3fdc757ea58cdc13457f5a66acacca26871e1759 master~2,"Fix argtopk split_every bug (#3810)

",1532530174,data split
3dddd1148d1050ea7eca16a9c6dd2bc187adb02a master~3,"Better error message on import when missing common dependencies (#3771)

",1532460074,
91ad15f1c6f755766d88cf27c56c792ee4e40c94 tags/0.18.2~5,"Handle missing `matmul` on Python 3.4 (#3791)

As `matmul` was added in Python 3.5 and we still support Python 3.4, we
can't get away with just adding this on Python 3 only as it will fail on
Python 3.4. To fix this, simply try to add `matmul` support everywhere
and skip it if `matmul` is not an operator.",1532126512,
358b9e8b30e737a5a02101e6f34c9242ee3d01db tags/0.18.2~8,"Ensure to_zarr with return_stored True returns a Dask Array (#3786)

* Use Dask Array `store` method in `to_zarr`

Normally `store` is designed for storing multiple arrays. So things like
`return_stored` return a `tuple` or Dask Arrays. However the `to_zarr`
use case involves storing a single Dask Array. Thus having
`return_stored` return a singleton tuple with a Dask Array is a little
unnecessary. This fixes that issue by switching to the Dask Array
`store` method, which returns a single Dask Array without a tuple when
`return_stored` is `True`.

* Test `to_zarr`'s `return_stored` behavior

Make sure that `to_zarr` returns a Dask Array and that contains the same
data as the Dask Array that was stored.
",1532086325,data structure
e214ea7a4fed9f2aa383984eba167b13f3fdaf1d tags/0.18.2~15,"Raise error when replace=False in da.choice (#3765)

* Raise error when replace=False in da.choice

* Expands check to exclude single-partition arrays

* Add test for replace=False for single-partition
",1531666282,data partition
16957a92d9c5f2a7fce2a494b0c9c9ec11a13652 tags/0.18.2~16,"Skip test if import fails. (#3768)

",1531622105,
1a9c1491299d0339edc97e9a56850e865c1512b4 tags/0.18.2~28,"Fix dtype-related issues of arange (#3722)

* arange dtype tweaks
* assert_raises -> pytest.raises
",1530989070,
c2fc8175c94b3a241b54356117dbc037908d6fe9 tags/0.18.2~34,"Avoid importing skimage on AttributeError (#3723)

When running Python in optimized mode it strips docstrings.
This causes and error in Pillow:

    dask/array/image.py:7: in <module>
	from skimage.io import imread as sk_imread
    ../../../miniconda/envs/test-environment/lib/python2.7/site-packages/skimage/io/__init__.py:7: in <module>
	from .manage_plugins import *
    ../../../miniconda/envs/test-environment/lib/python2.7/site-packages/skimage/io/manage_plugins.py:28: in <module>
	from .collection import imread_collection_wrapper
    ../../../miniconda/envs/test-environment/lib/python2.7/site-packages/skimage/io/collection.py:12: in <module>
	from PIL import Image
    ../../../miniconda/envs/test-environment/lib/python2.7/site-packages/PIL/__init__.py:27: in <module>
	__doc__ = __doc__.format(__version__)  # include version in docstring
    E   AttributeError: 'NoneType' object has no attribute 'format'

We now gracefully avoid this import",1530808049,io
cb3c51566cfdd7ee2fbb9b96aef9c9417240e0ee tags/0.18.2~40,"Add block_info keyword to map_blocks functions (#3686)

Fixes https://github.com/dask/dask/issues/3683

Your block function get information about where it is in the array by
accepting a special ``block_info`` keyword argument.

```python
>>> def func(block, block_info=None):
...     pass
```

This will receive the following information:

```python
>>> block_info
{0: {'shape': (1000,),
     'num-chunks': (10,),
     'chunk-location': (4,),
     'array-location': [(40, 50)]}}
```

For each argument and keyword arguments that are dask arrays (the positions
of which are the first index), you will receive the shape of the full
array, the number of chunks of the full array in each dimension, the chunk
location (for example the fourth chunk over in the first dimension), and
the array location (for example the slice corresponding to ``40:50``).",1530533861,job
fc71015ecf82e8416f6598377ab5d00e83a04dfb tags/0.18.2~41," fixed typo in tsqr documentation and improved clarity (#3698)

",1530533821,
105649e2fdb36981529c9f267630e48ed8e88ff2 tags/0.18.2~53,"Fix extra progressbar (#3669)

* Test case for triggering extra progress bar with Array.compute().

https://github.com/dask/dask/issues/3668

* Revert one line from 24fda1faf0da80a3d839d917372aeb7cd421e59c to fix a duplicate progress bar problem.

Closes #3668.
",1529958148,
36e235d7074f01830dd79a24fd656f4cda9e2c81 tags/0.18.2~55,"fix for `dask.array.linalg.tsqr` fails tests (intermittently) with arrays of uncertain dimensions (#3662)

* fix for issue https://github.com/dask/dask/issues/3659 where tsqr fails tests with uncertain dimensions

cause: uncertain dimensions mean the possibility of chunks with zero height, which np.linalg.qr does not accept
fix: a wrapper function for np.linalg.qr was written to handle the case

* regression test for case where tsqr encounters zero-height chunks (both in the case of known and unknown chunk sizes)

* modified _tensordot  to return the right shaped outcome (using np.zeros) in the edge case of a valid tensordot where some contraction axes has size 0

* added comments on workarounds
",1529932254,job
227d0301a47a62e34adcdab6bbf0a0851da3e467 tags/0.18.1~4,"Test gradient magnitude (#3647)

* Fix variable naming in gradient test

* Test computing the gradient magnitude

As there were some issues combining multiple gradient results into a
single computation (now fixed in `map_overlap`), add a simple test to
`gradient` to try combining multiple results together into something
typically used (i.e. gradient magnitude). Should help ensure this issue
doesn't crop up in the future.
",1529524453,"parallelism, job"
1755e866a421fc758bcd70a114702c3e3f46ab2f tags/0.18.1~8,"DASK_CONFIG dictates config write location (#3621)

* DASK_CONFIG dictates config write location

Previously existing configuration files could be specified by
`DASK_CONFIG`, but projects like `distributed` would always write the
default configuration file to `~/.config/dask/`. We now make the write
location configurable as well. This is important in environments where
`~/.config` may not exist of be writable.

* Overhaul ensure_file

- Don't ever write to destination if it's an existing file
- Catch errors when writing fails.
",1529503282,io
972b0182ca4cc01faa9bf4ad24261467df569f85 tags/0.18.1~9,"Add back dask.utils.effective_get (#3642)

This was removed in 0.18.0, which broke some XArray work.
See the following issues:

- https://github.com/pydata/xarray/issues/2238
- https://github.com/dask/dask/issues/3638",1529503211,
c691058d7a264ddef5748dca46d90ef0180024b8 tags/0.18.1~11,"revert requests fix (#3624)

",1529334153,
f75ebda49feb89c63c2b0fc4676e0affc2543223 tags/0.18.1~15,"Explain why `test_distributed` is skipped (#3627)

Appears `pytest` will error a test if it is skipped without a reason
now. This adds a reason explaining that this test won't be run if
`distributed` is not installed.",1529181113,
cfd0f7c223722bff489b8cb9ccb5e8f918a7f913 remotes/origin/jakirkham/mark_hdfs_tst_as_knwn_fail~1,"Add Dask Array implementation of pad (#3578)

* Add Dask Array implementation of `pad`

This provides a Dask Array implementation of NumPy's `pad`. It
dispatches through 1 of 4 different functions depending on the type of
padding (i.e. `mode`) used. If padding only involves edge chunks, then
NumPy's `pad` is applied to the edge chunks and internal chunks are left
untouched. If padding involves some sort of tiling, then the array is
sliced up into pieces that orientated and organized as need, which are
then combined with the original array using `block`. If the padding
involves the computation of some statistics of the array, the statistics
are computed and broadcast to match padding, which are then combined
with the original array using `block`. If a user defined function is
provided, then the array is padded with 0s and `map_blocks` is used
(with some `rechunk`ing) to apply the user function and get the
resulting array.

Some special cases like computation of the `median` and usage of
`reflect_type=""odd""` are currently not supported. The former could be
supported within some reasonable constraints. The latter has somewhat
mystifying behavior, which would need to be understood to be
implemented.

* Add `pad` to the `dask.array` namespace

* Include some basic tests of Dask Array's pad

* Add Dask Array's pad to the API docs

* Bump to NumPy 1.11.1

Appears a bug fix we need for NumPy's `pad` function is added in NumPy
1.11.1. So try bumping to NumPy 1.11.1 to see if that resolves the
issue.
",1529170947,data strcuture
0d7855e7e25fa8c33116104bda9dc5d1e1bfba16 remotes/origin/jakirkham/mark_hdfs_tst_as_knwn_fail~3,"Fix slicing of big arrays (#3619) (#3620)

Added slightly modified version of example which failed before as test",1529094333,
6b2058de4fa9bc97721843c0ee732349a536a417 tags/0.18.0~2,"Added recursion to array/linalg/tsqr to better manage the single core bottleneck (#3586)

Added recursion to tsqr to better manage the single core bottleneck; a balance is struck by only recursing when there is a need for it (the single core compute has to work on a block/chunk larger than the largest chunk) and when ""meaningful reduction can be made"" in the size of the single core compute (aspect ratio of the tall-and-skinny chunks is at least 2).

* simplify arguments in parametrized tests

* fix: tsqr SVD decomposition output shapes; enhanced SVD tests

* dask/array/tsqr resolved issues of uncertain array dimensions (rows & columns), added tests to cover cases of uncertainty for both qr & svd

* Use informative keynames in tsqr

This uses informative keynames like 'getitem' and 'qr` in the graph
for tsqr.  This helps the distributed scheduler make some decisions
and also improves the interpretability of visual diagnostics.

* tidied up docs; removed use_recursion (a classic case of ????????????...)

* Updated memory use in docs.",1528996709,"parallelism, job"
56174aca5f19b64f4f8fe72ec4086998955aac47 tags/0.18.0~3,"Support Zarr Arrays in `to_zarr`/`from_zarr` (#3561)

* Support a Zarr Array in `to_zarr`/`from_zarr`

This allows the user to provide a preconstructed Zarr Array for storing
with `to_zarr`. Also supports constructing a Dask Array with `from_zarr`
given a Zarr Array. Thus users can store the result in memory if they
like or to disk with a Zarr Array that may already exist as well as
easily load it into Dask.

* Test `to_zarr`/`from_zarr` with an existing array

Includes a roundtrip test for an existing Zarr Array.

* Disallow Distributed store to in-memory Zarr Array

Make sure that users are unable to store to in-memory Zarr Arrays from a
Distributed context as they do not make sense to access from multiple
processes.

* Test Distributed store of in-memory Zarr Array errs

Test that when trying to store an in-memory Zarr Array using
Distributed, this will error instead of attempting the operation.

* Make Client default so Zarr check works

The Zarr check for in-memory Arrays being used in Distributed context
looks at the globally registered scheduler (as that is what is used to
perform the computation and storage of the result). However the testing
framework doesn't seem to be registering the Client it creates globally.
Meaning the check will fail. This adds a flag to ensure the testing
framework's Client is registered globally so the check is exercised as
intended in the test.

* Include example of storing to a Zarr Array

Provides a simple example using `to_zarr` with a custom created Zarr
Array.
",1528982886,data strcuture
715f337ec6b49477b73806d2c71b283e158b9635 tags/0.18.0~7,"Replace token= keyword with name= in map_blocks (#3597)

* Replace token= keyword with name= in map_blocks

Behavior here was reversed from normal dask.array conventions.
This does an abrupt change of the name= semantics and issues a warning
when users use token=, pointing them to name=

See https://github.com/dask/dask/issues/3226#issuecomment-396419864

* Remove some uses of name= in map_blocks
",1528925923,
279fdf7a6a78a1dfaa0974598aead3e1b44f9194 tags/0.18.0~11,"Remove --verbose flag from travis-ci (#3477)

Fixes https://github.com/dask/dask/issues/3441",1528902471,
18649b1bcd94fcdd86b7b38cdf6705469906191c tags/0.18.0~13,"fixes dd.read_json to infer file compression (#3594)

* fixes dd.read_json to infer file compression

* add compression option to to_json, fix tests

* test to_json with multiple partitions

* change inferred compression test to use dd.to_json

* remove unused imports

* fix orc test

* fix http tests

* update travis to ignore test_http for optimize

* workaround for bug in requests 2.19.0
",1528895028,job
1a76ba3aa1482b3938be1b4b85059fd0f8309560 tags/0.18.0~16,"[array.linalg.qr] added sfqr (short-and-fat) as a counterpart to tsqr??? (#3575)

* [array.linalg.qr] added sfqr (short-and-fat) as a counterpart to tsqr (tall-and-skinny) to extend the applicability of qr; most ""big data"" cases using QR decompositions would probably be either short-and-fat or tall-and-skinny  #diversity

* Support non-square arrays in triu/tril

* Clean up da.linalg tests

* fix: dask.array.linalg.tsqr incorrectly reported shapes (https://github.com/dask/dask/issues/3583)

but also not using tsqr for the case of short and fat single blocks (inefficient)

* redirecting the single block case to sfqr (for efficiency)",1528663154,"data type, performance"
821f05e81308ee3213343f55eab5224d16232709 tags/0.18.0~21,"Correct internal dimension of Dask's SVD (#3517)

* Correct internal dimension of Dask's SVD

Previously the Dask Array SVD code had assumed the shared dimension
would be equal to the column in the Dask Array matrix always. However
the shared dimension in SVD is whichever is shortest (row or column) of
the provided matrix. Computing the results actually provides NumPy
Arrays of the expected shape. Meaning that the chunking is merely being
misreported when constructing the Dask Arrays from the Dask graph. Hence
this fixes how the shapes and chunks of the resulting Dask Arrays are
determined.

* Test that Dask's svd has correctly shaped results

Make sure that Dask's `svd` returns arrays that would have the correct
shapes compared to similar results from NumPy.
",1528406489,data strcuture
35354830ed4b029ab73e72e45022e03d381f43b4 tags/0.18.0~23,"Dataframe sample method docstring fix (#3566)

* Removes ""optional"" from frac parameter

* Adds space to fix code syntax
",1528384326,
1bb2284a14da738e5973d68d2456ce68d7d20018 tags/0.18.0~26,"Trivial/cosmetic changelog fixes (#3541)

",1527632549,
79c181ada85206e65617514d1dd3f891a9b7e836 tags/0.18.0~31,"add to/read_zarr (#3460)

* First stab at zarr

* flake

* return value and more tests

* Allow rechunk in to_zarr and check chunks for regularity

Dockstring for rechunk=  parameter suggests that rechunk will
allow various chunking schemes. Intent is to have None become the
default, and decide which scheme is best for it.

* Add import to examples for doctest

* Allow direct use pf mappers in to/read_zarr

* simpler length check

* fix

* Add chunks parameter to from_zarr

* comment typo [skip ci]

* Add doc and changelog [skip ci]

* remove rechunk parameter

* doc typo [skip ci]

* Simplify overwrite

* corrections
",1527302408,io
d87f838c869e6b3adf508e8abaa71c6d1db3d094 tags/0.18.0~35,"Fixes bug, where size of dimensions would not always be checked  (#3526)

",1527192188,data
b68ba1140c826c6ab5725bfec5c9dff35831b800 tags/0.18.0~37,"Update array-creation.rst (#3525)

fixing names to be correct / consistent (filenames, lazy_images, lazy_image)",1527078359,
fcf77748318bf1ccafac37211be1987534e6e5af tags/0.18.0~41,"Raise error if meta columns do not match dataframe (#3485)

* raises error is meta does not match df

* add check for NaN in meta and df columns

* Make error message more informative
",1527074021,data
771a045cd9196b37af54bee16761da7330c943a6 tags/0.18.0~42,"Merge 0.17.x with master (#3510)

* Fix rechunk with chunksize of -1 in a dict (#3469)

* Fix rechunk with chunksize of -1 in a dict

* Docstring updates

(cherry picked from commit 4756b59050ab3478b5dd197093d0d28e7b8a33fd)

* Faster slice_1d in dask.array (#3479)

This uses numpy and binary search to accelerate slicing performance 
when there are many chunks along a dimension

See also benchmarks in https://github.com/dask/dask-benchmarks/pull/15

(cherry picked from commit 7c419580037f552befc2650cb13967dd6bdef86a)

* einsum split_every parameter (#3472)

* da.einsum split_every support

* Check for invalid einsum parameters

(cherry picked from commit 5826ae0a108deedfca24e4d31db5f28a9e196026)

* Pandas 0.23.0 compat (#3499)

* Pandas 0.23.0 compat

Compatibility for result_type in apply

Avoid deprecated Index.summary

Avoid ambiguous column warning

Catch deprecation warning

Catch indexing warnings

Handle rolling warnings

Changelog fix

(cherry picked from commit 390fc144d917aa13c47ed59925a78ad220377f82)

* Updated changelog

* Removed distributed master install

* RLS: 0.17.5
",1527008093,"memory, data partiton"
83107230c0bad0cd901ea4f86667c42250aff649 tags/0.18.0~43,"Update sphinx readthedocs-theme (#3516)

Fixes https://github.com/dask/dask/issues/3515",1526778998,
bf5cfb01da7c6bc13f6e968df307f0352cb942ff tags/0.18.0~44,"Feature/dask.array.apply_gufunc (Issue #702) (#3109)

* Initial (still buggy) implementation of ``apply_gufunc``

* Finalize initial bugfree implementation

* Wrapup tasks

* Tiny out of topic linebreak fix for docu

* Add new methods to docu

* Minor

* Minor

* Minor docu fix

* Stylefixes, modify doc examples

* Fix conceptual issue: loop dims get broadcast fully before func is called

* Add docstring

* Remove `curry` from `gufunc`

* Stylefixes

* Use `apply_gufunc` into `dask.array.Array.__array_ufunc__`, provide `gufunc` wrapper (calls `apply_gufunc` directly) and provide `asgufunc` decorator.

* Finalize some tests and import `gufunc` methods to `dask.array` module

* Adds documentation for `gufunc` module

* Remove unused imorts

* Stylefixes

* Minor import fix

* Remove unused code

* Minor

* Fix doctests

* Limit some tests to newer versions of `numpy`

* Improve documentation

* Rename `asgufunc` to `as_gufunc`

* Minor enhancement of documentation

* Use `assert_eq` instead or `np.testing.assert_array_equal`

* Use `@pytest.mark.xfail(reason=""..."")` for one test

* Add test with mixed numpy dask array inputs

* Minor

* Changes `apply_gufunc(..., concatenate=)` to `apply_gufunc(..., allow_rechunk=)`

* Minor doc fix

* Put arguments in docstring in correct order.

* Make `signature` strictly a string input

* Make `output_dtypes` accept strings, add respective tests, and pass dtypes also to `np.vectorize`.

* Document passing of additinal `kwargs` to function, add test for it.

* Reimplement a more readable version of consistency check of chunk sizes

* Minor enhancement of few code lines for readability

* Minor stylefixes

* Fixes typo which broke tests

* Make some tests optional for numpy<1.12 because of missing `np.vectorize(..., signature=...)` support

* [skip ci] fix api link

* Minor typo

* Correct input validation to `TypeError`

* Add `pytest.mark.xfail` to tests that call private `numpy.linalg._umath_linalg` methods

* Revert ""Correct input validation to `TypeError`""

This reverts commit d2b27bfcb9e27367bb5e738cccdf433a6e949309.

* Correct input validation to `TypeError`
",1526668515,
20145c9a5d12d965ab1ffe52e0c37659aec70ac2 tags/0.18.0~45,"Add dask.config.update_defaults function (#3513)

This provides a uniform way for downstream library to register their
default values.

Previously our recommendation was for downstream libraries to update the
global configuraiton directly.  This had issues when the global config
would be reset, for example when refreshing the configuration.  Now we
remember these default values in a list of dicts and store them for
future use.",1526667709,
c493dbf1b5e4e9490fe8c48cbfc9a86c44751cb1 tags/0.18.0~47,"Adds synchronous scheduler syntax to debugging docs (#3509)

",1526554732,job
390fc144d917aa13c47ed59925a78ad220377f82 tags/0.18.0~48,"Pandas 0.23.0 compat (#3499)

* Pandas 0.23.0 compat

Compatibility for result_type in apply

Avoid deprecated Index.summary

Avoid ambiguous column warning

Catch deprecation warning

Catch indexing warnings

Handle rolling warnings

Changelog fix
",1526497336,
075698e2bee04dd09cb1f475ff271a6edd7ee7b7 tags/0.18.0~50,"Read whole files fix regardless of header for HTTP (#3496)

* Read whole files fix regardless of header for HTTP

* Should have been for read length -1

(not zero, that should produce b'')

* Add comments and allow buffer for small whole files

The latter comes with small addition to a test
",1526412568,io
1fd5f14af228d3f9b90b9f96213a55a014178a05 tags/0.18.0~53,"fix typo (#3495)

remove duplicate phrase",1526298317,
29ea899b724e0a518e9e6efd2af267225a615fbb remotes/origin/no_autoupdate_conda~3,"Use NumPy 1.11 with Python 3.4 (#3482)

Appears that Pandas 0.19 and NumPy 1.12 conflict due to the Conda
package for Pandas' version constraints (particularly for Python 3.4).
So try downgrading the NumPy version used in this CI matrix to fix this
issue.",1525858962,
f0c8e141d1dd28849085b7390e7e71437cd814ce remotes/origin/no_autoupdate_conda~8,"Support case in get_scheduler with no inputs (#3468)

Previously this failed on a check on collections",1525437974,job
9727d6667e85b9f41d2871ace9925de4d9e9ac74 remotes/origin/no_autoupdate_conda~10,"[skip ci] Remove changelog requirement from PR template (#3459)

See https://github.com/dask/dask/issues/3447",1525430571,
993f15363e2a7e5a3dc167af0800442eaaa98e9a tags/0.17.3~5,"Assume the current python executable is the one we should spawn. (#3450)

On Debian the Python executable is named python3, so the test fails
because it tries to run the Python 2 interpreter.

Using sys.executable should be the safest option as the test server
will be launched using the same interpreter that the test suite is
running under.",1525020265,
196c9b0d2656fb544945018bb68ce6ebce5c0870 tags/0.17.3~11,"Remove support for numpy < 1.11.0 (#3429)

* Remove support for numpy < 1.11.0

Downstream libraries have bumped the minimum numpy version they support,
and we should too.

* Fix failing __array_ufunc__ test

Numpy `out` parameter with dask dataframes requires numpy >= 1.13
",1524494002,
bc9f70c3eaed2bb7799cf73b6fc0a7ed1230cc4a tags/0.17.3~18,"Propagate optimize_grpah keyword in persist (#3420)

This was causing dask.distributed tests to fail",1524133376,performance
79528dbea664cc6438b9029102e0263516b59dbc tags/0.17.3~21,"Preserve DataFrame divisions when reading from parquet using pyarrow (#3387)

* Update test_parquet to check divisions for fastparquet and pyarrow >=0.9.0

pyarrow tests currently pass for <0.9.0 and fail for >=0.9.0

* WIP towards preserving divisions when reading parquet files using pyarrow

* Sort ParquetDatasetPieces naturally so that part numbers are processed in order

* Handle converting divisions to timestamps
But getting tests to pass required multiplying int64 time by 1000 and I'm not sure why.

* Use pyarrow schema to convert time to nanoseconds before passing to pandas

Refactor division logic into a new method and base it entirely on the
pyarrow schema (not the pandas metadata)

* Skip test_columns_name for write_parquet 'fastparquet' when reading with 'pyarrow'

This was causing a SIGSEGV error and the comments already state that fastparquet doesn't write column_names

* Encode string divisions

* Clarify parquet string encoding comment

* Use storage name when looking up column metadata for divisions

* Compute divisions column index on the row group of the first piece, rather than on the schema.
These differ in the case of partitioned datasets

* Get numpy datatype from the pandas metadata if available, otherwise get it from parquet schema as before

* Add `infer_divisions` kwarg to read_parquet function

* Updated changelog

* Fixes for pyarrow=0.8

* flake8

* Parquet testing updates so that more tests have 10+ parquet pieces
This ensures that we're testing natural sorting

* Refactor natural sorting logic
  - Move nat_sort_key into dask.utils
  - Natural sort paths resulting from globstring patterns before passing lists of paths to the engines
  - Only natural sort pyarrow parquet pieces if a single input path resulted in multiple pieces

* Compute division column index for each pyarrow parquet piece

* Move LooseVersion import to top of dataframe/io/parquet.py

* Don't assume unix file separators

* Refactor _to_ns to use a lookup table

* Rename nat_sort_key to natural_sort_key and add wikipedia reference

* Remove unnecessary  check_{engine} guards

* Remove unnecessary check_pa_divs assignment

* Rename check_divs -> should_check_divs

* flake8

* Fix natural_sort_key doctest

* Python 2 fix. str -> string_types
",1523979453,io
7cd5143820f58cc600bbaf553b542ccc5e760d9e tags/0.17.3~27,"Reset requests after using moto (#3381)

https://github.com/spulec/moto/pull/1553 fixed the bad interaction between
moto and requests. We now use master downstream of that, which means that
ORC py2 tests can be re-enabled

move moto reset to moto tests
",1523040031,
099a8f16f34fc82e0ec8f86ca0fa8c0e28cee1b3 tags/0.17.3~41,"Fix `isin` API documentation  [ci skip] (#3364)

Forgot to add `isin` with `autofunction` as well. This fixes that issue.",1522523568,
076d5eb333ecd3552488d56e8f9f0abb3e7af1d3 tags/0.17.3~46,"Fix broadcast_shapes handling of nan (#3356)

* Fix `broadcast_shapes` for `nan` lengths

Appears that Python's `max` will sometimes ignore `nan` values. To fix
this, we switch to NumPy's `max`, which will always prefer `nan` values
if they show up.

* Test `broadcast_shapes` handles `nan` correctly

Add some tests for `nan` lengths in shapes passed to `broadcast_shapes`.
Tries to catch some cases that were mishandled before.

* Add `broadcast_shapes` `nan` fix to changelog
",1522411480,job
7de2447e790a1394c00df5d04ec90d226677f596 tags/0.17.3~47,"Expand meta= error message (#3343)

* Expand meta= error message

Fixes #3342

* add udf= keyword to emulate and raise_on_meta_error
",1522411457,
ac660104fcb9b6c78c73366f831812d80feb96e9 tags/0.17.3~49,"Tweak piecewise tests (#3359)

* Tweak piecewise test

The two cases were a little off despite generating the same result. This
brings them into parity. Also adds a seed that is known to generate all
3 cases.

* Split out the piecewise otherwise test

This test just handles the case where `piecewise` has an extra
conditional argument used as an otherwise or else case.

* Skip piecewise otherwise test on pre-1.12 NumPy

Appears that `piecewise` runs into an issue on pre-1.12 NumPy when given
an extra conditional argument for an otherwise/else case. So we just
skip this test on old NumPy's a reference the relevant issue.
",1522410945,
31529d02ede1216c9e40943818508eac54b986ac tags/0.17.3~55,"fix typo in landing page
",1522179267,
bd9fed747ed01589edb0c2b28ed774b72498e1ac tags/0.17.3~62,"Typo fix (#3332)

",1522071636,
85e9a467103007c22affa9ba65447cb500bf5963 tags/0.17.3~64,"Typo fix: argument to DataFrame is called dsk, not dask (#3331)

",1522065403,
4ea114b2d8d6f5ef8ad063944ac388d4ef72435a tags/0.17.3~65,"Fix style issues in the changelog  [ci skip] (#3328)

Fixes various issues with inline code sections and PR references amongst
other things.",1522023458,
f5779130fc2a733cc3c3ad128124d88d0292190c tags/0.17.2~6,"Avoid sorting large stacks in order (#3298)

When performning task ordering we sort tasks based on the
number of dependents/dependencies they have.  This is critical to
low-memory processing.

However, sometimes individual tasks have millions of dependencies,
for which an n*log(n) sort adds significant overhead.  In these cases
we give up on sorting, and just hope that the tasks are well ordered
naturally (such as is often the case in Python 3.6+ due to sorted
dicts and the natural ordering that exists when constructing common
graphs)

See https://github.com/pangeo-data/pangeo/issues/150#issuecomment-373066066
for a real-world case",1521492162,"job, performance"
c0631b0f4cc66709aae48971609b77571b70b777 tags/0.17.2~11,"Loading of partitioned parquet should result in known categories (#3216)

* Loading of partitioned parquet should result in known categories

Fix for the fastparquet side

* Add tests and fix a bit

* Also test index categories

* fixes
",1521043330,data partition
55328a0fad511b81b06b5a078f484365658c81cf tags/0.17.2~14,"fixed typo (#3286)

",1521034271,
cc9f8db12e4c43b61fb79bb74bbd71b3cd8ba1f7 tags/0.17.2~19,"fix invalid gcsfs git repo url (#3257)

change gcsfs repo url from https://github.com/gcsfs/gcsfs to https://github.com/dask/gcsfs.git",1520602124,
e4c89249a922ed95d9fc833ab0c2b4ed2f5cb42d tags/0.17.2~25,"Fix handling of non-iterable non-string keys for task graphs (#3238)

* Fixed two issues with dask/core when keys are not strings

The first issue on lines 306-315 in subs is fixing the assumption that if type_arg is type(key) then arg is an ndarray, when it can actually be any non-core python type at this point in the branching.

The second issue is where '->'.join(cycle) will fail (in trying to create an error message) if the elements of cycle cannot be added to strings.  This is resolved by calling str(x) on each before joining.

* Forgot to check ndarray on type(key)

* Added test for user class substitution, modified subs to fall back to == if len() or zip() call fails

* Changelog

* Update to catch len() error in python <= 3.4 and >= 3.5

* Added comment and added name to changelog
",1520251305,joib
8bd36eb76a51a0cfc41ca77b82444ad7335bbd86 tags/0.17.2~34,"Support 0 or 1 args in broadcast_shapes and broadcast_chunks (#3218)

* Test `broadcast_shapes`/`broadcast_chunks` w/1 arg

Adds tests for `broadcast_shapes` and `broadcast_chunks` with one
argument. Didn't appear there was one before for either case. Hence this
adds one for this important base case.

* Support `broadcast_chunks` with no arguments

Make sure that if `broadcast_chunks` gets no arguments, it return an
empty tuple.

* Test `broadcast_shapes`/`broadcast_chunks` w/o arg

Adds tests for `broadcast_shapes` and `broadcast_chunks` when no
argument is provided. Didn't appear there was one before for either
case. Also it failed for `broadcast_chunks` previously. Hence this adds
test for this base case.
",1519739036,"parallelism,job"
23897769ed80f02882d44da2b41d59728c36a389 tags/0.17.1~1,"Support tokenizing partials when keywords is None (#3191)

* Support tokenizing partials when keywords is None
* pin travis numpy to 1.14.1 for Python 2.7, 3.6
* Fix test where ""add"" arises randomly in hex string
* bump pandas up as well
* fix docstrings with new numpy
",1519332377,
683d1c9082d8fc69e6a3dbc8299f18f0f9e75e30 tags/0.17.1~2,"Add HTTPFileSystem (#3160)

* Add HTTPFileSystem

* First test

Plus auto-import the back-end

* Simplify logic; parse query

* Add tests

* flake

* More tests, fixes, and working for Range-free servers

* Rever change in bytes.core

* fix test

* flake on tests

* Add checks for non-behaving HTTP servers

* one more flake

* Fix for comments

Use UUID for ukey; file may have changed at any time

Use temp directory for test server

* flaking

* Add httpfs docs to remote services, update changelog
",1519317029,"io, data"
246d2ce2ef5bb306b7b6e26e7bc6cfaea492b26b tags/0.17.1~4,"Introduced parameter for output stream for progress bar (#3185)

* Introduced parameter for output stream for progress bar

Using stderr instead of stdout for progress bars can be desirable, see
hyperspy/hyperspy#1842. This change allows users to specify the stream to use
and allows to easily change the default behavior.

* Implemented the change suggested by @sk1p to fix the failed tests

* Added spaces for code style
",1519257358,"parameter, io"
1893ebf938164d6dd80c347c3a31313dcd6b41a7 tags/0.17.1~12,"allow column assignment of pandas datetime to dask dataframe (#3164)

* replace np.isscaler with pd.api.types.is_scalar per Tom/Matt's advice

* update changelog

* add test

* fix changelog update - move note to correct session

* fix linting issues

* pd.datetime to pd.Timestamp, and spacing change as requested
",1518639327,data
571d7076d3fd881176bda18138814674f400b3ac tags/0.17.1~13,"Add daskernetes to kubernetes documentation (#3161)

* Add daskernetes to kubernetes documentation

* fix link and headers

* add docker doc page
",1518635349,
3ba1c3854293deb9a2ea7d3df693dddf5bce93a0 tags/0.17.1~16,"More tests of store's returned result (#3152)

* Clear delayed target before retesting store

* Move region tests with no result returned

Make sure all the region tests with no result returned are together.
This include with a single region and multiple regions.

* Wrap long line in tests

This line isn't over the limit, but it will soon be. So wrap it
proactively.

* Test immediate/delayed store and return of result

Was not testing whether `store` with result returned worked correctly
when the result was persisted to disk previously, this fixes that issue
by adding tests for this case.
",1518468935,
3f6fd6a12dc0162a03d74d550058457824d0585c tags/0.17.0~4,"df.query works with kwargs (#3144)

Previously this would error out. Also simplified the implementation and
cleaned up the tests for both query and eval.",1517986329,job
7769996ac5ed56ff11e7af96f2c4597207dda008 tags/0.17.0~7,"Pyarrow with partition_on forwards kwargs (#3142)

Previously extra keyword arguments were silently ignored when using
`to_parquet` with `partition_on` and `engine='pyarrow'`. This fixes that
and adds an appropriate test.",1517938418,data partition
cceb6e2ac50a85b3f34154612dc98508432f057c tags/0.17.0~12,"Full traversal in dask.order (#3066)

* Add failing test for short dependent chains

* add more ordering tests

* Implement full traversal in order

* relax tests and clean up old code

* pass on cycle test

* remove self references in dependencies in order

* add changelog entry

* remove graph cycle test

* remove reversed list in test
",1517520072,
d5407c286168e2240afd3d121dc7621bf26fe544 tags/0.17.0~15,"Remove `fastparquet` package before pip installing (#3085)

Recent versions of `pip` have gotten a bit smarter about detecting a
package is already installed instead of simply overwriting. This is
actually a good thing for the majority of use cases. However it means
that we have to adjust our logic for installing the development version
`fastparquet`. Though it is probably for the best so that we don't
accidentally have stray files floating around from a previous install.
To adapt to this new behavior from `pip`, we forcible remove
`fastparquet` after installing it (we only wanted to ensure the bulk of
its dependencies were present anyways) with `conda`. Then the `pip`
install can proceed without issues.",1517447528,usability
03e75ac7bdf4f102da86071bde7299175686e143 tags/0.17.0~16,"Cosmetic fixes to sql (#3120)

Following comments on #3100",1517426397,
4cd1f8846dbdec9801959441e34c609a92022c53 tags/0.17.0~21,"Support arrays with unknown chunk sizes in percentile (#3107)

* Support arrays with unknown chunk sizes in percentile

Fixes #3106

* fix use of _percentile in dataframes

* fix doctest

* [skip ci] add changelog
",1517407363,memory
35ee4dfea06392b3b359869b945d00202358d296 tags/0.17.0~22,"Fix travis parquet test issues (#3117)

* Fix non-deterministic parquet test failure

This was a bug in our test code that accidentally iterated over a set
instead of a list and assumed ordering. Should fix the non-deterministic
test failures for the parquet tests on travis.

* Remove `-q` from pip install on travis

Makes debugging future install issues easier.
",1517364828,
18d3afb681117f7b39bca418679d7eb4cad33ba4 tags/0.17.0~23,"Dtype rationalisation for sql queries (#3100)

* Dtype rationalisation for sql queries

Plus a few more options and docs

* flake

* revert frequency in sql

* More update to docstring

* Fix for older pandas

df.astype() apparently required dict in the past

* remove debug
",1517357719,data type
109c7676f845df3a097ccd1768b4a3059dd94ab8 tags/0.17.0~26,"Allow the CI dev build to fail (#3088)

* Allow the CI dev build to fail

- Move the CI dev build to only on merge
- Allow the CI dev build to fail
- A few other travis fixes
- Silence a weird bug when using xdist + py36, to be investigated later",1516735948,
071225f9acb8c5657e1ce85f91b770272b6e39ce tags/0.17.0~27,"Few travis fixes (#3091)

* Fix travis conditional build

* Fix ignored tests

`pytest_ignore_collect` overrides the usage of `pytest --ignore`, which
led to failures on travis.  Instead it's simpler to use the
`collect_ignore` option, since we just want to ignore a few files.

* Better comment
",1516652260,job
becbe1adf89d571cf03cfe3a58f9ca5a33580689 tags/0.17.0~30,"Pad result from _colorize (#3073)

Previously colorize would return results like `#FFFFF` (five Fs) rather than
the result left-padded to six characters `#0FFFFF`.  This provides a fix.",1516542223,
fef1532a6ceb040a18a84476562e79258a16df66 tags/0.17.0~33,"Remove `get_block_locations` handling (#3079)

* Remove `get_block_locations` handling

This was used for short circuit reads from hdfs using libhdfs3, but has
the following problems:

- `hdfs3` specific, other hdfs libraries currently don't expose this
feature. This would be less of an issue if `hdfs3` was a full featured
hdfs library, but due to its lack of handling various security features
other libraries (e.g. `libhdfs`) may be preferable instead (for now).
- This feature complicated testing and code, relying on tests in the
`hdfs3` library that have now started failing (due to lack of being
run).
- The feature wasn't fully thought out in a generic way. If we re-add it
later, we can think about how this might expand to other hdfs clients
(or other potential backends).

This optimization also only comes into play if:
- You have short-circuit reads enabled
- The data you want to read is on the same nodes you're computing on
- The data you want to read is evenly distributed among those nodes
- The scheduler decides it makes sense to read from those nodes instead
of moving the computation elsewhere.

* Add to changelog
",1516307053,io
bf5f7fe786664dff30990b82f35e380f35cd2535 tags/0.17.0~34,"Fixes slicing list/array issue (#3076)

* Adds slice isinstance check for b when a is None in fuse_slice

* Adds test to ensure proper NotImplementedError is raised

* Adds changelog entry and reference to issue in added test
",1516299077,"data partition, parallelism"
3dbdbc6730bae5ab1242f3b149b0992177724f78 tags/0.17.0~35,"Cleanup bytes docstrings [skip ci] (#3078)

Cleanup a few issues in `dask.bytes.open_files` and
`dask.bytes.read_bytes` docstrings.",1516294935,io
c8ba85599339d8f9ba8d2a1fd0ba2de8e31b1b12 tags/0.17.0~37,"XFAIL `test_partition_on` for pyarrow writing (#3075)

There's a race condition on pyarrow 0.7.0 that's fixed on master.
xfailing it for now. To make this easier in the future, made adding
custom marks for the parquet tests more extendable.",1516232472,job
b7be7d8007ed7c2b425b5998b85dde60d82052c5 tags/0.17.0~38,"A mild refactor of `dask.bytes` (#3069)

* Refactor bytes implementation

Refactor handling of paths/filesystems in bytes to fit a more generic
filesystem class. In particular:

- All protocol/parameters specified in the urlpath are stripped before
forwarding to any filesystem methods. Filesystems only see paths, making
this interface more uniform. Also avoids parsing the paths multiple
times.

- Cleanup Open/OpenFile classes.

* Refactor bytes backend a bit

- Remove `OpenFileCreator` class, as it's unnecessary
- Use `open_files` in more places
- Fix bug in hdfs specific block functionality to not check for client
  when block locations aren't known.
- Remove any non-standard use of methods on `filesystem`, in prep for
  merging arrow and dask filesystem backends. This means removing
  `get_block_locations` from required methods, and moving `logical_size`
  to a function (as it should be).

* parquet tokenizes names properly

* A few renames/docstrings

* Add a few tests

* Fix failing py2 test

* Fix windows error

* Actually fix windows bug

* Respond to comments
",1516225018,io
f29cf97d43e3b9df5f10abef92cfdef7fbbccdfc tags/0.17.0~39,"Fixes a metadata issue with store's return_stored (#3064)

* Test two arrays of different sizes and types

Make sure that each array's metadata is handled correctly and uniquely.

* Fix a bug in `store` when `return_stored=True`

There was an issue of applying the same metadata from the last array to
all preceding arrays when `return_stored=True`. This fixes that issue by
adding a `for`-loop and selecting out each array from `sources`
appropriately to match the data loaded from disk.

* Use generator expression to create arrays

To alleviate some style concerns raised, switch to using a generator
expression in a `tuple` using `zip`. Add an `assert` afterwards to
ensure we got the number of results we expected (same as number of
sources).

* Note `store`'s `return_stored` in the changelog

We forgot to include a changelog entry for the original PR. This
corrects that error.

* Add `store`'s metadata bug fix in the changelog
",1516127321,data type
6cbcf0813af48597a427a1fe6c71cce2a79086b0 tags/0.17.0~40,"Add more helpful error when len is called on arrays with empty chunks. Fixes 3058. (#3067)

* Add more helpful error when da's len is called on empty chunks. Fixes #3058

* Update changelog.

* Address PR review comments; use TypeError rather than ValueError, update error message.
",1515983375,data partition
1223f0167e1f760a93aaf286e6dcfa72ba21f773 tags/0.17.0~41,"Add color= option to visualize that supports coloring by task order (#3057)

* Add color= option to visualize that supports coloring by task order

This is helpful when exploring task priority issues.

Currently this feature feels a bit tacked on.  Suggestions on how to
make this less intrusive would be welcome.

* add docstring and changelog

* update color= section in docstring
",1515960064,job
d7db87b6b4028825125b99feee814f38789de7a5 tags/0.17.0~42,"Prefer fewer dependents and many dependencies in task ordering (#3056)

This change the task ordering policies to the following

1.  Avoid tasks with dependents
2.  Prefer tasks with many total downstream dependencies
3.  Break ties with the key

This also fixes an error where string comparison did not match value comparison

Fixes #3055",1515955513,job
e74161c9cb81e57b99cd291b3d26805b77de5156 tags/0.16.1~1,"[WIP] Add tests for order to prefer avoiding upwards branching (#3017)

* Add tests for order to prefer avoiding upwards branching

See https://github.com/dask/dask/issues/3013

* Remove computation of maxes from order

* flake8

* remove ordering test

* add changelog entry
",1515512879,
95d3a8b7da2b54407d5c18c21f17f3d8d028118b tags/0.16.1~2,"Update community conversation page (#3045)

Fixes https://github.com/dask/dask/issues/2945",1515335220,
423218fcd273af9e99f437378100aac2c50fe7a0 tags/0.16.1~3,"fix link typo in single-distributed docs
",1515334269,"parallelism, job"
6dd19e528b481df2006b49d1e538864289b851ae tags/0.16.1~5,"Fix naming bug of cumulative aggregations (#3042)

* Add test for cumulative using multiple columns (#3037)

* Fix cumulative aggregation isse (#3037)

* Update changelog

* Rename argument token to op_name.
",1515001615,job
98d85bc694a5b0bdb4ff113b0865f16beca60dd5 tags/0.16.1~9,"COMPAT: Pandas 0.23 duplicate names in MI (#3041)

* COMPAT: Pandas 0.23 duplicate names in MI

Pandas 0.23 is disallowing duplicate names in MultiIndexes. This
adjusts a test that relied on that behavior, and `groupby().nunique` which
produced it as a by-product.

Closes https://github.com/dask/dask/issues/3039

xref https://github.com/pandas-dev/pandas/pull/18882

* Cleanup renaming
",1514923913,"job,parallelism"
bee8e286d19e2cdc2bdad4ea2b589075baef3e8c tags/0.16.1~10,"derived_from gets full signature from __wrapped__ (#3026)

* derived_from gets full signature from __wrapped__

Previously this would fail for wrapped functions (e.g.
`pd.DataFrame.rename`, causing the not implemented parameters section to
miss a few parameters. We now traverse `__wrapped__` attributes fully
before inspecting the arguments.

* Fix chunk test

This is testing an invariant that I don't think is necessary.

* Use signature instead on py3

Supports more methods of inspecting the signature.
",1514922725,"parameter, job"
0e14191d1c6088d903dbeedcf1e36ee5ec088027 tags/0.16.1~15,"Cleanup insert_to_ooc (#3012)

* Add a docstring to `insert_to_ooc`

Provides a brief explanation of `insert_to_ooc`'s behavior, parameters,
and an example of how it works.

* Refactor out `insert_to_ooc`'s closure `store`

Create a global function `store_chunk`, which takes the place of
`insert_to_ooc`'s closure `store`. Should be a bit cleaner and faster to
work with when it comes to serialization between processes.

* Document `store_chunk`

Provides a basic docstring for `store_chunk` and an example of its
usage.

* fix doctest
",1514058405,"job, io"
7723be2d797165fc57c7e6061077d503b6df4216 tags/0.16.1~18,"Better error on empty bag to_dataframe (#3025)

When no metadata is passed in to `dask.bag.Bag.to_dataframe`, the first
element of the bag is read and used to infer the dtypes. If the first
partition is empty, this would fail with a non-intuitive error. We now
raise a helpful error.

Also added ability to silence warnings in take, and made take warnings
test actually test the warnings.",1514052877,"job,data"
330c2dc2383d5d7823186be8d43170829bbfedee tags/0.16.1~23,"Adds condition to dask.array.percentile to handle scalar percentile (#3021)

* Adds condition to dask.array.percentile to handle scalar percentile

* Adds changelog entry

* Fixes trailing whitespace flake8 issue

* Adds check for numbers.Number
",1513880494,job
076830c8e87c5f8806c85be639b92d97008ebf03 tags/0.16.1~24,"Read csv names (#3022)

* Add test for read_csv issue (#2976)

* Fix read_csv issue (#2976)

* Fix style

* Update docs

* Updated test using check_index=False
",1513868817,io
1fef002a32bd06b2300c386f2e15419be6b91170 tags/0.16.1~25,"Compat for pyarrow 0.8.0 (#2973)

* TST: Expand coverage

* COMPAT: For pyarrow 0.8

* Fixup docs

* fixup! Fixup docs

* PY2 compat

* Changelog

* CI: Test against pyarrow master

* BUG/ENH: Correctly set df.columns.name

* fixup! CI: Test against pyarrow master

* fixup! BUG/ENH: Correctly set df.columns.name

* fixup! fixup! BUG/ENH: Correctly set df.columns.name

* REF: User pandas metadata for fastparquet reader

* cleanup

* Various fastparquet updates

* one more

* skip it

* Removed data files

* Skip column index names on old arrow

* pep8

* Handle null index names

* Passing for 0.7.1

* pep8

* Bump skip for pyarrow

* Added historical to CI

* Remove historical tests

* REF: refactor columns

* REF: refactor index normalization

* Py2 compat

* Cleanups
",1513804768,io
64420cb6c8450b3f328f2ac631804c418c8df2d4 tags/0.16.1~27," Changes `dask.array.meshgrid` to use `broadcast_to` internally (#2981) (#3001)

* Changes `dask.array.meshgrid` to use `broadcast_to` internally.

* Cleanup

* Adds PR reference.

* Minor style fix.

* Removes unnecessary import.

* Coerces arguments to dask arrays and adds test for it.

* Minor stylefix
",1513536704,
d308eac6bac91f8501ae26ddf3ae217126cb5063 tags/0.16.1~30,"COMPAT: Pandas 0.22.0 astype for categorical dtypes (#2997)

* COMPAT: Pandas 0.22.0 astype for categorical dtypes

Change in https://github.com/pandas-dev/pandas/pull/18710 caused a dask failure
when reading CSV files, as our `.astype` relied on the old (broken) behavior.

Closes https://github.com/dask/dask/issues/2996

* Fix pandas version check

* Refactored

* update docs

* compat

* Simplify

* Simplify

* Update changelog.rst
",1513295428,
bae4dccdd8c17f7fa0c628eaed764e209567fd8e tags/0.16.1~40,"Add block (#2650)

* Vendor NumPy 1.13's _Recurser in numpy_compat

Unfortunately we need to vendor `numpy.core.shape_base._Recurser` as it
appears to get dropped midway through NumPy 1.13 and is replaced with a
new implementation, which will require some fiddling to get working with
Dask Arrays. In comparison, the `numpy.core.shape_base._Recurser`
basically just works with any so is pretty easy to get working with Dask
Arrays as well.

* Add block

Provide a Dask Array implementation of NumPy's `block` function. This is
basically verbatim the code from NumPy with some tweak to use Dask Array's
equivalents of NumPy operations like `concatenate`.

* Export block to Dask Array's public API

* Test Dask Array's block

Inspired by NumPy's tests of `block` except changed to compare Dask
Array's implementation of `block` to NumPy's implementation. Some of the
error tests are basically identical to those in NumPy.

* Document block in Dask Array's API docs

* Note `block` in the changelog
",1512748507,
9b4d60d56ad8367b7c33b821a24b634f4b138c37 tags/0.16.1~46,"Boolean coercion of Arrays and DataFrames (#2958)

* array coercion

The following has been implemented
* Arrays of (arr.size > 1) CANNOT be coerced into a scalar(1)
  The call to self.compute() will not be performed.

* Arrays where arr.size <= 1 can be converted to:
    * int
    * float
    * complex

* The bool() builtin will throw ""ambiguous truth value"" TypeError
  when arr.size > 1, just like NumPy

* Tests added for each of these cases.

(1) Borrowed from NumPy (lib/user_array.py::_scalarfunc())

* DataFrame coercion

__bool__ is now implemented for Series and DataFrames to raise
a ValueError, just like Pandas. Computation will not be triggered.

Scalar types will still trigger compute when __bool__ is called.

Tests added for
* Raise TypeError when using int(), float(), complex() on
  Series and DataFrames.
* Raise ValueError when using bool() on Series and DataFrames.
  Even boolean valued Series will raise ValueError.

* flake8 fixes

* update changelog

* separate calls to pytest.raises

* edits suggested by jakirkham

* copy edits to dask/dataframe/core
",1512479916,
dbc48cc06e68b09dfdca7ba735469e3b19934aec tags/0.16.1~48,"Bug issue2928: read sql table fails when reading empty table (#2935)

* If sql table is empty, return empty dask dataframe

* Flake8 compliance

* Flake8 compliance

* - Edit read_sql_table to comply with tables datatype when table is empty
- Unit testing
",1512158904,
da2cb1ff589e88bfe92e181e5f58701193a59dfe tags/0.16.1~51,"Improve compatibility with 32bit architectures (#2937)

This replaces np.int64 with np.intp in a few cases

See https://github.com/dask/dask/issues/20",1512069792,
430357b65802767a82e604bf939969d968a920cd tags/0.16.1~53,"Replace toolz.first with custom function in dask.delayed (#2919)

This will become a special value for the distributed scheduler to skip

See https://github.com/dask/distributed/issues/1567",1511365614,
42d2d190d21ab490515b130f508a3a977e32b074 tags/0.16.1~61,"Document instructions for minmal dask-core install (#2732)

* Document instructions for minmal dask-core install

This pull request's core purpose is to add in instructions for installing dask-core which has less dependency bloat than the default `conda install dask`. A use-case for example is  for developers building docker images that don't want a overly huge image size (see e.g. [here](https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#avoid-installing-unnecessary-packages)), or for those who would like to manage the optional dependencies themselves.

Aware of https://github.com/dask/dask/pull/2336 which removes confusion of two channels (default and conda-forge), but seeing as dask-core downloads overwhelming come from the conda-forge channel (>30k vs < 100 see https://anaconda.org/search?q=dask-core), I thought I'd add back in the instructions for conda-forge, but add in 'recommended' in brackets next to default channel so that new users won't be too confused. Open to recommendations on what's better.

External references:
https://pypi.python.org/pypi/dask
https://github.com/conda/conda/pull/4982 (note new optional dependency functionality from conda 4.4.0 onwards)
https://github.com/conda-forge/dask-feedstock/issues/22

* Retain only link to conda-forge

Remove `conda install -c conda-forge` so as to not add extra confusion, as per #2336. Using KISS philosophy.

* Fix up some more small style and english issues.
",1510967405,
1c5058d12bc5b5232d297d61921151b425627c28 tags/0.16.0~1,"Fix install of fastparquet on travis (#2897)

* Fix install of fastparquet on travis

Missing dependencies were causing this to silently fail.

* add snappy

* python-snappy instead of snappy

* Fix py27 fastparquet test
",1510951341,
fc1051e002b3016b803993893f5b629d461047aa tags/0.16.0~3,"fix hdfs3 version
",1510859879,
10a6f792ee5062dad83fd25b353da6d710ccd252 tags/0.16.0~7,"Avoid list index error in repartition_freq (#2873)

* Avoid list index error in repartition_freq

avoid list index error if old divisions result in only one new division

* Added test for list index error in repartition

added a repartition test that result in one division

* removed semicolon

fixed flake8 E703
",1510687734,
950ba9e7e54cf0a04acc4c51ce5e53c33d8cfa30 tags/0.16.0~9,"Support arrow in `to_parquet`. Several other parquet cleanups. (#2868)

- Support `engine` kwarg in `to_parquet`
- Add support for using `pyarrow` in `to_parquet`
- Switch `'arrow'` to `'pyarrow'` for engine kwarg, deprecating 
  old `'arrow'` name. This matches the pandas interface.
- Refactor how engines were handled, removing top-level imports
  in `dask.dataframe.io.parquet`
- Fix several bugs in both readers and writers, mostly having to do
  with how indices were handled
- Expand testing for `pyarrow` and `fastparquet` compatibility
- Allow running the test suite with neither, one, or both of
  `pyarrow` and `fastparquet` installed
- Add tests for reading and writing from both engines
- Cleanup bugs in the implementations of both engines that
   arose during testing.
- Change default of `compression` in `to_parquet` to `'default'`,
  meaning the use the default compression for the backend engine.
- Fix threadsafe bug in fastparquet where metadata was being mutated in
  a non-deterministic manner. This should be upstreamed, but fix here
  for now.
- Fix part of a metadata bug in fastparquet where the total number of
  rows isn't consistent when writing a file.
",1510603334,
486bb084d044870c4eb5e182e3728d748d08bcd7 tags/0.16.0~10,"Bugfix: Filesystem object not passed to pyarrow reader (#2527)

* Pass filesystem properly to pyarrow ParquetDataset

* Fix test

* Skip test without fastparquet

Also cleanup a few imports
",1510269671,
3232cb9cd4cced7f74635f43a1f027dae21af287 tags/0.16.0~12,"Fixup s3 tests (#2875)

* Fix travis env so s3 tests run

* Fix deprecation warning in s3 tests

* Fix s3 tests

One s3 test was missing an s3 fixture leading to a failure with a
credentials error on travis, and also causing all subsequent s3 tests to
fail.
",1510260821,
cd179aee77c6e988926b27310011ccc18dc3e5b5 tags/0.16.0~16,"Fixed fillna with Series (#2810)

Closes https://github.com/dask/dask/issues/2809",1510067411,
dcbd628db3471284bbde9d411bbed91fc5b2bc6d tags/0.16.0~17,"Error nicely on parse dates failure in read_csv (#2863)

In pandas, if a column specified in `parse_dates` has an invalid value,
the column is silently converted to `object` dtype. This can cause
issues if some columns infer as datetimes and others as object dtype, as
converting with `astype` is prone to failure.

To remedy this, we add a nice error message if any partition expecting a
datetime dtype finds an object dtype instead.",1510017293,
426de32563a44f844c0b6d2514a66e217efc16cf tags/0.16.0~18,"Fix empty dataframe partitioning for numpy 1.10.4 (#2862)

This was failing, as numpy 1.10.4 checks for `'NaT'` casting failed.
Rewritten to just try to create a missing value, and fallback to `NaN`.",1509732065,
bb005b179317e29066e448dfc697b613dbbbc667 tags/0.16.0~22,"fix and test for issue #2811 (#2818)

* check empty df in sample

* add test_sample_from_query

test fix for issue #2811

* Use jcrist's simpler test_sample_empty_partitions

* add blank line before test_sample_empty_partitions
",1509618446,
643ae878d7cc5204aa096b9f9946e5ccac0aea25 tags/0.16.0~35,"Array optimizations don't elide some getter calls (#2826)

Previously all no-op indexing operations were fully removed in the
optimization passes. This was fine for array-like's that follow numpy's
semantics, but not all objects fully follow these semantics.

Since chunks always should follow numpy slicing conventions, the only
thing we need to worry about is original backend storage (in calls to
`from_array`). We fix the optimization passes to never fully remove
slices if the indexing operation originated in `from_array`.",1509137293,
a41715be0cf10d3f7e9ce361c77fc82ac6c5b73e tags/0.16.0~37,"df.astype(categorical_dtype) -> known categoricals (#2835)

Previously using astype with a `CategoricalDtype` would always result in
unknown categoricals. This fixes it so that it results in known
categoricals if the categoricals on the dtype object are known.

Also fixes a bug where old known categoricals would be set to unknown
after calling `astype`.",1509137168,
c706f7eca6e9aa5ac48f9368bd8b1d1bab3f1edc tags/0.16.0~45,"Fix concat series bug (#2800)

* mkdir is part of os and not os.path

* fix #2798

* Added release note
",1508776353,
4f851be0ccf90534f0b6a8aacd22b123d3f85c4c tags/0.16.0~49,"da.random.choice works with array args (#2781)

* Fix da.random.choice

This method was implemented, but was broken for non-scalar inputs for
`a` and `p`. This fixes that, adds tests, and adds `choice` to the
top-level in `da.random`.

* Add `da.random.choice` to docs, add changelog",1508244557,
5cc5da6af22c13d5bfaec02f32a556ec8f6b7312 tags/0.16.0~51,"ResourceProfiler plot works with single point (#2778)

Previously if there was only a single point the (start, end) plot bounds
would be set to identical values, leading to js errors when trying to
view the plot. This fixes this issue and adds a test for it.

Note that this is only an issue for the ResourceProfiler - all other
plots already checked for this.",1508154774,
3be1d972739c738cd9412a009d61f5907f084376 tags/0.15.4~4,"Pandas 0.21 compatability (#2737)

* COMPAT: pandas TimeGrouper

xref https://github.com/pandas-dev/pandas/issues/16747

* COMPAT: For pandas 0.21 CategoricalDtype

* COMPAT: For pandas 0.21.0 HTML repr changes

https://github.com/pandas-dev/pandas/pull/16879

* COMPAT: For pandas 0.21.0 numexpr import

https://github.com/pandas-dev/pandas/pull/17710

* COMPAT: Default for inplace is now False in eval

https://github.com/pandas-dev/pandas/pull/11149
",1507229543,
42efc8bd3d4c96b17aa3e439bc317708d889ea7e tags/0.15.4~6,"Fix install failures

- Use pytables from defaults
- Pin conda
- Use `conda list` to give better debug support
",1507128539,
b5e27c254660d6744033b0b7c9297e3eb71afbc3 tags/0.15.4~14,"Call mkdir from correct namespace in array.to_npy_stack. (#2709)

* Calls mkdir from correct namespace in to_npy_stack.

Bugfix to correctly call mkdir from the os namespace,
not the os.path namespace.

* Tests to_npy_stack directory making

Test_to_npy_stack() now forces to_npy_array() to work without a pre-made
array, which forces it to exercise the additional code calling os.mkidr.

* Fix tests to use OS appropriate paths.

Use os.path.join to make a path, rather than adding a slash.  Makes test
portable across operating systems.
",1506339799,
983de64dd5d638d5e29f871354ad82bd24429967 tags/0.15.3~5,"Tweak apply_along_axis's pre-NumPy 1.13.0 error (#2703)

* Sum non-trivial dimensions as integers

NumPy will upcast the bool array that we are summing to integers as it
is a lower precision type. However there is no harm in forcing a higher
precision type to start with. Hence we force `sum` to convert the `bool`
array to `int` for summation.

* Tweak apply_along_axis's error message

The previous warning message implied that the returned array needed one
non-trivial dimension. However it could actually be a scalar. So instead
point out that more than one non-trivial dimension is an issue.
",1506187843,
e40bb537bea0f5238296df005703056cb25e5673 tags/0.15.3~6,"Add apply_along_axis (#2698)

* Add apply_along_axis to Dask Array

Implements a Dask Array equivalent of `apply_along_axis`. Does this by
leveraging a NumPy object Array to store all results before stacking
them all back together as one unified Dask Array.

* Export apply_along_axis to Dask Array's public API

* Test Dask Array's apply_along_axis

Provide some tests for the Dask Array implementation of
`apply_along_axis`, which compares it to the NumPy implementation. Try a
range of different functions and axes they are applied along. Also
explores functions that return something with some dimensionality to
them.

* Document Dask Array's apply_along_axis

* Rewrite apply_along_axis to use map_blocks

Previously the `apply_along_axis` implementation went through the Dask,
pulling out each 1D array, and applying the user provided function to
them. However this could create a rather large graph with a significant
amount of overhead for large Dask Arrays. A better solution ends up
being rechunking the data into 1D array chunks the size of which the
user provided function will operate on. Then simply using `map_blocks`
to apply the user provided function to each chunk. To ensure the right
size is determined for the result, some test data is run through the
user provided function first to estimate the results shape to provide to
`map_blocks`.

* Skip some apply_along_axis tests on old NumPys

Before NumPy 1.13.0, it was not permissible for `apply_along_axis` to
take a function that returned an array with dimensionality greater than
1-D. As such we should not bother testing cases where `apply_along_axis`
is given a function that returns something with higher dimensionality if
we don't have a NumPy version that can handle such a result and be
compared to. Thus we skip this case for any NumPy prior to 1.13.0.

* Use NumPy's apply_along_axis inside ours

To simplify our implementation of `apply_along_axis` and particularly
our wrapper function, just call `apply_along_axis` within our wrapper
function. This should allow it to be a bit more flexible, standardized,
and hopefully more efficient.

* Rechunk axis only in apply_along_axis

Instead of rechunking all other axes not being computed along to 1,
simply preserve their existing chunking. Only focus on having one chunk
for the axis that will be operated on by `apply_along_axis`. Due to
recent improvements under the hood in our wrapper function, this should
work seamlessly despite the fact there are some non-trivial dimensions.

* Raise for unsupported result shape

In the event a user has a NumPy older than 1.13.0, we cannot support
multiple dimension from the `func1d` function given to
`apply_along_axis`. So in this case, raise a `ValueError` just as old
version of NumPy would. Update the tests to make sure this error is
thrown.
",1506180995,
8a1286d6e4413722f32cd486a38da77a38f7154f tags/0.15.3~8,"Skip days in daily_stock that have nan values (#2693)

Some of the data sources fail to include complete data for some days.
We just skip those days.

I apologize for the lack of tests on this.  The support in
pandas_datareader is currently broken in a few ways that makes testing
this difficult.",1506014513,
3486d1773628567c56c1c82fa6a4197e7e10906a tags/0.15.3~9,"TST: Have array assert_eq check scalars (#2681)

* TST: Have array assert_eq check scalars

Closes https://github.com/dask/dask/issues/2680

* TST: Fixed broadcasting test

* TST: Fix svd_compressed

Make the right assertion
",1506014502,
95f0f6639293565e1cb5b0b7905deba1d4a06db4 tags/0.15.3~14,"Catch warning from numpy in subs (#2457)

When slicing a dask.array with an array of integers, an operation in `subs`
would emit a FutureWarning from NumPy.

```python
>>> import dask.array as da; import numpy as np
>>> X = da.random.random((100, 2), (2, 2))
>>> idx = np.array([0, 0, 1, 1])
>>> X[idx].compute()
```

```pytb
(Pdb++) pp arg
(array([0, 0, 1, 1]), slice(None, None, None))
(Pdb++) pp key
('da.random.random_sample-95ef33cc2e3f5ffd6248bd70b54d75ec', 0, 0)
(Pdb++) arg == key
/Users/taugspurger/Envs/dask-dev/lib/python3.6/site-packages/dask/dask/core.py:1: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  from __future__ import absolute_import, division, print_function
False
```

So we're doing `np.array([0, 0, 1, 1]) == 'a string'`. At the moment this
returns ``False``, but in the future it will be `array([False, False, False,
False])`, which means the tuple comparison will fail with a ValueError.

This prepares for that eventuality, while catching the warning today.",1505760157,
d80faefcb40d41f0da7a7cd86fb7918afa9bfdbc tags/0.15.3~16,"Fix norm keepdims (#2683)

* Avoid some keepdims branches in norm

To simplify some `keepdims` handling code in `norm`, pass `keepdims`
into a few more methods instead of tacking on singleton dimensions
afterwards. This makes the code a bit cleaner and avoids potential
errors sneaking in.

* In norm handle keepdims for svd-based results

There were a few cases in `norm` where `svd` was used and we forgot to
add back in the dimensions we removed when `keepdims` was `True`.
However this case is a little trickier as we want to operate on a vector
of singular values. However we need to get back two singleton
dimensions. In order to manage this, we take the result from `svd` and
tack on a singleton dimension before passing it to the appropriate
reduction function. That way the reduction function will either keep 2
singleton dimensions (the one we added and the one from the vector) or
it will get rid of both of them. No additional branch is required in
this case.
",1505474183,
a1a75a766243f1f84ac1df502de3faf8ce4ebc81 tags/0.15.3~17,"Dask array slicing with boolean arrays (#2658)

* Support slicing with boolean dask arrays

* add failing test for boolean slicing

* squash

* Partially support indexing with dimension 1 < ... < n

* remove cases other than 1 and all dimensions

* update error message
",1505246789,
b57ba9b59da70e2189efe2f7353ba94dd66c10e8 tags/0.15.3~18,"repartition works with mixed categoricals (#2676)

Previously this would fail, as mixed categories were dropped by
`pd.concat`. We use `dd.methods.concat` instead, which handles such
cases properly.",1505246238,
f30e935f43e1c5baafc47ed928edbf01a7457fce tags/0.15.3~25,"CLN: Remove redundant code, fix typos (#2652)

",1504651118,
bd7c00113c455d72a8ddbbb4efda6b71136645d4 tags/0.15.2~2,"Allow dataframe.loc with numpy array (#2615)

* fixes and tests

* improve isinstance calls

* Make .loc work for empty indexers
",1503579341,
ed35ff7b80904f0ae5cf08bea44c6e7278e26bfd tags/0.15.2~4,"Improve dtype inference and reflection (#2571)

* Add dask.array.core.result_type to the public API

Also adds some targeted unit tests for it: one to test it in isolation,
the other to test that it is consistent with elemwise. The latter
currently fails due to #2574 / #2575.

* Fixes for elemwise dtype inference for 0d arrays

This fixed #2574 and #2575. When given a 0d dask array, it now
passes a 1d dummy numpy array to `apply_infer_dtype`. This assumes that
the operation is capable of handling broadcasting - in particular if
elemwise is given a numpy 0d array and a dask 0d array, it will use
mixed dimensionalities in dtype inference. I had a quick look through
the array code and didn't see any problematic cases, and the unit tests
pass.

Additionally, when a 0d dask array is used, the result of
apply_infer_dtype might not match the actual returned type if numpy
type rules are in use. In this case, the operation is wrapped to cast
the output to the inferred type, to ensure that the Array has a dtype
that matches the end result.

I added a unit test to cover #2575. I deleted some existing code from
the test_elemwise_on_scalars test that seemed to have nothing to do with
elemwise.
",1503336170,
58c0f1e17d1d8cb8fb5744d06f8bce631ca766b1 tags/0.15.2~10,"Don't check for memory leaks in distributed tests (#2603)

The dask/dask test suite fails this for now",1502724852,
aafeedc1d7e406afb44ff18ad468a0b58b82c258 tags/0.15.2~14,"Normalize `meta` on call to `dd.from_delayed` (#2591)

Previously this would let non-normalized metadata sneak through, which
would lead to failures on computation.",1502133940,
6f8ec4772e7b905ebde2df3a61ba04afad717779 tags/0.15.2~16,"Adds choose method to dask.array.Array (#2584)

* Adds choose method to dask.array.Array

Also adds appropriate tests

* Fixes small error in test_array_core.py

* Makes the choose function call the choose method on input dask arrays

* Switches to having the choose method call the choose function
",1501907511,
12aa265e8cdd8900a2515ec9947cec2e781901e5 tags/0.15.2~17,"Generalize vindex in dask.array (#2573)

* Generalize vindex in dask.array

Extends dask.array.Array.vindex[] so it can handle multi-dimensional indexes
(using broadcasting like numpy) and non-full slices.

This will be useful for vectorized indexing in xarray (PR 1473).

* Raise error for vindex with boolean indexes
",1501893185,
a056b9f46f4a2d75f4c542337610f89158165832 tags/0.15.2~18,"Clear `_cached_keys` on name change in dask.array (#2572)

Previously a `_cached_keys` attribute was added to `dask.array` to store
the result of the `_keys` method for fast recompute. Later, mutating the
`name`/`dask` attributes of a `dask.array` was supported. Unfortunately,
mutating these attributes wouldn't clear the cache, which would lead to
hard to find bugs.

To fix this, we do the following:

- Make `Base` define `__slots__`, so that the `__slots__` attribute of
  `Array` is respected. Before the `__slots__` on `Array` were ignored,
  making `_cached_keys` an unexpected source of state (wasn't declared,
  and the declaration was thought to be respected)

- Make `name` a property, and clear the cache when set

- Add a test for the cache behavior.",1501606307,
62644f8a0a9177a1188fb18fd740830924308c5e tags/0.15.2~20,"Add missing initialization to CacheProfiler (#2550)

* Add missing initialization to CacheProfiler

This fixes using CacheProfiler.register (previously it would only work
if explicitly cleared or used through the context manager protocol).

* Add a unit test for profiling with register/unregister

Also improved the existing ResourceProfiler test to check that at least
one result came back.

* Make ResourceProfiler always collect at least 1 sample

This is to make the test more reliable, since it previously relied on
the subprocess starting up fast enough that it could get into the
collection loop before the parent asked it send results.
",1501535626,
a48abc1d6e84f8c597e0c58fba27661b45b558dc tags/0.15.2~21,"Add argwhere, *nonzero, where (cond) (#2539)

* Add an implementation of nonzero

Provides a basic implementation of NumPy's nonzero for Dask. Makes use
of `compress` to make it work. As `compress` seems to be eagerly
evaluated, this means `nonzero` is as well. So it is worth keeping that
in mind. However it is possible the `compress` implementation could be
revisited and made lazy, which would ensure `nonzero` is lazy as well.

* Adds a nonzero method for Dask Arrays

Simply calls the nonzero function from the method.

* Test nonzero function and method

Perform a simple comparison between the results gotten from the nonzero
function and method for Dask Arrays to those provided by NumPy.

* Export the nonzero function to the API

* Include nonzero in the API documentation

* Add flatnonzero for Dask Arrays

Simply takes a Dask Array, flattens it, uses `nonzero`, and drops the
unneeded singleton tuple. This simple strategy seems to simply reproduce
the behavior of NumPy's implementation.

* Test flatnonzero

Simply compare the behavior between the Dask Array implementation and
the NumPy implementation to verify that they are the same.

* Export flatnonzero as part of the Dask Array API

* Add flatnonzero to the API docs

* Add count_nonzero for Dask Arrays

Provides a simple implementation of `count_nonzero` for Dask Arrays.
Does the simplest thing one might expect in this scenario. Namely checks
which elements are non-zero. Then sums over the user provide axis or
axes (if any). This seems to match nicely to the NumPy implementation
without much work.

* Test count_nonzero

Provides some basic comparison tests for count_nonzero using both NumPy
and Dask implementations to make sure they are in line with each other.

* Add count_nonzero to the API docs

* Include count_nonzero in the Dask Array API docs

* Workaround an int type issue on Windows

As Windows treats `int` as 32-bit, we need to force 64-bit to get the
expected result.

* Stacking Dask Arrays with unknown dim doesn't work

Appears that Dask Array's `stack` does not work with Dask Arrays that
have an unknown dimension length. While that makes sense in the general
case (we don't know their lengths), it doesn't apply to this case where
we do know their lengths. In any event, we can update our tests
accordingly as we do here.

* Drop axis from compress in nonzero

This was only needed when `compress` was unable to handle the
unspecified `axis` case. However, as `compress` is now able to handle
this case, there is no need for this argument. After all both arrays are
flattened anyways.

* In nonzero, apply compress to the index array

To keep the computation of the nonzero indices compact, run `compress`
on the non-zero indices before splitting them out into separate arrays
in a `tuple`. This also improves readability a bit as well.

* Support condition only with Dask Array's where

As `nonzero` is now implemented for Dask Array's, it is now possible to
support the single argument (or condition only) case in Dask Array's
`where` since it merely dispatches to `nonzero`. Hence the error message
is updated and this special case is handled.

* Test Dask Array's where's condition only case

Make sure this behaves like NumPy's `where` case. Borrows the same
testing strategy used for `nonzero`.

* Test where for incorrect number of arguments

Dask Array's where must either get 1 or 3 arguments, but not 2. This
check verifies that when 2 arguments are provided an error is raised.

* Add argwhere for Dask Arrays

Basically mimics NumPy's argwhere function for use with Dask Arrays.

* Expose argwhere as part of Dask Array's API

* Test Dask Array's argwhere against NumPy's

* Document argwhere in Dask Array's API

* Rewrite nonzero to use argwhere

As `argwhere` basically does all the work up to the point of getting
transposed and returning `tuple`s, it does much of the work that
`nonzero` was doing. Given this, it makes sense to rewrite `nonzero` in
terms of `argwhere`. Since `argwhere` simply contains one array with all
of the data, it may be more useful in some circumstances than `nonzero`
for use with Dask Arrays.

* Drop extra imports from Dask Array into tests

To simplify the changes already present in this implementation and to
test that the API properly exposes the newly added functions, skip
importing them and use the functions directly from Dask Array's API.

* In argwhere, move transpose before compress

It appears to be a little bit more efficient to have the transpose occur
before the call to `compress` in `argwhere`. So we move the transpose on
the indices beforehand and update the `compress` call accordingly.

* Drop transpose in nonzero

As it is possible to have the desired effect by simply indexing the
second axis as opposed transposing and slicing the first axis, switch to
using the non-transposing strategy. This removes an unneeded operation
from the graph.

* Rewrite flatnonzero to use argwhere

Instead of calling `nonzero` from `flatnonzero` where the array will be
packed into a `tuple` unnecessarily, simply call `argwhere` directly
from `flatnonzero` and drop the singleton dimension in the second axis.

* In argwhere, improve handling of trivial arrays

If any array had trivial dimensions, an issue would occur in the
`reshape` step, which blocked getting a result for `argwhere`. To fix
this, we add a workaround that provides the same behavior by applying
`ravel` to each chunk. The result is an implementation that works just
as well on trivial and non-trivial arrays.

* Test nonzero operations on trivial arrays

Add some checks to ensure that trivial arrays are handled correctly by
all of the nonzero checking functions.

* Fix typo in count_nonzero test

Need to be comparing to NumPy's function not using Dask Array's
implementation in both cases.

* Refactor out count_nonzero calls in test

* Fix count_nonzero's test scalar comparison

As Dask Array represents `count_nonzero` as a zero dimensional array
when all axes are collapsed before it is computed while NumPy represents
it as a Python integer, simply compare the results between the two in
this case instead of comparing the arrays. This will ensure the values
are correct even if they may represent the result slightly differently.

* Test count_nonzero on trivial arrays

* Fix count_nonzero type on Windows

The result of `count_nonzero` should always be `int64` regardless if one
runs on Unix or Windows. To fix this, we need to convert the type to
`int64` before summing to ensure this constraint is met. Also this
explicit conversion is generally a good idea instead of relying on Dask
to implicitly do the promotion for us.

* Refactor out a nonzero function

Creates an internal nonzero function that can be used to work on numeric
arrays, string data, and generic object arrays. The latter two cases may
be notably less efficient due to the use of NumPy's `vectorize`.
Unfortunately that seems to be the best way to solve those issues, so
that is what we go with for now. Can always be replaced by a more
efficient method if one is found.

* Test argwhere and count_nonzero on strs and objs

As these are using our nonzero function directly, these are the two
function we want to be sure work for the different type cases. Hence the
additional tests on them. Higher level functions based off of these will
do the right thing as a consequence of these lower level functions'
behavior.

* Break up count_nonzero based on use of axis

NumPy versions prior to 1.12.0 do not support the axis argument to
`count_nonzero`. So we skip testing those cases on older NumPy's.

* Workaround an oddity with count_nonzero on Windows

Seems that `count_nonzero` will return `int32` for `object` arrays.
Though this is not the same behavior seen for all other cases (including
character arrays). So just force the output type to be `int64` in all
cases anyways.

* Support 0-d Dask arrays in argwhere and nonzero

In the particular case of 0-d Dask Arrays, there are a few code paths
that don't work as things may be 1-d instead of 2-d. So this adds a few
branches to workaround these special cases.

* Test 0-D arrays with various nonzero methods

* Add a note about string handling in isnonzero

We treat strings with whitespace only as empty. This matches NumPy's own
behavior. Though it does not match Python's. Hence this note is left
with a reference to the NumPy issue we opened about this behavior.

* Drop 0-d array case from count_nonzero axis tests

The axis test for `count_nonzero` is suppose to exercise a variety of
non-trivial axes. This does not mesh with 0-d array case as it cannot
have any axes specified. Hence we drop this case from the
`count_nonzero` axis test. After all it is still tested when the axes
are trivial.

* Drop duplicate skip from `count_nonzero` test

* Condense cases in isnonzero

Only special case string handling in `isnonzero`. For all other cases,
simply cast everything to `bool`. This seems to have the right behavior
for numeric and object arrays.

* Refactor out `_isnonzero_str`

Adds a private function that performs a non-zero check on Python strings
that acts the same way NumPy does. This is then used inside `isnonzero`.

* Vectorize `_isnonzero_str` at the module level

* Use count_nonzero in vectorized non-zero function

Renames `_isnonzero_str` to `_isnonzero_vec`. Also changes its behavior
to use `count_nonzero` on each value provided. This leverages NumPy's
own non-zero implementation to determine whether something is non-zero
instead of rolling our own. The result should be a more robust
implementation to changes in NumPy. Also this implementation can be used
on all manners of types (not just strings).

* Generalize isnonzero vectorization handling

Try a simple test conversion for the type in question to see if NumPy
can in fact convert it to `bool`. From quick testing, this seems to
properly pass through all numeric types and object arrays. It catches
string types correctly and passes them on to the vectorization function.
So it seems to retain the behavior had before, but a better forward
looking manner.

The nice thing about this check is it actually is testing the operation
Dask will need to perform to make the conversion happen. So it is a good
indicator if it can proceed or not. Further if NumPy fixes any cases
that currently fails (or vice versa), we should still be able to catch
them correctly and handle them appropriately. All of this being done
before generating the Dask array.

* Convert nonzero str test to argwhere str test

Given that `argwhere` is used to implement `nonzero` and other related
functions, it makes more sense to test `argwhere`'s ability to handle
`str`s instead of `nonzero`'s. The latter will effectively be proved by
the former working as intended.

* Move asarray calls to the top of API functions

Make sure `asarray` is called immediately after entering `argwhere` and
`count_nonzero`. Also drop the `asarray` call from `isnonzero` as it
should be handled before entering this function. This also provides the
added benefit of allowing `isnonzero` to work on NumPy arrays as is.

* Use nonzero function in where

To ensure the `condition` is properly converted to a Dask Array and uses
our implementation of `nonzero`, call the function `nonzero` (as opposed
to the method) on `condition`.
",1501247238,
6287749b67553da5c47516eafc045ef19dd9f5b6 tags/0.15.2~22,"Fix indices error message (#2565)

",1501247139,
3ba30276753205c97e8b4838048aaba608e1922e tags/0.15.2~32,"df.idxmax/df.idxmin work with empty partitions (#2542)

Fixes a bug where empty partitions in a DataFrame/Series would error for
`idxmax`/`idxmin`.",1500998443,
1e4cffedd827093f646c5ee2aa7e5a15dffd0546 tags/0.15.2~35,"da.repeat works with negative axis (#2544)

Previously this would fail with a confusing error message.",1500956866,
6d58b523a53bee22a76ea9860ca1a131b2f9312d tags/0.15.2~36,"Check metadata in `dd.from_delayed` (#2534)

Previously if you specified the wrong metadata in `dd.from_delayed`
you'd get a nonintuitive error message. We fix this by creating a
function to check metadata inline, and raise a nice error message when
incorrect metadata is found. We apply this to `from_delayed` only for
now.",1500931067,
faad4375e0308ccf56998d02c2b3ca2c295a4263 tags/0.15.2~40,"Updates read_hdf docstring (#2518)

* Updates read_hdf docstring to include lock and mode parameters. Also adds graphviz install line to Development Guidelines.

* Adds info regarding sorted_index to read_hdf

* Minor update to mode description

* Fixes flake8 issue
",1500467747,
a5d8e33d606de2b83bee25baa97018681a739bbf tags/0.15.2~42,"Better error message for read_csv (#2522)

Previously we'd provide a good error message only if the inferred dtypes
mismatched on `int->float` conversions. We now provide a nice error
message in all cases, including the following information:
- *All* mismatched columns, formatted nicely
- Suggested dtype dictionary to use
- If applicable, also suggest using `assume_missing=True`",1500403263,
9b3fef64ec1e987bc416c9f1ff7e76b6f98a18a5 tags/0.15.2~44,"Do not compare x.dim with None in array. (#1847)

* BUG: Do not compare x.dim with None in array.

I suspect this is the cause of failure in Python 3 at

https://travis-ci.org/bccp/nbodykit/jobs/183846633#L570

* Trying to fix flake8 errors

* blank line at end of file.
",1500327305,
bfcbd99ae646b82c40f0707a19eb37f98d34b524 tags/0.15.1~1,"Update doc building (#2512)

1.  We avoid numpydoc 0.7, which is particularly strict about extra sections
2.  We fix our docstrings to avoid duplicate section titles
3.  We fix a few links within prose docs
4.  Fix a spelling mistake

Even though we fixed the duplicate section titles the full class docstring
seems to get everything, causing more issues.  I'm just going to pin to 0.6 for
now.  It looks like they're releasing 0.8 rather soon, so I suspect that 0.7
had known issues.",1499521254,
50c4ac1ec755399cee9b87a9db48413dfdda6e51 tags/0.15.1~7,"TST: Don't trigger faulty NumPy lstsq problems. (#2505)

With `randint` and `seed=1`, the `np.linalg.lstsq` triggers some poor
rcond due to numpy/numpy#8740. Since dask does not trigger the same bug
itself, change the rcond as in numpy/numpy#9284 to not break.",1499256807,
2fa0eed9becdbd8eab212a74075ca3fc501a5668 tags/0.15.1~14,"Add `meta` kwarg to `Bag.to_dataframe` (#2482)

* Add `meta` kwarg to `Bag.to_dataframe`

Allows passing in a metadata specification instead of a DataFrame. Also
deprecate the overloading of the `columns` kwarg. This now has the
following behavior:

- If `meta` is specified use that (and error if columns is provided).
- If `meta` not specified, infer metadata using `columns` (if provided).",1498159165,
d01f68800089d49ac3c6ebcbf6de4054690ea8cb tags/0.15.1~15,"XFail test_interrupt with pytest-xdist (#2483)

This test started failing periodically again, but only when used with
`pytest-xdist`. I can't reproduce it locally, and I'm not sure that
`pytest-xdist` is actually the culprit. For now we xfail for xdist runs.
This test has used up too much dev time, and is still flakier than
baklava. Frustrating.",1498085397,
e70ee8abee773bc7b3d0307d1500c6be657d571c tags/0.15.1~18,"Better error messages for bad reductions on object series

The errors raised by pandas when calling a numeric reduction like `mean` on an
object series can be poor, making debugging difficult. To fix this we
explicitly error when an object series is passed to one of these reductions.
",1497984462,
37c3ae2e091412f5e2fdf3c957c383e89e4c8bb2 tags/0.15.1~22,"CI: Fix CI failures on master (#2459)

* CI: Fix failures on master

* Skip learn
",1497876615,
813eba43ce6f26765081a9147ac84fbb5920116a tags/0.15.1~23,"Bag.to_dataframe works with iterators (#2468)

Previously this would fail as the `data` argument to `pd.DataFrame`
can't be an `Iterator`. We now convert if needed.",1497640447,
fa970532da39d1190b70ccf09db60b11aa1c1e3f tags/0.15.1~24,"Slice delayed (#2467)

* delayed works with slicing

Previously we weren't pulling delayed objects out of `slice` objects,
causing slicing to fail on delayed objects. This fixes that.

* Fix xfailed delayed Iterator test

This was testing for behavior that has never been implemented, and
probably can't be. We fix the test to check that passing iterators to
delayed functions works as currently implemented (converts iterator to
a list).
",1497640410,
72a5f5104b22bd50de37329a99f0f5c623983171 tags/0.15.0~1,"Add __array_ufunc__ protocol to dask.array (#2438)

* Add __array_ufunc__ protocol to dask.array

Example
-------

```python
In [1]: import dask.array as da, numpy as np

In [2]: x = np.arange(24).reshape((4, 6))

In [3]: np.sum(np.sin(x), axis=0)
Out[3]:
array([-1.56697566,  2.06850183,  3.80220828,  2.04018197, -1.59757823,
       -3.76653238])

In [4]: d = da.arange(24, chunks=(4,)).reshape((4, 6))

In [5]: np.sum(np.sin(d), axis=0)
Out[5]: dask.array<sum-aggregate, shape=(6,), dtype=float64, chunksize=(6,)>

In [6]: _.compute()
Out[6]:
array([-1.56697566,  2.06850183,  3.80220828,  2.04018197, -1.59757823,
       -3.76653238])
```

* Support out parameters

* test non-supported ufunc methods

* fix cumreductions

* Use elemwise in __array_ufunc__ and return NotImplemented

* Test out= paramter with numpy array

* cleanup tests for numpy < 1.13.0

* generalize special cases in __array_ufunc__

* flake8

* skip out= test under old numpy

* skip fix test

* use numpy=1.13.0 in travis.yml

* Update travis.yml libraries

NumPy 1.13.0 is not yet compatible with pyarrow or pandas 0.20.1

* pull out matmul raise issue and skip

* remove comment within travis install

* update travis.yml

* add graphviz from conda-forge pyarrow from pip

* pin pytest

* pin pytest below 3.1.1
",1497186527,
a7fd47f2108054ba991df2ae83cd1873442eea7c tags/0.15.0~2,"Bag map partitions (#2445)

* Add db.map_partitions

- Add top-level `map_partitions` function
- Add support for Bag args and kwargs to `map_partitions`

* Fix docstring for map

We error now if partition lengths mismatch

* Use ensure_dict throughout

* Add to api docs
",1496957818,
1c0113d8bf2d5944fbe6b7b1331636e0b7d9d45f tags/0.15.0~3,"Add matrix multiplication operator for dask arrays (#2349)

* Add matrix multiplication operator for dask arrays

* Use operator.matmul instead of eval

* Test equivalence of swapping numpy and dask

* Also cast RHS to array if necessary

* Test RHS array cast before matmul

* Raise NotImplementedError for higher-D arrays

* Add tests to catch NotImplementedErrors

* Update error message to specify @ and point to tensordot

* Add forgotten raise
",1496922047,
88aea4271262f64b9a32471025e561dbc6bd6029 tags/0.15.0~4,"Don't error if writing parquet with no data (#2437)

* Don't error if writing parquet with no data

Also, allow reading where there is no data: produces empty dataframe.

Warning: partition_on does not mix with writing empty dataframe, since
there is nothing to base directory names on; should this still error?

Warning: writing empty dataframe will still produce data files for each
part (if not using partition_on), but each file will contain no rows.
This can be fixed on the fastparquet side, but it's not immediately
obvious what the behaviour should be.

* assert_eq instead
",1496882585,
a70f3c7a29f969c02a8591ffb8e91c77098dbbec tags/0.15.0~9,"Fixes some type issues in Dask Array shapes (#2375)

* Handle depth of other integral types

Instead of requiring that `depth` be of type `int`, allow `depth` to be
of any integral type. Also handle all integral types provided for
`depth` the same way.

* Reconstruct `depth` dictionary

If a dictionary for `depth` is provided (or formed), handle conversion
of the keys and values to `int` type. This is done so as to avoid weird
integral types showing up in the shape of Dask Arrays.

* Drop blank line

* Test shape value types after `map_overlap`

Make sure that even if the `depth` provided to `map_overlap` has a
different type from `int`, that `map_overlap` proceeds through and
properly converts the Integral values to `int` instances.

* normalize chunk values to integer type
",1496845059,
0949a1dca60cb23cd76ae32ef33337d8b946c386 tags/0.15.0~11,"Remove nans from divisions when shuffling (#2413)

* Remove nans from divisions when shuffling

When shuffling we first check to see if the column is already sorted, in which
case we can be much much faster.  To do this we compute the minimum and maximum
value of the column in every partition.  If there is an empty partition then
this can cause a problem, because the min/max value will be nan.

Now we remove these nans before proceeding

Fixes https://github.com/dask/dask/issues/2408

* flake8
",1496838448,
092d5c493e7af0adc05c2c2b7158e98501d02d70 tags/0.15.0~12,"TST/CLN: Catch more warnings (#2384)

* TST/CLN: Catch more warnings

Catches a handful of warnings from the dask.dataframe test suite. Remaining
warnings fall into a few classes:

1. ResoucreWarnings from cloudpickls (e.g.
  dask/dataframe/tests/test_arithmetics_reduction.py::test_frame_series_arithmetic_methods)
  Not sure what to do here. Probably a legitimate concern of dasks?
2. RuntimeWarnings from numpy (e.g.
  dask/dataframe/tests/test_arithmetics_reduction.py::test_reductions[False]
  I think dask should catch these, (followup PR)
3. ImportWarning from dependencies importing C code? e.g.
  dask/dataframe/tests/test_categorical.py::test_categorical_set_index[disk]
  I have a fix for partd (msgpack). Pandas has some too. Still investigating,
  may be a Cython issue
4. DeprecationWarning on regexes. e.g.
  dask/dataframe/tests/test_groupby.py::test_full_groupby_multilevel[grouper4]
  I think these are all pandas. xref
  https://github.com/pandas-dev/pandas/issues/16481#issuecomment-303836235
5. RuntimeWarning from pandas merge, e.g.
  dask/dataframe/tests/test_multi.py::test_merge_by_multiple_columns[disk-inner]
  Filing an issue on pandas

* CLN: catch warnings for dask/array

* TST: Reset warnings for test

Someone is globally setting Warnings to `'once'`. This will reset it
specifically for the test. Long-term solution is to figure out who
is setting that and fix it there.

* Skip on py27

* Fixed skip condition, linting

* Additional comments on warnings
",1496837123,
1eb54aa3b9b706cb83425bcf266d5e47a0c4a9e7 tags/0.15.0~14,"Optimize out reify calls in to_delayed (#2425)

* Optimize out reify calls in to_delayed

This has two major changes:

1.  The lazify optimization traverses down lists.  This is important
    after changes in https://github.com/dask/dask/pull/2339
2.  Call optimize when we convert from bags to delayed objects

See https://github.com/dask/dask/issues/2423

* Optimize in Array/DataFrame.to_delayed

* Use bag optimizations within to_textfiles
",1496772435,
6bd25dc940b6b418367fcd188fb335246e0fb173 tags/0.15.0~26,"Allow pathlib.Paths as URLs (#2310)

* Allow pathlib.Paths as URLs

* Test that using pathlib with URIs fails

* Check that pathlib is only used for local

* No paths test on windows

Because some sort of monkey-patch is breaking later moto tests

* add import
",1496257884,
bf709e2598286ce64c420ea5cb2f6b4c43d88ec4 tags/0.15.0~28,"BUG: Fixed partial string indexing with no freq (#2391)

Worked around an edge case in pandas datetime indexing by changing the data in
_meta_nonempty.index so that the index no longer looks monotonically decreasing.

Closes https://github.com/dask/dask/issues/2389",1496257714,
7da2c0722942ec84194f756b8cdfd08ca9edbdae tags/0.15.0~31,"Fuse slices works with alias in graph (#2364)

* Fuse slices works with alias in graph

In #2080 a bug was introduced that prevented fusing slices across
aliases in the graph. These would show up when several dask arrays were
concatenated together. This fixes that bug, and adds a test that fusing
works across aliases.

* Fuse slices works with locks
",1496161759,
802772eb386d9e4c0d1f139835bbf3ba8ff4327a tags/0.15.0~33,"Add `token` kwarg to `da.map_blocks` (#2383)

Add the `token` kwarg to `map_blocks`, mirroring the `token` kwarg of
`atop`. If provided, this is the prefix of the output key, but not the
key itself.",1495820793,
85e2b4f439dd93e742cf0004be1a8787af407623 tags/0.15.0~35,"Fix spelling errors (#2382)

",1495647126,
a698aa6184150217db4d2d6a7f5cfffb17d1c6d8 tags/0.15.0~38,"Improve docstring for read_csv/read_table (#2363)

- Add note about dtype inference, and what to do if it fails
- Fix formatting of docstring
- Fix a few typos

Also re-add the `mathjax` sphinx extension, as the array docs require
it.",1495239592,
4f7580aa0db7c1ae68b8029067ab7b40f0e513fb tags/0.15.0~40,"Support ufunc.outer (#2345)

* Support ufunc.outer

- Add support for `ufunc.outer` methods for dask versions
- Separate ufunc objects from wrapped elemwise functions. Ufuncs are now
  an instance of `da.ufunc.ufunc`, and have a number of attribute and
  methods mirrored from their numpy counterparts
- Fix a bug in `da.ufunc.angle`
- Improve test coverage for `da.ufunc`

* Py27 compat

* Respond to comment
",1495122770,
8a2893ab648e1287509a192605768c62df31cf0c tags/0.15.0~44,"xfail demo_stock dd.demo.dail_stock test

Yahoo changed/deprecated their api's, so this functionality no longer
works.
",1495038689,
f7a44ada3d47a55c9f92e2c7b20a2441a2c57ea6 tags/0.15.0~46,"Add `db.map`, `Bag.starmap`, fix `Bag.map` (#2339)

* Add db.map

Implement `db.map`, a mapping function that has the same semantics as
`builtins.map`, with the following extensions:
- Supports keyword arguments
- Non-bag arguments are ""broadcast"" across all function calls

* Add Bag.starmap

Similar to `itertools.starmap`, except also supports keyword arguments.

* Deprecate old magic Bag.map behavior

* Update bag docs",1494964583,
965c9e401801689a6a68cec5c0529f912a459960 tags/0.15.0~51,"[WIP]ENH: dask.stats (#2269)

* Added da.division

* stats

* ENH: Added dask.stats subpackage

Emulates scipy.stats. Added the easy statistical functions and
tests.

* API: Return Delayed(*Results)

* REF: Move to dask.array.stats

* Raise on nan-policy

* PKG: remove stats submodule

* Skip on import failure

* pep8

* remove 'stats' from setup.py

* add bias=False to da.stats tests

* Use da.moment for moment

Raise NotImplementedError on bias=False

* TST: a few missing cases

* Consolidate docs, fix order handling for tuple axis

* DOC: Move API section to array-api.rst

Also fixed up a few broken references

* Remove unused imports
",1494531559,
effcbf38c02fe3305c4b3e2160ee64f3a24395f0 tags/0.15.0~52,"Handle extra chunks in *fftfreq functions (#2330)

* Handle extra chunk in *fftfreq functions

Chunking is not handled correctly with the various *fftfreq. There seems
to be some subtle edge cases due to the fact that we construct arrays
with the chunks that are larger than the final result. To fix this
generally we have to normalize the chunks and tack on additional chunks
for the portion that will be disposed.

In the case of `fftfreq` and `ifftfreq`, only one extra value is added
by `linspace`. To adjust the provided chunks for this, we tack on a
singleton chunk to the end after normalization. This should allow it to
be trivial stripped off once we are done as it has no overlap with
anything else. So this shouldn't have any performance overhead.

As for the case of `rfftfreq`, we have to do a bit more. In this case,
we strip off about half of the `linspace` we construct and drop it. To
fix this case we tack on an additional chunk that is the length of this
pieces that will be dropped. As the chunks will be normalized again,
this last chunk will be probably be split into similar sizes as the
other chunks. Also the last chunk will probably have some overlap with
a portion of `linspace` that is to be removed. This might have some
effect on performance, but shouldn't be negligible when there are many
chunks and/or chunks are small.

* Improved chunk handling tests

Parameterize chunks in tests of *fftfreq functions.
",1494521232,
e8a2dca5f65b0513f437b63c145d8a3511342ba4 tags/0.15.0~53,"Check for tricky cases when fusing fancy indices (#2313)

* Check for tricky cases when fusing fancy indices

Previously we'd fuse normal and fancy indexing like:

```python
a[:, :, [0, 1], :][0, :, :, :] -> a[0, :, [0, 1], :]
```

This is incorrect, and was leading to bugs:

```python
>>> a = np.ones((11, 13, 20, 12))
>>> a[:, :, [0, 1], :][0, :, :, :].shape
(13, 2, 12)
>>> a[0, :, [0, 1], :].shape
(2, 13, 12)
```

Let `ind1` and `ind2` represent two index operations as in
`x[ind1][ind2]`. We now only fuse indices involving fancy indexing if:
- Only one of the two uses fancy indexing
- Dimensions where fancy indexing isn't used are non-integers

This means that the following will be fused:

```
a[:10, [1, 2, 3]][:5, 0]
```

but this won't:

```
a[:10, [1, 2, 3][0, 0]
```

This does exclude some valid cases, but is easier to write than
following all the checks required when mixing fancy and non-fancy
indexing.

* py2 compat fix
",1494519090,
af9d4b4297e5a65cd6cef16221d12fd0e77ee850 tags/0.15.0~55,"ERR: Better error message describe with non-numeric data (#2331)

* ERR: Better error message describe with non-numeric data

* Compat with older pandas, handle series

* Flake8
",1494515027,
917bbd4add38c7f2cc25b34ddc66f46d7f26fac0 tags/0.15.0~60,"Slicing with negative index in list (#2317)

* Slicing with negative index in list

There was a bug in `check_index` where we weren't properly checking that
a negative list index was still in-bounds. This fixes that.

* Respond to comment
",1494392593,
2cc7d96684f0fcca17dfc8b68df73fe132d75aa3 tags/0.15.0~63,"Better scheduler errors (#2312)

* Better scheduler errors

Previously we used our own ``RemoteException`` class to show errors when
using the threaded/multiprocessing schedulers. This provided suboptimal
tracebacks, which made debugging harder. We now do the following:

- Synchronous scheduler: no change

- Threaded scheduler: reraise the error in the main process with the
  original traceback.

- Multiprocessing scheduler: If ``tblib`` is installed, use it to
  serialize the traceback, and then reraise the error in the main
  process with the original traceback. If it's not installed, fallback
  to the old ``RemoteException`` class.

A benefit of this approach is that now pdb plays nice with the threaded
scheduler, making debugging a lot easier.

* Fix failing parquet test
",1494359601,
6c980a7e508c3f1522b9ef9e0fb5e2a1a77700ec tags/0.15.0~66,"Fix numpy pin to not upgrade secretly (#2302)

* Fix numpy pin to not upgrade secretly

Previously we'd install fastparquet on travis using pip as long as we
weren't running with PYTHONOPTIMIZE=2. This meant that fastparquet would
be installed for our 2.7 build, which would then secretly upgrade numpy
since fastparquet requires numpy > 1.11.0.

To fix this we do the following:

- Print version info for the pinned packages at the end of install, to
  visually see that they're correct in the travis logs.
- Add a check for numpy version before installing fastparquet
- Switch the 2.7 build to use numpy 1.11.3. I figured it's more
  important to test fastparquet with python 2.7 than it is to test old
  numpy with python 2.7.

* Skip sparse tests for numpy < 1.11.0
",1494000925,
4897da6c3f60f751bddef625f441fd39d1c58613 tags/0.14.3~1,"Catch missing pyarrow import (#2295)

See https://github.com/dask/fastparquet/issues/142",1493988154,
638bb983cfc422680197f8ee6cf7905774d0e579 tags/0.14.3~3,"Raise error in read_csv if sample is too small (#2291)

We raise an informative error if the sample isn't large enough to
include at least one row of data. Previously we could have a sample that
only included the header row (or just the skiprows).",1493939921,
bd5f3efadd587e87cc4dade705b49d64a4a180b2 tags/0.14.3~4,"xfail test_interrupt on py3 non-windows (#2297)

Still periodically failing, not sure why.",1493922080,
bdb021c7dcd94ae1fa51c82fae6cf4cf7319aa14 tags/0.14.2~1,"Raise informative error if dask.distributed is not present
",1493836441,
ad752cce5ee01fcc88f341ab707372aee08c050e tags/0.14.2~3,"Better error for set_index with sorted and divisions  (#2289)

* Better error for set_index with sorted and divisions

When calling `df.set_index(sorted=True, divisions=...)` divisions are
assumed to match the actual splits in the new index column. This means
that there must be the same number of divisions as in the original
frame, and the specified divisions must be valid.

This PR does the following:
- Adds a check that the specified divisions are valid, and throws an
  informative error if they aren't
- Improves the docstring for `set_index` to better reflect this
- Consolidates the `set_index` tests (which were scattered throughout
  the test files), and removes some duplicate tests as found

* Remove deprecated `set_partitions` method

* Remove `set_partition` from docs
",1493826872,
a5da73881d8c4afdcdbb04d7ab5b4555cc7771bd tags/0.14.2~5,"Make ResourceProfiler more robust (#2286)

Previously processes could be dropped after a tracking loop was started,
which may lead to an exception due to trying to get memory/cpu info from
a dead process. This PR fixes this to handle dead processes whenever
information is recorded.",1493750729,
ac3833d3e0370b219223b7657417b9253a75a971 tags/0.14.2~6,"Test dd.to_hdf with dask.distributed (#2283)

* Add few hdf tests using the distributed scheduler

This is to test the distributed scheduler is functioning properly with different dask operations

Signed-off-by: Nir Izraeli <nirizr@gmail.com>

* xfail test

* remove noqa flake comments
",1493750265,
7919d77c7d48cdc14b84a1c2670caca16198cb5d tags/0.14.2~9,"Note that map_partitions shouldn't change index (#2281)

Functions provided to `map_partitions` shouldn't modify the index, and
if they do the user should clear the divisions afterwards. Doing this
was causing issues for a user, and our docs didn't indicate that this
behavior was bad.",1493675729,
1d8dff894bd5d3a35dc0ecfddfaca16e526566d9 tags/0.14.2~13,"ENH: Support rechunk with some unknown dims (#2251)

* ENH: Support rechunk with some unknown dims

Expandas the rechunking methods to support rechunking arrays
with some unknown dimensions, along known dimensions.

Closes https://github.com/dask/dask/issues/2236

* Use math.isnan for nan checks

* REF: simplify by modifying callers

Still need to cleanup, but the tests pass

* Removed unneeded set_option

* Fixed old_to_new for unknown

* Ignore unknown dimensions in plan

* Avoid isfinite for py2

* No intermediate chunks with unknown

* assert_eq in test; update comment
",1493644566,
48f450bf5a2eaa702f598bb04986ce7cb692e134 tags/0.14.2~18,"da.stack properly handles mixed dtypes (#2274)

Previously we'd fail to convert partitions, leading to mismatched
dask/numpy dtypes.",1493416599,
272ede5fc0727a565c8d3e6e4d6678cad21f7473 tags/0.14.2~19,"Both new_axis and drop_axis are supported (#2264)

Previously we disallowed this as it wasn't clear what dropping and
adding axis at the same time meant. We now define drop_axis as coming
before new_axis, and through nice errors on bad parameter combinations.",1493409131,
918f2331e923c96a7c769993273c14464ac62aef tags/0.14.2~22,"Support sparse arrays (#2234)

* Support sparse arrays

This supports sparse arrays mostly by relying on ufuncs to do the right
thing.  Additionally we check for operations like concatenate and
tensordot on the object's module.

* add sparse to CI

* add package_of function to utils

* clean up package_of

-  Add test
-  Support missing module

* add comment around concatenate2/3

* fix typo

* Avoid testing sparse on old versions of Python/Numpy

This is a new and fringe feature.  Didn't want to spend the time
supporting old systems.

* Tensordot, rechunk, reshape, abs, and other elemwise operations

* support mixed dense/sparse arrays

* Use array_priority to determine concatenate function

* use __array_priority__ for concatenate3 as well

* handle corner cases with concatenate3

* flake8

* add sparse array docs

* skip more sparse tests in python 3.4

* flake8

* remove dead code
",1493392248,
68f9e417924a985c1f2e2a587126833c70a2e9f4 tags/0.14.2~23,"ENH: Refactoring to support multiple Parquet readers, add PyArrow reader (#2223)

* Refactor Parquet read path to support multiple implementations. Add
PyArrow-based implementation. Disable tests that don't yet work

* Remove superfluous global

* Comment typo

* Use functional style for Parquet read functions. Add failing test for #2177

* Fix renamed module

* Code review comments

* Add pyarrow to conda dependencies

* Code reviews

* Add documentation about Parquet engines in dataframe-performance.rst
",1493391466,
0298d408410dd730f46032e0c857124d170df24a tags/0.14.2~24,"Fix map_overlap with trivial depth (#2259)

* Test map_overlap with no depth

Appears that when the boundary condition was `""none""` and the depth was
trivial, running `map_overlap` would fail. This adds a test that
demonstrates that failure and hopefully guarantees it won't reoccur once
fixed.

* Fix map_overlap for trivial padding

In `add_dummy_padding` there was a special workaround for the `""none""`
boundary condition that failed when the `depth` was trivial (i.e. `0`).
To fix this, simply check to see that the boundary condition is `""none""`
and the `depth` is positive-definite before proceeding. This appears to
clear up the issue when working trivial depth and the `""none""` boundary
condition.
",1493299470,
d99fa2a4dd454b3b70d10c043e6b86a5cf05dd4f tags/0.14.2~25,"BUG: Handle grouping by and aggregating same colum (#2257)

Previosuly we always excluded a grouping column from the columns
to be aggregated. Pandas will allow you to group and aggregate the
same column, assuming it's the only column to be aggregated.

Fixed a related bug with multiple aggregations not reducing
to a single-level MI when possible.

Closes https://github.com/dask/dask/issues/2255",1493248791,
bab4942b4b9eab6862226c0da868389629ee074f tags/0.14.2~27,"Compat fixes for pandas 0.20.0rc1 (#2249)

Compat fixes for pandas 0.20.0rc1.
",1493067035,
7263fe81d3a8c6704590ed85f6b1bf07e40034ad tags/0.14.2~31,"Object arrays with missing values hash the same (#2243)

Due to a bug in the pandas hashing code, the presence of missing values
in an object series would cause it to hash differently. This was fixed
in pandas in
https://github.com/pandas-dev/pandas/commit/e351ed0fd211a204f960b9116bc13f75ed1f97c4
but was never backported to dask.",1492805865,
e33c7ee11fc62aeffaccc27d181a7e0267406809 tags/0.14.2~32,"Support custom optimizations (#2219)

* Support custom optimizations

* test and document foo_optimize=None

* Add defer_to_globals decorator

* Support custom optimizations

* fix typo in docs

* Fix docstring
",1492707782,
1c41702746c28c400e4910c39b507b74bda64062 tags/0.14.2~34,"Support concatenation with unknown chunksizes (#2235)

This is important for dask.dataframe / dask.array interactions but genreally a
messy proposition.  I have tried to provide warnings and informative errors.",1492539415,
5f9ddd2e07305d1dfc9201d03c25b43e7b041fbc tags/0.14.2~35,"Tokenize complex number types (#2229)

Fixes https://github.com/dask/dask/issues/2228",1492522943,
d549bb58db1d878ecce44059b93e5e4f1ce77e0d tags/0.14.2~37,"(Maybe) decrease flakiness of interrupt test (#2233)

- Increase timeout to maybe deal with travis slowness
- Use time.sleep() to make task length consistent across runs (help with
  debugging).",1492452966,
a5da00a3e5f8913c9014eb60faf0084be77c20e2 tags/0.14.2~38,"BUG: boundary_slice with missing and unsorted. (#2220)

* BUG: boundary_slice with missing and unsorted.

Missed a case in https://github.com/dask/dask/issues/2211 where
the index is not sorted and the key is not in the index and
either of the boundaries are not included.",1492430495,
d2dd77ee4feb5ec863eb80699b778ede2a97c128 tags/0.14.2~39,"CLN: fix usage of pandas.lib (#2226)

",1492362698,
ca3bccf2a62fc1f5cbe559e591e725cbb16a2998 tags/0.14.2~42,"Boundary slice excluded (#2217)

* BUG: boundary_slice assumes sorted indexes

Changes `boundary_slice` to handle cases where

- the index is not sorted
- using label-based indexing (loc)
- the start or stop is missing

See https://github.com/pandas-dev/pandas/issues/8613 for details on the pandas
side.

* Avoid sorting in `boundary_slice`

* Additional tests, falsey endpoints
",1492182619,
00c29df996521b79af178df554059527709559b7 tags/0.14.2~47,"Fix #2204: fix rechunking empty array with non-trivial shape (#2205)

* Fix #2204: fix rechunking empty array with non-trivial shape

* replace compute calls with assert_eq in test

* Reinstate assert statement
",1492010384,
472070866f3958ce72d5b64f151f1d069c929c67 tags/0.14.2~51,"Add machine learning doc page (#2194)

* add machine learning doc page

* add extra dependencies notes

* fix subtitle
",1491861703,
eaea248dd75a43df3fb190dbe165847c547ec800 tags/0.14.2~52,"Allow delayed targets in dask.array.store (#2181)

* Allow delayed targets in dask.array.store

* Resolve flake8 complaints and change test

* Resolve flake8 errors
",1491860876,
b7bd75fcd011f144d8061d329fe7676ec8ab9983 tags/0.14.2~54,"Add temporary_directory support to on-disk shuffles (#2163)

* Add temporary_directory support to on-disk shuffles

Fixes  https://github.com/dask/dask/issues/2139

Depends on https://github.com/dask/partd/pull/24 to be effective, but
doesn't break anything without it

Example
-------

dask.set_options(temporary_directory='/path/to/tmpdir')

df2 = df.set_index('x')...  # now stages in /path/to/tmpdir

* add git partd to continuous integration
",1491831042,
4874e97e08be7b9a1bd7da8649821d0bc051f4ba tags/0.14.2~55,"Fix CI pinnings (#2185)

* Pin NumPy and pandas in the environment

Use the `pinned` file for the test environment to fix the versions of
NumPy and pandas. This should ensure these don't need to be explicitly
listed in each step as these pins will always be matched no matter what.
Also by doing this, there is less opportunity of forgetting to
explicitly fix these and thus run into issues.

* Drop pinning of NumPy and pandas during install

These pinnings are now set in the pinning file. So we merely need to
install NumPy and pandas once and they will stick to the same version.

* Specify the NumPy patch version on AppVeyor

Appears that NumPy 1.12.1 has been used on AppVeyor recently. So picking
that patch version seems reasonable. Given we pin to the patch version
with pandas on AppVeyor, this doesn't seem unreasonable to do with
NumPy. Further on Travis CI we also pin to the patch version with NumPy.
In short, this is a totally reasonable and consistent change, which
should have minimal effect.
",1491826982,
9109119f8bdeb81e7d6d32564801040c7d4c8938 tags/0.14.2~58,"Run the FFT tests and fix a few failing tests (#2183)

* No longer ignore the FFT tests

It appears the FFT tests were being explicitly skipped by this
configuration file. A look into the commit history suggests this had
something to do with avoiding doctests. Though it is unclear whether
there were any doctests. In any event, there are none now and this is
unintentionally suppressing the unittests. So simply remove this
configuration file as it didn't do anything else.

* Enumerate the axes to properly order the chunks

The output shape was being incorrectly ordered in some cases as the
shape was remapped by the axis order, but the axes themselves. This
corrects that by enumerating the axes and using the enumeration to
reorder the output shape for use with chunks.

* Adjust handling of 1-D FFTs

Given the changes to how we handle the output shape in the FFTs, this
breaks the 1-D FFT special case code. So update it to reflect how the
output shape is now being handled.

* Fix output shape handling in real FFTs

* Re-add conftest so as to exclude fft doctests only

Seems the wrapped functions in the fft module expose docstrings with
doctests, which pytest picks up on and tries to run. So re-add the
existing configuration file, but update it to exclude the file `fft.py`
in `dask/array` only.
",1491777617,
33d7a791d9d4a103ff46c237844cf16fcc2b8a68 tags/0.14.2~60,"Raise error in persist if get functions differ. (#2176)

* Raise error in persist if get functions differ.

This patch adds a check in base.persist() to raise if the globally
set get function does not match a locally supplied instance.

Test is added.

* Allow set_options(get=) to override distributed in persist
",1491744222,
2f95291d326148d322dae7271ebd4ab4df930421 tags/0.14.2~63,"Support repartitioning dataframes to more partitions (#2168)

Fixes https://github.com/dask/dask/issues/2099",1491426160,
305d2f273dc89ce2aecfc734850e2219a29b1e00 tags/0.14.2~67,"Fix skiprows with multiple blocks (#2165)

* Fix skiprows with multiple blocks

* Make test_skiprows use user API functions

* Extend timeout in test_interrupt

This previously failed intermittently
",1491343659,
5fabc2e1d573f21a968f5dc58216488c5507604b tags/0.14.2~65^2^2,"Add index=False description to read_parquet

Discussion [here](https://github.com/dask/dask/issues/2161)",1491318396,
0d15d4bfa2f4e5761411fc50357983ebcd2b0647 tags/0.14.2~70,"Support repartition on series (#2157)

This failed previously because we had hardcoded in the DataFrame
constructor to a generic function",1491256843,
328884c82d9fb9ff0ffbae630c7dbcc05681e198 tags/0.14.2~71,"Add sql loader (#1181)

* Add sql loader

* Start attempt to use ntile instead of offsets

This is untestable as ntile and row_number are not implemented
in sqlite (but in the target sql servers, e.g., PG, they are).

* Tests pass on postgres

* Explicitly set index

* Remove dross

* Try simple SQL->dataframe loader

* Flake fixes

* Now with SQLalchemy

The code looks a little less tidy, and I don't like the isinstance(, str), but
seems to work well for a range of cases.

* Clean up

* flake fixes

* Requested fixes

Also, added some tests and uncovered some corner cases.

Not insisting that, if the index is an sqlalchemy compountd construct,
it should be explicitly named using `.label()`. Raises exception
otherwise. This gets around trying to guess what pandas will name
that column.

* Further fixes

Split out npartitions and divisions to avoid confusion.

* cosmetics

* Trailing u

* Furhter small changes

* Make upper limit inclusive

Similifies what happens with max value

* Create SQL engine within task

Tests only worked on multi-threaded, because the sqlalchemy engine is not
serializable.

* flake

* Auto-guess chunksize if npartition not given

Also: enable operating on any input sqlalchemy expression, not only
a named table.

* more flake

* add read_sql_table to imports and docs

* Ensure time divisions are are range limits
",1491141821,
da51f9bc6720bf2b069298c91fb77e9290ab9d09 tags/0.14.2~73,"`dask.threaded.get` can be interrupted in python 2 (#2144)

* `dask.threaded.get` can be interrupted in python 2

Due to a bug in `Queue.get` in python 2, `get` is not interruptible if
no timeout is set (see https://bugs.python.org/issue1360). To get around
this, we specify an extremely long timeout. Benchmarking shows no
noticeable overhead to specifying a timeout, and this allows
`dask.threaded.get` to be interrupted.

* More robust interrupt handling

- Properly handle windows with python 3
- Add test for keyboard interrupt

* Fix test for windows

* Remove slow mark on interrupt test

* Re-skip the test on appveyor
",1490996034,
d004c13e775fca0d7bd71448531590bc99f726f9 tags/0.14.2~77,"Add debug docs (#2056)

* add debug docs

* respond to comments

* respond to comments
",1490915962,
47c68462b0b2148fdcaf6d5bcb992e376a0dd56f tags/0.14.2~80,"Check and compare shapes in assert_eq (#2101)

* Check and compare shapes in assert_eq

Seems that shape comparison only occurred on the result of computing the
Dask array. In our case, we found the Dask array itself had the wrong
shape, but the computed result had the right shape. To fix this we add a
comparison of the shapes before computation occurs. This happens in
addition to the existing test comparing the shapes afterwards.

* Compare Dask shape to computed result shape

As an extra check of the shape, make sure the shape hasn't changed
between the Dask and the computed result. Have seen this occur in some
cases as well while working on other code. So this should show some of
these issues and help avoid them cropping up in the future.

* check dask.array shape against computed shape
",1490880713,
7b25cee2de212fa867da1cba29d7911c46996137 tags/0.14.2~89,"Relax timing test (#2130)

This was an intermittent failure on travis.ci before.",1490741705,
ab9c88b04d6fcb148eedcea2954056a443078f47 tags/0.14.2~87^2,"Revert test following fix in fastparquet

See https://github.com/dask/fastparquet/pull/117
",1490732954,
26bca8aeb0056dc3e3346998742fc2c6c11c107c tags/0.14.2~91,"Add Multidimensional FFTs (#2116)

* Add FFTs for 2 and N dimensions

This expands on `fft_wrap` to allow support for wrapping functions of
the form `*fft2` and `*fftn`. To provide this support separate chunk
generating functions were added to handle both the 2-D and N-D cases
together.

In addition to providing this support, the NumPy functions for 2-D and
N-D cases that match the supported API were also wrapped. Though it
should be possible to wrap other FFT functions from external libraries
(as long as the support the NumPy API) should that be desired.

* Consolidate FFT chunk generating functions

To simplify 1-D and N-D FFT support, this combines the chunk generating
functions for both cases and makes some tweaks to the 1-D case to treat
it as a special case of N-D. Should make the code a little more compact
and hopefully a bit easier to test.

* Consolidate 1-D and N-D functions

Instead of having separate wrappers for 1-D and N-D functions, add one
wrapper function for both cases. To resolve occasional discrepancies
between the functions add a few branches to handle the different
dimension cases. Also add a small additional wrapper for the 1-D case to
map and format arguments from the 1-D case into something the N-D case
can handle.

* Test multidimensional FFTs

Adds various tests to exercise 2-D and N-D FFT functions. Also tests
wrapping of functions from NumPy and SciPy with 2-D and N-D variants.

* Disallow repeated axes to FFT functions

As it is sort of strange for someone to take an FFT of an FFT, simply
disallow this behavior by raising an exception. Also check that this
exception is raised.

* Tweak `test_fft2n` to include another shape case

* Add better names for the N-D FFT tests

* Remove excess dim, dtype variables

This is just one less thing to track.  Instead we inline the kind check.

* Assign _dtype and fix indentation
",1490722235,
6d99bbac4af279b7adbe00d622730a9bc9980d16 tags/0.14.2~93,"Parameterize and consolidate FFT tests (#2118)

* Unwrap FFT wrapper test function names

* Consolidate and parameterize FFT no arg tests

* Consolidate and parameterize FFT arg tests

* Drop unused imports in FFT tests

* Swap location of a kwarg test

* Use a fixture for FFT function names

* Revert ""Use a fixture for FFT function names""

This reverts commit bfad2f2c36ff1ba3332d6be7b3a52716ab73b5d1.

* Drop fixture and just use a list with parameterize
",1490379222,
53bba35838d343fb39e8b2343fe5f85cea97dce7 tags/0.14.1~4,"Abstract FFT interface (#2093)

* Restructure FFT wrapper for custom wrapping

Change `_fft_wrap` to delay function specification. The goal being one
can create custom wrappers for different external FFT implementations.
This both keeps Dask flexible in the area of FFTs without necessarily
adding many implementations.

* Expose FFT wrappers in the public API and use them

Create public API FFT wrapper functions and make use of them to wrap the
exist NumPy FFTPACK interface used by Dask. Also provide some brief
explanation of how this API works in the docstrings.

* Test explicit wrapping NumPy's FFTPACK

Even though we are already doing this wrapping internally in Dask and
implicitly testing. We should make sure explicitly that it works in case
the implementation in Dask changes in the future.

* Test wrapping SciPy's FFTPACK

As another exercise of the wrappers, test wrapping SciPy's FFTPACK to
make sure our wrappers apply just as well to it.

* Consolidate to one fft_wrapper function

Instead of having a variety `*fft_wrapper` functions, simply provide one
`fft_wrapper` function that takes a specification as to the kind of FFT
function that is being wrapped.

* Parameterize wrapper tests to consolidate them

To cutdown on the amount of boilerplate in the tests, use
`pytest.mark.parametrize` so as to cycle through many different
combinations. This should keep the code small while allowing us to try
various combinations of NumPy and SciPy FFT functions with the wrappers.

* Raise unknown kind provided to `fft_wrapper`

If an unknown kind is provided to `fft_wrapper`, raise a `ValueError`
explaining that this happened.

* Handle FFT wrapping kinds with a dictionary

To simplify passing parameters to the wrapper, create a dictionary that
has all of the parameters for each supported kind so that they can be
easily passed through.

* Document the wrapper_fft API better

Explain the function's expected API more clearly. Also note the
supported kinds of functions that can be wrapped and how they should be
specified.

* Consolidate remaining FFT wrapping tests

Combine the remaining NumPy tests into the single parameterized test for
FFT wrappers. Also as SciPy's FFTPACK implements `rfft` and `irfft` in
ways that are not compatible with NumPy's API, skip testing the wrapping
of the functions and note why they are skipped. Should keep testing
relatively straightforward.

* clean up fft_wrapper

* support more scipy.fft functions

* clean up map_blocks call

This removes the call to partial

* Remove unnecessary indiretion with _fft_wrap

* stop using kind

* update style

* make out_chunk_fns private

* Test a different `n` with the wrappers

Just to make sure we don't fall victim to some faulty assumptions, test
a different `n` to see if the wrappers still do the right thing compared
to the non-wrapped versions.

* Fill out wrapped FFTs unknown type based on array

Determines the unknown `dtype` based on using a dummy array of the same
`dtype` as that of the array provided. That way, we are sure it should
match the result's `dtype` accurately based on the type we are provided.

* Hack around nonlocal variable issue

Seems the closure doesn't bind the argument from the outer scope
function. So hack around this issue by storing it to a variable that is
local to the outer function and then copy it over to the inner function.
Despite being quite ugly, this does seem to work correctly.

* Add some FFT wrapper tests for single precision

* Parameterize FFT wrapper type testing

To make sure types are properly handled throughout, try converting the
data to single and double precision before proceeding.

* Use assert to compare types from FFT wrappers

Was using `assert_eq`, which is geared for arrays. So fixed this to do a
simple equal comparison in an `assert`.

* Test that FFT wrapper shape matches actual shape

Just to make sure the correct shape is computed for the Dask array, add
a test that verifies that the shape of the Dask after the FFT matches
that of the actual result.

* Fix handling of *rfft style functions

Pass around the expected result type of the functions to all of the
chunking operations. Use this information to determine the proper chunk
sizes particularly for functions like `rfft` and `irfft`, which differ
between NumPy and SciPy. However this comes with one caveat, `irfft`
will be dispatched according to whether the data is complex (NumPy) or
real (SciPy).

* Fix some linting issues

* Revert ""Fix handling of *rfft style functions""

This reverts commit 4257a2bb64b5d5e6215e3d3d0b6abcc690901302.

* Error on SciPy's `rfft` and `irfft` functions
",1490113428,
4018206e26115385c4678b8a4c10424e9a6cf884 tags/0.14.1~5,"Avoid shuffle when calling set_index on a sorted column (#2091)

* Avoid shuffle when calling set_index on a sorted column

When we're already going through the index column to get divisions we
now also check to see if the column is sorted.  If so then we avoid the
shuffle entirely.

* Fix test

And properly xfail another

* flake8
",1490054021,
85609920aa24fd03386ab5b1c939fdb9f67ccbda tags/0.14.1~8,"Enhance Reshape (#2089)

* Add reshape rechunk logic

* replace old reshape with new reshape

* fix inshape-outshape bug

* flake8

* add docstring to reshape

* from future import division

* extend reshape tests
",1490029912,
175ae5a91ac09940c1065ea007a6a93a95d39207 tags/0.14.1~13,"Support arithmetic with row-series (#2085)

* Support arithmetic with row-series

Sometimes a Series is just one row and is intended to be broadcast
across a Dataframe, rather than in an elementwise fashion.

This creates a convention, that a Series with divisions (0, 1) signals
that it is just a single row, and is thus appropriate for broadcasting.

This enables computations like the following (which used to err):

    df = df - df.mean()

However it also introduces silent failures if elementwise operation
against this series is intended, despite its divisions of (0, 1)

Fixes https://github.com/dask/dask/issues/1759

* Switch from (0, 1) to min/max of columns
",1489606845,
6927d73a8ddd914e162a4b165e033ba8018bb80e tags/0.14.1~16,"Help open-ended loc ranges (#2087)

Fixes https://github.com/dask/dask/issues/1962",1489580693,
0c39da3493891891830ab9e8eb5d5d8db203f826 tags/0.14.1~18,"Err when setting with a multi-dimensional array (#2076)

Fixes https://github.com/dask/dask/issues/2005",1489507532,
741e3d50cfc4df8e98c12b827b2ff2407c624448 tags/0.14.1~21,"Update toolz version requirement (>=0.7.2 to >=0.7.3) (#2062)

* Update toolz version requirement (>=0.7.2 to >=0.8.2)

With toolz 0.7.2, `import dask.bag` fails.

```
$ pip freeze | grep toolz
toolz==0.7.2
$ python -c 'from toolz import peek'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: cannot import name 'peek'
$ pip install toolz==0.8.2
...
$ pip freeze | grep toolz
toolz==0.8.2
$ python -c 'from toolz import peek'
```

* Update setup.py
",1489429957,
ee499df1f147f46360673fa9e0b10462ee7ebdea tags/0.14.1~29,"Update array-stack.rst (#2061)

For input:
```
>>> data = [from_array(np.ones((4, 4)), chunks=(2, 2))
... for i in range(3)]
```

I get the following error:

```
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
NameError: name 'from_array' is not defined
```
I have proposed a small fix for this error.

Thank you!",1489095212,
91d2c5cb18c565ff56299df19223090bbc7e83f7 tags/0.14.1~30,"Parquet: Remove exist_ok from makedirs (#2050)

* Remove exist_of from makedirs

Py2 doesn't have this option, so skip it.

* Enable py2 build with fastparquet

* Parquet py2 fixes

* Reorder install lines; fix flake

* re-flake

* Don't test fastparquet if pythonoptimize

The optimization tries to import the thrift file as an ordinary
python module

* wrong comparison

* Simplify unicode for py2
",1489081596,
49683151e756ce9d4a51e2fd83a18e055b7bd951 tags/0.14.1~32,"Set_index automatically repartitions (#2025)

* Set_index automatically repartitions

When we have to look through a dataset to find approximate quantiles
lets also look through and find memory use.  Based on that use lets repartition
our data into chunks of different size

* wip

* various fixes to repartition in set_index

1.  support non-numeric types
2.  don't increase npartitions
3.  docstrings

* cleanup some tests

* fix tests

* add sizeof from distributed to dask

* use dask.sizeof

* flake8

* cleanup shuffle tests for 32 bit systems

* add name to sizeof

* change default back to nout = nin

* set default to npartitions=None
",1488990974,
5f0cbdd617aabb4fc2972ee54bfe5b8a4d0893b7 tags/0.14.1~33,"Add issue template (#2049)

* Add issue template

* add request for version information
",1488837167,
bfa63da2c67ecd1ca69556c2a064ca481216bfbc tags/0.14.1~34,"Avoid naming collisions in dd.demo (#2031)

* Make_timeseries has different keys for different seeds

* more tests on demo names

* fix test to work with PYTHONOPTIMIZE=2
",1488812132,
e5781f7aaa4900aa0af9a7e4731824501b87c7d7 tags/0.14.1~36,"Raise informative errors on bad concatenation (#2035)

* Raise informative errors on bad concatenation

* Appease flake8
",1488658386,
490ec6370ddeee8e9d8ae5a6c4d6f1bbbcc7b23c tags/0.14.1~40,"Fix 32bit issues in dask.array reductions (#2023)

* Fix 32bit issues in dask.array reductions

* use np.argfoo to determine dtype

* copy=False
",1488378468,
2429ef5a38877e8e22d8fe0a6d58148fe23e15f9 tags/0.14.1~43,"#2017 fixed with a test. (#2018)

* #2017 fixed with a test.

* #2017 improved test.
",1488225920,
76d237ab98f559464ce548a5eef4c9834beb78ed tags/0.14.1~49,"#1964. Fuse tasks that form reductions (aka diamond fuse) (#1979)

* WIP #1964.  Probably broken.  Fuse tasks that form reductions.

This trades parallelism opportunities for faster scheduling by making
tasks less granular.

There are many options (and opinions) for what the parameterization
of this function should be.  For the sake of experimentation, I have
included many parameters as described below.  My gut feeling right
now is that we only need one parameter, `ave_width`, and to choose
a reasonable value of `max_depth_new_edges` based on `ave_width`
(this controls against pathologies while allowing desirable behavior).

This optimization operation is more general than ""single input, single
output"".  It applies to all reductions, so it may be ""multiple input,
single output"".  I have attempted to make this well-behaved and
robust against pathologies.  Notably, by allowing multiple inputs
(what I refer to as `edges` in the code), there may be parallelism
opportunities w.r.t. the edges that are otherwise not captured by
analyzing the fusible reduction tasks alone.  I added a cheap-to-
compute heuristic that is conservative; i.e., it will never
underestimate the degree of edge parallelism.  This heuristic
increases the value compared against `min_width`--a measure of
parallizability--which is one of the reasons I prefer `min_width`.

I think it will be easy for this operation to supercede `fuse`.
I'm ignoring task renaming for now.

Parameters
----------
dsk: dict
    dask graph
keys: list or set
    Keys that must remain in the dask graph
ave_width: float
    Limit for `width = num_nodes / height`, a good measure of
    parallelizability.
max_depth_new_edges: int
    Don't fuse if new dependencies are added after this many levels
max_height: int
    Don't fuse more than this many levels
max_width: int
    Don't fuse if total width is greater than this

* Begin adding tests (and a fix).

* Another test, another fix.

I'm beginning to get confused.  I think I should make the traversal strategy clearer.

* Another test, another fix.

I'm still a little confused and should rework things to be more explicit.
Sorry for the tangle of logic with implicit behaviors.

* A little more clear.

* Add a few more tests.  Still not ready for prime time.

* Small tweaks.

* Remove the use of `irreducible` and change `deps` to sets.

We now allow for fusing reducible tasks below tasks that weren't fused, but
only in strict circumstances (such as allowing for linear fusing).

* Remove unnecessary indent, as `num_processing` may only contain positive numbers

* For clarity.  I think.  Goodnight!

* Remove `num_processing` stack, as it no longer provides a benefit.

Time for more tests!

* More reorganization and cleanup.

* Cleaner control flow.  Things are looking better.

* Handle linear fusing so this can replace `fuse` optimization.

This actually performs a superset of linear fusing that `fuse` performs.
Specifically, `fuse_reductions(dsk, ave_width=1)` will fuse linear tasks
that may refer to the inputs used in the top-level task.  In `fuse`, we
only fuse chains with single-input, single-output.

* No more `continue`s or implied fall-through behavior.  More explicit and clear?

I think the clarity and cleanliness is worth the cost of sometimes performing
`item = list.pop()` followed by `list.append(item)`, which is what we avoided
doing previously.

* Accept and return `dependencies` like `fuse` does.  Barely tested.

* Oups

* Optionally get parameters from global context.

* Fix updating `dependencies`, and make it easy to run `fuse` tests.

* Test heuristic, and include dependencies in tests.

Also, test `fuse_reductions` in `fuse` tests.

* Improve the heuristic and track the fudge factor separately.

This is still cheap.  Notably, we don't need to look at and track the
dependencies or dependents of the non-reducible nodes.

* Update docstring and clean up tests.

* Silly micro-optimizations (but, hey, these add up!).  I couldn't help myself.

* Create fast branches when fusing a single child (i.e., linear chains). This is worth it.

* More optimizations.  More aggressive fast path for linear chains.

This did become a little uglier and has a little bit of duplicated code, but
the performance is pretty good!

* The solution to task renaming was to think.  Go brain!

I changed the parameter name from `rename_fused_keys` to `rename_keys`, and
for consistency the global parameter is now `fuse_rename_keys`.  Should we
also change the parameter name of `rename_fused_keys` to `fuse_rename_keys`
in the `optimize` function for the dask collections?

* Experiment with a cheaper task renamer:

a sorted set of keys returned by key_split except for the root node, which is
the last item in the joined keys.  This should be cheap enough to compute,
nice enough to look at, and well-behaved enough to test.  Of course, this
also breaks existing tests that check renamed keys, which will need fixed
if this scheme is approved.

* Experiment with fancy fusing!

* A little better, and basic docstrings.

* Update tests to pass with new renaming method.

* Rename `fuse` to `fuse_linear`, and `fuse_reductions` to `fuse`

* Cleanup.  Remove fancy fuse.
",1487962295,
5e2677ee77b6b3affb1e63d89e491446365596a2 tags/0.14.0~1,"delayed(dask_object) -> Delayed (#2006)

This used to work, but a regression was introduced in #1985 where the
graph was mistakenly assumed to be a `sharedict.ShareDict`. This PR
fixes this, along with some general cleanups:

- Rename `to_task_dasks` to `to_task_dask`
- Update docstring to be consistent with new behavior introduced in
  #1985
- Fix bug when calling `delayed` on a Base object, and add test for it
- Unskip test for `to_task_dask`, and update to pass",1487798505,
3c3fe2a418da1a1528c77a8f1579f47bf7a4bd85 tags/0.14.0~2,"Fix #2002: fix test on ASCII locale (#2004)

",1487782463,
b7d8aa768dad885a9e56fd6d79ab8ca100ffee7c tags/0.14.0~5,"Fix outdated link in resample error message (#1994)

* Fix outdated link in resample error message

The file was moved in 33725e8.

* Fix another link
",1487593808,
ecf9727f520883c1057c1057fd038b0ac879e7fb tags/0.14.0~8,"fix link in examples docpage
",1487333019,
2502c3ef7d38c8ec0af7f19fb0fd2913ada408bf tags/0.14.0~10,"Fix groupby multilevel issues (#1914)

* Fix groupby multilevel issues

* remove pandas ver check

* remove unused args
",1487092252,
a07bb0be539621ec04ee789667fc5d6c8718123a tags/0.14.0~12,"Allow different category labels in parquet partitions (#1930)

* Allow different category labels in parquet partitions

Define the labels in the metadata using UNDEFINED_CATEGORIES.

* Use assert_eq

* Fix categories=None in read_parquet

* Do not accidentally add columns to meta

Specifying a column to categories= would put it in meta; no more. Now
it is simply ignored (no exception), as in fastparquet.

* Use column validation for categories= at load

* Jim's fixes
",1486999866,
a5b308f7e74c43d971dd7a56a531e39dcd840ebf tags/0.14.0~17,"delayed avoids repeat calls to normalize_function (#1970)

We previously called ``normalize_function`` on every call to a delayed
function. This is expensive, and can be even more so if the function
requires cloudpickle. This fixes that to use a cached (but equivalent)
argument to represent the function.",1486652071,
01bc173456fa8e827d0636fdad758e9bdc0888fc tags/0.14.0~18,"`dask.bag.from_delayed` may create iterators.  Reify to lists if necessary (#1966)

* `dask.bag.from_delayed` may create iterators.  Reify to lists if necessary.

If the `delayed` functions used to create bags return iterators, then we
should convert them to lists iff they are directly used by multiple tasks.
If we don't do this, then the first task will exhaust the iterator, thereby
leaving nothing for the second task to operate on.

`reify` and `list` will be optimized out by the bag optimizations as
appropriate.

* Oups.  That test wasn't needed anyway.

Still, it's interesting that `pluck` in `toolz` and `cytoolz` behave
differently in this exceptional case.  I should fix that.
",1486420592,
df83847de0f903b1892dedf911c30a3315f8270e tags/0.14.0~21,"Propagate errors= keyword through dask.bytes (#1954)

Previously we didn't pass the errors= keyword cleanly through dask.bytes
This affects db.read_text",1485989748,
4b0b2405ba49054c12765b66e955ce858290a800 tags/0.14.0~24,"Fix bug in da.map_blocks (#1952)

Previously we were accidently calling `reversed` instead of `sorted(...,
reverse=True)`, which would cause failures in `map_blocks` if chunks
were provided explicitly. Fixes #1947.",1485903199,
64a480a96fc0aa4af64228cd1db2fb621f6a9d68 tags/0.14.0~25,"Improve error message for dtype inference error (#1951)

- Don't raise in the `except` block, as this makes a less clear
  traceback
- Suggest to use the `dtype` kwarg to address the error.",1485903175,
c7cef9875276f20be435b6afe7e8fef7fc32697b tags/0.14.0~26,"Allow `dask.bag` reductions to work on very sparse data. (#1950)

* Allow `dask.bag` reductions to work on very sparse data.

Now, the `aggregate` function that merges results can return `no_result` if
all the inputs are also `no_results`.  However, the very last call to
the `aggregate` function will not return `no_results`, so it will still
error as appropriate.

Fix `merge_frequencies` and test `frequencies` with empty partitions.

Fix and test sparse `dask.bag.distinct` too.",1485894772,
d34080c45faa66ca4bdc8c5b235da716ab780676 tags/0.14.0~27,"Raise error if groupby key is unaligned with df (#1941)

Pandas allows grouping by a key that isn't aligned with the grouped
DataFrame/Series. If this happens, then groupby first aligns the indices
before performing the grouping. Unfortunately this operation isn't
threadsafe, and can lead to incorrect results. Since grouping by an
unaligned key isn't recommended, we just error loudly in these cases.

Fixes #1876.",1485891447,
30c199343fafb661840b5b2fef08a094d888fdb4 tags/0.14.0~29,"Issue #1883 - Unicode errors on string columns with NaNs  (#1945)

* Test and fix for Issue #1883 Unicode errors on string columns with NaNs",1485822024,
74784d5a63941e4f93af73e80a14bd80fae54d18 tags/0.14.0~30^2~2,"Added more tests, adjusted the code and improved error messaging #1940, #1936 .
",1485543776,
274706a8bed93b652a3496bca2bcd434fa87a479 tags/0.14.0~33,"Added Presentations section to docs for issue #987 (#1937)

* Added Presentations section to docs for issue #987

* Added PLOTCON Presentation
",1485531360,
7382d5e49823a79f911b42fdc9296a61f971a3a8 tags/0.14.0~37,"Add known/unknown status to repr (#1932)

Change the repr for dask dataframe so that categorical dtypes repr
including their known/unknown status. This makes `category` repr as
`category[known]`/`category[unknown]` depending on known/unknown status.

Also fix a bug in the repr methods where the repr data was cached,
leading to the same repr even if metadata/divisions changed.",1485318093,
42986b0b31d2e972b027fc2b730178365adee05a tags/0.14.0~41,"Support non-uniform categoricals (#1877)

Add support for non-uniform categoricals.

This adds support for different partitions having different categorical
mappings. This is accomplished through `union_categoricals`, a method added in
pandas 0.19.0. As such, support for pandas < 0.19.0 has been dropped.

### Changes to metadata:

Categorical columns/indices now have two states:

- known categoricals have the `categories` known statically (on the `_meta`
  attribute)
- unknown categoricals don't know the categories statically, and are indicated
  by the presence of `dd.utils.UNKNOWN_CATEGORIES` in the categories on the
  meta attribute. This is nice, because operations on the categorical in most
  cases propagate the presence of this value, so the known/unknown state
  carries through inference.

### Changes to the categorical accessor

To query and convert between these states, following has been added to the
categorical accessor:

- `.cat.known` is a property that indicates if the categories are known
- `.cat.as_known()` converts unknown to known (and is a no-op if already
  known). Note that this requires computation.
- `.cat.as_unknown()` converts known to unknown. This requires no computation -
  it's just a change in metadata.

If the categories are known, then `.cat.categories` returns statically (no
computation needed). If they are unknown, then an error is raised (rather than
computing them)

### Changes to concat

Concatenation of *dask* dataframes with different categoricals is now
supported. For each column/index in a dataframe, the output *dask* column/index
will have known categories only if all categories are known. Otherwise the
categories will be unknown.

After a call to concat, each partition is guaranteed to match the metadata on
the containing dask dataframe. This fixes a bug from before where in certain
cases partitions could have different columns/datatypes.

### Changes to `pivot_table` and `get_dummies`

These functions only work if the categories are known. If unknown, an error is
raised

### Changes to `read_csv`

For pandas > 0.19.2, dask now supports reading from csvs directly into
categorical columns. These will be unknown categories

### Changes to `merge`

No changes are made to merge. Pandas merge doesn't support merging
categoricals, *even if the categoricals are identical*. As such, dask doesn't
error, but the categorical datatype is dropped. When this is fixed in pandas,
support for merge on differing categoricals can be added.

### Changes to `hash_pandas_object`

Previously, this would hash categories based on codes only. We now hash based
on the values the categoricals represent. This was patched in pandas, and will
be in the next release. The fix is backported to dask for versions 0.19.0 to
0.19.2. The faster (and more stable) object hash will be used for pandas
0.19.2.

### Changes to `categorize`

Categorize now takes an optional ``index`` parameter, indicating whether to
categorize the index or not. Additionaly, by default it will convert all
unknown categoricals to known categoricals.
",1485199004,
a8db711e0d1a37dc1346bddb7b6ca51ba3a84f98 tags/0.14.0~42,"ENH: Add DataFrame tabular repr (#1637)

* ENH: Add DataFrame tabular repr

* Update format

* Fixed format

* fix lint

* add dask name and task nums

* use key_split in dataframe repr
",1485096704,
c2c7b541c16084d91ec696dca63ca054a8d029fd tags/0.14.0~43,"Merge dask.array.concatenate_axes logic with dask.array.concatenate3 (#1923)

* Merge dask.array.concatenate_axes logic with dask.array.concatenate3 (#1923)

* Fix docstring and use of xrange

* Fix flake8 errors
",1485089631,
2d949f2f6d9307c31fa439a0efbe1abc24b9e4b8 tags/0.14.0~44,"groupby().cumsum()/cumprod() implementation (#1909)

* groupby().cumsum()/cumprod() implementation

* groupby().cum{sum,prod} do not support skipna, but axis as pandas

* added support for groupby().cumcount()

* fixed na handling in groupby().cum...

* cleaned up groupby().cum... code
",1484963563,
259b51f50a26b4839ea117f1d2c48dab1170dcb7 tags/0.14.0~46,"Rename tasks when fusing (#1919)

* WIP: Rename tasks when fusing. #1911

* Add `rename_fused_keys=` keyword to `fuse` and copy `key_split` from `distributed.utils`.

Other than the impressive number of test failures, I think this is virtually done.

* Keep the `{old_key: new_key}` aliases in the fused dict.  It makes tests much happier.

* Fix tests and add tests for `rename_fused_keys=True`

Every test in `test_fuse` and `test_fuse_keys` is repeated for
`rename_fused_keys` True and False.  I didn't add any new, aggressive
tests for True.

* Install conda=4.2 on travis-ci (for now)

* Make flake8 happy.

* Fix doctests for `key_split`?

* Oh yeah, update docstring examples for `fused`.

* Improve perfomance: don't call `inline` in fuse.  Also, jimc formatting change.

The alias substitutions in `fuse` are simple replacements, which we can do
much more efficiently without calling `inline`.

* Oups.  Still has `@profile`.

* Only keep aliases given in `keys=` (if specified)

* optionally use lru_cache for key_split

* Use type is rather than isinstance in default_fused_keys_renamer

* Revert ""optionally use lru_cache for key_split""

This reverts commit 22dd5d2431971ee0416259f5f7f6c46f13b98e62.

* Undo changes to .travis.yml (`conda update conda`).

conda error has been fixed by latest conda.
",1484950865,
c68f277ba46280b0018aef149616641cffcad10e tags/0.14.0~50,"`meta_nonempty` works with empty categories (#1907)

Previously this expected all categorical series/indices to have at least
one category. This was causing issues, in particular with parquet
support. Fixes #1905.",1484240669,
b0567c4ffd01f1ea0ae7c6136e11c0dcf0051cc2 tags/0.14.0~51,"Fix edge behavior for floating step arange (#1904)

Previously this would fail due to inconsistent boundary handling in
numpy. Fixes #1903.",1484097300,
b03486c3bee75d4292f501249163a516e3b64b2d tags/0.14.0~56,"Update scheduler-overview.rst (#1893)

* Update scheduler-overview.rst

A user emailed the documentation team that the example caused errors and was fixed by this change. Please review.

* Update scheduler-overview.rst
",1483756429,
51fa9af4acf114895a69c380df90086d97616212 tags/0.14.0~60,"Remove toolz dependency in async.py (#1882)

* Remove toolz dependency in async.py

- provide an 'identity' function right inside dask/async.py so that toolz
  isn't a hard requirement for a ""default"" pip installation

- closes #1849

* PEP8 and doctest fixes in dask/async.py
",1483497982,
91d102ef098b7723bc72820153db7d3ef2ea6d17 tags/0.13.0~2,"minor doc fixes
",1483286632,
76fffb27a977826d70e8c056e3724fd7cc5737b5 tags/0.13.0~7,"Create new partd for every bag groupby (#1867)

Fixes https://github.com/dask/dask/issues/1851",1482789576,
956f9f56b8cb2ecb08077eb0b57b6e153c359d32 tags/0.13.0~9,"Add parquet roundtrip test (#1863)

* add parquet roundtrip test

* add more test cases to roundtrip

* fix some tests

* tweak tests

* flake8

* use categories= in parquet roundtrip test
",1482617937,
80839f7b28a6509e9fa836b78358fd5c65559866 tags/0.13.0~20,"Parquet fixes (#1843)

* Test simple parquet scheme

* Fix test and flake

* Small doc and parameter changes

* Fix for file opener changes in fastparquet

* Fixes for changes in fastparquet

* Flake update

* Test in s3 needed updating

* Update for changes in fastparquet

Need to preallocate dataframe before calling real row-group

* follow rename of _read_parquet_row_group

* make importerror more apparent

* flake8
",1481901325,
4ae1a203860bd67f09057778c0da5af533b9fcc6 tags/0.13.0~21,"Allow non-informative chunk sizes in dask.array (#1838)

* Support nan chunk sizes

* support no-chunk svd

* add to_records method to dataframe

* add Frame.values

* raise informative errors when chunks are unknown

* remove visualize debug call

* add warnings to dataframe-to_array methods

* remove _flatten_seq
",1481419143,
21e9c5ca20f9a07ee782d8ed4ff952b67ee53a78 tags/0.13.0~22,"Micro-optimize computation of the rechunk graph (#1827)

* Fix #417: iterative (progressive) rechunking

* Fix failing test

* New heuristic

* Make the cost estimate computation much less expensive, and add tests

* A docstring to plan_rechunk()

* Implementation partial optimization in case we hit the memory limit

* wip

* Add pretty-printing function for rechunk plans

* Fix tests and remove prints

* Use threshold to compute when to trigger the splitting pass

* Split and merge both at the same time, as suggested by @shoyer

* Micro-optimize computation of rechunk graph

* Remove multislice optimization

* More tweaks
",1481402496,
c23852d859196bba57bb7d5a49c84cdc2ba252d7 tags/0.13.0~23,"Fix #417: iterative (progressive) rechunking (#1737)

* Fix #417: iterative (progressive) rechunking

* Fix failing test

* New heuristic

* Make the cost estimate computation much less expensive, and add tests

* A docstring to plan_rechunk()

* Implementation partial optimization in case we hit the memory limit

* wip

* Add pretty-printing function for rechunk plans

* Fix tests and remove prints

* Use threshold to compute when to trigger the splitting pass

* Split and merge both at the same time, as suggested by @shoyer

* Address review comments

* Add sortedcollections to travis.yml
",1481211254,
58332a6e8a4fe744aaa059188b3bae61a1560872 tags/0.13.0~24,"`dd.get_dummies` raises on object dtype (#1829)

Previously `dd.get_dummies` would pass through object dtype unchanged,
differing from the pandas implementation. This was confusing for users.
Now we error when behavior would differ between pandas and dask.

Fixes #1809.",1481128899,
205f91b352fbe156bf09a6c79135a1c661b57237 tags/0.13.0~25,"Cleanup `funcname` (#1831)

- Supports `partial`, `curry`, `methodcaller`, `multipledispatch` (fixes
  #1820), and generic callables.

- Don't trim the `<>` from the ends of `str(func)` in the fallback, it
  looks odd (ends up like `'function foo at 0x10d137d90'`). This is also
  what `distributed` does.

- Remove unused (and broken) `full` option. Not sure what this for.

- Add tests for this function.",1481128853,
abc5d9b27b4f08f7940d59f7d3a03d569631d24b tags/0.13.0~29,"ProgressBar exits on error with get_sync (#1826)

Due to some convoluted logic in when and how the `finish` callbacks were
run in `get_async`, the `finish` method wouldn't be called if
`raise_on_exception` was `True`. Since `get_sync` sets this as `True`,
the `ProgressBar` would never exit if exceptions occurred.

This simplifies the logic, and ensures that `finish` is always called on
exceptions.",1480628720,
e73546a10b89df00db65f42fa8c4be85f1f6bf50 tags/0.13.0~30,"Fix `drop_duplicates` with `split_out` (#1828)

The `split_out` kwarg was added to `aca` in 303d038. This allows for
outputs to be split by hashing the rows, improving efficiency for large
outputs. However, this defaulted to hashing all columns in the output
dataframe, which didn't play well with the `subset` kwarg to
`drop_duplicates`. This was resulting in not all duplicate rows being
dropped, as they'd hash to different partitions since the hashed columns
weren't being subset.

To fix this, we replace the `split_index` kwarg with `split_out_setup`,
which optionally takes a function to apply to each chunk before being
hashed. `split_out_setup_kwargs` is also available to pass keywords to
this function. `drop_duplicates` was modified to adjust for this change.",1480627569,
dd6b00707b5b6e08ae6660d7d14323ee8e55c166 tags/0.13.0~31,"Bug fix: Use a dtype that can support negation. (#1824)

* Bug fix: Use a dtype that can support negation.

* add test for nunique_approx
",1480626772,
f062e9a287740fe2e0fb07798255d812d7197724 tags/0.13.0~32,"Don't force sort on groupby levels (#1825)

https://github.com/pandas-dev/pandas/issues/14776",1480598371,
b8beea01a3d9e6c1ed4783ac8c0ba0fabfff1ceb tags/0.13.0~34,"Two fixes for split_out (#1823)

1.  Support new hashing code in pandas 0.19.2
2.  Support MultiIndex objects",1480531809,
303d038056ef715a2ad0d7a754b6ce8a477e0846 tags/0.13.0~35,"Hash split (#1808)

* Initial implementation of hyperloglog cardinality estimation for dataframe.

Addresses #1288.

* Refactor hashing into one place, update/readd some tests.

Also, minor doc tweak.

* Resolve partitioning_index testing issue

We were depending on partitioning_index to pass through values like 0..n
Now we encode this explicitly.

* flake8

* fix another test

* fix additional test

* test nunique_approx series

* Enable apply_concat_apply to generate output partitions

This allows apply_concat_apply to split the dataset with a hash during
aggregation, allowing for reducitons with multiple output partitions.
This can help to reduce tail communication for reductions that may have
very large outputs, such as drop_duplicates.

* support split_out in groupby aggregations

* add split_out to value_counts and drop_duplicates

* respond to comments

* flake8
",1480526037,
42e45287e08a0548c67dfefc7ec7e7f065232051 tags/0.13.0~36,"Cardinality estimation using HyperLogLog (#1358)

* Initial implementation of hyperloglog cardinality estimation for dataframe.

Addresses #1288.

* Refactor hashing into one place, update/readd some tests.

Also, minor doc tweak.

* Resolve partitioning_index testing issue

We were depending on partitioning_index to pass through values like 0..n
Now we encode this explicitly.

* flake8

* fix another test

* fix additional test

* test nunique_approx series

* add more tests to nunique approx

* Bugfix

* flake8

* extend hashing tests

* use pandas hash if available

* style edits

* unify file names

* Enable tree aggregation.

* Enable tree aggregation.

* add split_every tests

* More standard coding cookie.

* Remove stale comment and move complex test in with the other, normal tests. (Was previously separate because it was an xfail.)

* Improve docstring.

* remove b parameter from nunique_approx

* Update docstring for new API.
",1480514356,
695850cc357019e728b9095d3ddfbb9cc55a12d2 tags/0.13.0~42,"Add df.first and df.last

Also fix autocorr test.
",1479747487,
b0e119c280dd934e835cda5f14515f260156e8be tags/0.13.0~43,"Cleanup drop implementation

- Remove non-standard `dtype` kwarg. This was used inside `groupby` and
  `shuffle` functions to ensure the columns index dtype was correct.
  This has been replaced with an explicit setting use of map_partitions
  and a separate function.
- Add `errors` keyword
- Use `map_partitions` instead of `elemwise`
",1479747487,
a6d2160ff952b16e2184f89fefb20be5a725575f tags/0.13.0~51,"Add str and repr methods to methodcaller (#1788)

This helps debugging",1479342856,
e37db071de1fc9eb40df94898140cd200c05cd03 tags/0.13.0~52,"Create default pool on demand (#1781)

Fixes https://github.com/dask/dask/issues/1780",1479313523,
ecf55575c565a62da03ec4f255dc613427b867d1 tags/0.13.0~56,"Fix colormap lookup in diagnostics plots (#1775)

Some newer bokeh palettes contain non-contiguous integer variations
(e.g. Blues3, Blues4, ...). This was causing problems in our plotting
code when choosing a good colormap.

This fix changes to the following heuristic:
- If there is an integer variation that matches nfuncs, use that
- If nfuncs > the max integer variation, then cycle through
- Otherwise, find the next largest variation, shuffle that palette to
  prevent using just the low range, and then use that. The shuffle has a
  fixed seed to produce consistent plots.

Fixes #1772.",1479138177,
afbc2711c7ef3ea66319693b667fbae93000be7a tags/0.13.0~56^2,"Fix colormap lookup in diagnostics plots

Some newer bokeh palettes contain non-contiguous integer variations
(e.g. Blues3, Blues4, ...). This was causing problems in our plotting
code when choosing a good colormap.

This fix changes to the following heuristic:
- If there is an integer variation that matches nfuncs, use that
- If nfuncs > the max integer variation, then cycle through
- Otherwise, find the next largest variation, shuffle that palette to
  prevent using just the low range, and then use that. The shuffle has a
  fixed seed to produce consistent plots.

Fixes #1772.
",1478882513,
40213cc62c8d0f87f320d999f2a58a87c08513f7 tags/0.13.0~62,"Feature/groupby reductions multiple levels (#1692)

* Fix var, std, and nunqiue in groupby with multiple levels

* Rename multilevel aggregation test, decrease test data size

* Handle multiple series in groupby

* Allow to use multiple derived columns in groupby

* Fix style problems

* Build meta correctly for list of series in groupby

* Add tests to show limitations, Remove restriction in SeriesGroupBy init

* Fix segfault, add todos of open topics

* Skip name-check for series-groupby-mean

* Remove duplicate _normalize_index introduced in rebase

* Remove xfail for mean/var dtype recast

* Remove xfail for multidimensional series aggregate: now supported

* Fix unique for groupbys on lists

* Unify nunique impl for series and dataframe

* Skip name check for nunique also

* Simplify checks and meta creation, add implementation notes

* Inline helper functions
",1478539010,
1b188294f637cdd55911bf0c7c866e5ccfed5eca tags/0.12.0~4,"Faster array slicing (#1731)

* Faster array slicing

* Take a bit more shortcuts

* Appease flake8

* Remove unwanted change in docstring

* Optimize inline_functions()

* Faster iterative _deps()

* Optimize cull() and friends

* Appease flake8 again

* Add tests for new functions

* Appease flake8 again

* Use deterministic initialization in test

* Standardize around get_dependencies()

* Install up-to-date distributed in .travis.yml

* Try to fix dependencies on Travis

* Avoid pulling dependencies when upgrading distributed and zict from git
",1478182632,
b68aee9d042c7f3ca0a6dc582a6f0fe468b60421 tags/0.12.0~6,"Fix slicing error with None and ints (#1743)

This adds a comprehensive test around slicing with ellipses, Nones, and ints.
This uncovers two bugs

*  when slicing with both a new axis and destroying an axis with an int.
*  slicing with -n

I originally did this with parametrized tests but have commented this out to
keep down the visual noise when running tests normally.",1478090805,
b6d23117e7991230a5561ad7439757d0db733fec tags/0.12.0~14^2,"Correct shape when indexing with Ellipsis and None

Fixes https://github.com/dask/dask/issues/1730
",1477918770,
bbd414b05e357afb11d7ce9862067cff2cdf546d tags/0.12.0~20,"A few small tweaks to da.Array.astype (#1721)

- Support `copy` and `casting` arguments
- Error at graph build time when casting doesn't work
- Fix a bug when dtype is unknown and casting to `f8`. Previously this
  would result in a no-op, as `np.dtype('f8') == None`, now `_astype` is
  applied as it should be.",1477865145,
b929a084475bc8aa86ab5d3ff0b0737ff40f7cd1 tags/0.12.0~21,"BUG: Fixed metadata lookup failure in Accessor (#1706)

Closes https://github.com/dask/dask/issues/1705

Specifically for CategoricalAccessor with all missing values
in the first division. This solution feels a bit wrong. I think
the better solution would be to fix _meta_nonempty, but maybe
this is OK?",1477865080,
ae717129b6d06d62c9edc9fa0679007d878ab971 tags/0.12.0~24,"Add rename support for multi-level columns (#1712)

* Add rename support for multi-level columns

Fixes #1694.

Also found a bug in the method versions of operators when applied to
Series with axis=1. `map_partitions` was automatically repartitioning
this, resulting in an incorrect answer. However, the old way of renaming
columns was actually creating new columns filled with `NaN`. Since the
data it was being with tried had no matches between columns and index, the
output was also all `NaN`, masking the issue. This changes the test so
it would have caught the issue, and fixes the implementation of the
operator.

Also fixed a few bugs in `_rename` and `map_partitions`:

- `_rename` now returns `Index` when passed an `Index`. Previously this
  would return a `Series`.

- `map_partitions` now returns partitions with the same columns in all
  occasions. Previously partitions with non-matching columns could sneak
  through, which would cause issues later.
",1477669952,
64814107694ada104381b066df68155ee5696a8f tags/0.12.0~29,"WIP: Feature/dataframe aggregate (implements #1619) (#1678)

* Add partial aggregate support (missing: std, var, nunique)

* Add aggregate docs

* Handle agg for numpy funcs and for non dictionary specs

* Use pytest parametrize fixtures

* Remove use of partial and namedtuple

* Reuse intermediates between results in aggregates.

* Implement aggregate for std / var

* Expose the split_every argument of apply_concat_apply in aggregate

* Update docs of _build_arg_s

* Adapt test: remove duplicate test cases, add different group specs

* Add support for using series as groupers in aggregate

* Adapt tests: decrease work performed, mark known failures

* Test properties of the aggregate dask

* Pass combine function explicitly to document the dask construction

* Add the function/column to intermediate names

* Recursively normalize the index in groupby

Example:

    # previously supported:
    df['a'] -> 'a'

    # additionally supported
    [df['a'], df['b]] -> ['a', 'b']

* Fix style (flake8)

* Remove non ascii dashes

* Address PR comments:

- inline _aggregate_meta
- add arg docs, examples for _normalize_spec
- add arg docs for _groupy_apply_funcs
- inline token arguments in aggregate

* Brush up comment strings

* Rebase to master, remove duplicated pytest import

* Fix doctests: ignore whitespace, use OrderedDicts

* Implement series-groupby, use tokenize for intermediate ids
",1477485865,
f69c4e9a3c46ec8d08e69683b99e1f837ae4c967 tags/0.12.0~33,"Prepend optimization keywords with name of optimization (#1690)

Fixes https://github.com/dask/dask/issues/1688

This passes particular keywords to particular optimizations by creating
a convention that we prepend the name of the optimization to the
particular keyword argument.",1477309717,
b663bf67615b17837d2411c6c46c6f7baa9994e7 tags/0.12.0~36,"da.random with state is consistent across sizes (#1687)

* da.random with state is consistent across sizes

Previously seeds were culled down in `different_seeds`, and then
returned in sorted order. This causes problems when changing the size of
the output array, as the seeds might be in different order, leading to:

```python
x1 = da.random.RandomState(123).random(20, chunks=20)
x2 = da.random.RandomState(123).random(200, chunks=20)[:20]
assert (x1 == x2).all()
```

to fail. This PR fixes that.

Instead of passing arounds seeds (which have a small independent space),
we pass also change to pass around the internal states of
``numpy.random.RandomState`` or ``random.Random`` objects.",1477072226,
44d65707934188053454aaa8f45cff285b79b562 tags/0.12.0~46,"ENH: add dd.pivot_table (#1665)

* ENH: add dd.pivot_table

* Remove dynamically generated funcs

* Update error message
",1476705889,
f2df4fbd5e7321f7d97a04ab451e4adb8616f005 tags/0.12.0~49,"Dataframe elemwise (#1660)

* add applymap (#1259)

* add DataFrame.round (#1259)

* add series.round (#1259)

* add dataframe.to_timestamp (#1259)

* add dataframe and series elementwise comparisons. (#1259) Series tests not yet passing due to unexpected fill_values error.

* removed fill_value from Series (in pandas 19.0 but not 18.0). Passes tests now

* meta parameter for applymap

* update to_timestamp tests

* removed copy kwarg in to_timestamp

* removed unused StringIO import

* flake8 is picky

* Moved comparison tests to test_arithmetics_reduction

* updated timestamp tests. divisions are properly cast to TimeStamp

* remove old dt accessors

* apply to_timestamp to divisions

* add Series.to_timestamp
",1476467127,
42719215a8c06b311181a6d28c24f772f7f99544 tags/0.12.0~52,"Fix #1656: failures when parallel testing (#1657)

* Fix #1656: failures when parallel testing

Parallel testing works using the `pytest-xdist` plugin with
a hard-coded PYTHONHASHSEED, e.g.:

$ PYTHONHASHSEED=1 py.test -n=auto dask

* Appease flake8; also, use a hardcoded number of workers
",1476457170,
9b0aec9d974383dde3031b01ef850dc6377f22ec tags/0.12.0~53,"Remove use of multiprocessing.Manager (#1652) (#1653)

* Remove use of multiprocessing.Manager (#1652)

Using a Manager process makes multiprocessing.get fragile on Windows
as concurrent unpickling of Numpy objects may result in sporadic errors
(the numpy module being seen uninitialized by one of the manager threads).

Instead of queueing explicitly in execute_task(), simply return the
result and queue it in the parent.

* Appease flake8

* Address review comments @jcrist

* Remove `queue` parameter to get_async()

* Simplify handling of unserialization error in control process
",1476454408,
0b84cb1853aa516da27aad52ab3c0a1340c800d0 tags/0.12.0~54,"A few fixes for `map_blocks` (#1654)

* map_blocks block_id works with other kwargs

Fixes #1481.

* map_blocks with drop_axis/new_axis works

Fix a few bugs in drop_axis/new_axis. Fixes #1297.
",1476380960,
24c652e71e95b029e7b32cd4fc1448b021f8ebe8 tags/0.12.0~55,"Automatically expand chunking in atop (#1644)

* Automatically expand chunking in atop

Fixes #1641

* warn if auto-rechunking increases chunks by 10x

* fix doctests

* explain common_blockdim
",1476376941,
c6cb638294a27f318c3f0d988a936c5a0bf3fd77 tags/0.12.0~60,"Use strides, shape, and offset in memmap tokenize (#1646)

Previously `tokenize` on `np.memmap` objects would return the same token
for slices of the same array. To fix this, we now include all the
metadata needed to fully define the array in the token. This includes
the array strides, shape, and the pointer offset.

Fixes #1645.",1476185923,
7460556c6e5e60fc0a5b82c4b442e2f8cc00fe94 tags/0.12.0~61,"Validate scalar metadata is scalar (#1642)

Previously we only validated metadata is of the correct type for
DataFrame/Series/Index. This allowed a bug in `quantile`, where the
output would be a `Scalar`, but the metadata would be a `Series`. This
fixes that bug, and adds validation in the `Scalar` constructor.

Fixes #1640.",1476112341,
8136bb34b145ff515118979b0529dc993138923f tags/0.12.0~65,"Error nicely when indexing Array with Array (#1629)

Raise a nice error instead of accidentally calling compute when a user
indexes a dask Array with a dask Array.

Fixes #1628.",1475941615,
318fbbe3470b5eec2ee1921ced9808af72b199b7 tags/0.12.0~67,"PEP8: some fixes (#1633)

",1475934902,
d74e80ecc3246fbb9b553321e315ed3942d7425a tags/0.11.1~1,"Groupby works with multiprocessing (#1625)

Previously this would fail, as the partd was buffered, and wouldn't
write to disk some/all of the data. Since we still want to use the
buffer with the threaded scheduler, we solve this by switching behavior
if the partd creation function is serialized. If the function isn't
serialized, a buffer is used, if it is serialized then a file based
partd is used.

Fixes #1587.",1475794266,
10af53a5cdad4e81fa252b0db8d0c68cef3e72cf tags/0.11.1~2,"Use a nonempty index in _maybe_partial_time_string

Fixes the regression due to
https://github.com/pydata/pandas/issues/14354.
",1475794075,
4f8343f6b867f6cb84424a4ee780dd7998fea09e tags/0.11.1~6,"work-around for ddf.info() failing because of https://github.com/pydata/pandas/issues/14368 (#1623)

",1475782964,
669aae7d2b125e7b4070221aaa27be21cc4ee3e4 tags/0.11.1~10,"Use assert_eq everywhere in dask.array tests (#1617)

Previously we had a bunch of custom eq functions defined.
These were not as comprehensive as dask.array.utils.assert_eq.
In particular many of them failed to check dtypes.

Now we use assert_eq everywhere and we ensure that all dask.array
functions propagate dtype information.",1475704863,
34660a2215f80918e62c9ab03d9b75dea159e0c3 tags/0.11.1~17,"Better error message on metadata inference failure (#1598)

Previously we'd just throw an error stating that metadata inference
failed. This made debugging the cause of these failures tricky. Now we
raise an error indicating that metadata inference failed, and include
the original error and traceback as well.",1475508987,
b7fd210d810620cf08bc528ba5076a501de791d7 tags/0.11.1~18,"ENH/API: Enhanced Categorical Accessor (#1574)

Closes https://github.com/dask/dask/issues/1573

Implements a richer `dd.Series.cat` accessor that mimics the
behavior of `pd.Series.cat`.",1475507111,
38dc88ad4e8514e9658bed50423fd6dd0a5bd5b4 tags/0.11.1~19,"PEP8: dataframe fix except E127,E402,E501,E731 (#1601)

",1475499881,
f5173c38c70073c96a9cfcc7fb66aa6f0525f33a tags/0.11.1~21,"PEP8: some fixes (#1605)

",1475499859,
2f627248e9c35c95ba375d6f44d32af442458388 tags/0.11.1~33,"ENH: Support Series in read_hdf (#1577)

Closes https://github.com/dask/dask/issues/1564

Modifies the reader to check for `name` in the case of Series,
and `columns` in the case of `DataFrame`.",1474755009,
57285aa1cd721f0845b4f2b3756cb82d5f4597cf tags/0.11.1~34,"COMPAT/API: DataFrame.categorize missing values (#1578)

Closes https://github.com/dask/dask/issues/1565

For compatability with https://github.com/pydata/pandas/pull/10929
where it was decided that

`pd.Categorical(['a', np.nan], categories=['a', np.nan])`

Should raise a `FutureWarning`. Now we just drop missing values
before computing the distincts for the categories.",1474754943,
8b12ddb66c1d91786d9fa8a57c293ea295a40fe4 tags/0.11.1~36,"Sample from `read_bytes` ends on a delimiter (#1571)

If a delimiter is provided to `read_bytes`, the sample will end with the
delimiter.  This was specified in the docstring, but wasn't actually
implemented, since the delimiter wasn't forwarded. The `sample` kwarg
was also dropped in a few places throughout.

This PR fixes these issues, and removes the need to adjust the end of
the sample in ``dd.read_csv`` added in #1495. Pandas won't error a
numeric field is errored, which led to the failure in #1532. A test has
been added for this, and confirmed to fix on the provided csv.

Fixes #1532.",1474651474,
ab2aca3adc0a534ba106bc2f7143447601d88244 tags/0.11.1~38,"Tokenize mmap works without filename (#1570)

Due to a bug in numpy, ufuncs performed on `memmap`'s return an
instance of `memmap`, even though the data is already in memory. In
these cases `memmap.filename` is `None`, which causes `tokenize` to
error. We now treat `memmap` files where `filename is None` the same as
an in-memory file.

Fixes #1562.",1474651296,
bc82dce5beb6fdc161cff7e9f120559b5c59e062 tags/0.11.1~39,"String accessor works with indexes (#1561)

Previously the string accessor would error on indexes due to a pandas
peculiarity. Fixes #1560.",1474421044,
ddc3b14ac76bcd796cd905af5b40869a4c5d37f2 tags/0.11.1~44,"Improve slicing performance (#1539)

* Improve slicing performance

This avoids excessive function calls and isinstance checks by taking
certain behaviors based on the first element in lists, rather than
checking on all elements of a list

* use numpy to accellerate partition_by_size and check_index

* fix doctest
",1474024668,
6fad482d4cffc880002d0369603dcd2d30da1456 tags/0.11.1~45^2~1,"Fix metadata in Series.getitem

Fixes https://github.com/dask/dask/issues/1548
",1473961039,
87db8c0f3bafa538518c3708f6096d8e7280ad67 tags/0.11.1~46,"A few changes to `dask.delayed` (#1542)

* Add nout to delayed, allow for splatting results

This makes a few separate changes to `dask.delayed`:

- Adds an optional `nout` keyword to `delayed` objects, allowing for
  splatting and iteration over results.  If not provided, iteration will
  still error.

  ```python
  @delayed(pure=True, nout=2)
  def foo(a):
      return a, -a

  x, y = foo(1)
  ```

- Cleanup method calling behavior through use of small `DelayedAttr`
  object. Use `dask.utils.methodcaller` to do the actual method calls.
  This is cleaner than trying to inline the method getattrs later.

- Cleanup the test suite for delayed, removing a few duplicate tests.

* Remove deprecated `imperative` and `value`

Remove long since deprecated `dask.imperative` and
`dask.imperative.Value` namings.
",1473892972,
6ca1eaf45e8de690522684f5cf75144ab900ef41 tags/0.11.1~47,"Fixed read_hdf example (#1544)

The keyword 'path` raises error in the three `dd.read_hdf` examples",1473891743,
3218209426c9a64259b51ae2956dab1ecf84b9b8 tags/0.11.1~49,"Fix spelling (#1535)

This fixes spelling in various documentation, strings, comments, and
code.",1473557361,
5ff63a8675fd6e9a0f9e409db0de788aca0067a3 tags/0.11.1~51,"Deemphasize graphs in docs (#1531)

This moves the graphs section down in the index and provides warnings
that it is for core developers only.

This also updates the optimize docs so that they can be run through
cleanly.

Fixes https://github.com/dask/dask/issues/1530",1473420606,
abde2826b9f3e591dd5a8b0f34286f5cbcfea6fe tags/0.11.1~52,"Avoid pickle when tokenizing __main__ functions (#1527)

* Avoid pickle when tokenizing __main__ functions

Previously two functions defined in main with the same name would
tokenize to the same value.  This is because `pickle` would return
the same value in both cases without raising an exception (as might be
expected)

This now checks the result for the bytestring b`__main__` and if present
continues on to use cloudpickle.

Fixes https://github.com/dask/distributed/issues/487

* add test for normalizing functions in main

* use dedent in tokenize_function test
",1473248471,
4b31ffaa69df1b1cd09b798854d46b18a1569b0a tags/0.11.1~53,"Add changelog doc going up to dask 0.6.1 (2015-07-23). (#1526)

* Add changelog doc going up to dask 0.6.1 (2015-07-23).

Release notes are based on blaze-dev release emails:
https://groups.google.com/a/continuum.io/forum/#!topicsearchin/blaze-dev/release

* Add comment to dev guidelines about changelog updating.

* Add development changelog and configure doc extension to link to GH issues/prs.
",1473161533,
84546e36f845702810c1615027b63b7b88a095b1 tags/0.11.1~60,"Remove lambda/inner functions in dask.dataframe (#1516)

These increased serialization costs, often contained closures making
debugging more difficult, and were sometimes defined in multiple
places. Instance methods were also removed, as these weren't
serializable with pickle either.

Where possible a `methodcaller` was used to create the functions
to call methods, instead of defining a function separately for each
one. In other cases the functions were moved to
`dask.dataframe.methods`, where they can be reused.",1472661276,
32d8f9616b31e4bd9ff02507222f29bb62c11ac5 tags/0.11.1~62,"``dd.map_partitions`` works with scalar outputs (#1515)

Previously if a user defined function returned a scalar, the output of
`dd.map_partitions` would be a `Scalar`, even if the input type was
a series. This fixes that so that scalar returning functions return a
series, unless all inputs are scalars (in which case the output is a
scalar as before). Fixes
http://stackoverflow.com/questions/39215617/what-is-map-partitions-doing.",1472570646,
f445116cdb50cff80c9730d16fa4ef3354bd6e62 tags/0.11.1~63,"meta_nonempty returns types of correct size (#1513)

This was a bug with `pd.Series([instance_of_correct_dtype])` upcasting
implicitly. This was leading to only `meta_nonempty(int32_series)`
returning a series of dtype `int64`. Fixes #1512.

Also fixed units passing through for objects of type `np.timedelta64`
and `np.datetime64`.",1472496558,
4ab97394426f78497268b547c113e158f68a57cd tags/0.11.1~66,"Chunks check (#1504)

* Explain chunks= with examples

Taken from this StackOverflow post:
http://stackoverflow.com/questions/34895846/correct-choice-of-chunks-specification-for-dask-array/34904445#34904445

* Raise exceptions on bad chunks inputs

* Remove same size restriction on chunks

* fix linalg tests
",1472145775,
27b977d0ba6319ed495d767a590497fa1c5dfd1e tags/0.11.1~67,"Fix last 'line' in sample; prevents open quotes. (#1495)

The last line of the sample var may be an incomplete line. This causes the 'head' value to throw a CParserError:

> CParserError: Error tokenizing data. C error: EOF inside string starting at line 821

This occurs because of a hanging quote in a line due to the incompleteness. This patch will prevent the error during the head assignment.",1471956403,
5fe11e2fcb3805da0da3474de2a969ee05bb235b tags/0.11.1~69^2,"Add finalize- prefix to dask.delayed collections
",1471867539,
112b541cbb81db04a5753d80da0217aa7f31feeb tags/0.11.1~70,"State that delayed values should be lists in bag.from_delayed (#1490)

Fixes https://github.com/dask/dask/issues/1488",1471692275,
cc3615bee811d88b46f751ba58ab86d860c36e38 tags/0.11.1~71,"Use lists in db.from_sequence (#1491)

This avoids putting callables in tuples, which can be dangerous.

Fixes https://github.com/dask/dask/issues/1489",1471692268,
8906453bf03720f0cc5b825a0c8fcb740de92250 tags/0.11.1~73,"Field access works with non-scalar fields (#1484)

Previously field access in structured dtype arrays would fail if the
field had a shape. This was due to the output array not being the same
shape as the input array. This fixes that by inferring the output shape
and adjusting accordingly.

Fixes #1482.",1471572357,
2a685e2e143243589ee72a54ec98b176c97c1b7d tags/0.11.0~6,"fix typo in use case doc
",1471472167,
8bdf14d9067bb7c9c8a513599633681c7543f8b9 tags/0.11.0~7,"Add idxmax/min to DataFrame (#1446)

* #1310 add idxmax/min functions, works by chunk, need to add for overall

* #1310 switch to reusable function

* #1310 move chunk and agg outside of function

* 1310 use transpose to get nicer concat

* #1310 add simple test case

* #1310 process agg

* #1310 pass min/max to agg

* #1310 specify meta and use eq for tests

* #1310 both axis work, scalar still not 100% on the type returned

* #1310 remove idxmaxmin, pass as keyword arguments

* 1310 use map_partitions for axis=1

* #1310 parametrize test and explicitly set type

* #1310 remove transpose as caused type errors.

* #1310 merge in fix to eq, uncomment assertions, fix typo

* #1310 fix flake8 error

* #1310 add NaN to test

* #1310 appropriately test skipna

* #1310 parametrize skipna

* #1310 remove extra line
",1471463035,
725697fddf8bffe9db773ee92cfdcdbdaa89420c tags/0.11.0~9,"Make align_partitions more strict (#1475)

Previously when aligning DataFrame/Series with unknown divisions, and
ones with known divisions, bad things would occur. Now we explicitly
check if any of the input objects have unknown divisions, and fail if
they do.

A separate function used elsewhere is `_maybe_align_partitions` - this
only calls `align_partitions` if the divisions are unequal. This means
that *equal but unknown* divisions can propogate through, which is
desired for many elementwise operations. The case of mixing known and
unknown is then caught in `align_partitions`, failing as desired.

Tests for these behaviors are then added to ensure the fail cases hold.

Also Added tests for known and unknown divisions for `assign` (testing
that this fixes #1373).",1471452574,
6fa1c1582205e82156ac5baf0d37cf287703a5e2 tags/0.11.0~10,"fix link to the bag examples (#1477)

",1471438617,
c950905873c052d921aab77ab50143825c7cd7da tags/0.11.0~11,"Add document about choosing the right scheduler (#1473)

* Add document about choosing the right scheduler

Fixes https://github.com/dask/distributed/issues/364

* respond to comments
",1471432824,
9ee688ec89a57af7bd3d7b58a55a4c8f8a73d6fb tags/0.11.0~13,"Refactor Scalar class (#1472)

* Remove `__new__` in dask.dataframe

`_Frame` is no longer callable, with that functionality moved to a
constructor function. This simplifies a few things, and seems to fit
better as a function than a `__new__` method on `_Frame`.

* Refactor `Scalar` class

Previously the `Scalar` class wouldn't track metadata at all. This
caused problems with metadata inference if `Scalar` was used in any
operation. To fix this, the `Scalar` class has been refactored as
follows:

- Each scalar has a `_meta` attribute, same as all other
  `dask.dataframe` objects. This attribute is a non-empty scalar of the
  appropriate type. Supported types include any subclass of
  `np.generic`, as well as `str`/`unicode`, and custom pandas scalars
  like `pd.Timestamp` and `pd.Timedelta`. A corresponding
  `_meta_nonempty` attribute is also provided - this just returns
  `_meta`.

- Inference is added to `reduction` and `Scalar` returning
  `map_partitions`. Operators that work on scalars are modified to infer
  metadata based off the scalar dtype as well.

- `make_meta` is modified to dispatch on raw `dtype` arguments. This
  means that `make_meta('i8')` will produce the appropriate metadata for
  a scalar containing an int64.

Caveat: If a type is supported as a numpy scalar, a numpy scalar is
used. This does mean that for some (rare) operations, the `_meta` object
on a scalar may be a `np.int64`, but the computed result is an `int`.

* Add `__repr__` and `dtype` to `dd.Scalar`

- Nice repr showing name and `dtype`/`type`
- Add `dtype` attribute if underlying object has a `dtype`
",1471358304,
d057a3f8058af490083b181673c1f9597a91fd0a tags/0.11.0~8^2~1,"Rewrite dask.array.reshape

It now handles more cases efficiently, including the one identified in GH1468. I
*think* I've now covered every scenario where it's possible to reshape in
contiguous order by calling `np.reshape` on each chunk, but I haven't proved it.

In cases where we cannot reshape without creating a ridiculously large number of
tasks, it now fails noisily with a ValueError.

This is a breaking change, but one that I think will result in a better user
experience. Let's save it for a major dask release (0.11?).
",1471276188,
84190d6da23f827c28e0bff26155ccb7331bf557 tags/0.11.0~15,"Fix dataframe return type inference (#1466)

The old logic was causing issues when trying to infer the output type
from the input metadata. Now we infer directly from the metadata, which
simplifies a lot of things. This also results in the `return_scalar`
constant going away, which is much simpler.",1471274739,
904f8e21ba7611e92c0f1abdf97f4cf1dae0feba tags/0.11.0~16,"Add code quality tests (#1464)

* Add code quality tests

Add code quality tests, and squash all issues to make them pass. Code
quality is checked with `flake8`, using a subset of errors (see
`setup.cfg`). A test in `dask/tests/test_flake8.py` is added to check
that there are no flake8 errors from this subset. This can be easily run
using `py.test`:

```
py.test -m quality dask
```

To ignore certain errors locally you can use `# noqa: errors` per
line, or `# flake8: noqa` at the top of a file. This is used sparingly
throughout to silence complaints that are invalid/not useful.

* Hack to get ensure_protocol to pass

`distributed` registers the `hdfs` backend *even if* `hdfs3` isn't
installed. This goes against the logic implemented in `ensure_protocol`,
which is designed to give informative errors if `hdfs` or `s3` support
isn't available.
",1471011083,
b5f521a76558a10c702fa11ce5acc4009981e56d tags/0.11.0~19,"PR: Write bytes simpler (#1454)

* Instead of two-step delayed, pass opener function

Simplification to allow workign with HDFiles without
serialization.

* Use dask graph instead of passing function to write bytes

* Remove sync debug mode

* tokenize writing tasks for all inputs
",1470924348,
af142046938cc909e0bab6426e8319a822c566f3 tags/0.11.0~20,"Test dask.bag with unicode strings (#1459)

Related to https://github.com/dask/partd/pull/16

Fixes https://github.com/dask/dask/issues/1452",1470872709,
d95f72cbbd0473c2401170e1ed9cefae072e7c8a tags/0.11.0~21,"Raise informative error when resampling without divisions (#1458)

* Raise informative error when resampling without divisions

* Add function to compute sorted divisions

This could use better API

* Permit divisions='sorted' in dd.from_delayed

* improve error message in compute_divisions

* add back in meta
",1470871248,
58b0ae0d8a6d9aff1112266736cb14188c08e5ae tags/0.11.0~22,"Fix pickle for tseries dataframes (#1462)

This was caused by two issues:
- `__getstate__` and `__setstate__` weren't defined for dataframes,
  which was causing the default `__dict__` implementation to be used.
- `__getattr__` for timeseries dataframes wouldn't fail properly when
  fake columns were `getattr`'d. This was causing issues in cloudpickle,
  as it checks for various attributes differently than pickle.

This fixes both of these issues, and tests that dataframes are picklable
with both pickle and cloudpickle.",1470857955,
a7cb96b6f063156a278078a483ca9c7ea75ced44 tags/0.11.0~24,"Fix pickle for all classes (#1457)

* Fix pickle for all classes

Not all dask collections were pickleable. Further, the dataframe
implementation was broken, resulting in a shared `__dict__` between
objects. This fixes that, and adds tests for all classes to ensure
pickleable.

* Fix bag pickle on py3.3

Use __getstate__ and __setstate__ instead. This required changing a
`str` to a property. To forward the docstring, `StringAccessor` had to
be moved before `Bag` in the file.
",1470847173,
6d2eb0e7d5714516a7449cf86dbd1771e8dab9f6 tags/0.11.0~25,"Re-add the castra tests. (#1456)

These were xfailed in https://github.com/dask/dask/pull/1422, as the
last release of castra was incompatible with the new metadata. This was
fixed in https://github.com/blaze/castra/pull/64, and a new release was
made (0.1.8). We now only run these tests if castra version is >= 0.1.8",1470799603,
135d0f02de2626876aa1eaf24a8e349e67d65398 tags/0.11.0~26,"A few pep8 cleanups in dataframe (#1455)

- Kill use of tabs instead of spaces
- Remove unused imports
- Fix only the most glaring errors:
    - Visual indent lines up with nested scope
    - Missing/extra blank lines
    - `not foo is bar` and `not foo in bar`
    - Whitespace at end of lines",1470781761,
8d3db66467e5aa3f6dbb934da0ab6b13e843e986 tags/0.11.0~27,"DataFrame metadata cleanup (#1422)

This commit rewrites how dask dataframe handle metadata.

- Forbid untyped dataframes, removing optional `dtype` compute. This
  causes lots of errors that need to be squashed

- Propagate metadata properly everywhere

- Add in inference for user passed functions. This tries to run the function
  on a nonempty sample of data, and grab the metadata from that. If that
  fails, we throw an informative error.

- Rename `nonempty_sample_df` to `meta_nonempty`. Make work with
  `pd.Series`, `pd.DataFrame` and `pd.Index` objects. Ensure that names,
  dtype, index type, and index name are all appropriately copied.

- Move `_pd_nonempty` property to `_Frame` base class

- Rename `_pd` -> `_meta`, `_pd_nonempty` -> `_meta_nonempty`

- Deprecate `_pd` and `_pd_nonempty` attributes

- `eq` now checks that the inferred dtypes match the computed dtypes.
  Note that all numeric types are compared equally, as pandas will
  implicitly convert integers to floats in the presence of missing data,
  which makes inference *hard*. There's not an obvious good way around
  this, so this will do for now.

- `from_dask_array` inference with struct dtype now matches.
Cleaned up a few implementation details of that inference function.

- castra tests are xfailed until they can be fixed in castra master.

- Add in `meta` keyword to dask.dataframe use functions (`map_partitions`,
`apply`, `map`, etc...).

- Change signature of `map_partitions` function and method to allow for easier
  calling without passing in `meta` keyword. This is a backwards incompatible
  change without deprecation.

- Rename columns/dummy->meta in a few places

- Deprecate `metadata` keyword to `dd.from_delayed`, replace with
  `meta`.

- a few docstring and pep8 cleanups",1470758118,
c9dcba5eb5b412107f6b86b64c3cdfeccee69eff tags/0.11.0~31,"Move try-except KeyboardInterrupt around get() setup (#1444)

* Move try-except KeyboardInterrupt around get() setup

Previously when calling dask.async.get_async we would only catch
KeyboardInterrupts that happened during computation, not during setup
and teardown.  This could cause an issue if a ProgressBar callback
was setup but then the KeyboardInterrupt happened either during further
setup or after computation during teardown (but before ProgressBar
teardown)

This commit expands the range of the KeyboardInterrupt try-except to
include everything after plugin registration.

Fixes #1439

* Use started_cbs in dask.async.get_async

This avoids a rare case where un-started plugins can be ""finished""
before they start if another plugin fails
",1470492922,
d451f77b88f8425859c90985abcf786aabf0e5fc tags/0.11.0~33,"Fix no_default/no_result comparison against custom types. (#1442)

* Fix no_result comparison against custom types.

When the intermediate output of reduction is a type with strict
comparison, the code may break (for instance, when the output is a
``csr_matrix`` instance).

* Test reduction with non-comparable objects and sparse matrices.

This test fails in previous versions with a TypeError due to the empty
partition check with a ``!=`` operator against a string.
",1470394501,
9e84d7f183973cbc112fcf3520860f4e642f694c tags/0.11.0~36,"PR: write_bytes (#1346)

* First write_bytes, local

Tested with and without gzip.

Tests may have a more preferable location, and the couple of lower-level functions
may use some tests of their own.

* Propagate write_bytes and write_files to bag and dataframe/csv

* Squash some errors

* Try to squash again

* Squashed commit of the following:

commit 09d9e2f7085a97df28ad5e8c2985cad2add85355
Author: Martin Durant <martin.durant@utoronto.ca>
Date:   Tue Jul 19 13:51:45 2016 -0400

    dataframe and bag tests pass

commit a596bd82d8826245a04bf8c5ea2682bdab9f801f
Author: Martin Durant <martin.durant@utoronto.ca>
Date:   Mon Jul 18 15:51:54 2016 -0400

    Successful to_csv test

    Made simple test - previous tests not changed. Some of those will
    comtain corner cases no longer relevant.

commit f390ae449a5672ca75a16c54bc18d07ee757947d
Author: Martin Durant <martin.durant@utoronto.ca>
Date:   Mon Jul 18 13:43:51 2016 -0400

    A version that works

commit 2182ad72510da0cd10e74f8691e2a0647395eacb
Author: Martin Durant <martin.durant@utoronto.ca>
Date:   Sat Jul 16 20:46:59 2016 -0400

    stash

* Py2 fixes

* Fix unicode ting

* Avoid bz2 test on py2

* remove other bz2

* Fixes

* Hook for s3 write_bytes

* revert test dir in s3

* Cosmetic changes

* Fix series

Don't attempt to pass down get= keyword, rely on global or context definition
instead.

* More cosmetic fixes

* dumb remaining slashes

* A few more
",1470173030,
643004cc58bcd4ba24b85bd310bb700cf5666f46 tags/0.11.0~37,"Support concat with unknown divisions (#1421)

* support concat with unknown divisions

* Improve error messaging around concat(axis=1)
",1470137939,
98ff5337a612d8cd443fb10a287418c0c036d375 tags/0.11.0~38,"bag.to_dataframe works with scalars (#1433)

Previously this would error on construction, now it returns a
`dask.DataFrame` with a single column. This brings it into compliance
with the same iterables accepted by `pd.DataFrame`. Fixes #1429.",1470137929,
8b4be199a783a65811781275157bf826f0562cc3 tags/0.11.0~39,"Error if type not supported in dataframe assign (#1427)

Previously assigning to a column with an unsupported type wouldn't fail
until runtime. This adds an error message if the assignment is
unsupported, and improves our test coverage for what is supported. This
was pointed out in #1426.",1470083071,
9fac06b1f9429e4879a91973c51955bf31b05f68 tags/0.11.0~43,"dd.to_hdf: Format param doc + remove warning + cleanups (#1423)

If we consider supporting format='fixed' we shouldn't warn on a user specifying format
moves silently passing parameters to kwargs
add format parameter to to_hdf method's documentation",1469755031,
56596c738c424c02c00389219fd3cdd153a2db3b tags/0.10.2~1,"raise informative error on merge(on=frame)
",1469563731,
320f5ab8d246cc5e7d5a561bd6f02d57a973cdf9 tags/0.10.2~2,"Fix crash with -OO Python command line (#1388)

* Fix for Python with -OO

* Update .travis.yml

* Update fft.py

* Update random.py

* Update fft.py

* Update core.py

* Update .travis.yml

* Update core.py

* Update .travis.yml

* Update core.py

* Update core.py

* Update core.py

* Update .travis.yml

* Update .travis.yml

* Update .travis.yml

* Update test_array_core.py

* Update .travis.yml

* Update .travis.yml

* Update test_profiler.py

* Update test_progress.py

* Update test_profiler.py

* Update test_profiler.py

* Update test_progress.py

* Update test_profiler.py

* Update test_progress.py

* Update test_progress.py

* Update test_profiler.py

* Update test_profiler.py

* Try to skip some -OO errors on external packages

* update Travis.yml

* Update .travis.yml

* Update test_dot.py

* Update .travis.yml

* Uptdate test_dot.py

* -OO job move to crown job

* Update .travis.yml
",1469556614,
732aaf422ab06909a12b5dde947e406d14ec7211 tags/0.10.2~6,"Unify shuffle algorithms (#1404)

* rename method= to shuffle=

* Unify shuffle around rearrange_column

This unifies shuffles and set_index around a single standard approach.
We create a new column that dictates the target partition for each row
and then depend on either partd or task-shuffling to rearrange into
rows.

Various functions that use shuffling create this column and consume the
resulting dataframe in different ways, but they all share this core

* fix various errors after shuffle unification

* Rearrange shuffle code and remove dead code

* reassign original column dtype

* extend tests to disk/tasks
",1469396166,
456a98c928e7135861655c58921267b469ba6ee1 tags/0.10.2~7,"dd.read_hdf: clear errors on exceeding row numbers (#1406)

Explicitly error on invalid values for start, stop.
Previously user input was trusted. For example stop
could be greater than number of rows when manually
specified, althought the actual maximum as at hand.

We now validate stop is below/equals max rows, start
is below stop (for None values as well), chunksize
is positive.

Signed-off-by: Nir Izraeli <nirizr@gmail.com>",1469325367,
b51f2a200a9027985f7ff9423eea84e677cc85c4 tags/0.10.2~8,"Merge pull request #1402 from jcrist/nice-errors-on-imports

Add nice error messages on import failures",1469289621,
0b9a5aa885b2c2c98a56c7db60ecccf7012f7f9c tags/0.10.2~8^2,"Add nice error messages on import failures

- Throw a nice error when imports fail on optional dependencies.
- Make the `dask.diagnostics.visualize` function always importable,
  moving the missing dependency failure to runtime.

Fixes #1372.
",1469216187,
59c4eb3588ccf53d1984e96a08503ae92899fed7 tags/0.10.2~13^2~1,"Improve checks for dtype and shape in dask.array

The testing functions in numpy broadcast their arguments, which was
silently ignoring some bugs in dask.array. This tightens up those
checks, and fixes the bugs that appeared. Summary:

- Fix repr bug with empty arrays
- Ensure that `dtype` is passed through in reductions (Fixes #1394).
  Turns out `bool(np.dtype(...))` is always False (not what I would
  expect).
- `residuals` in `da.linalg.lstsq` returns an array of shape (1,)
  instead of a scalar.
- `da.array.Array._finalize([])` returns an empty array instead of a
  scalar with uninitialized memory.
",1469131629,
b4d3dba54fa488f60a808b9f7629aea4c156176e tags/0.10.2~14,"Merge pull request #1395 from nirizr/fix_unstoppable_progress

Progess bar process should be deamon",1469122136,
a43dfeda6c6f541284f795a8ac2c1172d1ae117f tags/0.10.2~15,"Merge pull request #1393 from jcrist/a-few-df-fixes

A few df fixes",1469121974,
10786c7f08ae3afa30812cc30e7d27831d5feffb tags/0.10.2~14^2,"Progess bar process should be deamon

Fix usage of progress bar making compute hang on error

Signed-off-by: Nir Izraeli <nirizr@gmail.com>
",1469117885,
86c48a6d3a61cb93143ed5cee8348cc4be2e3659 tags/0.10.2~16,"Merge pull request #1392 from jcrist/to_bag_fix

Fix pickling issue in dataframe to_bag",1469110101,
621a1e8ddf50ef44629da554fa8ac4f33b7d502a tags/0.10.2~17,"LZMA may not be available in python 3 (#1391)

Add extra case for python 3 in case lzma fails to import.",1469102932,
6d77360ae1621999380daf54f75b9009d8745d70 tags/0.10.2~18,"dd.to_hdf: multiple files multiprocessing avoid locks (#1384)

* dd.to_hdf: multiple files multiprocessing avoid locks

When writing to multiple files with multiple processes we're using a single hdf core per file.
Thus there are no potential thread safety issues nor any file locking issues.

These specific conditions allow an entirely parallel operation and improved performance

Signed-off-by: Nir Izraeli <nirizr@gmail.com>

*  consult context._globals for _actual_get

Signed-off-by: Nir Izraeli <nirizr@gmail.com>

* keep backwards compat for sequential writes

by defaulting to the get_sync scheduler, but keep the correct _actual_get value for param optimization

Signed-off-by: Nir Izraeli <nirizr@gmail.com>

* minor comment change
",1469102825,
37fc73a696762006d39c6047a65d104389f75ed5 tags/0.10.2~15^2,"dir works with numeric column names

`pd.compat.isidentifier` throws an error if called on a non string type.
This was causing our `__dir__` implementation to fail if column names
weren't strings.
",1469053280,
36e591051e57093d572681c9081af33767297d80 tags/0.10.2~15^2~1,"Dataframe groupby works with numeric column names

`df.groupby(0)[df.columns]` fails in pandas if all column names are
numbers (then columns is an `Int64Index` instead of just an `Index`).
This ensures that we cast to list before subselecting columns in our
groupby implementation.
",1469052698,
3d2da22a981893699a59c78519a271c5514c3dac tags/0.10.2~15^2~2,"Use fsync when appending to partd

In some configurations missing this would result in data consistency
issues.
",1469051160,
57c3ed92f0154fbd830980e67b3d4b75af8e8327 tags/0.10.2~16^2,"Fix pickling issue in dataframe to_bag

This was caused due to pandas returning namedtuples in itertuples, which
can't be pickled. To fix, we cast the namedtuples to normal tuples.

Also pulled out some lambdas into a separate function to reduce
serialization cost.
",1469050483,
5a3c3abdfbdd2b46e6972ead6fe80a3d920cac33 tags/0.10.2~21,"Fix 'visualize' import location in diagnostics documentation (#1376)

In issue #1372 a user complains he cannot import 'visualize' from 'dask.diagnostics'. This is because 'visualize' is exposed from 'dask' and the docs are misleading.

Signed-off-by: Nir Izraeli <nirizr@gmail.com>",1468606926,
2b7120470d1b37a51191745cd8d5b586ab8bd786 tags/0.10.1~3,"Merge pull request #1348 from nirizr/bloscmaxbufsize_noxfail

remove xfail mark for blosc missing const",1468254972,
454c61da9cb18301da5af1d5e83b7ff84b82b2a1 tags/0.10.1~4,"Protect reductions against empty partitions (#1361)

Previously reductions would fail if any partition was empty
Now we filter out empty partitions ahead of time to protect given
reductions.",1468244915,
80dd822ada17fdba22aa428060d7915ff99740ed tags/0.10.1~3^2,"remove xfail mark for blosc missing const

Some constants were removed in python-blosc v1.3.0 and brought back in
python-blosc v1.3.1, released 7 days later.  the short v1.3.0 timeframe suggest
these tests actually fail for a small portion of dask users.  The xfail marks
were added before v1.3.1 was released

In this commit I suggest removing the mark xfail and replacing it with
`pytest.skip` for python-blosc v1.3.0

dataframe/tests/test_io.py:test_to_castra was additionally failing because it
wasn't adjusted by commit 3b2e397, I also fix that.

dd.to_castra(compute=False) returned a delayed list instead of a castra object,
this also fixes that.

Signed-off-by: Nir Izraeli <nirizr@gmail.com>
",1467907178,
c8596bd6e62520448a4a12954584b7ed28815c13 tags/0.10.1~9,"Merge pull request #1359 from jcrist/s3_compat_fix

Add `anon=True` for read from s3 test",1467836603,
92c1eb7d1255e10d430a9f1e115d2a149cb79a72 tags/0.10.1~9^2,"Add `anon=True` for read from s3 test

Previously this would fail due to credentials error.
",1467835636,
5a60f0fce326c669a5a1c6915530201044ad94f7 tags/0.10.1~8^2,"`subs` doesn't needlessly compare keys and values

Numpy throws a deprecation warning on `bool(array == scalar)`, which
would come up in `subs` periodically, polluting the terminal. This fix
ensures comparison only happens if the key and task are the same type,
removing these warnings.
",1467827763,
eeb502d9ecae8eb496e68112c93dd84ebebc21dd tags/0.10.1~10,"Merge pull request #1343 from jcrist/mp_get_deserialize_errors

Multiprocess scheduler handles unpickling errors",1467310028,
ef190528c77722769a31183ca4982f5d84602fe3 tags/0.10.1~12,"Merge pull request #1344 from jcrist/bokeh-fixes

Fixes for bokeh 0.12.0",1467216810,
deb74696e859cd50fcb2271a810811364aa27a59 tags/0.10.1~10^2,"Multiprocess scheduler handles unpickling errors

Previously if a task failed to be cleanly deserialized, the error wasn't
handled gracefully and it would cause the scheduler to lock up. This
fixes that issue by special casing the `apply_async` implementation in
`dask.multiprocessing`. This means that it no longer matches the call
signature of `pool.apply_async`, but in reality we always call
`apply_async` with the same arguments. The special casing allows for
deserialization errors to be handled the same as all other errors, and
allows for a clean exit.

Also did a few small pep8 cleanups in the touched files.
",1467210703,
cb566375ce9375d7c1b6ec0773869c42c6bf6d5d tags/0.10.1~13,"arra.random with array-like parameters (#1327)

* enable using non-scalar inputs to array.random

* the two test that broke + a fix

* checking shape in broadcasting test

* more tests

* broadcasts as appropriate

* proper broadcasting and tests

* properly add numpy and dask array to the graph

* skip test if older numpy (<1.10), pep8

* numpy arrays are stored contiguous, dask.Array in tests

* array.random broadcasting tests a la @mrocklin
",1467052597,
a9088d8e53c83012e1c77051098664c7236b9770 tags/0.10.1~14,"Fixes issue #1337 (#1338)

",1467052531,
8a242a7abd32cc98bd54afb24b2c147f473e99ec tags/0.10.1~17,"Add center argument to Series/DataFrame.rolling (#1280)

* Add center=True capability to Series/DataFrame.

Also, some argument broadening.

* Remove scary comment--this case is handled.

* Add back in a docstring explaining the level of support.

* Parameterize the unit test to more fully explore the pandas API.

* Run tests on 0.18.0

* Rework some tests:
 - xfail some tests for some weird behavior in pandas
 - reduce the number of cases were parametrize to

* Lower precision for some checks, as I saw a case where numerical error caused a test for kurt to fail.

* Use standard utility function rather than our local one.
",1467043223,
d6d9db45ba349bfa6d7ca82b6b411c5e9d61c253 tags/0.10.1~24,"Few travis fixes, pandas version >= 0.18.0 (#1314)

* Travis pip install no-deps

Without --no-deps, pip would upgrade all packages that were out of date,
even if this was intentional by the pandas/numpy build matrix.

* More travis modifications

- pandas >= 0.18.0 for all versions
- only run coverage once

* Remove checks for pandas version < 0.18.0

Removed old code for pandas versions < 0.18.0.

* Add cloudpickle for python 3.3 to travis
",1466783134,
76fc6e2ff9d9e73c8d1372fb76650edb37675bd7 tags/0.10.1~27,"Support datetimes in DataFrame._build_pd (#1319)

Previously set_index(..., sorted=True) failed because pandas Timestamps
aren't numpy scalars.  This adds a special case for them in _build_pd.

More general solutions welcome.",1466695362,
5b5526dabcdb568e1cfda76d79a90c12dbb67bad tags/0.10.1~35,"quantiles for repartitioning (#1261)

* Create a new method, `Series._repartition_quantiles`, to compute quantiles for repartitioning.

This calculates fewer quantiles than before, which will reduce the time required
to merge the quantiles together.  This is based on some hand-wavy heuristics, and
we introduce some random percentiles to try to avoid certain types of pathologies.
Give it a try!

* Use number of partitions proportional to chunk length.

This requires us to calculate the len of series before we create percentiles.

Also, slightly reduce the number of random samples.

* Handle case when number of percentiles would be more than number of elements

* Add docstring.

* Assume partitions are of comparable lengths.  Don't block to take a sum.

* Convert percentile `merge_sorted` to a reduction.  Add `upsample` keyword.

* Fix most failing tests.

The random percentiles is causing a test that compares names to fail.

* Use deterministic random seeds.

* Combine steps 1-4 into a single step.

These were very linear and inter-dependent anyway, so we might as well make
the graph simpler and reduce the number of tasks.

* docstring for `create_merge_tree`

* Docstring for `_prepare_percentile_merge`, which could use a better name.

* typo

* Don't close over df.name in lambda, because this closes over df, which is bad.

* Move code for determining new partition divisions using quantiles to a new file.

* Handle categoricals explicitly and convert values to native Python for more efficient serialization.

Serialization with `cloudpickle` should now be 10-20x faster than before, but it is
still the dominant factor.  `cloudpickle` remains about 50x slower than `cPickle`.

* update doctest.

* Add more docs, including a high level discussion in the module docstring.

* Don't create empty partitions if possible.  Also, merge equal values together.

* If undersampled, interpolate extra divisions if given a number dtype.

* Test interpolating divisions for undersampled data.
",1466540508,
f2ef3e744430c0e1636725fd90208f4828252821 tags/0.10.1~43,"Add validation and tests for order breaking name_function (#1275)

* Add validation and tests for order breaking name_function

This bug was recently introduced by myself to dataframe.to_hdf and preexisted in bag.to_textfiles.
Saving to multiple files (as possible with asterisks in those functions) did not preserve the
partitions order.

Signed-off-by: Nir Izraeli <nirizr@gmail.com>

* name_function bug fix

Signed-off-by: Nir Izraeli <nirizr@gmail.com>
",1465967725,
7e781b3cde4d86c9f10f6960026ab06fa64c6d23 tags/0.10.1~46,"added note and verbose exception about CSV parsing errors (#1287)

* added note and verbose exception about CSV parsing errors

Parsing CSV blocks can fail if a line terminator appears in a quoted value.
This just makes that a little clearer to users since there's probably not an easy solution.

https://github.com/dask/dask/issues/1284

* removed CSV parsing try-except
",1465927362,
333d83523405f5a6f49e535edafdc2b191be07cd tags/0.10.0~1,"Merge pull request #1281 from mrocklin/windows-3

Windows fixes",1465834418,
45cdc395e70174d84a506c38d4dcdaea2ac88394 tags/0.10.0~2,"Merge pull request #1222 from necaris/fix-columns-in-merge

Fix columns in merge",1465831173,
73d60ac7e3df2fab50409e56054efb1dc3595e8b tags/0.10.0~1^2,"Windows fixes
",1465756366,
893183ea18f04d9256f86ea5282dc5ea7aeaa356 tags/0.10.0~4,"Merge pull request #1250 from jcrist/sched_key_fix

[WIP] Fix scheduler behavior for top-level lists",1465599155,
c6a1f9644a1065604816fbd0ea153e2f7199e59f tags/0.10.0~2^2~5,"Add failing test for merge case
",1465508389,
966cc36782a9ae0c7e0c047615092ba6258f0b57 tags/0.10.0~15,"Merge pull request #1257 from mikegraham/master

Fix some bugs in the rolling implementation.",1465483140,
2c07378afc4bba6733ec2782dd9e133afe039f0e tags/0.10.0~15^2,"Fix some bugs in the rolling implementation.

 - off-by-one error when considering partition size
 - incorrect check for first-partition/axis=0 (should just give nan / not consider partition respectively)
",1465444642,
92c39eadd0d34196e4c9e7a83299389b731a1403 tags/0.10.0~17,"Merge pull request #1246 from jcrist/opt-slice-fix

Opt slice fix",1465409221,
991a0d1ab9dadab3850016e3dbf24cbe9994a718 tags/0.10.0~21,"Merge pull request #1254 from nirizr/dot_graph_filename_fix

Fix dot_graph filename split bug",1465399082,
df228f3a9d6511fdd57a6e403c9405297a26934c tags/0.10.0~21^2,"Fix dot_graph filename split bug

Signed-off-by: Nir Izraeli <nir@sentinel-labs.com>
",1465398285,
a4fb8f96fd813044dbe7d8d91d8788dafce6a183 tags/0.10.0~24^2^2,"Fix tokenize in dask.delayed

Previously kwargs weren't being tokenized properly in
``dask.delayed.tokenze``. Fixed the bug, as well as one with `pure` in
delayed object creation wrapping literates containing dask objects.
Added tests for both cases.
",1465338359,
d60e482a31ce55044cee26bd0b3a2eafe92a61eb tags/0.10.0~17^2~1,"Remove unused imports, pep8 fixes

- Removed unused imports left over from refactor
- A few pep8 fixes (mostly indenting related). Flake8 passes 100% for
  this file now.
",1465336386,
cc4abc1caedcf59a6c7821819c274a68ed560796 tags/0.10.0~17^2~2,"Fix bug in slicing optimization

When slicing an unknown array-like object, ``getarray`` is used to
ensure that the output of the slicing operation is a numpy array,
instead of another array-like. When optimizing nested slices, we need to
ensure that if ``getarray`` is used at all in the chain, then the
slicing still happens. A bug in this logic was introduced in the recent
work on speeding up the optimization passes. This patch fixes this bug,
and reverts the changes to the tests made that allowed it to pass.
",1465336043,
0a6940e35a6e8d62d719afc0e173258be7bc296b tags/0.10.0~26,"Add Task Shuffle (#1186)

* Add task-based shuffle for dask.bag

This uses the transpose trick originally described in
https://github.com/dask/dask/issues/417 to affect a shuffle using the
task scheduling framework.  This is slower than partd on a single
machine but can work with the distributed scheduler.

This operates in log(n) / log(32) full memory copies and
n_partitions * 32 * n_copies tasks.

* push grouper into shuffle_tasks

* move task shuffle to main groupby function

* use tasks by default for distributed scheduler

Test is in the distributed repository

* Add set_partition_tasks for dataframe

* use set_partition_tasks in set_index

* support set_index(series)

* wip

* ensure shard results are dataframes in set_partition_tasks

* Support set_partitions_tasks with series index

* pandas 0.16 support

* fix shuffle

* add groupby name test

* squashme: respond to comments
",1465335559,
33fbbcf707be63cb539b6015b7742faaf331d9c0 tags/0.10.0~29,"Merge pull request #1241 from jcrist/delayed_dask_key_name

Add dask_key_name to docs, fix bug in methods",1465327047,
2e4b0dd36ba0ea4af77ffe4807e9424f2744cd2d tags/0.10.0~29^2,"Add dask_key_name to docs, fix bug in methods

- Add ``dask_key_name`` examples to docstring
- Fix bug with using ``dask_key_name`` on method calls
",1465326251,
00b08396bb42dc1d070ea4bb6b6e9ae1beae0a71 tags/0.10.0~33^2~1,"Add `optimize_graph` keyword to `compute`

If true, the optimizations for each collection won't be run. Default is
false. This can be useful for debugging, and mirrors the keyword for
`visualize`. Also added docstrings for `compute`.
",1464990661,
f8bf19a27e9b7b316c120c2b9ec7e22aac7f72e5 tags/0.10.0~40,"Merge pull request #1210 from jcrist/doc-errors

A few docfixes",1464888274,
a70c32982d707366d2d2ba7f288ad84ce9f90f75 tags/0.10.0~40^2,"A few docfixes

- Fix math formatting in `da.top`
- Add in missing hdf5 array doc example (was referenced, but didn't
  exist)
- Many small formatting fixes to squash errors in the sphinx build
",1464885338,
fcab781b5d2a976d3050ad3aa0d7e42f45af3edb tags/0.10.0~42,"Merge pull request #1204 from jcrist/cat-part-fix

Fix categorical partitioning",1464815449,
444dfb9f54d156af8ee8c1ab09c0db87d64751a6 tags/0.10.0~42^2,"Fix categorical partitioning

- Ensure categories are properly re-encoded
- Fix sharding issue with categoricals all mapping to same partition

Fixes #1200.
",1464810464,
2f6ed3dac972cd96f9ce9227bf5b98578b442b02 tags/0.10.0~45,"Fix failing from_url doctest

This is why I shouldn't push to master :/
",1464736970,
315f255f8e9ddd0e20eebf9f9ae9d0a2eba763b4 tags/0.10.0~44^2,"Ensure to_task_dasks always returns a task

Previously lists were special cased to return a list of keys. This
worked fine, except when fusing was used. Fusing resulted in a list
of tasks, which the scheduler wouldn't recognize as a task unless there
were external dependencies involved.

This fix ensures that any task returned by `to_task_dasks` is an actual
task. Nested lists can still contain lists, as long as the outer thing
is a task.
",1464711291,
e9c81113c9d3f5c0ea76243ce9d73bf09e7ae1e4 tags/0.10.0~50,"Merge pull request #1190 from jcrist/dd_dir_fix

Fix dir for dataframe objects",1464656064,
8862e26c59cf912f85de9a2a677854569e4551f2 tags/0.10.0~52,"Merge pull request #1185 from jcrist/df-closure-fixes

Fix some closure issues in dask.dataframe",1464377326,
985157945a8aca812e4f2c82c15ee1b6db77daeb tags/0.10.0~52^2,"Fix some closure issues in dask.dataframe

Remove spots where `self` was accidently being contained in a closure,
resulting in errors when using distributed.
",1464376244,
070f5a7c162afe82e91f15ed7e2d0311ad6c31d8 tags/0.10.0~63,"Merge pull request #1173 from jcrist/opt-doc-fix

Fix optimize docs",1464044502,
006e7e391799a7fe5ed2c6e4a4f6a2571296d8fe tags/0.10.0~63^2,"Fix optimize docs

Update example to match changes in scheduler, and fix serialization
issue. Fixes #1172.
",1464028677,
38fc4f9f7b9da92ff6380de87ac89d0f8145a224 tags/0.10.0~64^2,"Few pep8 cleanups to dask.dataframe.groupby

- Fix indentation issues
- 2 blanklines at top-level
",1463777717,
2363975ab467f4011097c06b9cffcd364fd8f7c6 tags/0.9.0~3^2,"Inline getattr for dask.delayed objects

When using methods with `Delayed` values, a task is first created for
the attribute access, and a second task is created for the method call.
While the scheduler overhead of doing two tasks is minimal, this causes
problems when using delayed with distributed/multiprocessing, as methods
aren't necessarily pickleable. To fix this, we inline all calls to
`getattr` (which should also improve performance, since these calls are
cheap).
",1462572416,
df84b3e16ce6b19edd1296efaf70ee54a1ce40cd tags/0.9.0~4,"Compat fix for pandas 0.18.1 (#1146)

Location of CategoricalDtype moved, but with the new metadata we no
longer need to import that anyway.",1462494353,
d7f4ded25a40f940b1ae96d9b8b1f55dce66ddcd tags/0.9.0~19^2~10,"fix from_s3 test
",1461867445,
59d9fcf572b0c575d705880ce9cf24a2e9ae0ce5 tags/0.9.0~19^2~30,"Seekable compressed files

1.  Add separate dict of seekable compression formats
2.  Add SeekableFile object for Py2 xz compatibility
3.  Add warnings to read_csv on compression issues
",1461867442,
b85881d4a97af56743db359b315a2b9f853f40c5 tags/0.9.0~19^2~35,"Work around xz Py2 failures
",1461867442,
b88b462f1448883eb8773c4447444bbd40971c9c tags/0.9.0~19^2~36,"avoid bz2 in python 2

bz2.BZ2File doesn't support Python file objects

https://bugs.python.org/issue5863
",1461867441,
ffd78154a90d49e134c85a572c80b9aa132cae09 tags/0.9.0~20,"Separate Delayed into DelayedFunction (#1118)

* Separate Delayed into DelayedFunction

Added a `prefix=` keyword to delayed

    dfunc = delayed(function, prefix='add')

added a `dask_key_name=` keyword to calling a delayed function:

    dfunc(1, 2, dask_key_name='add-1-2')

It felt odd adding extra function-specific keyword arguments to the
`Delayed` class.  It feels like we're overloading one class with two
very different meanings so I'm also reseparating `Delayed` to
`DelayedFunction`

* DelayedFunction inherits from Delayed

* squash

* Tokenize delayed functions consistently across sessions

* Improve delayed tokenization

1.  Respect arguments in partials
2.  Keep consistent names across sessions
3.  Tokenize before to_task_tasks to preserve order

* DelayedLeaf -- squashme

* tokenize functions with protocol=0

* squash
",1461867274,
d31886b249e6fed90715364aca4b9c12746701c3 tags/0.9.0~25^2~1,"fix repartition to handle repeated end divisions
",1461339960,
a8d55b68e2683528f26fa50e50aa49e77ab7ee6e tags/0.9.0~31,"Merge pull request #1105 from jcrist/rprof_cleanup

Fix bug in ResourceProfiler resource cleanup",1461184532,
41773a734be11aa1b3a033367d8e5a2d8869551c tags/0.9.0~33,BUG fixed utils.file_size with compression (#1097),1461183615,
730d178828571bfefd91d1cf229e905063580874 tags/0.9.0~31^2,"Fix bug in ResourceProfiler resource cleanup

Previously a `__del__` method was on the `_Tracker` class, which would
join the tracker upon cleanup. This worked fine if the parent process gc
hit it first, but failed if the child process gc did (as child processes
can't join themselves). After this happens, if a new resource profiler
was created, then it would try to grab information from all of its
sibling processes, including the one that failed to cleanup properly.
Process information can't be gathered from a zombie process, so this
would error, leading to another zombie process, and so on...

The fix is to move the `__del__` cleanup to the `ResourceProfiler`
class, and also filter out zombie processes in the `_Tracker` class.

Fixes #1094.
",1461178244,
d7f4ca519eccd8129a58d6d92a5f19015302d53d tags/0.9.0~34^2~18,"Fix doc build warnings/errors
",1461080271,
b6fcfa40b0e74f3e0699538f4fb4cf9605727c16 tags/0.9.0~34^2~19,"Minor docs fixes
",1461080271,
7dba9ede5cac25b299b85a4f7a003652d70a7a71 tags/0.9.0~37,"Merge pull request #1102 from jcrist/fix_shuffle

Improve hashing in dd.shuffle",1460592094,
819b37646adf59a071d21597c5c1aa9d5d88b96c tags/0.9.0~37^2,"Improve hashing in dd.shuffle

Previously, python's `hash` function was used to hash indices for
partitioning in shuffle. This could lead to non-ideal behavior, as
python's hash function could have low entropy (on my machine `hash((a,
b)) % 5 == 1` for any integers `a` and `b`. This also was slow-ish for
large chunks, as it doesn't take advantage of numpy/pandas vectorized
operations.

This implements a custom elementwise hashing operation for use in
shuffle partitioning. Each pandas dtype has a small function to map a
series of that type deterministically to an integer array (the hashes),
which are then combined iteratively with a simple `new_hash = 3 * hash1
+ hash2`. This is both faster (as it takes advantage of vectorization
for non object columns), and seems to partition more evenly in many
cases.

This also fixes a bug where empty partitions would lack the requisite
metadata (column names or dtypes), which caused problems with subsequent
operations.
",1460590441,
44e01880fb2afceceadf3669bb10229d6f36f754 tags/0.8.2~1,"Remove pandas nan/inf series test

Reporting this as a pandas bug upstream for now
",1460563799,
504159eed88581c7ce098e45308cc9ef77d99955 tags/0.8.2~3,"Merge pull request #1089 from mrocklin/webinar

Various fixes associated to upcoming webinar",1460477722,
ca6f01fcab0aabf472fc8778b940ebcfdc53baef tags/0.8.2~5^2,"Fix numeric stability for dd.corr and dd.cov

- Improve numeric stability of `corr` and `cov`
- Fix issue with `_get_numeric_data` when dtype is unknown
",1459981515,
aa1c0ad14a33e0bd74c03e46d44daadb1a83f092 tags/0.8.2~8,"Merge pull request #1083 from dask/shoyer-fix-colon

DOC: missing colon in diagnostics.rst",1459871732,
863279ebe6e2aec1499c579f5353e6266c01b0f6 tags/0.8.2~16,"Merge pull request #1080 from mrocklin/xfail-castra

deprecate castra tests",1459802313,
a389a02f39e1f8213ad038214d2254eaad903340 tags/0.8.2~16^2,"xfail castra tests
",1459800328,
36d08139f3a0822cebb2321fc02a0aaa0189f48a tags/0.8.1~1,"Merge pull request #1043 from jcrist/pandas_018rc2

Pandas compat fixes, 0.18.0rc2",1457717799,
fceeafbfe994c51aa40b2e8511808982f11f48e2 tags/0.8.1~1^2,"Pandas compat fixes, 0.18.0rc2

- Fix resample tests to use the appropriate api, depending on pandas
  version installed.
- Fix bug in resample implementation for 0.18.0
- Fix bug in loc implementation that was exposed by new assert statement
  in pandas code.
",1457651585,
3f35739dc6a657f2a91e209c27f69664bcbd5745 tags/0.8.1~6,"Merge pull request #1038 from jcrist/xr_issue

Fix slicing bug",1457479983,
d8641ae69785ab1b8c18399ee5458ba6e1d8a733 tags/0.8.1~6^2,"Fix slicing bug

This fixes a pretty bad bug due to iterating through a dictionary,
relying on it to be sorted (but never actually sorting it). This was
causing slices to be inserted incorrectly in the generated slicing
graphs, which led to incorrect output.
",1457475447,
0698d9261b5a0fc1b91426a2227c9f5e27dc224e tags/0.8.1~4^2,"Compatibility fix with numpy 1.9.2

This is the last numpy available on conda. It doesn't have `stack`, so
we use `concatenate` instead. Also a few minor edits to docstrings.
",1457466905,
a0e297c3989e9416f8b035a210f385e96bb0ff4c tags/0.8.1~7,"Merge pull request #1029 from thequackdaddy/bcolz_fix

FIX: bcolz indexing",1457369508,
0771dba0b7504847bf2143f4c9ebb67467ac983b tags/0.8.1~9,"Merge pull request #1028 from jcrist/nanarg_fix

`nanarg` reductions handle all NaN slices",1457109226,
ac1ac10fc49d6595ff877f2eb45469f519d9776a tags/0.8.1~10,"Merge pull request #1027 from jcrist/graphviz_error

Throw useful error if graphviz fails.",1457042276,
22571777fc9cb59b69fc3b2547db7c8804dab313 tags/0.8.1~9^2,"`nanarg` reductions handle all NaN slices

Previously chunks with all NaN slices would cause errors at runtime,
even if the whole array didn't have an all NaN slice. This is now
handled at runtime by replacing `NaN`s with inf/-inf in each chunk upon
error. An error is still thrown if the whole array contains an all `NaN`
slice, matching numpy behavior. Fixes #777.
",1457041731,
82e7f7d0f1fff5f4b61a7afabd7d3bc59bec6e15 tags/0.8.1~10^2,"Throw useful error if graphviz fails.

The conda version of graphviz (the c library, not the python binding of
the same name) is missing png support. In this case, an empty string is
returned from the command. Now we throw a useful error in this case,
pointing the user to the relevant github issue.
",1457034483,
2d68573b125b8f713caf15fb68a17bb151d1cc61 tags/0.8.1~12^2,"Move cachey import to local import

Displays nicer error on failed Cache creation. Fixes #868.
",1457026279,
2597f35e094b1b972d1d8b37a5c7b7de787a81c1 tags/0.8.1~13,"Merge pull request #1024 from jcrist/argmin_fixes

Bugfixes for arg reductions",1457020568,
e6b242bf68b28bd1cb2bf4b4105f513ce8d6970a tags/0.8.1~13^2,"Bugfixes for arg reductions

- Properly handle `axis=None` case (default). In this case, the result is
  equivalent to `a.ravel().argfunc(axis=0)`. This is handled in an
  efficient way, avoiding the need to actually ravel the array.
- Properly handle arrays larger than 2 dimensions.
",1456990768,
41c38104e8fa35a06d4e6d210c4cef432eba5956 tags/0.8.1~24^2~1,"Fix imperative tests when .array, .bag missing

Running pytest.importorskip('dask.array') at the module level skips the
whole module's tests if the import fails - that's not what we want if
we're just making sure 'pip install dask[imperative]' works.
",1455828275,
a9ec183a42be2350989450c1f9829072f6c08ca5 tags/0.8.0~3,"Merge pull request #998 from jcrist/dep_fixes

Update requirements",1455641675,
9d9a9efc06eb69cd2943ccd7046e53f3cd93bb86 tags/0.8.0~4^2,"Update resample to use lazy object instead of how

`how` kwarg for resample is deprecated in pandas 0.18.0. This fix is
backwards compatible with the previous pandas release, but internally
uses the same lazy `Resampler` object for both implementations.
",1455578625,
7cb2428705fea3555f7269ba4be456783e02e194 tags/0.8.0~4^2~1,"Compat fix in resample for pandas 0.18.0
",1455568359,
058d637c7ea4e68778d5dcececa5e655864ce26c tags/0.8.0~6,"fix reduction in case of single partition
",1455390946,
228fc79a239efe95d3bef9e143b04babeeed72ed tags/0.8.0~12^2,"Improve error message in diag
",1455292255,
b41b638afdf25a5ebf2233773a3082f4ef7ed74a tags/0.8.0~13^2~1,"fix multi-level nested frequencies
",1455229508,
d00db8bde806962974ab3ed80698adb79cf5b14b tags/0.8.0~20,"Merge pull request #969 from jrenner/patch-1

fixed raising exception when try to use from_pandas with multi-index ???",1454990672,
1d8781ff7fe3610ee99dd6126d33d73525b511ef tags/0.8.0~20^2,fixed raising exception when try to use from_pandas with multi-index dataframe,1454988326,
6089e92b37210a124a8cb616bcd1dcdd84da482c tags/0.8.0~26^2,"Cleanup a few typos in diagnostics docs.

Typos and formatting fixes.
",1454385278,
dad681244783146e3b99bd67f2fba19bd2c4ebb6 tags/0.8.0~27^2,"Catch np.divide warning

Fixes https://github.com/blaze/distributed/issues/89
",1454344041,
01f1fa99d6bda5037fe7cce2254ba5a91eda7761 tags/0.8.0~36^2~1,"fix cumulative reductions
",1453415806,
c9458d8b35653e9248865eba1d85fefbca06b0d1 tags/0.8.0~40^2~1,"Remove unnecessary calls to ``__getattribute__``

``__getattr__`` is only called if the base lookup fails. So these
try/except blocks using ``object.__getattribute__`` will always hit the
fail case - no use to test for them.
",1453249554,
9bbf15d632b11473b7b7c088aea4ec2a5c3e52bd tags/0.8.0~41^2,"resolve docstring issues.  add imports
",1453150442,
6423046697d67c0b6f579c819a500a18c81bb563 tags/0.8.0~41^2~2,"docstring fix
",1453143439,
3a43f085fc6158635ad29871783ac1e798478109 tags/0.8.0~47,"Merge pull request #914 from jcrist/bokeh-compat

Compat fixes for new bokeh release",1452668096,
587c2ea9b4ab08e4a3ec1764e3eb519d1b7d834f tags/0.8.0~47^2,"Compat fixes for new bokeh release
",1452667717,
28e3835e99f8bc468cc98ba02e6386b441116774 tags/0.7.6~3,"Merge pull request #904 from mrocklin/windows

Various windows fixes",1452019104,
e9c0cfaac7180bd63fd580a78a9e8a67ec4bb7c5 tags/0.7.6~3^2,"Various windows fixes
",1452018443,
5c6607d6f4739eee6616c1fbd7d2ba091ce8f177 tags/0.7.6~9,"Merge pull request #895 from mrocklin/remote-exception-check

Return old exception if remote_exception fails",1451850916,
05e2e6679a5bd43435f57c58d77cb41d1314b4fc tags/0.7.6~8^2~2,"Finalize no longer accepts the dask object

This was tricky when putting finalize into the dask graph itself,
as is necessary when we ask a remote worker to compute a dask object.

In these cases we had a full copy of the dask object within the dask
graph, which caused some serialization issues.

Now the _finalize staticmethods of each collection only accept the
results and don't have access to any of the object metadata.
",1451781862,
c2002b8444233826069cede757f1b3f52f4ae2d2 tags/0.7.6~9^2,"return old exception if remote_exception fails
",1451406376,
0083d5f6d0d8b3c28599ef48334c2e95f5cf8747 tags/0.7.6~7^2~3,"Fixed Python 3.x bug in tests.
",1450916150,
813d5c4ef4a5bea9c42a98520f90a8cd83298b8b tags/0.7.6~7^2~4,"Fixed Python 3.x bug in 'textblock'.
",1450913763,
b9ae4b5e79e34495d4c87f8f9b8679a520ead7b0 tags/0.7.6~7^2~5,"Added io.BaseIO compatibile classes of GzipFile and BZ2File for Python 2.6. Fixed bug in 'textblock' and moved it to dask.utils. Adapted dask.dataframe code that uses 'textblock' accordingly. Added centralized 'open' function which handles compression inference and modified code to use this function. Added compression argument to dask.bag.from_filenames and dask.bag.to_textfiles.
",1450892118,
9a0bce43d17c02763601961bbfa2edee06aa7367 tags/0.7.6~24,"Merge pull request #870 from mrocklin/raise-bad-chunks

Raise error on bad chunks argument",1449547852,
5fc84d1b75f8eeabfd8e2df280f94c75745ef17d tags/0.7.6~24^2,"Raise error on bad chunks argument
",1449546777,
d37b24bbe0602723b3675ad9e6c94626f07c6b8a tags/0.7.6~21^2~2,"fixed line breaks in docstring
",1449436123,
9626cb9dbda2099480ae6b18b98b9048d9199e3e tags/0.7.6~21^2~3,"attempt to fix deep recursion error when pickling
",1449350974,
f8410941cbcb2e810c399877d3959171773cf3da tags/0.7.6~21^2~5,"add a test that fails with dill (and should pass with cloudpickle)
",1449344470,
c3be56d1fa611f615c60bd62efcae8a977ceaa38 tags/0.7.6~37^2,"fix and new test for filename and format handling in dot_graph
",1448770765,
e443d1884405ca6cde72511988af90613e4c137b tags/0.7.6~7^2~9,"Fixed python3 issue. Added arbitrary linesep argument to non-chunked file reader. Added linesep argument to from_filenames function. Modified tests accordingly.
",1448487834,
0d42a83ef1d2c29bae38b06ecc7e731da12a7a70 tags/0.7.6~7^2~10,"Added more efficient readline function that supports arbitrary line separators. Bug fixes and modifications to support the new readline function in the _textblock function.
",1448405547,
92c8e67be0336a14dd1151271230ab36729ab31a tags/0.7.6~7^2~12,"pep8 fixes
",1448307078,
2e2fe9fcecc482c5a5a612386ba1c8665534b18c tags/0.7.6~41^2~2,"Simplify Bag.from_castra

We were running into serialization issues of the `Pandas` namedtuple
object generated within pd.DataFrame.iteritems.  This replaces those
with dictionaries for the time being.

I've also simplified the from_castra function to always match the partitions
found in the castra file rather than possibly bundle them up.  This behavior
seems orthogonal.
",1448301538,
4d1f8352878f20cb6fbff63b57a1a783e9466a8a tags/0.7.6~28^2~6,"Cleanups to tree reductions

- Input cleanup
- More tests
- Fix integer `max_leaves` input bug
",1448058995,
686d0ac6d15c79bed16a0e13fde2fe2b60d67081 tags/0.7.6~43^2,"fix doctest
",1447696652,
3e159f0c165ca1c966693c1f8b972244a8ad660c tags/0.7.6~46^2~6,"Use uuid to tokenize unknown types

Fixes #829

Previously we used `repr` as a catch-all for tokenize.  This failed
when different objects repred identically.  Now we default to random
uuids.  This requires us to be more diligent in the simple case but
makes us much more conservative about avoiding collisions.
",1447369783,
dc2b6fa78491b929708ba299864e342237d57367 tags/0.7.6~45^2,"Mark svd_compressed test as slow

This was failing on ARM.  Now we increase the size and number of
power iterations to improve relative accuracy.  We also mark the test
as slow to avoid slogging down tests.

Fixes #825
",1447013754,
518a963fe7b679468c3484d9967d66de6c0230c0 tags/0.7.6~52,"Merge pull request #824 from mrocklin/csv-hdf-fixes

Csv hdf fixes",1446999481,
71d1a54b60086a6fa365817200e67c4373199c56 tags/0.7.6~52^2~1,"Support single partition in to_hdf

See https://github.com/blaze/dask/issues/823#issuecomment-154778026
",1446994698,
b8defc89909bb355f113ddfd5dd5b881bc485fa2 tags/0.7.5~1,"Merge pull request #800 from kwmsmith/docs/fix-typo

Fix small typo in docs Optimization section.",1445715327,
5a27f6a4a140eda9b373089e26aa1edffcc5ea13 tags/0.7.4~4,"fix array doctest
",1445365710,
cd74eb1c69f27aca60020fe9592d977a1a17e95b tags/0.7.4~3^2,"fix dataframe_from_ctable doctest
",1445362446,
f08c178b60b49c95a3001df8d28206afcf700929 tags/0.7.4~12^2,"Fixups for CacheProfiler

- Add tests
- Allow zoom on y-axis
- Some docstring fixes
",1445023359,
5af933ae74c2ba63200fa155fe6bb0c2caba7bd9 tags/0.7.4~15,"Merge pull request #785 from jcrist/doc-fixes

Doc fixes",1444864563,
1880b3fbfbeb3f203cb50ffd694b7e9e0075d9cc tags/0.7.4~15^2~3,"Fixes for docs

- Pin pandas version so docs build on RTD
- Fix headings in many docstrings to standard numpydoc formatting
- Add numpydoc setting to conf.py, removes 1000+ lines of warnings on
  each build
- Remove references to non-existant functions
- A few rst formatting fixes throughout

Docs now build with minimal terminal output, and without silently
dropping parts of docstrings.
",1444859674,
40fa04ffebe4a547cf23880976f136be0b4e3a28 tags/0.7.4~17,"Merge pull request #782 from jcrist/numpy-pandas-upgrade

Version compatibility fixes",1444755164,
b6b7bd9f45652a1a790dab2f36fc729bb5a71886 tags/0.7.4~17^2~1,"xfail distributed test

Not sure why it's failing - something to do with ipyparallel.
",1444687559,
d8d9286bbb11440294324038c3cc15a13c6edfb6 tags/0.7.4~17^2~4,"Fix read_csv compatibility issues with pandas 0.17
",1444670697,
46a2f4a491f14a338661caa438b556fc0daa1d19 tags/0.7.4~17^2~5,"Fix for numpy 1.10 divide

Need to test for another fail-case for np.divide, and use our custom
version for that as well.
",1444428436,
fb26f3712329f17ae943dd918b2ae0eaea70db28 tags/0.7.4~18,"Merge pull request #780 from jcrist/imperative-fixes

A few imperative fixes",1444425696,
cb8e13ace5f7dc16e1ddc9c14110204626059b2d tags/0.7.4~18^2~1,"A few imperative fixes

- Key names in do prepend function name
- Value keys use type name as prelude
- Correct intend of tokenize(..., True) to pure=True
",1444420825,
84a6013f0d72664b8d5376fd1fa06f3ee0d53df1 tags/0.7.4~19,"Merge pull request #776 from shoyer/nan-reduce-fixes

BUG: fixes for nan-skipping aggregation functions",1444323030,
98ce640b461d499b564f692629fa06fa0009d86a tags/0.7.4~19^2,"BUG: fixes for nan-skipping aggregation functions

Fixes GH773
",1444251354,
4b56a224cfe27d1b748df8f3e0d895f751bed300 tags/0.7.4~21^2,"ravel array in tokenize

This supports 3.2

See https://github.com/scikit-image/scikit-image/issues/1740#issuecomment-145275737
",1443897801,
f8a6ab4d0559ed74145d6bd112214e2051389a6f tags/0.7.4~22,"Merge pull request #767 from mrocklin/random-ignore-choice

Ignore importerror when defining RandomState.choice",1443834371,
c1fbac63d1b267223fdd6e6f564b3647c7ecb919 tags/0.7.4~22^2,"Ignore importerror when defining RandomState.choice

Fixes #766
",1443831292,
aea7edc8cb79d653fe07a4163ea40f4d41a11fb4 tags/0.7.4~25^2,"fix name test
",1443534323,
c163c30596898d661512fc33d493fa29683d5e58 tags/0.7.4~27^2,"Support struct dtypes in dd.from_dask_array

Example
-------

In [1]: import numpy as np

In [2]: x = np.array([(1, 'a'), (2, 'b')], dtype=[('a', 'i4'), ('b',
'object')])

In [3]: import dask.array as da

In [4]: y = da.from_array(x, chunks=(1,))

In [5]: import dask.dataframe as dd

In [6]: df = dd.from_dask_array(y)

In [7]: df
Out[7]: dd.DataFrame<from-dask-arrayd332533ff03b86defb56f10bc4fd3468,
divisions=(0, 1, 1)>

In [8]: df.compute()
Out[8]:
   a  b
   0  1  a
   1  2  b

Fixes https://github.com/blaze/dask/issues/747
",1443369021,
372a1908013b6657a2a0dd7577a71abb3cfd5e1e tags/0.7.2~2,"Merge pull request #739 from cowlicks/bom

Fixing issues with UTF-16/32 encodings and dataframe.read_csv",1443124809,
5d2be4f2f63617f2f7a10072965c9ecb779226eb tags/0.7.2~1^2,"Close memmap'd file when finished.

Fixes a bug in windows.
",1443051588,
bb5e07dcdfbe3c4dcdb2f150f0ecbaf900f77df9 tags/0.7.2~4^2,"Ensure args to elemwise have compatible shapes

Previously we errored later on, and the message was more cryptic. This
replicates numpy's error message, and validates arguments immediately.
",1443050623,
7333f6ef0886583910c19742caf740114d881a02 tags/0.7.2~2^2,"Windows fix dataframe test_encoding_gh601
",1443027413,
114d6c1f27b96b57056554cd71dc15f030b6033b tags/0.7.2~2^2~1,"Windows fix test_utils.py::test_gh606
",1443025657,
58de150b21e37a71756042637da338c07905cace tags/0.7.2~2^2~4,"fix test for which broke from changes to _read_csv
",1443024068,
02afdf585570a3dcdb8f6374aa5a3a8fec357738 tags/0.7.2~2^2~5,"fix bug in textblock
",1443024068,
1ec8a2dd507d50fab7b99a8669020b7d8862a944 tags/0.7.2~2^2~7,"Prefix blocks in _read_csv with correct bom.
",1443024068,
e86daf98211e68e4fc660c1f7ed687433479d67d tags/0.7.2~5^2,"Fixups to pprint_task

- Add `apply` from `dask.compatibility` in tests
- Fix div-by-zero issue in pprint_task
- Sort kwargs before printing
",1442953489,
f343a3d4915eb0a517c5419116222850b7b118d2 tags/0.7.2~7^2,"Clean up read_csv

A few changes to read_csv

1.  Remove categorize and index functionality to read_csv
    This is now handled in a less efficient but simpler way that is
    easier to maintain
2.  Clean up style issues from #720
3.  Minor fixes to other tests
",1442939777,
43ba4b3617257059b8a9d7823db6a57605158a08 tags/0.7.2~7^2~4,"Fixes to read_csv for DateTimeIndex; header titles can have spaces; kwarg nrows fix
",1442939728,
ea58d12438a343583ebc5c33ae75b78ba35342e6 tags/0.7.2~8,"Merge pull request #740 from mrocklin/subs-except

Except all errors on task==key check in subs",1442939694,
32423a8686b6a0bc69931bbd50d94767b9564e3d tags/0.7.2~8^2,"Except all errors on task==key check in subs
",1442871586,
8faffd4015a585ad6ee9418302350a5c4491f9a8 tags/0.7.2~11,"Merge pull request #724 from cowlicks/gh-714

Add error for using `set_index` in read_csv",1442867553,
f09d024b9cdf1679d3fc7e53048b1b51e57a1297 tags/0.7.2~12,"Merge pull request #723 from cowlicks/textblock

WIP fixes to textblock and read_csv encodings",1442865379,
01ab838b1387ad06d80008e680115d238d28b07f tags/0.7.2~16^2~1,"Compatability fix for pandas versions

Addition wasn't associative for timestamps/series in previous version:(.
",1442603257,
911e373068f998ad2b820f140045fa3d04fd3012 tags/0.7.2~11^2~1,"Add error for using `set_index` in read_csv
",1442527102,
a701d0b5420d571712f631b73f9a125a6aaab2f9 tags/0.7.2~12^2~3,"Add next_newline, and fix textblock.
",1442518934,
97c28ccf86907bb936f1555feab8805164d20cf4 tags/0.7.2~23,"Merge pull request #711 from mrocklin/error-multi-index

Raise informative error on attempted multi-index",1442416323,
47e72e9745047c5329c2ee0074318b1fc658e0dc tags/0.7.2~25^2,"Fix series.groupby.nunique

This follows on to #709 which passed only because the test data was trivial.

This changes the test data and fixes the uncovered issue.
",1442335913,
d5cb53f16060fa3bb9eacd24764ddcd249dfd684 tags/0.7.2~23^2,"Raise informative error on attempted multi-index
",1442335382,
b951cb4c75cf5ba7f5bf3658b81cfc4bbc751641 tags/0.7.2~26^2,"fix series groupby nunique
",1442333092,
c1c5d1307facdc3e44bd20cda940ac7529d3a6c7 tags/0.7.2~29^2~10,"Better error message for read_hdf start/stop
",1442242929,
a0b16facf9b7affa852fd114c61f1add41ea087a tags/0.7.2~35,"Merge pull request #698 from sinhrks/elemwise_bug

Fix DataFrame elemwise results in incorrect columns metadata",1442153415,
5526bc1d00cce4864d45d19eccc6f60999df0488 tags/0.7.2~36^2,"Cleanups to ResourceProfiler

- Stacked plots now align edges
- Bugfix for _finish callback
- Test sleep time shortened, removed slow decorators
",1441989981,
64d9b1361e2691dddff273af85ddebdc970278d6 tags/0.7.2~36^2~1,"Add psutil to travis, py26 compat fix
",1441944809,
4ec8bb22c15de8262ff2a52ca6efa04e6de3c856 tags/0.7.2~36^2~3,"Use Pipe instead of Event in ResourceProfiler

Fixes lockup issue
",1441930872,
a36774ae1322a7f3cbe12f5f1c5fc08f971f1cf4 tags/0.7.2~38^2~6,"None -> 'none', fix accidentally removed test
",1441754728,
a896338bbd60d7c580df1e6b200f301f42a21920 tags/0.7.1~3,"Merge pull request #656 from mrocklin/csv-error-message

CSV error message",1441375761,
f694e239f7fbce74e39225187b7af297ce2b19a6 tags/0.7.1~2^2~1,"fix SeriesGroupBy for df.gb('key').foo
",1441340852,
86e63dbd1eb71490415e61428271679e3e0eba89 tags/0.7.1~5,"Merge pull request #657 from mrocklin/remove-debug-message

Remove verbose debug message from exception",1441321306,
bc05e96e3a0888b7614d022b259c45df01c50940 tags/0.7.1~5^2~1,"Remove verbose debug message from exception

Many error reports that we get don't seem to understand this message.
It may be more direct to give them the underlying error report without
explanation.
",1441318815,
adc3b104003634663918ec2139c9cbb5d13f108d tags/0.7.1~3^2~1,"use regexes in read_csv error messaging
",1441316660,
214631936ea89de1b024847841a0fc6fb851fd50 tags/0.7.1~2^2~2,"Test bad indices raise errors.
",1441314650,
1019c883d39deef06b6b0c647eb05304202d2d00 tags/0.7.1~8^2~1,"Use seed in svd_compressed

This uses RandomState(seed).standard_normal rather than just
standard_normal in `svd_compressed`.  This allows for the following:

1.  Parallelism within the random call
2.  Determinism
3.  We avoid an intermittent failure
",1441232203,
7beb4e39e7111bf59aee5c1c08e58ac3d8763bf1 tags/0.7.1~14,"Merge pull request #638 from mrocklin/profiler-robust-to-errors

Make Profiler robust to errors",1441209924,
d1fa450f8858647f04f1d23a5dd096e5a1772d5c tags/0.7.1~21,"Merge pull request #645 from shoyer/fix-da.sign

BUG: fix da.sign",1441055561,
7003c9dd9d10fe74b1bfd5bd491e4e4bdcce428f tags/0.7.1~21^2,"BUG: fix da.sign

This was a copy & paste error...
",1441054799,
1df3d78759bb1bf8e680814d25d28fda1b9450b7 tags/0.7.1~14^2~2,"fix profile error test
",1440800162,
eb8aca8b21b4134786c93bf4055090d65cc6473d tags/0.7.1~24,"Merge pull request #632 from mrocklin/to-csv-gzip

Add xfailing to_csv gzip test",1440796594,
8c6ae0bbfb743838d801bae2f5ae1cd292df8645 tags/0.7.1~14^2~5,"Make Profiler robust to errors

In particular we remove incomplete results from the log
",1440795831,
b5a746bddf069011622f07991a323d75d170a2c1 tags/0.7.1~25^2,"remove debug code
",1440712074,
7e370c61d53f60054fb6d73e593200a8a61a82b0 tags/0.7.1~24^2,"Add xfailing to_csv gzip test
",1440697444,
cade3990c8a17a6368cdff7f6cc1dce5b538df5c tags/0.7.1~29,"Merge pull request #628 from mrocklin/castra-selections

Fix fusion optimize bug with castra",1440684845,
978da9f4d84479f7435409d175e00ea018bf953b tags/0.7.1~29^2,"Fix fusion optimize bug with castra

Previously we would fuse selections because they used getitem, like column
access.
",1440619803,
03803dbd008e240d14f929461bd1f91751eb244f tags/0.7.1~33,"Merge pull request #626 from mrocklin/bar-error

Provide update_bar elapsed in error situation",1440564587,
04e1770835b0c38d96ad2b9e38ca74faeae3199c tags/0.7.1~33^2,"Provide update_bar elapsed in error situation
",1440540641,
facf1ad2d8e87a4bba10fd0b09eb3212c2d8d640 tags/0.7.1~34^2~2,"Fix DataFrame hash join bug
",1440505154,
564932d65d6b360fe77af78420ac8ce1828b8978 tags/0.7.1~27^2~4,"A couple fixups for resample

Previously this failed if the index was [17:00, ...] and the frequency
was 6h+. This was because pandas resample boundary algorithm would move
the bounds below the start of the data (6h frequency would have the
first window start at 12:00), which resulted in duplicate indices and
other failures.
",1440097101,
bc9bc5d8432b3d9045cfb694a198061aa6f0e6e1 tags/0.7.1~45,"Merge pull request #604 from wiso/issue-603

fix argument",1439940752,
cec312232acc22be2d04fb955a468831c6d36f03 tags/0.7.1~45^2~1,"fix argument
",1439939389,
46a23b9862d2b469d93ffaff6f47b813f846b70a tags/0.7.1~52,"Merge pull request #590 from jcrist/dist_fixes

Compat fix for new IPython release",1439588505,
a23fc744596afb16440c607da02ae260d26614d6 tags/0.7.1~52^2,"Travis fixes
",1439587456,
e2eb321d27dd2c3283f8ad557c6c90c0481d3918 tags/0.7.1~52^2~1,"Compat fix for new IPython release

- Distributed test robust to versions of IPython
- Add ipyparallel to travis install
",1439586172,
d2222b919f5b8f7fe7e42ad9cf105b2e1e733294 tags/0.7.0~3,"Merge pull request #581 from jcrist/progressbar_fixes

A few callback related fixes",1439526160,
d0a6ef87364e47d82ca5327897edd1d795408390 tags/0.7.0~5^2~1,"fix doctest
",1439510409,
c287bdece440ebbb67359afbdaf1969580b45839 tags/0.7.0~6^2~1,"Some dataframe tokenize cleanups

- Normalization of dask.dataframe objects now happens in
  `normalize_token` when it makes sense.
- A few bug fixes for non-deterministic cases, added tests to cover
  these.
",1439502005,
4508044d36cd3c7adf89e5079662a86f13a9f454 tags/0.7.0~2^2~3,"Add __array/wrap/prepare__ to fix gh-580
",1439492705,
94a538cc03612d51eb42cdecc4b19be10eaf6e26 tags/0.7.0~6^2~3,"A few fixes to normalize_token

- `normalize` -> `normalize_token`
- Compatability fixes for new version of toolz, which changed the
  implementation of `Compose`.
",1439491784,
3b2fb7830d89475fa8bbb5d48314be8dcf1c2d78 tags/0.7.0~6^2~4,"Compatibility fixes for array tokens

- Sorting function as str & tuple comparison is forbidden in python 3
- Apparently sometimes keywords is None on functools.partial objects.
  But not always. And it never is in 2.7, but can be in 2.6 and 3+.
  Which is both odd and annoying :/
",1439489127,
fcdcb3140c69620a851e8dd3d3b98e54637e369f tags/0.7.0~6^2~5,"Fix failing dataframe test in osx

The while loop was causing the test to lockup. Sleeping for 1 second
fixes it on my computer.
",1439489127,
ba12555787c48239ab8d7fd99a985c1f8e233736 tags/0.7.0~6^2~6,"Add tests for deterministic tokens in dask.array

Fixes added for bugs as they were found. Biggest issue was deterministic
naming of functions that also knew about partial/compose/curry.
",1439489127,
0eca1504925b704a340326e05e83dc877120a22d tags/0.7.0~6^2~8,"Fix failing doctest
",1439489127,
b2307eec82a759e140c8666356839a00fec8efd3 tags/0.7.0~3^2~1,"Py 2.6 compat fix for progressbar test
",1439488421,
5c1910ce57021895b235399e019cfef3a5850fcc tags/0.7.0~8,docfix in dask.array.ghost,1439480618,
ccf8366a8801de7a3a03ddd36921ddbba348ab19 tags/0.7.0~9,"Merge pull request #579 from sinhrks/df_doc

DataFrame related doc fixes",1439441214,
a18492a1cc14c768c2c59aa88a772adab517c385 tags/0.7.0~9^2,"DataFrame related doc fixes
",1439432547,
4a2b382ee3a1b302b564f43a64863c5d470fdaee tags/0.7.0~10,"Merge pull request #577 from mrocklin/various-fixes

Various fixes",1439417397,
65fab44c257c09c8fc55864da54e22cbc2a11213 tags/0.7.0~17,"Merge pull request #568 from mrocklin/ignore-progress-stdout-error

Ignore errors on ProgressBar printing",1439229895,
0e035edf43ae122c3b4d938f35c36063ca886860 tags/0.7.0~17^2,"Ignore errors on ProgressBar printing
",1439229083,
b097b163595dc134ae6b6e8c4f26a4889d2fc57a tags/0.7.0~20^2,"ignore importerror around scipy
",1439225801,
2d94da1cad5b22718b03c92cb400f76ca23b2cf5 tags/0.7.0~22,"Merge pull request #563 from mrocklin/dot-fixes

Dot fixes",1439223974,
682853c10716c0efeba759a6bf29f642db6ad7ab tags/0.7.0~23,"Merge pull request #548 from mrocklin/distributed-fix

Fix intermittent error in test_collect_retry",1439220985,
c3cf4bfdc6ec8e26863354fc70f8fe61837938ab tags/0.7.0~24,"Merge pull request #561 from sinhrks/dd_merge

Fix repartition edge case bugs",1439216591,
2f384b830b22b3a8190145f5fe46b400dcc70bc8 tags/0.7.0~24^2,"Fix repartition edge case bugs
",1439202501,
45a13fc534a61b2df8a54c24da1210d4a5c348b2 tags/0.7.0~32,"Merge pull request #550 from mrocklin/progressbar-debug

Make progressbar robust to no tasks",1438975872,
d6683a2086e63f07173287c96558ebd80abb6286 tags/0.7.0~23^2~1,"Fix intermittent error in test_collect_retry
",1438961577,
a751ba272fc154733c7b766ec162ac3ddbcaa732 tags/0.7.0~42,"Merge pull request #530 from mrocklin/to-castra-fix

Fixes around to-castra",1438886629,
763ddbc729354e1965976d2978a1e4cbf2fd3a10 tags/0.7.0~37^2,"raise informative error on mismatched columns, return_type
",1438885851,
7664703d2dfac12860a5468bc926b7259f10ae64 tags/0.7.0~53,"Merge pull request #526 from mrocklin/castra-fix

to_castra supports compute=False",1438785782,
b6eb72bc92b4873496d16a8b23b196eefc4c0c30 tags/0.7.0~43^2~14,"Add rfft, irfft, hfft, ihfft. better errors.
",1438744222,
e388ed8e20097d708758c4002272fb92feb09777 tags/0.7.0~57^2~1,"py34 fix
",1438723236,
f134f9c8b3fdd0d33cb9b6caacb903ecf7bd2147 tags/0.7.0~57^2~15,"read_csv fails on no filenames
",1438722969,
dda9e75d427977763b21f811761d25ed9aa72446 tags/0.7.0~69^2,"Forbid `how` kwarg in rolling_* functions

Also add tests for asserting these errors.
",1438376624,
9cdc8d784c6da911926f2e4dc7c5b14a8ed8e072 tags/0.7.0~69^2~2,"Small fixes to dask.dataframe.rolling

- Use iloc
- Use common tokens from dask.dataframe.core
",1438373822,
f907799f1e283ec27e4652ed99f2519e5e4af3f3 tags/0.7.0~70,"Merge pull request #503 from cpcloud/bag-bug

Fix bug where entire dask was being computed in concat",1438370411,
82b5d43ff0ffbd8ef831289913536b817ebe6829 tags/0.7.0~70^2,"Fix bug where entire dask was being computed in concat
",1438370021,
1ab72aa532868405829932c18e47fbaf9af336c4 tags/0.7.0~71,"Merge pull request #501 from jcrist/callback_on_error

Finish callback called on error in scheduler",1438356843,
d38a0b284770e900c8e3b1e2a007f713b6a4a80a tags/0.7.0~71^2,"Fix add_callbacks.__exit__ to handle errors

More resilient to errors in enclosed code block.
",1438356452,
b35adb3c153904e4a2e0b468da2d73a38778bcf6 tags/0.7.0~71^2~1,"Finish callback called on error in scheduler

Allows for cleanup of diagnostics resources on error.
",1438354355,
fd62919ce0c007e9e02ffe74fb01922d8797038b tags/0.7.0~68^2,"vindex: raise error on mismatched lengths
",1438305676,
603e3906b12d3ab203776f16d8f5a439e6393925 tags/0.7.0~79,"Merge pull request #490 from cpcloud/fix-k-bigger-than-chunk

Fix k when k bigger than chunk in topk",1438189809,
25076003412725ac7b3ae56fe7b0eb9928427134 tags/0.7.0~68^2~5,"raise error on multi-dimensional key in vindex
",1438126764,
291643957c3e9ae1781ecacc33959c3cfa49a0cf tags/0.7.0~68^2~7,"fix doctests
",1438122942,
a016461ece5901106a6d20a29f5405dee222e81f tags/0.7.0~68^2~8,"unite vindex error
",1438122466,
0a158b83b3137969285db5e3f5b8883107b0ca08 tags/0.7.0~72^2~5,"Added arity detection for builtins and fixed arity detection for constructors, added tests for both
",1438038121,
5f123587fd4bc73d1ad9eae6c33b86fa703da412 tags/0.7.0~72^2~4^2,"Added arity detection for builtins and fixed arity detection for constructors, added tests for both
",1437976088,
963871cadf5be49ff931215c9ec5ced88ed4bbd7 tags/0.7.0~85^2,"fix slicing optimization test
",1437611134,
9432207ca9deaa6f8aa2fad46a2d17a3938e0d9a tags/0.7.0~84^2,"support nrows in read_csv

This creates a dask DataFrame with a single partition with the
desired number of rows.

Some issues:

1.  This adds a significant amount of complexity
2.  This does not work with multiple csv files
",1437585195,
95de57a2c7eb63cea1f6950073b6420f8280aec8 tags/0.6.1~10,"Merge pull request #446 from mrocklin/dataframe-fixes-3

[WIP] Various fixes to dataframe",1437516699,
23b321bb851b7d0a966b29233acb9497acc6a662 tags/0.6.1~11^2~1,"Add `pure` kwarg to `do`

Can now specify if a wrapped function is pure or not (default is False).
This is important for functions like `random` or `time`.

Also changed `tokenize` to default to incremental token if hashing
fails. This means we don't do common-subexpression-elimination as
aggressively as we did earlier, but people shouldn't be relying on dask
preventing repeat computations anyway. This is probably a more sane
default than we had originally.
",1437513752,
0a27ceb78da42ab3ad764e2409ce4f2590da65d8 tags/0.6.1~12^2,"getarray handles newaxis for non-numpy arrays

Previously computations that uesd `np.newaxis` on top of array-like
objects like `h5py.Datasets` that didn't support it we got a runtime
error.  This offloads newaxis handling to the `getarray` function which
protects the downstream array objects from having to implement this.
",1437512070,
3e7e1890382d0f10d0fa6827c4c1eb7f8f886c79 tags/0.6.1~13,"Merge pull request #453 from mrocklin/long-slice

Fix long slicing from dask.keys() sorted issue",1437510476,
d4bb5638d4eab9758c4d8eed832d62d448416cf0 tags/0.6.1~13^2,"Fix long slicing from dask.keys() sorted issue

Fixes #452
",1437509514,
e9298ce9aa13f0c10e705f7192e8186bbc268efe tags/0.6.1~9^2,"Don't test conda.recipe/run_test.py with py.test

Before, when running py.test from the dask root directory, py.test would
try to ""test"" the conda.recipe/run_test.py file, and fail, since it is
not for unit-tests.
",1437445076,
08c60e6e4800592696c72b91f1564c2339079e26 tags/0.6.1~10^2~2,"fix windows dtype error
",1437423720,
319e1e869f36fe4457e3d7f44af36d269b08479e tags/0.6.1~16,"Merge pull request #444 from shoyer/fix-dot

Fix dot_graph",1437367463,
038b1550e5d557e85d73c96628393e4b078c77d4 tags/0.6.1~16^2,"Fix dot_graph

Use subprocess.check_call so we get an error message if the command
fails (e.g., because graphviz is not installed).

Actually generate pngs (not pdfs) so visualization in notebooks works.
",1437360592,
2378f7741fc98af58641d8f15aef4728d121a858 tags/0.6.1~18^2~4,"Check for closed socket.

This avoids the ""socket operation on non-socket"" errors we were seeing.
",1437150105,
1ab60a0a22bdbf2e93197e316826a55b0cb36639 tags/0.6.1~21,"fix broken link in distributed docs

Fixes #435

Thanks @jakirkham
",1437142818,
7dc2b2ef7f064b9231e0025afb0530df74525b41 tags/0.6.1~22^2,"README.rst: Switch to shields.io as pypip.in appears to be down ( https://github.com/badges/pypipins/issues/37 ) and shields.io has an SVG. [ci skip]
",1437137376,
7417b95120a60af88857dd81a07b72819e455fd1 tags/0.6.1~24,"Merge pull request #430 from mrocklin/raise-kwargs

Raise informative error on bad keyword arguments",1437093531,
9280fc00e91f0add1f94f54ae8d959c270185b1a tags/0.6.1~24^2,"raise informative kwarg error in atop
",1437075424,
4e7be4164cdbc551b30ead0c43f3750f5e59bdc7 tags/0.6.1~24^2~1,"Raise informative error on bad keyword arguments

Example
-------

In [1]: import dask.array as da

In [2]: x = da.ones(6, chunks=3)

In [3]: da.minimum(x, out=None)
TypeError: minimum does not take the following keyword arguments ['out']

Fixes #429
",1437075122,
6f8b3d61515a926555264cf3dd77f9b8981c1ab9 tags/0.6.1~26^2~2,"Raise when worker.collect fails to find key. Test.
",1436984638,
07c7b2e2261a66cfb79979763babff441d84bfbb tags/0.6.1~26^2~5,"Queue msg m['got_key'] -> m['status']

and the values changed from True/false to 'success'/'failed'
",1436981647,
10aa5069bc29221e17219cc734296af93bd9d8ed tags/0.6.1~31,"Merge pull request #425 from mrocklin/dataframe-fixes

A few small dataframe fixes",1436976289,
5405d79b7a5a1c022c848d226e983298946630a1 tags/0.6.1~30^2~2,"Improve tests for compose, doc fixes
",1436976031,
04ee1805ee3692418b4f659a4e888a1d13fa18ea tags/0.6.1~36,"Merge pull request #387 from mrocklin/async-debug

Add rerun_on_exception kwarg to async scheduler",1436886956,
4bbed51f62e1bb893846bd7065728a7dfd4281b7 tags/0.6.1~36^2~1,"expand rerun_exceptions_locally error message
",1436886420,
e64077d2f68a3fc3e4638d75c49bd0b72d18238f tags/0.6.1~36^2~6,"add rerun_on_exception kwarg to async scheduler

This reruns the failing task in the main process when an exception
occurs.  This enables the use of tools like ``pdb`` on parallel code
with errors.
",1436886373,
74fe4650c33442361390c45775f281a917e05b19 tags/0.6.1~26^2~26,"Resiliency bugs.
",1436827351,
985f9042a9e4a27644db4248cf0ea34758c7bf29 tags/0.6.1~26^2~28,"fix test_worker to work with heartbeat registration
",1436827351,
453a6cb7e06973a44a2aff4c00c9c4ff171c1886 tags/0.6.1~26^2~29,"fix test_scheduler to work w heartbeat registration
",1436827351,
3250c3fca0090e66757c634108b9b1ee65d48a71 tags/0.6.1~26^2~32,"fix worker to track keys being collected.
",1436827351,
42a1af94415da391fe41135202b8318c96d82078 tags/0.6.1~26^2~33,"fix tests
",1436827351,
551aa54092f343fe7e8d7a50ceaeac9af351454e tags/0.6.1~30^2~8,"Fix failing doctest
",1436826684,
f542a2fdf4ca6f85f2bad9e8ab4bafbaf8cc5ea5 tags/0.6.1~39,"Merge pull request #416 from mrocklin/chunks-error

Raise informative error on chunks=None",1436820980,
2099bf579a5359f6cd4bdb38aba72d5fc62f4999 tags/0.6.1~39^2~3,"Raise informative error on chunks=None

Fixes #415

Example
-------

In [1]: import dask.array as da

In [2]: da.random.random(size=(100, 100))
ValueError: You must specify a chunks= keyword argument.
This specifies the chunksize of your array blocks.

See the following documentation page for details:
  http://dask.pydata.org/en/latest/array-creation.html#chunks

cc @jdwarner
",1436646910,
788de3259c59f31eec2bce90de3ed37fedfdb0c7 tags/0.6.1~46,"fix typo
",1436475415,
8ad9a4cbb1e21a3b8bb28213d7137400b091b66e tags/0.6.1~57^2,"improve informative error around columns
",1436193010,
0ad5a0461f2f40b09c34ecd385112ea94019b508 tags/0.6.1~57^2~1,"raise value error on bad ndims
",1436192861,
5c19c4343d62ad71aacc50cfdc15e91e385f6314 tags/0.7.1~27^2~17,"Implement failure on df.index[0] not in our new divisions
",1436023440,
05517975c97189ea8e441c3bdb395ca78c6db4d9 tags/0.7.1~27^2~18,"Xfail on cases where our divisions do not match df.index[0]
",1436023413,
be394674bbe6d20f040e8f4fde1034c47160674d tags/0.7.1~27^2~30,"Add a different failing test case
",1435624969,
77496044ffd0e54ee5bf7a9e9fac07e9f7312eda tags/0.6.0~15^2~7,"fix test broken from heartbeat

test did not expect scheduler.workers dict to have 'last-seen' times in
it.
",1435616895,
85ab12917a64d8cb902130278a13f267e57e022c tags/0.6.0~17^2,"Replace use of curry with partial

Curry is more serializable but also opens us up to uncaught errors

Partial isn't very serializable but should be ok with dill.  Errors
are caught and raised more quickly.
",1435602579,
58c9e4d256ff73b4603bf0bdc14a89c781b33ba6 tags/0.6.0~23^2,"fix typos
",1435595590,
02d8a632c7ad8bc2319348021df47d4f20c4b461 tags/0.6.0~28,"Merge pull request #368 from mrocklin/windows-fixes2

Various Fixes for Windows Compatibility",1435376718,
685490080aab96f3fd79106605192492582264c6 tags/0.6.0~28^2,"fix doctest
",1435376536,
95a95c92f06828322e38c9ce09fa6284e2d75230 tags/0.6.0~16^2~12,"Cleanups for callbacks, few minor fixes

- Few minor cleanups for callbacks
- Removed extraneous imports for async
",1435246695,
1b97de500bfbc9a5392eaddce25abf9032fe39da tags/0.6.0~32^2~8,"Make subs robust to eq returning non-nonzero-able

Subs on DataFrame objects fails because

    >>> DataFrame() == 'x'
    DataFrame()

This wraps up the eq in a try-except block.  It's a bit ugly.
Impovements welcome.

cc @eriknw
",1435166379,
77a91b2086384f94cb13fb1856d8273523edbcf8 tags/0.6.0~38^2~1,"remove xfail from castra test
",1435076220,
3705d3106bd487d3bf08c4c184ce8f4ea793df46 tags/0.6.0~38^2~6,"fix redivide_divisions test
",1435071939,
263f2568888b6c7cdea916edc6dbb983a2600cca tags/0.6.0~41^2~3,"xfail castra test
",1435006826,
6e9eb724f9f2229d8bef2ca7e2f988a1672d71e3 tags/0.6.0~44^2~2,"fix doctests in dataframe/utils
",1434984344,
109cbc6bfe1cf417f2245d68298d93bfeb9436e1 tags/0.6.0~44^2~9,"fix read_csv index=... test
",1434918479,
60220c7bd5dd5df4134f6a1aecd548d5933e4f66 tags/0.6.0~50,"Merge pull request #339 from mrocklin/zmq-errors

Various fixes for dask.distributed",1434751986,
d1fe1e3fa20e11997f233d3d4dd4a5222e626f1d tags/0.6.0~51^2,"threadsafe pseudorandom function

Previously we were using

    np.random.seed(...)
    x = np.random.random(...)

But this would have failed if another call to a random function occured
between those two calls.

Now we use RandomState to handle this cleanly
",1434739176,
f8f518cd320e771786c18d814d4a4550b8f6f656 tags/0.6.0~56^2~6,"Add to and fix docstring
",1434585597,
a845161fe933d339008ec75a1153525afc48e5bd tags/0.6.0~55^2~4,"Catch when mantissa != 0, Don't block other errors.
",1434494041,
223adbbeef1c02a44a0e852c73f89bb8f5b89165 tags/0.6.0~58^2,"Small cleanups, pep8 fix
",1434410213,
4e38d47ca50bec6ba33e8701a208bf8a8ae0fdd5 tags/0.6.0~60,"Merge pull request #310 from jcrist/pandas_mean_name_fix

Keep series name in mean on SeriesGroupBy",1434407124,
2ee7204030f77b618457ff59f94b25fef9d48643 tags/0.6.0~65^2~1,"raise informative error if chest is not found
",1434140937,
25a44f69e8e721583ff3941c5c9209104f9d6f5d tags/0.6.0~66^2,"Merge pull request #301 from cowlicks/update-recipe

fix and update conda recipe",1434137661,
557791997543e8a79cd0e396629b4a880f92f174 tags/0.6.0~66^2~1^2,"Numpy compatibility fix, minor cleanups

- Added compatibility fix for numpy.divide until it works properly with
  dtypes. See numpy issue 3484.
- Add reference to moment paper in docstring
- Move custom getargspec to utils
",1434127812,
7ee287abdd95dff43b448ce2332adfc045c5f9e8 tags/0.6.0~66^2~2^2~4,"py34 fix with gzip write bytes
",1434121376,
328258475fc6b221fd64484870006bfeb89c47fc tags/0.6.0~66^2~2^2~12,"partd name is tuple to avoid sorting issues
",1434121340,
0bf9396210fa98d83eadc0606a78f8b914542939 tags/0.6.0~66^2~2^2~36,"dask.bag shuffle works multicore

partd has gc issues
",1434121223,
0748e883a8d6d2eaa472464da5350be8e3ce8aff tags/0.6.0~68^2~10,"Fix bz2 decompress stripping newlines bug
",1434036790,
e2790c0ee79a9c38c8881c16715b885fef0debc4 tags/0.6.0~70,"Merge pull request #295 from cowlicks/gh-210

Fix py2.7 always passing on travis bug.",1434014142,
d593586d924b26d7e857d8a915511f305c543ab0 tags/0.6.0~76^2~14,"Bug fixed
",1433683674,
9903a3f719a2aca46a0b9444b147b5498e95db02 tags/0.6.0~81^2,"fix typo in docstring
",1433453119,
0549d95fc54765c0def946b40abd1ff78d575d82 tags/0.6.0~89^2,"raise informative error on map_blocks arg switch
",1433275669,
61707c2e0f99454fddf3c9af7e470ee561bb7873 tags/0.6.0~90,"fix typo in doctest
",1433262982,
d63bef2e6675e8771f14f4fbc1ac22ce8bc6e1c2 tags/0.6.0~92^2~3,"fix rechunk_method test for valid chunk shape
",1433244240,
73cf47eeca49974c0c1819074e7b93052ee2a0e3 tags/0.6.0~94^2,"Correct error in test.
",1433187548,
fe8b882271fbc588b16a9dea6ce2179602427101 tags/0.6.0~95,"Merge pull request #256 from mrocklin/dot-docs

Dot error and docs",1433169850,
3b5e895b70f2552d6c29ab72876fbe621df4085d tags/0.6.0~95^2~1,"raise informative error when pydot is missing
",1433165141,
323ab507212b91e953f92fdc7da7a82059449ae9 tags/0.6.0~96,"Merge pull request #253 from esc/fix_typos

fix typos",1433100293,
1f979c20a48220f8571734f2c1c5e61eb263ea30 tags/0.6.0~96^2,"fix typos
",1433085136,
b9f43467705020cfca7728310603b4a15f0f7c87 tags/0.6.0~109,"Merge pull request #240 from mrocklin/travis-fix

move bcolz and chest to conda install",1432337854,
307c2140681f30a885db6dc00b6c0e551172fea4 tags/0.6.0~118,"Merge pull request #232 from mrocklin/zmq-fix

Fix zmq issues",1432234089,
35e05ef775619a3a26d28e1e4b6818294ea872bd tags/0.6.0~118^2~3,"worker test raises errors on bad router connections
",1432217863,
e0036c1a71f3d78be6ab28aced6fc7e116b3913a tags/0.6.0~115^2~3,"use more permanent github, fix exist test
",1432048249,
a9da4c9f842825939448fc5f4a0e26f2f1f1819f tags/0.6.0~126,"Merge pull request #219 from mrocklin/where-error

Informative error for where",1431966994,
f24e84f8e48f5a5363c083b6b0ac50a934f8ede7 tags/0.6.0~126^2,"add informative error message for da.where
",1431912901,
c769487bd67af85d2fe23aa4b86fcd6d625d10d6 tags/0.5.0~11,"Merge pull request #207 from mrocklin/windows-fixes

Windows fixes",1431557476,
49e1f859dbf44184c221f32078a897dca16255a5 tags/0.5.0~17^2~1,"Make scheduler test robust to slow runner

Some scheduler tests have race conditions that should resolve
themselves very very quickly (e.g. sleep(0.001)).  Of course this
approach is fraught and fails on very slow machines (like travis.ci
and binstar-build).

In many cases we have blocking logic built into the Scheduler/Worker
In some cases adding this just doesn't make sense.

This commit adds a poll/sleep loop to a scheduler test
",1431460733,
d434e50ce0a0e7cb56404cde59a4dd4eb3b512ed tags/0.5.0~18,"Merge pull request #203 from cowlicks/check-run-test-exit

Check run test exit value, and workaround conda-build bug",1431459254,
ba3fcb73f2ffca0732ff637947f072e91ba73229 tags/0.5.0~18^2,"Add default package version as workaround for conda-build bug
",1431457932,
c459d86ee4fc284a84b948556cecfb309df16179 tags/0.5.0~22^2~10,"more error logging in worker.py
",1431389892,
b49883601b83e437c4aa32f70b26c0bba7165899 tags/0.5.0~22^2~11,"better error reporting on scheduler/worker
",1431389892,
1e87300dcb9638ff2bcdec91db16e936a1a1bcd0 tags/0.5.0~24^2~1,"avoid empty dataframe(dtype=...) error
",1431377739,
a15a43bfad18fb45498bac05a44e222edbcede58 tags/0.5.0~33,"Merge pull request #182 from cowlicks/more-ghost-stuff

bug fixes is dask.array.ghost and make importable under numpy 1.6",1431097663,
421fe9e83b114c24dcfd1cf083de6c60cf71f737 tags/0.5.0~33^2~7,"Fix bug with constant boundaries.
",1431016159,
6351869cd63cd6da50245d4a08c64e1ad26353cf tags/0.5.0~33^2~8,"Fix bug with ghost and one chunk along axis
",1431016159,
0547b72ba30534ee09dcb5af59a7f4b947318ca4 tags/0.5.0~37^2~7,"Add failing tests for some depth=0 others nonzero
",1430765225,
7d411805e8fcfa0936bfd916f2b357662377da0b tags/0.5.0~37^2~13,"Fix NameError and spelling error
",1430750698,
b08f847283d2b6a8f9e2da990b0e946bae544f49 tags/0.5.0~37^2~16,"make fractional_slice return an empty slice

This was the intended behaviour but there was a bug.
",1430714210,
a8b4e0655290366c42377e96e4484317243746ce tags/0.5.0~37^2~22,"test_depth_equals_boundary_length now works

Previously there were errors when ghosting depth equaled chunk size.
",1430497975,
7c422444be96db3441dfa9c4c6c3409a1e31687d tags/0.5.0~39^2~4,"Scheduler doesn't record data in case of worker error
",1430358261,
8e5243e120a59163534e5bb7b002de29f6e7b852 tags/0.5.0~42,"Merge pull request #170 from cpcloud/fixes

Various fixes",1429819879,
007293fab0768c659a57047804ad09950dcdb1f1 tags/0.5.0~39^2~28,"fix random_names test
",1429810530,
4b50a08a807aae70bbae56381adbb4841cfc14ea tags/0.5.0~39^2~29,"fix scatter test
",1429810287,
8e4c57d34e31d3275facda851bf714e41845bbe2 tags/0.5.0~39^2~37,"fix doctest
",1429802512,
f81d5378d7a2a2560b7ff75ca453c217c812a0e5 tags/0.5.0~39^2~58,"small fixes to scheduler

1.  bring context into object state
2.  destroy context with linger=3
3.  Fix gethostname calls
",1429761464,
cee697dc58ef803c89e55d5b205854c16100d6af tags/0.4.0~9,"lingering dtype test issue
",1429552703,
42d7b23669aa95247d6cbd628888dfdc786957b3 tags/0.4.0~5^2^2~2,"failing test case for repeated insertion
",1429474825,
e7f8a15bb73b8975d1297034d022d86df0747e21 tags/0.4.0~16,"Merge pull request #159 from mrocklin/more-slicing

Some slicing fixes",1429470647,
5fe98329f018810ff9e7d5d2c8de8f59f260e898 tags/0.4.0~16^2~2,"fix doctest
",1429467427,
b49f0f235cbca9b4959593c7c5c9c071452da867 tags/0.4.0~16^2~7,"TST: exhaustive (currently failing) tests for dask.array slicing

Indexing has a lot of edge cases. This PR adds tests for many more of them.

Here is the particular case that I caught this morning because it broke
one of my tests for xray (which yes, are currently running against dask
master):

    assert eq(x[:, [0, 1, 2]][[0, 1]], a[:, [0, 1, 2]][[0, 1]]), i
",1429467427,
6c7231dd8e4feb6a262b737a9386eceaadf3bc58 tags/0.4.0~22^2,"fix doctest
",1429116108,
7d7fac55a87b2ac49290581f8fe6657d8b1cf8c7 tags/0.4.0~23,"Merge pull request #152 from mrocklin/various

Couple of small fixes",1429025460,
1c51d564748124f885ab69cda8e0ecf6fab2ebd5 tags/0.4.0~19^2~19,"fix groupby tests
",1429016276,
d3e6ac6b2a726f787b33da90d9a0517d39015e8c tags/0.4.0~25^2,"wraps respsects dtype kwargs

but now fails on dtype args
",1428338887,
3af964101472f39a72aad1cc2b6ce9ae9beba078 tags/0.4.0~22^2~3,"getarray ensures ndarray type for subclasses

and other various fixes
",1428337626,
9b5701d668e8ff42d02c7dfebebe2163168039ed tags/0.4.0~27^2,"da.compute always returns a tuple, never a single array

The current implementation of the compute function has an unpredictable
signature when provided with a programmatically determined sequence of arrays.
For example, `da.compute(*my_arrays)` returns a list of numpy arrays, except
when `my_arrays` has length one, in which case it returns a single array.

This sort of thing not only leads to all sorts of bugs, but also meant that
there was no way to reliable compute multiple dask arrays at once short of
using isinstance checks.

I considered trying to do something clever, like using `da.compute(my_arrays)`
(no `*`) as an indication that the result should always be returned a sequence,
and while that arguably makes the final API nice to use (nobody ever has to
type `*`!) it's not really worth the added complexity.

This solution is simpler: `da.compute` always returns a tuple of numpy arrays.
To return a single array, use `my_array.compute()`.
",1428274924,
9f1ce4d26ecb1cb1870b5c42da7f294a7cda5b0c tags/0.4.0~30^2,"fix doctest
",1428109151,
e5a50f8773dfdeeaaa4b250118a896394488ac44 tags/0.4.0~31,"informative error around empty percentile
",1428079901,
47ec5e8b3e3a188b926068a8380fa3432ae0c50d tags/0.4.0~33^2~1,"Fix tests. Mark arange failure cases with xfail.
",1428004625,
155c64cafd23684d551a2a92746afdb68a09a907 tags/0.4.0~41,"Merge pull request #129 from cowlicks/slice-stop

Fix slicing bug where slice.stop == 0",1427921106,
4a93a474107109cc973a4073deb6ec74b1d22af8 tags/0.4.0~41^2~1,"Fix bug where Array.compute() fails on empty arrays.

This came up fixing the indexing bug where the slice.stop == 0.
",1427919970,
d349a58cb89ae65805169a00fa72733410312122 tags/0.4.0~48^2,"Fix unpack singleton

It currently breaks on scalar datetime64 array, which incorrectly report
that they are Iterable. This is undoubtedly a numpy bug, but we need to
work around it, somehow...
",1427854789,
16fb34cdf19acbdfa202e31c9db1220e9fbf9214 tags/0.4.0~32^2,"`merge_sync` applies merge in topo-ordering

Previous ordering was hash dependent, and would fail with name conflicts
detected after inserting dependent terms.
",1427837975,
ee500b978fef6c5ef86559ff94ab2777f6709bb9 tags/0.4.0~50,"Merge pull request #112 from shoyer/around

misc fixes, including da.around and da.isclose",1427812824,
bdcf5c841250021fce92645182c63bc7048bab31 tags/0.4.0~49^2~1,"fix reblock doctests
",1427812720,
e3b6924cd60388234f1347bb9f43f3295ea79c1d tags/0.4.0~50^2~2,"Fix type coersion from dask array to bool, int, float and complex

Previously these failed with a variation on:

TypeError: __float__ returned non-float (type int)
",1427701159,
ebc0a0524d91275c66b51fdc7f2c67779390b5ed tags/0.4.0~57,"Merge pull request #102 from shoyer/fix-slicing-ndarray

Fix slicing dask arrays with ndarrays",1427507518,
af0f56b2f8dda9defaa9303b32d81b5410dc78e0 tags/0.4.0~57^2,"Fix slicing dask arrays with ndarrays

Previously, this raised the error:

	--> 619         if all(i == slice(None, None, None) for i in index):
	    620             return self
	    621

	ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
",1427506403,
a59777b76d21ff4ed91d656e0be04fb18e5d09d7 tags/0.4.0~58^2~5,"fix old doctests
",1427498845,
5c7d9d4757da6e2dfe5d0291946910fe5abc6d06 tags/0.4.0~58^2~9,"fix bcolz columnstore rule
",1427498845,
c7bfa9ddd3406c2f20e797cfbb1e2e292a8dcd43 tags/0.4.0~67^2,"raise value error if different number of sources/targets
",1427316309,
aadadbb5e7d86587a8a62a8a3848ca338b922881 tags/0.4.0~67^2~1,"Store many arrays simultaneously

Creates a top-level store function that accepts either a single
dask.array/output pair

    >>> store(x, out)

Or a list/tuple of such pairs

    >>> store([x, y, z], [out1, out2, out3])

Fixes https://github.com/ContinuumIO/dask/issues/84
cc @shoyer
",1427301723,
10630043dc25434d2ab9a38944d31f15a8593ad7 tags/0.4.0~71^2^2~1,"fix modf test
",1427299341,
d27db9941ec91f80f6693d33795d81cc353d4d1d tags/0.4.0~71^2^2~3,"fix astype test
",1427296203,
a5978366d8c6fc8475abdef42207dc6865a744ff tags/0.4.0~71^2^2~8,"add failing test for elemwise dtypes
",1427260318,
475ad60e045a7bcee5cd1a6300dffbd18215bf90 tags/0.4.0~73^2~3,"Dask.array tracks dtypes when possible

The Array class now holds a `_dtype` attribute.  Various dask.array functions
propagate dtype information, repeating a bit of numpy logic where necessary.

If this logic fails then we fall back on computation of a small element of the
dask array.

Fixes https://github.com/ContinuumIO/dask/issues/64
",1427216711,
f483d022c4df9f9304fa4c3a81c7b7ebca140687 tags/0.4.0~63^2~1,"Bug fixes, code cleanup

- Cleaned up some missed debugging code
- Cleaned up ``Traverser``, now allows iteration. Removed
  ``preorder_traversal``.
- Added ``END`` token to represent end of traversal. This is necessary,
  because the previously used ``None`` could also be an argument in an
  expression/traversal.
- Some minor speed/readability improvements
",1427204129,
7e81c62d34b8b5d4f8b5bddf0d356f971e466b9c tags/0.4.0~75^2,"Add isnull and notnull

The pandas functions are more robust than np.isnan (e.g., they don't
fail on integer dtypes).
",1427178486,
c939526fb2c6e71b65bcd4c1aa650c64a6ad9a58 tags/0.4.0~63^2~2,"Add syntactic rewriting

Allows for creation of a ``RuleSet`` of rewrite rules, and applying
these rules to a dask, or a single task. Matching is done syntactically
only, and assumes all functions have fixed arity.

Current capabilities:
- Bottom up rewriting of terms
- Lazy generation of matches from a matchset
- Nonlinear patterns (variables can be in a pattern more than once)
",1427172757,
68541403cfa60195682d6a9ec62d0032e91d8ee3 tags/0.4.0~66^2~34,"raise error on object dtype
",1426961567,
0274ec8bc277e49f6028d7b111ac66b1d2033ed7 tags/0.4.0~79^2,"np.array(dask.Array) works even with no shape

Example
-------

    In [1]: import dask.array as da

    In [2]: import numpy as np

    In [3]: x = da.ones(5, blockshape=(2,))

    In [4]: np.array(x.sum())
    Out[4]: array(5.0)

Fixes https://github.com/mrocklin/dask/issues/3
Reported by @rhattersley
",1426869250,
e4d7b3ee1897776720d72a876f3023791fb3f402 tags/0.4.0~80^2~13,"fix groupby issues

1.  Pass along column information
2.  support case with one partition
",1426606570,
a208fb729e7c5be4a64b446228829a5cf41058c1 tags/0.4.0~80^2~25,"fix from_array length
",1426520230,
635e4d1ddf50961fdc69a725f599a5f148958d7d tags/0.4.0~80^2~26,"fix from_array dask length
",1426215952,
9573c76b73b74c14fade9cc7d403647ec8ef799d tags/0.4.0~84,"Merge pull request #69 from mrocklin/raise-error-on-none-blockshape

Raise informative error on blockshape=None",1426109506,
808e1fb8dda6d5d02fde148edad71f024983340c tags/0.4.0~84^2,"Raise informative error on blockshape=None

Thanks @cowlicks for bringing this up
",1426107287,
f7439df9bb60e63ce98929ae595cb03220f3d9ef tags/0.4.0~80^2~67,"fix readlines parition counts
",1425849890,
c90dd840457e18c711271f68bfb4cc5a38e85dbf tags/0.4.0~49^2~12,"merge conficts fixed
",1425681527,
8a2a711ce0fa13add7103e6a6eefc081cdb1369b tags/0.3.0~16^2~13,"py3 fix
",1424973469,
b5459c9a57626a8480ad122853c0edc88d39842a tags/0.3.0~20,"remove debug false import line

cc #47
",1424797244,
1cc35f7bb3dbb781e365f0df496dae11a4a7d6fd tags/0.3.0~21,"fix hashable error
",1424797221,
d5772f803a2e9957a7fc04c3fefb7b017c39abf7 tags/0.3.0~27^2,"Better error reporting on axis=None
",1424551025,
2efcdd31c31027eac2a46c5cafcccc414b16130b tags/0.3.0~32,"doc: fix eye example to use delayed functions
",1424390578,
897859d19a57209f471b08a2cbf827c073fb7c90 tags/0.3.0~29^2~5,"Compatibility fixes for python 3 and python 2.6
",1424360771,
d44291b645293a744f007aa67c250fa09aea369b tags/0.2.1~1^2~2,"fix abs and neg
",1423867809,
0d4e5791ca4dd6de4e83426ec4c07fe95c5a989a tags/0.2.1~6^2,"Merge pull request #33 from mrocklin/get-sync

get_sync function for debugging",1423777222,
d3b3c34423576d010c0f12baa89899ba78c1b375 tags/0.2.1~6^2^2,"get_sync function for debugging
",1423775127,
c94ffc5a4d9ac9c47b7e09e8801dc898b46fefe8 tags/0.2.1~16^2~4,"fix doctests
",1423451165,
94329936c114d034a2be5d44aa1c791612734c7a tags/0.2.1~42,"fix array tests

1.  posify negative int/list entries
2.  fill out index with trailing :
",1423328160,
7a52686c1ea077eec18a7228f36e1d41976790dd tags/0.2.1~52^2~5,"fix doctests
",1423004320,
a1fcfa3efaf38990aca3cb5178be8312d52fbdd1 tags/0.2.1~54^2~6,"catch pickling errors
",1422902418,
b71db2d2d2d3e52bb44d6c981b0b44bf2b6b1568 tags/0.2.1~65,"fix error with empty flatten
",1422672247,
4dc49bb44885679271dc0b01276d3c5d610ef353 tags/0.2.1~60^2~2,"Added `getcycle`, which is like `isdag` but returns the first detected cycle if not DAG.

Also changed exception raised by non-recursive `get` to RuntimeError.  This
is the same error type that a recursive `get` would return (if `isdag` weren't
checked).  The exception is more informative and shows the nodes in the cycle.
",1422595820,
b825cc601c04ea3c55be90b832cd0671c05a98de tags/0.2.0~5,"don't fail if pydot isn't present
",1422490482,
0b8c4f2a44830102a634a565ce6d5893cac0ae3c tags/0.2.0~15^2~18,"Addressed most of the previous PR comments
 - non-slice index examples added to doctest
 - Reformatted and updated docstrings
 - tested against python 3.4 and fixed integer vs. float division problems
 - reformatted to have 80 max 80 columns per line in code, and max 72 columns per line in comments
Added optimization where if all slices are slice(None,None,None), the original object is returned.
",1421785536,
4d0de04cda71e0480d06fdc14041f6358a0fce59 tags/0.2.0~15^2~20,"Bugfix on block step offsets. Added comments and doctests.
Adding comments and doctests revealed some bugs in the previous code that didn't properly handle steps with offsets.
The updated code fixes these errors, but I'm not happy with it.
I think that the code can be significantly simpler and clearer, but I don't understand it well-enough yet.
As it stands, steps and starting offsets should be properly handled now.
",1421785536,
f8f7215ce7847401114eea5719c8f911ba81d8eb tags/0.2.0~16,"Merge pull request #10 from eriknw/cull

Add `cull` to remove unnecessary tasks.  See issue #2.",1421688205,
67b6d05af60b1e50646857eb890038c8e5c29592 tags/0.2.0~16^2~1,"Add `cull` to remove unnecessary tasks.  See issue #2.
",1421680398,
bf5ebe0e40d6a80f474cd1eaf9054bb437f51797 tags/0.2.0~24,"Merge pull request #6 from nevermindewe/memory_leak

This changeset fixes a problem where memory usage grows as all results a...",1421170432,
c83575eb885e54a9d9a41109bdc6e19ac7105478 tags/0.2.0~24^2,"This changeset fixes a problem where memory usage grows as all results are computed.
The desired behavior is for memory usage to remain small since this is designed for out-of-core computation.
execute_task no longer returns the results of the actual task execution. Instead it now relies solely on the queue that it puts results into.
A few comments have been updated, and some minor code clarifications have been added.
",1421166651,
e389d9bb892ba469167dafb46d90b9f6dd467141 tags/0.2.0~28,"debug code in threaded
",1420489048,
77d69cd826fdbad53a4750e48621ec941520eb71 tags/0.2.0~34,"fix bug in use of many
",1420397115,
83efca903e45ea0306ba253c82a7077757208299 tags/0.2.0~57,"capture and print tracebacks on errors
",1420218431,
61060ff63ed13bc151878cb6b9671482a56e0424 tags/0.2.0~67,"add visualization debug  logic
",1420065445,
aba8e6378be02dd141608e884f6a6293b8cdf49d tags/0.2.0~73,"better error reporting / debug info
",1420052574,
d52846ab8ff5a8f6572bff274e39950076a23213 tags/0.2.0~80,"Add lock and fix lost data from results
",1420043646,
413438348c3bc94ecd0567d50ca700d2b8e5dfe0 tags/0.2.0~93,"various changes

1.  various generators become list-comprehensions (for debug)
2.  get on iteratable yields generators (for lazy maps)
3.  Array top function

None of this has tests
",1419439797,
