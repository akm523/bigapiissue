0adc932add413a1754107b21d5ecfb38c0c3a4eb master,"[SPARK-1357 (fix)] remove empty line after :: DeveloperApi/Experimental ::

Remove empty line after :: DeveloperApi/Experimental :: in comments to make the original doc show up in the preview of the generated html docs. Thanks @andrewor14 !

Author: Xiangrui Meng <meng@databricks.com>

Closes #373 from mengxr/api and squashes the following commits:

9c35bdc [Xiangrui Meng] remove the empty line after :: DeveloperApi/Experimental ::
",1397088497
87bd1f9ef7d547ee54a8a83214b45462e0751efb master~3,"SPARK-1093: Annotate developer and experimental API's

This patch marks some existing classes as private[spark] and adds two types of API annotations:
- `EXPERIMENTAL API` = experimental user-facing module
- `DEVELOPER API - UNSTABLE` = developer-facing API that might change

There is some discussion of the different mechanisms for doing this here:
https://issues.apache.org/jira/browse/SPARK-1081

I was pretty aggressive with marking things private. Keep in mind that if we want to open something up in the future we can, but we can never reduce visibility.

A few notes here:
- In the past we've been inconsistent with the visiblity of the X-RDD classes. This patch marks them private whenever there is an existing function in RDD that can directly creat them (e.g. CoalescedRDD and rdd.coalesce()). One trade-off here is users can't subclass them.
- Noted that compression and serialization formats don't have to be wire compatible across versions.
- Compression codecs and serialization formats are semi-private as users typically don't instantiate them directly.
- Metrics sources are made private - user only interacts with them through Spark's reflection

Author: Patrick Wendell <pwendell@gmail.com>
Author: Andrew Or <andrewor14@gmail.com>

Closes #274 from pwendell/private-apis and squashes the following commits:

44179e4 [Patrick Wendell] Merge remote-tracking branch 'apache-github/master' into private-apis
042c803 [Patrick Wendell] spark.annotations -> spark.annotation
bfe7b52 [Patrick Wendell] Adding experimental for approximate counts
8d0c873 [Patrick Wendell] Warning in SparkEnv
99b223a [Patrick Wendell] Cleaning up annotations
e849f64 [Patrick Wendell] Merge pull request #2 from andrewor14/annotations
982a473 [Andrew Or] Generalize jQuery matching for non Spark-core API docs
a01c076 [Patrick Wendell] Merge pull request #1 from andrewor14/annotations
c1bcb41 [Andrew Or] DeveloperAPI -> DeveloperApi
0d48908 [Andrew Or] Comments and new lines (minor)
f3954e0 [Andrew Or] Add identifier tags in comments to work around scaladocs bug
99192ef [Andrew Or] Dynamically add badges based on annotations
824011b [Andrew Or] Add support for injecting arbitrary JavaScript to API docs
037755c [Patrick Wendell] Some changes after working with andrew or
f7d124f [Patrick Wendell] Small fixes
c318b24 [Patrick Wendell] Use CSS styles
e4c76b9 [Patrick Wendell] Logging
f390b13 [Patrick Wendell] Better visibility for workaround constructors
d6b0afd [Patrick Wendell] Small chang to existing constructor
403ba52 [Patrick Wendell] Style fix
870a7ba [Patrick Wendell] Work around for SI-8479
7fb13b2 [Patrick Wendell] Changes to UnionRDD and EmptyRDD
4a9e90c [Patrick Wendell] EXPERIMENTAL API --> EXPERIMENTAL
c581dce [Patrick Wendell] Changes after building against Shark.
8452309 [Patrick Wendell] Style fixes
1ed27d2 [Patrick Wendell] Formatting and coloring of badges
cd7a465 [Patrick Wendell] Code review feedback
2f706f1 [Patrick Wendell] Don't use floats
542a736 [Patrick Wendell] Small fixes
cf23ec6 [Patrick Wendell] Marking GraphX as alpha
d86818e [Patrick Wendell] Another naming change
5a76ed6 [Patrick Wendell] More visiblity clean-up
42c1f09 [Patrick Wendell] Using better labels
9d48cbf [Patrick Wendell] Initial pass
",1397031286
9689b663a2a4947ad60795321c770052f3c637f1 master~4,"[SPARK-1390] Refactoring of matrices backed by RDDs

This is to refactor interfaces for matrices backed by RDDs. It would be better if we have a clear separation of local matrices and those backed by RDDs. Right now, we have

1. `org.apache.spark.mllib.linalg.SparseMatrix`, which is a wrapper over an RDD of matrix entries, i.e., coordinate list format.
2. `org.apache.spark.mllib.linalg.TallSkinnyDenseMatrix`, which is a wrapper over RDD[Array[Double]], i.e. row-oriented format.

We will see naming collision when we introduce local `SparseMatrix`, and the name `TallSkinnyDenseMatrix` is not exact if we switch to `RDD[Vector]` from `RDD[Array[Double]]`. It would be better to have ""RDD"" in the class name to suggest that operations may trigger jobs.

The proposed names are (all under `org.apache.spark.mllib.linalg.rdd`):

1. `RDDMatrix`: trait for matrices backed by one or more RDDs
2. `CoordinateRDDMatrix`: wrapper of `RDD[(Long, Long, Double)]`
3. `RowRDDMatrix`: wrapper of `RDD[Vector]` whose rows do not have special ordering
4. `IndexedRowRDDMatrix`: wrapper of `RDD[(Long, Vector)]` whose rows are associated with indices

The current code also introduces local matrices.

Author: Xiangrui Meng <meng@databricks.com>

Closes #296 from mengxr/mat and squashes the following commits:

24d8294 [Xiangrui Meng] fix for groupBy returning Iterable
bfc2b26 [Xiangrui Meng] merge master
8e4f1f5 [Xiangrui Meng] Merge branch 'master' into mat
0135193 [Xiangrui Meng] address Reza's comments
03cd7e1 [Xiangrui Meng] add pca/gram to IndexedRowMatrix add toBreeze to DistributedMatrix for test simplify tests
b177ff1 [Xiangrui Meng] address Matei's comments
be119fe [Xiangrui Meng] rename m/n to numRows/numCols for local matrix add tests for matrices
b881506 [Xiangrui Meng] rename SparkPCA/SVD to TallSkinnyPCA/SVD
e7d0d4a [Xiangrui Meng] move IndexedRDDMatrixRow to IndexedRowRDDMatrix
0d1491c [Xiangrui Meng] fix test errors
a85262a [Xiangrui Meng] rename RDDMatrixRow to IndexedRDDMatrixRow
b8b6ac3 [Xiangrui Meng] Remove old code
4cf679c [Xiangrui Meng] port pca to RowRDDMatrix, and add multiply and covariance
7836e2f [Xiangrui Meng] initial refactoring of matrices backed by RDDs
",1397023275
fa0524fd02eedd0bbf1edc750dc3997a86ea25f5 master~5,"Spark-939: allow user jars to take precedence over spark jars

I still need to do a small bit of re-factoring [mostly the one Java file I'll switch it back to a Scala file and use it in both the close loaders], but comments on other things I should do would be great.

Author: Holden Karau <holden@pigscanfly.ca>

Closes #217 from holdenk/spark-939-allow-user-jars-to-take-precedence-over-spark-jars and squashes the following commits:

cf0cac9 [Holden Karau] Fix the executorclassloader
1955232 [Holden Karau] Fix long line in TestUtils
8f89965 [Holden Karau] Fix tests for new class name
7546549 [Holden Karau] CR feedback, merge some of the testutils methods down, rename the classloader
644719f [Holden Karau] User the class generator for the repl class loader tests too
f0b7114 [Holden Karau] Fix the core/src/test/scala/org/apache/spark/executor/ExecutorURLClassLoaderSuite.scala tests
204b199 [Holden Karau] Fix the generated classes
9f68f10 [Holden Karau] Start rewriting the ExecutorURLClassLoaderSuite to not use the hard coded classes
858aba2 [Holden Karau] Remove a bunch of test junk
261aaee [Holden Karau] simplify executorurlclassloader a bit
7a7bf5f [Holden Karau] CR feedback
d4ae848 [Holden Karau] rewrite component into scala
aa95083 [Holden Karau] CR feedback
7752594 [Holden Karau] re-add https comment
a0ef85a [Holden Karau] Fix style issues
125ea7f [Holden Karau] Easier to just remove those files, we don't need them
bb8d179 [Holden Karau] Fix issues with the repl class loader
241b03d [Holden Karau] fix my rat excludes
a343350 [Holden Karau] Update rat-excludes and remove a useless file
d90d217 [Holden Karau] Fix fall back with custom class loader and add a test for it
4919bf9 [Holden Karau] Fix parent calling class loader issue
8a67302 [Holden Karau] Test are good
9e2d236 [Holden Karau] It works comrade
691ee00 [Holden Karau] It works ish
dc4fe44 [Holden Karau] Does not depend on being in my home directory
47046ff [Holden Karau] Remove bad import'
22d83cb [Holden Karau] Add a test suite for the executor url class loader suite
7ef4628 [Holden Karau] Clean up
792d961 [Holden Karau] Almost works
16aecd1 [Holden Karau] Doesn't quite work
8d2241e [Holden Karau] Adda FakeClass for testing ClassLoader precedence options
648b559 [Holden Karau] Both class loaders compile. Now for testing
e1d9f71 [Holden Karau] One loader workers.
",1397021403
ce8ec5456169682f27f846e7b8d51e6c4bcf75e3 master~7,"Spark 1271: Co-Group and Group-By should pass Iterable[X]

Author: Holden Karau <holden@pigscanfly.ca>

Closes #242 from holdenk/spark-1320-cogroupandgroupshouldpassiterator and squashes the following commits:

f289536 [Holden Karau] Fix bad merge, should have been Iterable rather than Iterator
77048f8 [Holden Karau] Fix merge up to master
d3fe909 [Holden Karau] use toSeq instead
7a092a3 [Holden Karau] switch resultitr to resultiterable
eb06216 [Holden Karau] maybe I should have had a coffee first. use correct import for guava iterables
c5075aa [Holden Karau] If guava 14 had iterables
2d06e10 [Holden Karau] Fix Java 8 cogroup tests for the new API
11e730c [Holden Karau] Fix streaming tests
66b583d [Holden Karau] Fix the core test suite to compile
4ed579b [Holden Karau] Refactor from iterator to iterable
d052c07 [Holden Karau] Python tests now pass with iterator pandas
3bcd81d [Holden Karau] Revert ""Try and make pickling list iterators work""
cd1e81c [Holden Karau] Try and make pickling list iterators work
c60233a [Holden Karau] Start investigating moving to iterators for python API like the Java/Scala one. tl;dr: We will have to write our own iterator since the default one doesn't pickle well
88a5cef [Holden Karau] Fix cogroup test in JavaAPISuite for streaming
a5ee714 [Holden Karau] oops, was checking wrong iterator
e687f21 [Holden Karau] Fix groupbykey test in JavaAPISuite of streaming
ec8cc3e [Holden Karau] Fix test issues\!
4b0eeb9 [Holden Karau] Switch cast in PairDStreamFunctions
fa395c9 [Holden Karau] Revert ""Add a join based on the problem in SVD""
ec99e32 [Holden Karau] Revert ""Revert this but for now put things in list pandas""
b692868 [Holden Karau] Revert
7e533f7 [Holden Karau] Fix the bug
8a5153a [Holden Karau] Revert me, but we have some stuff to debug
b4e86a9 [Holden Karau] Add a join based on the problem in SVD
c4510e2 [Holden Karau] Revert this but for now put things in list pandas
b4e0b1d [Holden Karau] Fix style issues
71e8b9f [Holden Karau] I really need to stop calling size on iterators, it is the path of sadness.
b1ae51a [Holden Karau] Fix some of the types in the streaming JavaAPI suite. Probably still needs more work
37888ec [Holden Karau] core/tests now pass
249abde [Holden Karau] org.apache.spark.rdd.PairRDDFunctionsSuite passes
6698186 [Holden Karau] Revert ""I think this might be a bad rabbit hole. Started work to make CoGroupedRDD use iterator and then went crazy""
fe992fe [Holden Karau] hmmm try and fix up basic operation suite
172705c [Holden Karau] Fix Java API suite
caafa63 [Holden Karau] I think this might be a bad rabbit hole. Started work to make CoGroupedRDD use iterator and then went crazy
88b3329 [Holden Karau] Fix groupbykey to actually give back an iterator
4991af6 [Holden Karau] Fix some tests
be50246 [Holden Karau] Calling size on an iterator is not so good if we want to use it after
687ffbc [Holden Karau] This is the it compiles point of replacing Seq with Iterator and JList with JIterator in the groupby and cogroup signatures
",1397006159
fac6085cd774a4dba73ad1618537ef1817b2bcf3 master~9,"[SPARK-1397] Notify SparkListeners when stages fail or are cancelled.

[I wanted to post this for folks to comment but it depends on (and thus includes the changes in) a currently outstanding PR, #305.  You can look at just the second commit: https://github.com/kayousterhout/spark-1/commit/93f08baf731b9eaf5c9792a5373560526e2bccac to see just the changes relevant to this PR]

Previously, when stages fail or get cancelled, the SparkListener is only notified
indirectly through the SparkListenerJobEnd, where we sometimes pass in a single
stage that failed.  This worked before job cancellation, because jobs would only fail
due to a single stage failure.  However, with job cancellation, multiple running stages
can fail when a job gets cancelled.  Right now, this is not handled correctly, which
results in stages that get stuck in the “Running Stages” window in the UI even
though they’re dead.

This PR changes the SparkListenerStageCompleted event to a SparkListenerStageEnded
event, and uses this event to tell SparkListeners when stages fail in addition to when
they complete successfully.  This change is NOT publicly backward compatible for two
reasons.  First, it changes the SparkListener interface.  We could alternately add a new event,
SparkListenerStageFailed, and keep the existing SparkListenerStageCompleted.  However,
this is less consistent with the listener events for tasks / jobs ending, and will result in some
code duplication for listeners (because failed and completed stages are handled in similar
ways).  Note that I haven’t finished updating the JSON code to correctly handle the new event
because I’m waiting for feedback on whether this is a good or bad idea (hence the “WIP”).

It is also not backwards compatible because it changes the publicly visible JobWaiter.jobFailed()
method to no longer include a stage that caused the failure.  I think this change should definitely
stay, because with cancellation (as described above), a failure isn’t necessarily caused by a
single stage.

Author: Kay Ousterhout <kayousterhout@gmail.com>

Closes #309 from kayousterhout/stage_cancellation and squashes the following commits:

5533ecd [Kay Ousterhout] Fixes in response to Mark's review
320c7c7 [Kay Ousterhout] Notify SparkListeners when stages fail or are cancelled.
",1396993322
e25b593447a2e0aab9e5066f755e41be9068ecdc master~10,"SPARK-1445: compute-classpath should not print error if lib_managed not found

This was added to the check for the assembly jar, forgot it for the datanucleus jars.

Author: Aaron Davidson <aaron@databricks.com>

Closes #361 from aarondav/cc and squashes the following commits:

8facc16 [Aaron Davidson] SPARK-1445: compute-classpath should not print error if lib_managed not found
",1396993220
6dc5f5849c0e0378abc6648c919412827d831641 master~13,"[SPARK-1396] Properly cleanup DAGScheduler on job cancellation.

Previously, when jobs were cancelled, not all of the state in the
DAGScheduler was cleaned up, leading to a slow memory leak in the
DAGScheduler.  As we expose easier ways to cancel jobs, it's more
important to fix these issues.

This commit also fixes a second and less serious problem, which is that
previously, when a stage failed, not all of the appropriate stages
were cancelled.  See the ""failure of stage used by two jobs"" test
for an example of this.  This just meant that extra work was done, and is
not a correctness problem.

This commit adds 3 tests.  “run shuffle with map stage failure” is
a new test to more thoroughly test this functionality, and passes on
both the old and new versions of the code.  “trivial job
cancellation” fails on the old code because all state wasn’t cleaned
up correctly when jobs were cancelled (we didn’t remove the job from
resultStageToJob).  “failure of stage used by two jobs” fails on the
old code because taskScheduler.cancelTasks wasn’t called for one of
the stages (see test comments).

This should be checked in before #246, which makes it easier to
cancel stages / jobs.

Author: Kay Ousterhout <kayousterhout@gmail.com>

Closes #305 from kayousterhout/incremental_abort_fix and squashes the following commits:

f33d844 [Kay Ousterhout] Mark review comments
9217080 [Kay Ousterhout] Properly cleanup DAGScheduler on job cancellation.
",1396944213
83ac9a4bbf272028d0c4639cbd1e12022b9ae77a master~14,"[SPARK-1331] Added graceful shutdown to Spark Streaming

Current version of StreamingContext.stop() directly kills all the data receivers (NetworkReceiver) without waiting for the data already received to be persisted and processed. This PR provides the fix. Now, when the StreamingContext.stop() is called, the following sequence of steps will happen.
1. The driver will send a stop signal to all the active receivers.
2. Each receiver, when it gets a stop signal from the driver, first stop receiving more data, then waits for the thread that persists data blocks to BlockManager to finish persisting all receive data, and finally quits.
3. After all the receivers have stopped, the driver will wait for the Job Generator and Job Scheduler to finish processing all the received data.

It also fixes the semantics of StreamingContext.start and stop. It will throw appropriate errors and warnings if stop() is called before start(), stop() is called twice, etc.

Author: Tathagata Das <tathagata.das1565@gmail.com>

Closes #247 from tdas/graceful-shutdown and squashes the following commits:

61c0016 [Tathagata Das] Updated MIMA binary check excludes.
ae1d39b [Tathagata Das] Merge remote-tracking branch 'apache-github/master' into graceful-shutdown
6b59cfc [Tathagata Das] Minor changes based on Andrew's comment on PR.
d0b8d65 [Tathagata Das] Reduced time taken by graceful shutdown unit test.
f55bc67 [Tathagata Das] Fix scalastyle
c69b3a7 [Tathagata Das] Updates based on Patrick's comments.
c43b8ae [Tathagata Das] Added graceful shutdown to Spark Streaming.
",1396940417
11eabbe125b2ee572fad359c33c93f5e6fdf0b2d master~15,"[SPARK-1103] Automatic garbage collection of RDD, shuffle and broadcast data

This PR allows Spark to automatically cleanup metadata and data related to persisted RDDs, shuffles and broadcast variables when the corresponding RDDs, shuffles and broadcast variables fall out of scope from the driver program. This is still a work in progress as broadcast cleanup has not been implemented.

**Implementation Details**
A new class `ContextCleaner` is responsible cleaning all the state. It is instantiated as part of a `SparkContext`. RDD and ShuffleDependency classes have overridden `finalize()` function that gets called whenever their instances go out of scope. The `finalize()` function enqueues the object’s identifier (i.e. RDD ID, shuffle ID, etc.) with the `ContextCleaner`, which is a very short and cheap operation and should not significantly affect the garbage collection mechanism. The `ContextCleaner`, on a different thread, performs the cleanup, whose details are given below.

*RDD cleanup:*
`ContextCleaner` calls `RDD.unpersist()` is used to cleanup persisted RDDs. Regarding metadata, the DAGScheduler automatically cleans up all metadata related to a RDD after all jobs have completed. Only the `SparkContext.persistentRDDs` keeps strong references to persisted RDDs. The `TimeStampedHashMap` used for that has been replaced by `TimeStampedWeakValueHashMap` that keeps only weak references to the RDDs, allowing them to be garbage collected.

*Shuffle cleanup:*
New BlockManager message `RemoveShuffle(<shuffle ID>)` asks the `BlockManagerMaster` and currently active `BlockManager`s to delete all the disk blocks related to the shuffle ID. `ContextCleaner` cleans up shuffle data using this message and also cleans up the metadata in the `MapOutputTracker` of the driver. The `MapOutputTracker` at the workers, that caches the shuffle metadata, maintains a `BoundedHashMap` to limit the shuffle information it caches. Refetching the shuffle information from the driver is not too costly.

*Broadcast cleanup:*
To be done. [This PR](https://github.com/apache/incubator-spark/pull/543/) adds mechanism for explicit cleanup of broadcast variables. `Broadcast.finalize()` will enqueue its own ID with ContextCleaner and the PRs mechanism will be used to unpersist the Broadcast data.

*Other cleanup:*
`ShuffleMapTask` and `ResultTask` caches tasks and used TTL based cleanup (using `TimeStampedHashMap`), so nothing got cleaned up if TTL was not set. Instead, they now use `BoundedHashMap` to keep a limited number of map output information. Cost of repopulating the cache if necessary is very small.

**Current state of implementation**
Implemented RDD and shuffle cleanup. Things left to be done are.
- Cleaning up for broadcast variable still to be done.
- Automatic cleaning up keys with empty weak refs as values in `TimeStampedWeakValueHashMap`

Author: Tathagata Das <tathagata.das1565@gmail.com>
Author: Andrew Or <andrewor14@gmail.com>
Author: Roman Pastukhov <ignatich@mail.ru>

Closes #126 from tdas/state-cleanup and squashes the following commits:

61b8d6e [Tathagata Das] Fixed issue with Tachyon + new BlockManager methods.
f489fdc [Tathagata Das] Merge remote-tracking branch 'apache/master' into state-cleanup
d25a86e [Tathagata Das] Fixed stupid typo.
cff023c [Tathagata Das] Fixed issues based on Andrew's comments.
4d05314 [Tathagata Das] Scala style fix.
2b95b5e [Tathagata Das] Added more documentation on Broadcast implementations, specially which blocks are told about to the driver. Also, fixed Broadcast API to hide destroy functionality.
41c9ece [Tathagata Das] Added more unit tests for BlockManager, DiskBlockManager, and ContextCleaner.
6222697 [Tathagata Das] Fixed bug and adding unit test for removeBroadcast in BlockManagerSuite.
104a89a [Tathagata Das] Fixed failing BroadcastSuite unit tests by introducing blocking for removeShuffle and removeBroadcast in BlockManager*
a430f06 [Tathagata Das] Fixed compilation errors.
b27f8e8 [Tathagata Das] Merge pull request #3 from andrewor14/cleanup
cd72d19 [Andrew Or] Make automatic cleanup configurable (not documented)
ada45f0 [Andrew Or] Merge branch 'state-cleanup' of github.com:tdas/spark into cleanup
a2cc8bc [Tathagata Das] Merge remote-tracking branch 'apache/master' into state-cleanup
c5b1d98 [Andrew Or] Address Patrick's comments
a6460d4 [Andrew Or] Merge github.com:apache/spark into cleanup
762a4d8 [Tathagata Das] Merge pull request #1 from andrewor14/cleanup
f0aabb1 [Andrew Or] Correct semantics for TimeStampedWeakValueHashMap + add tests
5016375 [Andrew Or] Address TD's comments
7ed72fb [Andrew Or] Fix style test fail + remove verbose test message regarding broadcast
634a097 [Andrew Or] Merge branch 'state-cleanup' of github.com:tdas/spark into cleanup
7edbc98 [Tathagata Das] Merge remote-tracking branch 'apache-github/master' into state-cleanup
8557c12 [Andrew Or] Merge github.com:apache/spark into cleanup
e442246 [Andrew Or] Merge github.com:apache/spark into cleanup
88904a3 [Andrew Or] Make TimeStampedWeakValueHashMap a wrapper of TimeStampedHashMap
fbfeec8 [Andrew Or] Add functionality to query executors for their local BlockStatuses
34f436f [Andrew Or] Generalize BroadcastBlockId to remove BroadcastHelperBlockId
0d17060 [Andrew Or] Import, comments, and style fixes (minor)
c92e4d9 [Andrew Or] Merge github.com:apache/spark into cleanup
f201a8d [Andrew Or] Test broadcast cleanup in ContextCleanerSuite + remove BoundedHashMap
e95479c [Andrew Or] Add tests for unpersisting broadcast
544ac86 [Andrew Or] Clean up broadcast blocks through BlockManager*
d0edef3 [Andrew Or] Add framework for broadcast cleanup
ba52e00 [Andrew Or] Refactor broadcast classes
c7ccef1 [Andrew Or] Merge branch 'bc-unpersist-merge' of github.com:ignatich/incubator-spark into cleanup
6c9dcf6 [Tathagata Das] Added missing Apache license
d2f8b97 [Tathagata Das] Removed duplicate unpersistRDD.
a007307 [Tathagata Das] Merge remote-tracking branch 'apache/master' into state-cleanup
620eca3 [Tathagata Das] Changes based on PR comments.
f2881fd [Tathagata Das] Changed ContextCleaner to use ReferenceQueue instead of finalizer
e1fba5f [Tathagata Das] Style fix
892b952 [Tathagata Das] Removed use of BoundedHashMap, and made BlockManagerSlaveActor cleanup shuffle metadata in MapOutputTrackerWorker.
a7260d3 [Tathagata Das] Added try-catch in context cleaner and null value cleaning in TimeStampedWeakValueHashMap.
e61daa0 [Tathagata Das] Modifications based on the comments on PR 126.
ae9da88 [Tathagata Das] Removed unncessary TimeStampedHashMap from DAGScheduler, added try-catches in finalize() methods, and replaced ArrayBlockingQueue to LinkedBlockingQueue to avoid blocking in Java's finalizing thread.
cb0a5a6 [Tathagata Das] Fixed docs and styles.
a24fefc [Tathagata Das] Merge remote-tracking branch 'apache/master' into state-cleanup
8512612 [Tathagata Das] Changed TimeStampedHashMap to use WrappedJavaHashMap.
e427a9e [Tathagata Das] Added ContextCleaner to automatically clean RDDs and shuffles when they fall out of scope. Also replaced TimeStampedHashMap to BoundedHashMaps and TimeStampedWeakValueHashMap for the necessary hashmap behavior.
80dd977 [Roman Pastukhov] Fix for Broadcast unpersist patch.
1e752f1 [Roman Pastukhov] Added unpersist method to Broadcast.
",1396939236
0d0493fcf7fc86d30b0ddd4e2c5a293c5c88eb9d master~16,"[SPARK-1402]  Added 3 more compression schemes

JIRA issue: [SPARK-1402](https://issues.apache.org/jira/browse/SPARK-1402)

This PR provides 3 more compression schemes for Spark SQL in-memory columnar storage:

* `BooleanBitSet`
* `IntDelta`
* `LongDelta`

Now there are 6 compression schemes in total, including the no-op `PassThrough` scheme.

Also fixed a bug in PR #286: not all compression schemes are added as available schemes when accessing an in-memory column, and when a column is compressed with an unrecognised scheme, `ColumnAccessor` throws exception.

Author: Cheng Lian <lian.cs.zju@gmail.com>

Closes #330 from liancheng/moreCompressionSchemes and squashes the following commits:

1d037b8 [Cheng Lian] Fixed SPARK-1436: in-memory column byte buffer must be able to be accessed multiple times
d7c0e8f [Cheng Lian] Added test suite for IntegralDelta (IntDelta & LongDelta)
3c1ad7a [Cheng Lian] Added test suite for BooleanBitSet, refactored other test suites
44fe4b2 [Cheng Lian] Refactored CompressionScheme, added 3 more compression schemes.
",1396934652
a3c51c6ea2320efdeb2a6a5c1cd11d714f8994aa master~25,"SPARK-1432: Make sure that all metadata fields are properly cleaned

While working on spark-1337 with @pwendell, we noticed that not all of the metadata maps in JobProgessListener were being properly cleaned. This could lead to a (hypothetical) memory leak issue should a job run long enough. This patch aims to address the issue.

Author: Davis Shepherd <davis@conviva.com>

Closes #338 from dgshep/master and squashes the following commits:

a77b65c [Davis Shepherd] In the contex of SPARK-1337: Make sure that all metadata fields are properly cleaned
",1396890120
accd0999f9cb6a449434d3fc5274dd469eeecab2 master~27,"[SQL] SPARK-1371 Hash Aggregation Improvements

Given:
```scala
case class Data(a: Int, b: Int)
val rdd =
  sparkContext
    .parallelize(1 to 200)
    .flatMap(_ => (1 to 50000).map(i => Data(i % 100, i)))
rdd.registerAsTable(""data"")
cacheTable(""data"")
```
Before:
```
SELECT COUNT(*) FROM data:[10000000]
16795.567ms
SELECT a, SUM(b) FROM data GROUP BY a
7536.436ms
SELECT SUM(b) FROM data
10954.1ms
```

After:
```
SELECT COUNT(*) FROM data:[10000000]
1372.175ms
SELECT a, SUM(b) FROM data GROUP BY a
2070.446ms
SELECT SUM(b) FROM data
958.969ms
```

Author: Michael Armbrust <michael@databricks.com>

Closes #295 from marmbrus/hashAgg and squashes the following commits:

ec63575 [Michael Armbrust] Add comment.
d0495a9 [Michael Armbrust] Use scaladoc instead.
b4a6887 [Michael Armbrust] Address review comments.
a2d90ba [Michael Armbrust] Capture child output statically to avoid issues with generators and serialization.
7c13112 [Michael Armbrust] Rewrite Aggregate operator to stream input and use projections.  Remove unused local RDD functions implicits.
5096f99 [Michael Armbrust] Make HiveUDAF fields transient since object inspectors are not serializable.
6a4b671 [Michael Armbrust] Add option to avoid binding operators expressions automatically.
92cca08 [Michael Armbrust] Always include serialization debug info when running tests.
1279df2 [Michael Armbrust] Increase default number of partitions.
",1396854840
87d0928a3301835705652c24a26096546597e156 master~28,"SPARK-1431: Allow merging conflicting pull requests

Sometimes if there is a small conflict it's nice to be able to just
manually fix it up rather than have another RTT with the contributor.

Author: Patrick Wendell <pwendell@gmail.com>

Closes #342 from pwendell/merge-conflicts and squashes the following commits:

cdce61a [Patrick Wendell] SPARK-1431: Allow merging conflicting pull requests
",1396843485
1440154c27ca48b5a75103eccc9057286d3f6ca8 master~29,"SPARK-1154: Clean up app folders in worker nodes

This is a fix for [SPARK-1154](https://issues.apache.org/jira/browse/SPARK-1154).   The issue is that worker nodes fill up with a huge number of app-* folders after some time.  This change adds a periodic cleanup task which asynchronously deletes app directories older than a configurable TTL.

Two new configuration parameters have been introduced:
  spark.worker.cleanup_interval
  spark.worker.app_data_ttl

This change does not include moving the downloads of application jars to a location outside of the work directory.  We will address that if we have time, but that potentially involves caching so it will come either as part of this PR or a separate PR.

Author: Evan Chan <ev@ooyala.com>
Author: Kelvin Chu <kelvinkwchu@yahoo.com>

Closes #288 from velvia/SPARK-1154-cleanup-app-folders and squashes the following commits:

0689995 [Evan Chan] CR from @aarondav - move config, clarify for standalone mode
9f10d96 [Evan Chan] CR from @pwendell - rename configs and add cleanup.enabled
f2f6027 [Evan Chan] CR from @andrewor14
553d8c2 [Kelvin Chu] change the variable name to currentTimeMillis since it actually tracks in seconds
8dc9cb5 [Kelvin Chu] Fixed a bug in Utils.findOldFiles() after merge.
cb52f2b [Kelvin Chu] Change the name of findOldestFiles() to findOldFiles()
72f7d2d [Kelvin Chu] Fix a bug of Utils.findOldestFiles(). file.lastModified is returned in milliseconds.
ad99955 [Kelvin Chu] Add unit test for Utils.findOldestFiles()
dc1a311 [Evan Chan] Don't recompute current time with every new file
e3c408e [Evan Chan] Document the two new settings
b92752b [Evan Chan] SPARK-1154: Add a periodic task to clean up app directories
",1396837300
4106558435889261243d186f5f0b51c5f9e98d56 master~30,"SPARK-1314: Use SPARK_HIVE to determine if we include Hive in packaging

Previously, we based our decision regarding including datanucleus jars based on the existence of a spark-hive-assembly jar, which was incidentally built whenever ""sbt assembly"" is run. This means that a typical and previously supported pathway would start using hive jars.

This patch has the following features/bug fixes:

- Use of SPARK_HIVE (default false) to determine if we should include Hive in the assembly jar.
- Analagous feature in Maven with -Phive (previously, there was no support for adding Hive to any of our jars produced by Maven)
- assemble-deps fixed since we no longer use a different ASSEMBLY_DIR
- avoid adding log message in compute-classpath.sh to the classpath :)

Still TODO before mergeable:
- We need to download the datanucleus jars outside of sbt. Perhaps we can have spark-class download them if SPARK_HIVE is set similar to how sbt downloads itself.
- Spark SQL documentation updates.

Author: Aaron Davidson <aaron@databricks.com>

Closes #237 from aarondav/master and squashes the following commits:

5dc4329 [Aaron Davidson] Typo fixes
dd4f298 [Aaron Davidson] Doc update
dd1a365 [Aaron Davidson] Eliminate need for SPARK_HIVE at runtime by d/ling datanucleus from Maven
a9269b5 [Aaron Davidson] [WIP] Use SPARK_HIVE to determine if we include Hive in packaging
",1396831721
7ce52c4a7a07b0db5e7c1312b1920efb1165ce6a master~31,"SPARK-1349: spark-shell gets its own command history

Currently, spark-shell shares its command history with scala repl.

This fix is simply a modification of the default FileBackedHistory file setting:
https://github.com/scala/scala/blob/master/src/repl/scala/tools/nsc/interpreter/session/FileBackedHistory.scala#L77

Author: Aaron Davidson <aaron@databricks.com>

Closes #267 from aarondav/repl and squashes the following commits:

f9c62d2 [Aaron Davidson] SPARK-1349: spark-shell gets its own command history separate from scala repl
",1396831424
856c50f59bffbf76ad495eaab837febaf65cf02d master~32,"SPARK-1387. Update build plugins, avoid plugin version warning, centralize versions

Another handful of small build changes to organize and standardize a bit, and avoid warnings:

- Update Maven plugin versions for good measure
- Since plugins need maven 3.0.4 already, require it explicitly (<3.0.4 had some bugs anyway)
- Use variables to define versions across dependencies where they should move in lock step
- ... and make this consistent between Maven/SBT

OK, I also updated the JIRA URL while I was at it here.

Author: Sean Owen <sowen@cloudera.com>

Closes #291 from srowen/SPARK-1387 and squashes the following commits:

461eca1 [Sean Owen] Couldn't resist also updating JIRA location to new one
c2d5cc5 [Sean Owen] Update plugins and Maven version; use variables consistently across Maven/SBT to define dependency versions that should stay in step.
",1396831261
7012ffafad8fa876aa8bcb0b848445eec6734ef1 master~34,"Fix SPARK-1420 The maven build error for Spark Catalyst

Author: witgo <witgo@qq.com>

Closes #333 from witgo/SPARK-1420 and squashes the following commits:

902519e [witgo] add dependency scala-reflect to catalyst
",1396825386
6e88583aef7d8caf59d53c9fcb659a62d2cd6051 master~37,"[SPARK-1371] fix computePreferredLocations signature to not depend on underlying implementation

Change to Map and Set - not mutable HashMap and HashSet

Author: Mridul Muralidharan <mridulm80@apache.org>

Closes #302 from mridulm/master and squashes the following commits:

df747af [Mridul Muralidharan] Address review comments
17e2907 [Mridul Muralidharan] fix computePreferredLocations signature to not depend on underlying implementation
",1396736617
7c18428fac1403eb9c69b61890453964b255c432 master~39,"HOTFIX for broken CI, by SPARK-1336

Learnt about `set -o pipefail` is very useful.

Author: Prashant Sharma <prashant.s@imaginea.com>
Author: Prashant Sharma <scrapcodes@gmail.com>

Closes #321 from ScrapCodes/hf-SPARK-1336 and squashes the following commits:

9d22bc2 [Prashant Sharma] added comment why echo -e q exists.
f865951 [Prashant Sharma] made error to match with word boundry so errors does not match. This is there to make sure build fails if provided SparkBuild has compile errors.
7fffdf2 [Prashant Sharma] Removed a stray line.
97379d8 [Prashant Sharma] HOTFIX for broken CI, by SPARK-1336
",1396676959
0acc7a02b4323f4e0b7736bc1999bdcedab41f39 master~40,"small fix ( proogram -> program )

Author: Prabeesh K <prabsmails@gmail.com>

Closes #331 from prabeesh/patch-3 and squashes the following commits:

9399eb5 [Prabeesh K] small fix(proogram -> program)
",1396672320
b50ddfde0342990979979e58348f54c10b500c90 master~42,"SPARK-1305: Support persisting RDD's directly to Tachyon

Move the PR#468 of apache-incubator-spark to the apache-spark
""Adding an option to persist Spark RDD blocks into Tachyon.""

Author: Haoyuan Li <haoyuan@cs.berkeley.edu>
Author: RongGu <gurongwalker@gmail.com>

Closes #158 from RongGu/master and squashes the following commits:

72b7768 [Haoyuan Li] merge master
9f7fa1b [Haoyuan Li] fix code style
ae7834b [Haoyuan Li] minor cleanup
a8b3ec6 [Haoyuan Li] merge master branch
e0f4891 [Haoyuan Li] better check offheap.
55b5918 [RongGu] address matei's comment on the replication of offHeap storagelevel
7cd4600 [RongGu] remove some logic code for tachyonstore's replication
51149e7 [RongGu] address aaron's comment on returning value of the remove() function in tachyonstore
8adfcfa [RongGu] address arron's comment on inTachyonSize
120e48a [RongGu] changed the root-level dir name in Tachyon
5cc041c [Haoyuan Li] address aaron's comments
9b97935 [Haoyuan Li] address aaron's comments
d9a6438 [Haoyuan Li] fix for pspark
77d2703 [Haoyuan Li] change python api.git status
3dcace4 [Haoyuan Li] address matei's comments
91fa09d [Haoyuan Li] address patrick's comments
589eafe [Haoyuan Li] use TRY_CACHE instead of MUST_CACHE
64348b2 [Haoyuan Li] update conf docs.
ed73e19 [Haoyuan Li] Merge branch 'master' of github.com:RongGu/spark-1
619a9a8 [RongGu] set number of directories in TachyonStore back to 64; added a TODO tag for duplicated code from the DiskStore
be79d77 [RongGu] find a way to clean up some unnecessay metods and classed to make the code simpler
49cc724 [Haoyuan Li] update docs with off_headp option
4572f9f [RongGu] reserving the old apply function API of StorageLevel
04301d3 [RongGu] rename StorageLevel.TACHYON to Storage.OFF_HEAP
c9aeabf [RongGu] rename the StorgeLevel.TACHYON as StorageLevel.OFF_HEAP
76805aa [RongGu] unifies the config properties name prefix; add the configs into docs/configuration.md
e700d9c [RongGu] add the SparkTachyonHdfsLR example and some comments
fd84156 [RongGu] use randomUUID to generate sparkapp directory name on tachyon;minor code style fix
939e467 [Haoyuan Li] 0.4.1-thrift from maven central
86a2eab [Haoyuan Li] tachyon 0.4.1-thrift is in the staging repo. but jenkins failed to download it. temporarily revert it back to 0.4.1
16c5798 [RongGu] make the dependency on tachyon as tachyon-0.4.1-thrift
eacb2e8 [RongGu] Merge branch 'master' of https://github.com/RongGu/spark-1
bbeb4de [RongGu] fix the JsonProtocolSuite test failure problem
6adb58f [RongGu] Merge branch 'master' of https://github.com/RongGu/spark-1
d827250 [RongGu] fix JsonProtocolSuie test failure
716e93b [Haoyuan Li] revert the version
ca14469 [Haoyuan Li] bump tachyon version to 0.4.1-thrift
2825a13 [RongGu] up-merging to the current master branch of the apache spark
6a22c1a [Haoyuan Li] fix scalastyle
8968b67 [Haoyuan Li] exclude more libraries from tachyon dependency to be the same as referencing tachyon-client.
77be7e8 [RongGu] address mateiz's comment about the temp folder name problem. The implementation followed mateiz's advice.
1dcadf9 [Haoyuan Li] typo
bf278fa [Haoyuan Li] fix python tests
e82909c [Haoyuan Li] minor cleanup
776a56c [Haoyuan Li] address patrick's and ali's comments from the previous PR
8859371 [Haoyuan Li] various minor fixes and clean up
e3ddbba [Haoyuan Li] add doc to use Tachyon cache mode.
fcaeab2 [Haoyuan Li] address Aaron's comment
e554b1e [Haoyuan Li] add python code
47304b3 [Haoyuan Li] make tachyonStore in BlockMananger lazy val; add more comments StorageLevels.
dc8ef24 [Haoyuan Li] add old storelevel constructor
e01a271 [Haoyuan Li] update tachyon 0.4.1
8011a96 [RongGu] fix a brought-in mistake in StorageLevel
70ca182 [RongGu] a bit change in comment
556978b [RongGu] fix the scalastyle errors
791189b [RongGu] ""Adding an option to persist Spark RDD blocks into Tachyon."" move the PR#468 of apache-incubator-spark to the apache-spark
",1396669100
1347ebd4b52ffb9197fc4137a55dff6badb149ba master~43,"[SPARK-1419] Bumped parent POM to apache 14

Keeping up-to-date with the parent, which includes some bugfixes.

Author: Mark Hamstra <markhamstra@gmail.com>

Closes #328 from markhamstra/Apache14 and squashes the following commits:

3f19975 [Mark Hamstra] Bumped parent POM to apache 14
",1396664388
d956cc251676d67d87bd6dbfa82be864933d8136 master~46,"[SQL] Minor fixes.

Author: Michael Armbrust <michael@databricks.com>

Closes #315 from marmbrus/minorFixes and squashes the following commits:

b23a15d [Michael Armbrust] fix scaladoc
11062ac [Michael Armbrust] Fix registering ""SELECT *"" queries as tables and caching them.  As some tests for this and self-joins.
3997dc9 [Michael Armbrust] Move Row extractor to catalyst.
208bf5e [Michael Armbrust] More idiomatic naming of DSL functions. * subquery => as * for join condition => on, i.e., `r.join(s, condition = 'a == 'b)` =>`r.join(s, on = 'a == 'b)`
87211ce [Michael Armbrust] Correctly handle self joins of in-memory cached tables.
69e195e [Michael Armbrust] Change != to !== in the DSL since != will always translate to != on Any.
01f2dd5 [Michael Armbrust] Correctly assign aliases to tables in SqlParser.
",1396657397
198892fe8d39a2fad585fa2a7579d8b478456c33 master~47,"[SPARK-1198] Allow pipes tasks to run in different sub-directories

This works as is on Linux/Mac/etc but doesn't cover working on Windows.  In here I use ln -sf for symlinks. Putting this up for comments on that. Do we want to create perhaps some classes for doing shell commands - Linux vs Windows.  Is there some other way we want to do this?   I assume we are still supporting jdk1.6?

Also should I update the Java API for pipes to allow this parameter?

Author: Thomas Graves <tgraves@apache.org>

Closes #128 from tgravescs/SPARK1198 and squashes the following commits:

abc1289 [Thomas Graves] remove extra tag in pom file
ba23fc0 [Thomas Graves] Add support for symlink on windows, remove commons-io usage
da4b221 [Thomas Graves] Merge branch 'master' of https://github.com/tgravescs/spark into SPARK1198
61be271 [Thomas Graves] Fix file name filter
6b783bd [Thomas Graves] style fixes
1ab49ca [Thomas Graves] Add support for running pipe tasks is separate directories
",1396656991
f1fa617023d30d8cdc5acef0274bad8cc3e89cea master~50,"[SPARK-1133] Add whole text files reader in MLlib

Here is a pointer to the former [PR164](https://github.com/apache/spark/pull/164).

I add the pull request for the JIRA issue [SPARK-1133](https://spark-project.atlassian.net/browse/SPARK-1133), which brings a new files reader API in MLlib.

Author: Xusen Yin <yinxusen@gmail.com>

Closes #252 from yinxusen/whole-files-input and squashes the following commits:

7191be6 [Xusen Yin] refine comments
0af3faf [Xusen Yin] add JavaAPI test
01745ee [Xusen Yin] fix deletion error
cc97dca [Xusen Yin] move whole text file API to Spark core
d792cee [Xusen Yin] remove the typo character ""+""
6bdf2c2 [Xusen Yin] test for small local file system block size
a1f1e7e [Xusen Yin] add two extra spaces
28cb0fe [Xusen Yin] add whole text files reader
",1396635167
ee6e9e7d863022304ac9ced405b353b63accb6ab master~53,"SPARK-1337: Application web UI garbage collects newest stages

Simple fix...

Author: Patrick Wendell <pwendell@gmail.com>

Closes #320 from pwendell/stage-clean-up and squashes the following commits:

29be62e [Patrick Wendell] SPARK-1337: Application web UI garbage collects newest stages instead old ones
",1396588436
33e63618d061eeaae257a7350ea3287a702fc123 master~54,"Revert ""[SPARK-1398] Removed findbugs jsr305 dependency""

This reverts commit 92a86b285f8a4af1bdf577dd4c4ea0fd5ca8d682.
",1396569606
9231b011a9ba5a2b25bd3d1a68be7d1a7cb735da master~55,"Fix jenkins from giving the green light to builds that don't compile.

 Adding `| grep` swallows the non-zero return code from sbt failures. See [here](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13735/consoleFull) for a Jenkins run that fails to compile, but still gets a green light.

Note the [BUILD FIX] commit isn't actually part of this PR, but github is out of date.

Author: Michael Armbrust <michael@databricks.com>

Closes #317 from marmbrus/fixJenkins and squashes the following commits:

7c77ff9 [Michael Armbrust] Remove output filter that was swallowing non-zero exit codes for test failures.
",1396569215
a599e43d6e0950f6b6b32150ce264a8c2711470c master~57,"[SPARK-1134] Fix and document passing of arguments to IPython

This is based on @dianacarroll's previous pull request https://github.com/apache/spark/pull/227, and @joshrosen's comments on https://github.com/apache/spark/pull/38. Since we do want to allow passing arguments to IPython, this does the following:
* It documents that IPython can't be used with standalone jobs for now. (Later versions of IPython will deal with PYTHONSTARTUP properly and enable this, see https://github.com/ipython/ipython/pull/5226, but no released version has that fix.)
* If you run `pyspark` with `IPYTHON=1`, it passes your command-line arguments to it. This way you can do stuff like `IPYTHON=1 bin/pyspark notebook`.
* The old `IPYTHON_OPTS` remains, but I've removed it from the documentation. This is in case people read an old tutorial that uses it.

This is not a perfect solution and I'd also be okay with keeping things as they are today (ignoring `$@` for IPython and using IPYTHON_OPTS), and only doing the doc change. With this change though, when IPython fixes https://github.com/ipython/ipython/pull/5226, people will immediately be able to do `IPYTHON=1 bin/pyspark myscript.py` to run a standalone script and get all the benefits of running scripts in IPython (presumably better debugging and such). Without it, there will be no way to run scripts in IPython.

@joshrosen you should probably take the final call on this.

Author: Diana Carroll <dcarroll@cloudera.com>

Closes #294 from mateiz/spark-1134 and squashes the following commits:

747bb13 [Diana Carroll] SPARK-1134 bug with ipython prevents non-interactive use with spark; only call ipython if no command line arguments were supplied
",1396565322
c1ea3afb516c204925259f0928dfb17d0fa89621 master~59,"Spark 1162 Implemented takeOrdered in pyspark.

Since python does not have a library for max heap and usual tricks like inverting values etc.. does not work for all cases.

We have our own implementation of max heap.

Author: Prashant Sharma <prashant.s@imaginea.com>

Closes #97 from ScrapCodes/SPARK-1162/pyspark-top-takeOrdered2 and squashes the following commits:

35f86ba [Prashant Sharma] code review
2b1124d [Prashant Sharma] fixed tests
e8a08e2 [Prashant Sharma] Code review comments.
49e6ba7 [Prashant Sharma] SPARK-1162 added takeOrdered to pyspark
",1396564937
5d1feda217d25616d190f9bb369664e57417cd45 master~60,"[SPARK-1360] Add Timestamp Support for SQL

This PR includes:
1) Add new data type Timestamp
2) Add more data type casting base on Hive's Rule
3) Fix bug missing data type in both parsers (HiveQl & SQLParser).

Author: Cheng Hao <hao.cheng@intel.com>

Closes #275 from chenghao-intel/timestamp and squashes the following commits:

df709e5 [Cheng Hao] Move orc_ends_with_nulls to blacklist
24b04b0 [Cheng Hao] Put 3 cases into the black lists(describe_pretty,describe_syntax,lateral_view_outer)
fc512c2 [Cheng Hao] remove the unnecessary data type equality check in data casting
d0d1919 [Cheng Hao] Add more data type for scala reflection
3259808 [Cheng Hao] Add the new Golden files
3823b97 [Cheng Hao] Update the UnitTest cases & add timestamp type for HiveQL
54a0489 [Cheng Hao] fix bug mapping to 0 (which is supposed to be null) when NumberFormatException occurs
9cb505c [Cheng Hao] Fix issues according to PR comments
e529168 [Cheng Hao] Fix bug of converting from String
6fc8100 [Cheng Hao] Update Unit Test & CodeStyle
8a1d4d6 [Cheng Hao] Add DataType for SqlParser
ce4385e [Cheng Hao] Add TimestampType Support
",1396564397
92a86b285f8a4af1bdf577dd4c4ea0fd5ca8d682 master~62,"[SPARK-1398] Removed findbugs jsr305 dependency

Should be a painless upgrade, and does offer some significant advantages should we want to leverage FindBugs more during the 1.0 lifecycle. http://findbugs.sourceforge.net/findbugs2.html

Author: Mark Hamstra <markhamstra@gmail.com>

Closes #307 from markhamstra/findbugs and squashes the following commits:

99f2d09 [Mark Hamstra] Removed unnecessary findbugs jsr305 dependency
",1396559327
9c65fa76f9d413e311a80f29d35d3ff7722e9476 master~64,"[SPARK-1212, Part II] Support sparse data in MLlib

In PR https://github.com/apache/spark/pull/117, we added dense/sparse vector data model and updated KMeans to support sparse input. This PR is to replace all other `Array[Double]` usage by `Vector` in generalized linear models (GLMs) and Naive Bayes. Major changes:

1. `LabeledPoint` becomes `LabeledPoint(Double, Vector)`.
2. Methods that accept `RDD[Array[Double]]` now accept `RDD[Vector]`. We cannot support both in an elegant way because of type erasure.
3. Mark 'createModel' and 'predictPoint' protected because they are not for end users.
4. Add libSVMFile to MLContext.
5. NaiveBayes can accept arbitrary labels (introducing a breaking change to Python's `NaiveBayesModel`).
6. Gradient computation no longer creates temp vectors.
7. Column normalization and centering are removed from Lasso and Ridge because the operation will densify the data. Simple feature transformation can be done before training.

TODO:
1. ~~Use axpy when possible.~~
2. ~~Optimize Naive Bayes.~~

Author: Xiangrui Meng <meng@databricks.com>

Closes #245 from mengxr/vector and squashes the following commits:

eb6e793 [Xiangrui Meng] move libSVMFile to MLUtils and rename to loadLibSVMData
c26c4fc [Xiangrui Meng] update DecisionTree to use RDD[Vector]
11999c7 [Xiangrui Meng] Merge branch 'master' into vector
f7da54b [Xiangrui Meng] add minSplits to libSVMFile
da25e24 [Xiangrui Meng] revert the change to default addIntercept because it might change the behavior of existing code without warning
493f26f [Xiangrui Meng] Merge branch 'master' into vector
7c1bc01 [Xiangrui Meng] add a TODO to NB
b9b7ef7 [Xiangrui Meng] change default value of addIntercept to false
b01df54 [Xiangrui Meng] allow to change or clear threshold in LR and SVM
4addc50 [Xiangrui Meng] merge master
4ca5b1b [Xiangrui Meng] remove normalization from Lasso and update tests
f04fe8a [Xiangrui Meng] remove normalization from RidgeRegression and update tests
d088552 [Xiangrui Meng] use static constructor for MLContext
6f59eed [Xiangrui Meng] update libSVMFile to determine number of features automatically
3432e84 [Xiangrui Meng] update NaiveBayes to support sparse data
0f8759b [Xiangrui Meng] minor updates to NB
b11659c [Xiangrui Meng] style update
78c4671 [Xiangrui Meng] add libSVMFile to MLContext
f0fe616 [Xiangrui Meng] add a test for sparse linear regression
44733e1 [Xiangrui Meng] use in-place gradient computation
e981396 [Xiangrui Meng] use axpy in Updater
db808a1 [Xiangrui Meng] update JavaLR example
befa592 [Xiangrui Meng] passed scala/java tests
75c83a4 [Xiangrui Meng] passed test compile
1859701 [Xiangrui Meng] passed compile
834ada2 [Xiangrui Meng] optimized MLUtils.computeStats update some ml algorithms to use Vector (cont.)
135ab72 [Xiangrui Meng] merge glm
0e57aa4 [Xiangrui Meng] update Lasso and RidgeRegression to parse the weights correctly from GLM mark createModel protected mark predictPoint protected
d7f629f [Xiangrui Meng] fix a bug in GLM when intercept is not used
3f346ba [Xiangrui Meng] update some ml algorithms to use Vector
",1396472472
1faa57971192226837bea32eb29eae5bfb425a7e master~66,"[SPARK-1371][WIP] Compression support for Spark SQL in-memory columnar storage

JIRA issue: [SPARK-1373](https://issues.apache.org/jira/browse/SPARK-1373)

(Although tagged as WIP, this PR is structurally complete. The only things left unimplemented are 3 more compression algorithms: `BooleanBitSet`, `IntDelta` and `LongDelta`, which are trivial to add later in this or another separate PR.)

This PR contains compression support for Spark SQL in-memory columnar storage. Main interfaces include:

*   `CompressionScheme`

    Each `CompressionScheme` represents a concrete compression algorithm, which basically consists of an `Encoder` for compression and a `Decoder` for decompression. Algorithms implemented include:

    * `RunLengthEncoding`
    * `DictionaryEncoding`

    Algorithms to be implemented include:

    * `BooleanBitSet`
    * `IntDelta`
    * `LongDelta`

*   `CompressibleColumnBuilder`

    A stackable `ColumnBuilder` trait used to build byte buffers for compressible columns.  A best `CompressionScheme` that exhibits lowest compression ratio is chosen for each column according to statistical information gathered while elements are appended into the `ColumnBuilder`. However, if no `CompressionScheme` can achieve a compression ratio better than 80%, no compression will be done for this column to save CPU time.

    Memory layout of the final byte buffer is showed below:

    ```
     .--------------------------- Column type ID (4 bytes)
     |   .----------------------- Null count N (4 bytes)
     |   |   .------------------- Null positions (4 x N bytes, empty if null count is zero)
     |   |   |     .------------- Compression scheme ID (4 bytes)
     |   |   |     |   .--------- Compressed non-null elements
     V   V   V     V   V
    +---+---+-----+---+---------+
    |   |   | ... |   | ... ... |
    +---+---+-----+---+---------+
     \-----------/ \-----------/
        header         body
    ```

*   `CompressibleColumnAccessor`

    A stackable `ColumnAccessor` trait used to iterate (possibly) compressed data column.

*   `ColumnStats`

    Used to collect statistical information while loading data into in-memory columnar table. Optimizations like partition pruning rely on this information.

    Strictly speaking, `ColumnStats` related code is not part of the compression support. It's contained in this PR to ensure and validate the row-based API design (which is used to avoid boxing/unboxing cost whenever possible).

A major refactoring change since PR #205 is:

* Refactored all getter/setter methods for primitive types in various places into `ColumnType` classes to remove duplicated code.

Author: Cheng Lian <lian.cs.zju@gmail.com>

Closes #285 from liancheng/memColumnarCompression and squashes the following commits:

ed71bbd [Cheng Lian] Addressed all PR comments by @marmbrus
d3a4fa9 [Cheng Lian] Removed Ordering[T] in ColumnStats for better performance
5034453 [Cheng Lian] Bug fix, more tests, and more refactoring
c298b76 [Cheng Lian] Test suites refactored
2780d6a [Cheng Lian] [WIP] in-memory columnar compression support
211331c [Cheng Lian] WIP: in-memory columnar compression support
85cc59b [Cheng Lian] Refactored ColumnAccessors & ColumnBuilders to remove duplicate code
",1396468042
78236334e4ca7518b6d7d9b38464dbbda854a777 master~67,"Do not re-use objects in the EdgePartition/EdgeTriplet iterators.

This avoids a silent data corruption issue (https://spark-project.atlassian.net/browse/SPARK-1188) and has no performance impact by my measurements. It also simplifies the code. As far as I can tell the object re-use was nothing but premature optimization.

I did actual benchmarks for all the included changes, and there is no performance difference. I am not sure where to put the benchmarks. Does Spark not have a benchmark suite?

This is an example benchmark I did:

test(""benchmark"") {
  val builder = new EdgePartitionBuilder[Int]
  for (i <- (1 to 10000000)) {
    builder.add(i.toLong, i.toLong, i)
  }
  val p = builder.toEdgePartition
  p.map(_.attr + 1).iterator.toList
}

It ran for 10 seconds both before and after this change.

Author: Daniel Darabos <darabos.daniel@gmail.com>

Closes #276 from darabos/spark-1188 and squashes the following commits:

574302b [Daniel Darabos] Restore ""manual"" copying in EdgePartition.map(Iterator). Add comment to discourage novices like myself from trying to simplify the code.
4117a64 [Daniel Darabos] Revert EdgePartitionSuite.
4955697 [Daniel Darabos] Create a copy of the Edge objects in EdgeRDD.compute(). This avoids exposing the object re-use, while still enables the more efficient behavior for internal code.
4ec77f8 [Daniel Darabos] Add comments about object re-use to the affected functions.
2da5e87 [Daniel Darabos] Restore object re-use in EdgePartition.
0182f2b [Daniel Darabos] Do not re-use objects in the EdgePartition/EdgeTriplet iterators. This avoids a silent data corruption issue (SPARK-1188) and has no performance impact in my measurements. It also simplifies the code.
c55f52f [Daniel Darabos] Tests that reproduce the problems from SPARK-1188.
",1396466857
11973a7bdad58fdb759033c232d87f0b279c83b4 master~69,"Renamed stageIdToActiveJob to jobIdToActiveJob.

This data structure was misused and, as a result, later renamed to an incorrect name.

This data structure seems to have gotten into this tangled state as a result of @henrydavidge using the stageID instead of the job Id to index into it and later @andrewor14 renaming the data structure to reflect this misunderstanding.

This patch renames it and removes an incorrect indexing into it.  The incorrect indexing into it meant that the code added by @henrydavidge to warn when a task size is too large (added here https://github.com/apache/spark/commit/57579934f0454f258615c10e69ac2adafc5b9835) was not always executed; this commit fixes that.

Author: Kay Ousterhout <kayousterhout@gmail.com>

Closes #301 from kayousterhout/fixCancellation and squashes the following commits:

bd3d3a4 [Kay Ousterhout] Renamed stageIdToActiveJob to jobIdToActiveJob.
",1396460152
ea9de658a365dca2b7403d8fab68a8a87c4e06c8 master~70,"Remove * from test case golden filename.

@rxin mentioned this might cause issues on windows machines.

Author: Michael Armbrust <michael@databricks.com>

Closes #297 from marmbrus/noStars and squashes the following commits:

263122a [Michael Armbrust] Remove * from test case golden filename.
",1396421678
8b3045ceab591a3f3ca18823c7e2c5faca38a06e master~71,"MLI-1 Decision Trees

Joint work with @hirakendu, @etrain, @atalwalkar and @harsha2010.

Key features:
+ Supports binary classification and regression
+ Supports gini, entropy and variance for information gain calculation
+ Supports both continuous and categorical features

The algorithm has gone through several development iterations over the last few months leading to a highly optimized implementation. Optimizations include:

1. Level-wise training to reduce passes over the entire dataset.
2. Bin-wise split calculation to reduce computation overhead.
3. Aggregation over partitions before combining to reduce communication overhead.

Author: Manish Amde <manish9ue@gmail.com>
Author: manishamde <manish9ue@gmail.com>
Author: Xiangrui Meng <meng@databricks.com>

Closes #79 from manishamde/tree and squashes the following commits:

1e8c704 [Manish Amde] remove numBins field in the Strategy class
7d54b4f [manishamde] Merge pull request #4 from mengxr/dtree
f536ae9 [Xiangrui Meng] another pass on code style
e1dd86f [Manish Amde] implementing code style suggestions
62dc723 [Manish Amde] updating javadoc and converting helper methods to package private to allow unit testing
201702f [Manish Amde] making some more methods private
f963ef5 [Manish Amde] making methods private
c487e6a [manishamde] Merge pull request #1 from mengxr/dtree
24500c5 [Xiangrui Meng] minor style updates
4576b64 [Manish Amde] documentation and for to while loop conversion
ff363a7 [Manish Amde] binary search for bins and while loop for categorical feature bins
632818f [Manish Amde] removing threshold for classification predict method
2116360 [Manish Amde] removing dummy bin calculation for categorical variables
6068356 [Manish Amde] ensuring num bins is always greater than max number of categories
62c2562 [Manish Amde] fixing comment indentation
ad1fc21 [Manish Amde] incorporated mengxr's code style suggestions
d1ef4f6 [Manish Amde] more documentation
794ff4d [Manish Amde] minor improvements to docs and style
eb8fcbe [Manish Amde] minor code style updates
cd2c2b4 [Manish Amde] fixing code style based on feedback
63e786b [Manish Amde] added multiple train methods for java compatability
d3023b3 [Manish Amde] adding more docs for nested methods
84f85d6 [Manish Amde] code documentation
9372779 [Manish Amde] code style: max line lenght <= 100
dd0c0d7 [Manish Amde] minor: some docs
0dd7659 [manishamde] basic doc
5841c28 [Manish Amde] unit tests for categorical features
f067d68 [Manish Amde] minor cleanup
c0e522b [Manish Amde] updated predict and split threshold logic
b09dc98 [Manish Amde] minor refactoring
6b7de78 [Manish Amde] minor refactoring and tests
d504eb1 [Manish Amde] more tests for categorical features
dbb7ac1 [Manish Amde] categorical feature support
6df35b9 [Manish Amde] regression predict logic
53108ed [Manish Amde] fixing index for highest bin
e23c2e5 [Manish Amde] added regression support
c8f6d60 [Manish Amde] adding enum for feature type
b0e3e76 [Manish Amde] adding enum for feature type
154aa77 [Manish Amde] enums for configurations
733d6dd [Manish Amde] fixed tests
02c595c [Manish Amde] added command line parsing
98ec8d5 [Manish Amde] tree building and prediction logic
b0eb866 [Manish Amde] added logic to handle leaf nodes
80e8c66 [Manish Amde] working version of multi-level split calculation
4798aae [Manish Amde] added gain stats class
dad0afc [Manish Amde] decison stump functionality working
03f534c [Manish Amde] some more tests
0012a77 [Manish Amde] basic stump working
8bca1e2 [Manish Amde] additional code for creating intermediate RDD
92cedce [Manish Amde] basic building blocks for intermediate RDD calculation. untested.
cd53eae [Manish Amde] skeletal framework
",1396413649
afb5ea62786e3ca055e247176def3e7ecf0d2c9d master~73,"[Spark-1134] only call ipython if no arguments are given; remove IPYTHONOPTS from call

see comments on Pull Request https://github.com/apache/spark/pull/38
(i couldn't figure out how to modify an existing pull request, so I'm hoping I can withdraw that one and replace it with this one.)

Author: Diana Carroll <dcarroll@cloudera.com>

Closes #227 from dianacarroll/spark-1134 and squashes the following commits:

ffe47f2 [Diana Carroll] [spark-1134] remove ipythonopts from ipython command
b673bf7 [Diana Carroll] Merge branch 'master' of github.com:apache/spark
0309cf9 [Diana Carroll] SPARK-1134 bug with ipython prevents non-interactive use with spark; only call ipython if no command line arguments were supplied
",1396405766
ada310a9d3d5419e101b24d9b41398f609da1ad3 master~76,"[Hot Fix #42] Persisted RDD disappears on storage page if re-used

If a previously persisted RDD is re-used, its information disappears from the Storage page.

This is because the tasks associated with re-using the RDD do not report the RDD's blocks as updated (which is correct). On stage submit, however, we overwrite any existing information regarding that RDD with a fresh one, whether or not the information for the RDD already exists.

Author: Andrew Or <andrewor14@gmail.com>

Closes #281 from andrewor14/ui-storage-fix and squashes the following commits:

408585a [Andrew Or] Fix storage UI bug
",1396332074
33b3c2a8c6c71b89744834017a183ea855e1697c master~79,"SPARK-1365 [HOTFIX] Fix RateLimitedOutputStream test

This test needs to be fixed. It currently depends on Thread.sleep() having exact-timing
semantics, which is not a valid assumption.

Author: Patrick Wendell <pwendell@gmail.com>

Closes #277 from pwendell/rate-limited-stream and squashes the following commits:

6c0ff81 [Patrick Wendell] SPARK-1365: Fix RateLimitedOutputStream test
",1396308343
841721e03cc44ee7d8fe72c882db8c0f9f3af365 master~81,"SPARK-1352: Improve robustness of spark-submit script

1. Better error messages when required arguments are missing.
2. Support for unit testing cases where presented arguments are invalid.
3. Bug fix: Only use environment varaibles when they are set (otherwise will cause NPE).
4. A verbose mode to aid debugging.
5. Visibility of several variables is set to private.
6. Deprecation warning for existing scripts.

Author: Patrick Wendell <pwendell@gmail.com>

Closes #271 from pwendell/spark-submit and squashes the following commits:

9146def [Patrick Wendell] SPARK-1352: Improve robustness of spark-submit script
",1396292834
95d7d2a3fc2adc0bbca90d015c6ca319fffb26aa master~83,"[SPARK-1354][SQL] Add tableName as a qualifier for SimpleCatelogy

Fix attribute unresolved when query with table name as a qualifier in SQLContext with SimplCatelog, details please see [SPARK-1354](https://issues.apache.org/jira/browse/SPARK-1354?jql=project%20%3D%20SPARK).

Author: jerryshao <saisai.shao@intel.com>

Closes #272 from jerryshao/qualifier-fix and squashes the following commits:

7950170 [jerryshao] Add tableName as a qualifier for SimpleCatelogy
",1396199068
df1b9f7b1a07bf8d806695a7684f9d69bf705093 master~84,"SPARK-1336 Reducing the output of run-tests script.

Author: Prashant Sharma <prashant.s@imaginea.com>
Author: Prashant Sharma <scrapcodes@gmail.com>

Closes #262 from ScrapCodes/SPARK-1336/ReduceVerbosity and squashes the following commits:

87dfa54 [Prashant Sharma] Further reduction in noise and made pyspark tests to fail fast.
811170f [Prashant Sharma] Reducing the ouput of run-tests script.
",1396159383
92b83959cacbc902ff0b50110261f097bf2df247 master~86,"Don't swallow all kryo errors, only those that indicate we are out of data.

Author: Michael Armbrust <michael@databricks.com>

Closes #142 from marmbrus/kryoErrors and squashes the following commits:

9c72d1f [Michael Armbrust] Make the test more future proof.
78f5a42 [Michael Armbrust] Don't swallow all kryo errors, only those that indicate we are out of data.
",1396155689
af3746ce0d724dc624658a2187bde188ab26d084 master~88,"Implement the RLike & Like in catalyst

This PR includes:
1) Unify the unit test for expression evaluation
2) Add implementation of RLike & Like

Author: Cheng Hao <hao.cheng@intel.com>

Closes #224 from chenghao-intel/string_expression and squashes the following commits:

84f72e9 [Cheng Hao] fix bug in RLike/Like & Simplify the unit test
aeeb1d7 [Cheng Hao] Simplify the implementation/unit test of RLike/Like
319edb7 [Cheng Hao] change to spark code style
91cfd33 [Cheng Hao] add implementation for rlike/like
2c8929e [Cheng Hao] Update the unit test for expression evaluation
",1396131163
1617816090e7b20124a512a43860a21232ebf511 master~89,"SPARK-1126.  spark-app preliminary

This is a starting version of the spark-app script for running compiled binaries against Spark.  It still needs tests and some polish.  The only testing I've done so far has been using it to launch jobs in yarn-standalone mode against a pseudo-distributed cluster.

This leaves out the changes required for launching python scripts.  I think it might be best to save those for another JIRA/PR (while keeping to the design so that they won't require backwards-incompatible changes).

Author: Sandy Ryza <sandy@cloudera.com>

Closes #86 from sryza/sandy-spark-1126 and squashes the following commits:

d428d85 [Sandy Ryza] Commenting, doc, and import fixes from Patrick's comments
e7315c6 [Sandy Ryza] Fix failing tests
34de899 [Sandy Ryza] Change --more-jars to --jars and fix docs
299ddca [Sandy Ryza] Fix scalastyle
a94c627 [Sandy Ryza] Add newline at end of SparkSubmit
04bc4e2 [Sandy Ryza] SPARK-1126. spark-submit script
",1396129296
75d46be5d61fb92a6db2efb9e3a690716ef521d3 master~91,"fix path for jar, make sed actually work on OSX

Author: Nick Lanham <nick@afternight.org>

Closes #264 from nicklan/make-distribution-fixes and squashes the following commits:

172b981 [Nick Lanham] fix path for jar, make sed actually work on OSX
",1396038815
632c322036b123c6f72e0c8b87d50e08bec3a1ab master~93,"Make sed do -i '' on OSX

I don't have access to an OSX machine, so if someone could test this that would be great.

Author: Nick Lanham <nick@afternight.org>

Closes #258 from nicklan/osx-sed-fix and squashes the following commits:

a6f158f [Nick Lanham] Also make mktemp work on OSX
558fd6e [Nick Lanham] Make sed do -i '' on OSX
",1395985500
3d89043b7ed13bc1bb703f6eb7c00e46b936de1e master~94,"[SPARK-1210] Prevent ContextClassLoader of Actor from becoming ClassLoader of Executo...

...r.

Constructor of `org.apache.spark.executor.Executor` should not set context class loader of current thread, which is backend Actor's thread.

Run the following code in local-mode REPL.

```
scala> case class Foo(i: Int)
scala> val ret = sc.parallelize((1 to 100).map(Foo), 10).collect
```

This causes errors as follows:

```
ERROR actor.OneForOneStrategy: [L$line5.$read$$iwC$$iwC$$iwC$$iwC$Foo;
java.lang.ArrayStoreException: [L$line5.$read$$iwC$$iwC$$iwC$$iwC$Foo;
     at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88)
     at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870)
     at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:870)
     at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)
     at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:859)
     at org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:616)
     at org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)
     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
     at akka.actor.ActorCell.invoke(ActorCell.scala:456)
     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
     at akka.dispatch.Mailbox.run(Mailbox.scala:219)
     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
     at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
     at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
     at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
     at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
```

This is because the class loaders to deserialize result `Foo` instances might be different from backend Actor's, and the Actor's class loader should be the same as Driver's.

Author: Takuya UESHIN <ueshin@happy-camper.st>

Closes #15 from ueshin/wip/wrongcontextclassloader and squashes the following commits:

d79e8c0 [Takuya UESHIN] Change a parent class loader of ExecutorURLClassLoader.
c6c09b6 [Takuya UESHIN] Add a test to collect objects of class defined in repl.
43e0feb [Takuya UESHIN] Prevent ContextClassLoader of Actor from becoming ClassLoader of Executor.
",1395983835
6f986f0b87bd03f4df2bf6c917e61241e9b14ac2 master~95,"[SPARK-1268] Adding XOR and AND-NOT operations to spark.util.collection.BitSet

Symmetric difference (xor) in particular is useful for computing some distance metrics (e.g. Hamming). Unit tests added.

Author: Petko Nikolov <nikolov@soundcloud.com>

Closes #172 from petko-nikolov/bitset-imprv and squashes the following commits:

451f28b [Petko Nikolov] fixed style mistakes
5beba18 [Petko Nikolov] rm outer loop in andNot test
0e61035 [Petko Nikolov] conform to spark style; rm redundant asserts; more unit tests added; use arraycopy instead of loop
d53cdb9 [Petko Nikolov] rm incidentally added space
4e1df43 [Petko Nikolov] adding xor and and-not to BitSet; unit tests added
",1395960547
53953d0933c0a7c3bd3bc1003954426363912e4b master~96,"SPARK-1335. Also increase perm gen / code cache for scalatest when invoked via Maven build

I am observing build failures when the Maven build reaches tests in the new SQL components. (I'm on Java 7 / OSX 10.9). The failure is the usual complaint from scala, that it's out of permgen space, or that JIT out of code cache space.

I see that various build scripts increase these both for SBT. This change simply adds these settings to scalatest's arguments. Works for me and seems a bit more consistent.

(I also snuck in cures for new build warnings from new scaladoc. Felt too trivial for a new PR, although it's separate. Just something I also saw while examining the build output.)

Author: Sean Owen <sowen@cloudera.com>

Closes #253 from srowen/SPARK-1335 and squashes the following commits:

c0f2d31 [Sean Owen] Appease scalastyle with a newline at the end of the file
a02679c [Sean Owen] Fix scaladoc errors due to missing links, which are generating build warnings, from some recent doc changes. We apparently can't generate links outside the module.
b2c6a09 [Sean Owen] Add perm gen, code cache settings to scalatest, mirroring SBT settings elsewhere, which allows tests to complete in at least one environment where they are failing. (Also removed a duplicate -Xms setting elsewhere.)
",1395946529
d679843a39bb4918a08a5aebdf113ac8886a5275 master~99,"[SPARK-1327] GLM needs to check addIntercept for intercept and weights

GLM needs to check addIntercept for intercept and weights. The current implementation always uses the first weight as intercept. Added a test for training without adding intercept.

JIRA: https://spark-project.atlassian.net/browse/SPARK-1327

Author: Xiangrui Meng <meng@databricks.com>

Closes #236 from mengxr/glm and squashes the following commits:

bcac1ac [Xiangrui Meng] add two tests to ensure {Lasso, Ridge}.setIntercept will throw an exceptions
a104072 [Xiangrui Meng] remove protected to be compatible with 0.9
0e57aa4 [Xiangrui Meng] update Lasso and RidgeRegression to parse the weights correctly from GLM mark createModel protected mark predictPoint protected
d7f629f [Xiangrui Meng] fix a bug in GLM when intercept is not used
",1395887420
1fa48d9422d543827011eec0cdf12d060b78a7c7 master~100,"SPARK-1325. The maven build error for Spark Tools

This is just a slight variation on https://github.com/apache/spark/pull/234 and alternative suggestion for SPARK-1325. `scala-actors` is not necessary. `SparkBuild.scala` should be updated to reflect the direct dependency on `scala-reflect` and `scala-compiler`. And the `repl` build, which has the same dependencies, should also be consistent between Maven / SBT.

Author: Sean Owen <sowen@cloudera.com>
Author: witgo <witgo@qq.com>

Closes #240 from srowen/SPARK-1325 and squashes the following commits:

25bd7db [Sean Owen] Add necessary dependencies scala-reflect and scala-compiler to tools. Update repl dependencies, which are similar, to be consistent between Maven / SBT in this regard too.
",1395883934
3e63d98f09065386901d78c141b0da93cdce0f76 master~101,"Spark 1095 : Adding explicit return types to all public methods

Excluded those that are self-evident and the cases that are discussed in the mailing list.

Author: NirmalReddy <nirmal_reddy2000@yahoo.com>
Author: NirmalReddy <nirmal.reddy@imaginea.com>

Closes #168 from NirmalReddy/Spark-1095 and squashes the following commits:

ac54b29 [NirmalReddy] import misplaced
8c5ff3e [NirmalReddy] Changed syntax of unit returning methods
02d0778 [NirmalReddy] fixed explicit types in all the other packages
1c17773 [NirmalReddy] fixed explicit types in core package
",1395883495
32cbdfd2887f7a792f360ac3224f8c38cc97d21f master~104,"[SQL] Un-ignore a test that is now passing.

Add golden answer for aforementioned test.

Also, fix golden test generation from sbt/sbt by setting the classpath correctly.

Author: Michael Armbrust <michael@databricks.com>

Closes #244 from marmbrus/partTest and squashes the following commits:

37a33c9 [Michael Armbrust] Un-ignore a test that is now passing, add golden answer for aforementioned test.  Fix golden test generation from sbt/sbt.
",1395883155
345825d97987b9eeb2afcf002f815a05ff51fc2e master~105,"Unified package definition format in Spark SQL

According to discussions in comments of PR #208, this PR unifies package definition format in Spark SQL.

Some broken links in ScalaDoc and typos detected along the way are also fixed.

Author: Cheng Lian <lian.cs.zju@gmail.com>

Closes #225 from liancheng/packageDefinition and squashes the following commits:

75c47b3 [Cheng Lian] Fixed file line length
4f87968 [Cheng Lian] Unified package definition format in Spark SQL
",1395873378
8237df8060039af59eb387f5ea5d6611e8f3e526 master~109,"Avoid Option while generating call site

This is an update on https://github.com/apache/spark/pull/180, which changes the solution from blacklisting ""Option.scala"" to avoiding the Option code path while generating the call path.

Also includes a unit test to prevent this issue in the future, and some minor refactoring.

Thanks @witgo for reporting this issue and working on the initial solution!

Author: witgo <witgo@qq.com>
Author: Aaron Davidson <aaron@databricks.com>

Closes #222 from aarondav/180 and squashes the following commits:

f74aad1 [Aaron Davidson] Avoid Option while generating call site & add unit tests
d2b4980 [witgo] Modify the position of the filter
1bc22d7 [witgo] Fix Stage.name return ""apply at Option.scala:120""
",1395779293
5140598df889f7227c9d6a7953031eeef524badd master~115,"SPARK-1128: set hadoop task properties when constructing HadoopRDD

https://spark-project.atlassian.net/browse/SPARK-1128

The task properties are not set when constructing HadoopRDD in current implementation, this may limit the implementation based on

```
mapred.tip.id
mapred.task.id
mapred.task.is.map
mapred.task.partition
mapred.job.id
```

This patch also contains a small fix  in createJobID (SparkHadoopWriter.scala), where the current implementation actually is not using time parameter

Author: CodingCat <zhunansjtu@gmail.com>
Author: Nan Zhu <CodingCat@users.noreply.github.com>

Closes #101 from CodingCat/SPARK-1128 and squashes the following commits:

ed0980f [CodingCat] make SparkHiveHadoopWriter belongs to spark package
5b1ad7d [CodingCat] move SparkHiveHadoopWriter to org.apache.spark package
258f92c [CodingCat] code cleanup
af88939 [CodingCat] update the comments and permission of SparkHadoopWriter
9bd1fe3 [CodingCat] move configuration for jobConf to HadoopRDD
b7bdfa5 [Nan Zhu] style fix
a3153a8 [Nan Zhu] style fix
c3258d2 [CodingCat] set hadoop task properties while using InputFormat
",1395723303
dc126f2121d0cd1dc0caa50ae0c4cb9137d42562 master~116,"SPARK-1094 Support MiMa for reporting binary compatibility accross versions.

This adds some changes on top of the initial work by @scrapcodes in #20:

The goal here is to do automated checking of Spark commits to determine whether they break binary compatibility.

1. Special case for inner classes of package-private objects.
2. Made tools classes accessible when running `spark-class`.
3. Made some declared types in MLLib more general.
4. Various other improvements to exclude-generation script.
5. In-code documentation.

Author: Patrick Wendell <pwendell@gmail.com>
Author: Prashant Sharma <prashant.s@imaginea.com>
Author: Prashant Sharma <scrapcodes@gmail.com>

Closes #207 from pwendell/mima and squashes the following commits:

22ae267 [Patrick Wendell] New binary changes after upmerge
6c2030d [Patrick Wendell] Merge remote-tracking branch 'apache/master' into mima
3666cf1 [Patrick Wendell] Minor style change
0e0f570 [Patrick Wendell] Small fix and removing directory listings
647c547 [Patrick Wendell] Reveiw feedback.
c39f3b5 [Patrick Wendell] Some enhancements to binary checking.
4c771e0 [Prashant Sharma] Added a tool to generate mima excludes and also adapted build to pick automatically.
b551519 [Prashant Sharma] adding a new exclude after rebasing with master
651844c [Prashant Sharma] Support MiMa for reporting binary compatibility accross versions.
",1395721223
8043b7bc74ff3640743ffc3f1be386dc42f3f44c master~117,"SPARK-1294 Fix resolution of uppercase field names using a HiveContext.

Fixing this bug required the following:
 - Creation of a new logical node that converts a schema to lowercase.
 - Generalization of the subquery eliding rule to also elide this new node
 - Fixing of several places where too tight assumptions were made on the types of `InsertIntoTable` children.
 - I also removed an API that was left in by accident that exposed catalyst data structures, and fix the logic that pushes down filters into hive tables scans to correctly compare attribute references.

Author: Michael Armbrust <michael@databricks.com>

Closes #202 from marmbrus/upperCaseFieldNames and squashes the following commits:

15e5265 [Michael Armbrust] Support for resolving mixed case fields from a reflected schema using HiveQL.
5aa5035 [Michael Armbrust] Remove API that exposes internal catalyst data structures.
9d99cb6 [Michael Armbrust] Attributes should be compared using exprId, not TreeNode.id.
",1395714262
80c29689ae3b589254a571da3ddb5f9c866ae534 master~120,"[SPARK-1212] Adding sparse data support and update KMeans

Continue our discussions from https://github.com/apache/incubator-spark/pull/575

This PR is WIP because it depends on a SNAPSHOT version of breeze.

Per previous discussions and benchmarks, I switched to breeze for linear algebra operations. @dlwh and I made some improvements to breeze to keep its performance comparable to the bare-bone implementation, including norm computation and squared distance. This is why this PR needs to depend on a SNAPSHOT version of breeze.

@fommil , please find the notice of using netlib-core in `NOTICE`. This is following Apache's instructions on appropriate labeling.

I'm going to update this PR to include:

1. Fast distance computation: using `\|a\|_2^2 + \|b\|_2^2 - 2 a^T b` when it doesn't introduce too much numerical error. The squared norms are pre-computed. Otherwise, computing the distance between the center (dense) and a point (possibly sparse) always takes O(n) time.

2. Some numbers about the performance.

3. A released version of breeze. @dlwh, a minor release of breeze will help this PR get merged early. Do you mind sharing breeze's release plan? Thanks!

Author: Xiangrui Meng <meng@databricks.com>

Closes #117 from mengxr/sparse-kmeans and squashes the following commits:

67b368d [Xiangrui Meng] fix SparseVector.toArray
5eda0de [Xiangrui Meng] update NOTICE
67abe31 [Xiangrui Meng] move ArrayRDDs to mllib.rdd
1da1033 [Xiangrui Meng] remove dependency on commons-math3 and compute EPSILON directly
9bb1b31 [Xiangrui Meng] optimize SparseVector.toArray
226d2cd [Xiangrui Meng] update Java friendly methods in Vectors
238ba34 [Xiangrui Meng] add VectorRDDs with a converter from RDD[Array[Double]]
b28ba2f [Xiangrui Meng] add toArray to Vector
e69b10c [Xiangrui Meng] remove examples/JavaKMeans.java, which is replaced by mllib/examples/JavaKMeans.java
72bde33 [Xiangrui Meng] clean up code for distance computation
712cb88 [Xiangrui Meng] make Vectors.sparse Java friendly
27858e4 [Xiangrui Meng] update breeze version to 0.7
07c3cf2 [Xiangrui Meng] change Mahout to breeze in doc use a simple lower bound to avoid unnecessary distance computation
6f5cdde [Xiangrui Meng] fix a bug in filtering finished runs
42512f2 [Xiangrui Meng] Merge branch 'master' into sparse-kmeans
d6e6c07 [Xiangrui Meng] add predict(RDD[Vector]) to KMeansModel
42b4e50 [Xiangrui Meng] line feed at the end
a4ace73 [Xiangrui Meng] Merge branch 'fast-dist' into sparse-kmeans
3ed1a24 [Xiangrui Meng] add doc to BreezeVectorWithSquaredNorm
0107e19 [Xiangrui Meng] update NOTICE
87bc755 [Xiangrui Meng] tuned the KMeans code: changed some for loops to while, use view to avoid copying arrays
0ff8046 [Xiangrui Meng] update KMeans to use fastSquaredDistance
f355411 [Xiangrui Meng] add BreezeVectorWithSquaredNorm case class
ab74f67 [Xiangrui Meng] add fastSquaredDistance for KMeans
4e7d5ca [Xiangrui Meng] minor style update
07ffaf2 [Xiangrui Meng] add dense/sparse vector data models and conversions to/from breeze vectors use breeze to implement KMeans in order to support both dense and sparse data
",1395621242
8265dc7739caccc59bc2456b2df055ca96337fe4 master~121,"Fixed coding style issues in Spark SQL

This PR addresses various coding style issues in Spark SQL, including but not limited to those mentioned by @mateiz in PR #146.

As this PR affects lots of source files and may cause potential conflicts, it would be better to merge this as soon as possible *after* PR #205 (In-memory columnar representation for Spark SQL) is merged.

Author: Cheng Lian <lian.cs.zju@gmail.com>

Closes #208 from liancheng/fixCodingStyle and squashes the following commits:

fc2b528 [Cheng Lian] Merge branch 'master' into fixCodingStyle
b531273 [Cheng Lian] Fixed coding style issues in sql/hive
0b56f77 [Cheng Lian] Fixed coding style issues in sql/core
fae7b02 [Cheng Lian] Addressed styling issues mentioned by @marmbrus
9265366 [Cheng Lian] Fixed coding style issues in sql/core
3dcbbbd [Cheng Lian] Fixed relative package imports for package catalyst
",1395613300
57a4379c031e5d5901ba580422207d6aa2f19749 master~122,"[SPARK-1292] In-memory columnar representation for Spark SQL

This PR is rebased from the Catalyst repository, and contains the first version of in-memory columnar representation for Spark SQL. Compression support is not included yet and will be added later in a separate PR.

Author: Cheng Lian <lian@databricks.com>
Author: Cheng Lian <lian.cs.zju@gmail.com>

Closes #205 from liancheng/memColumnarSupport and squashes the following commits:

99dba41 [Cheng Lian] Restricted new objects/classes to `private[sql]'
0892ad8 [Cheng Lian] Addressed ScalaStyle issues
af1ad5e [Cheng Lian] Fixed some minor issues introduced during rebasing
0dbf2fb [Cheng Lian] Make necessary renaming due to rebase
a162d4d [Cheng Lian] Removed the unnecessary InMemoryColumnarRelation class
9bcae4b [Cheng Lian] Added Apache license
220ee1e [Cheng Lian] Added table scan operator for in-memory columnar support.
c701c7a [Cheng Lian] Using SparkSqlSerializer for generic object SerDe causes error, made a workaround
ed8608e [Cheng Lian] Added implicit conversion from DataType to ColumnType
b8a645a [Cheng Lian] Replaced KryoSerializer with an updated SparkSqlSerializer
b6c0a49 [Cheng Lian] Minor test suite refactoring
214be73 [Cheng Lian] Refactored BINARY and GENERIC to reduce duplicate code
da2f4d5 [Cheng Lian] Added Apache license
dbf7a38 [Cheng Lian] Added ColumnAccessor and test suite, refactored ColumnBuilder
c01a177 [Cheng Lian] Added column builder classes and test suite
f18ddc6 [Cheng Lian] Added ColumnTypes and test suite
2d09066 [Cheng Lian] Added KryoSerializer
34f3c19 [Cheng Lian] Added TypeTag field to all NativeTypes
acc5c48 [Cheng Lian] Added Hive test files to .gitignore
",1395601735
abf6714e27cf07a13819b35a4ca50ff9bb28b65c master~123,"SPARK-1254. Supplemental fix for HTTPS on Maven Central

It seems that HTTPS does not necessarily work on Maven Central, as it does not today at least. Back to HTTP. Both builds works from a clean repo.

Author: Sean Owen <sowen@cloudera.com>

Closes #209 from srowen/SPARK-1254Fix and squashes the following commits:

bb7be47 [Sean Owen] Revert to HTTP for Maven Central repo, as it seems HTTPS does not necessarily work
",1395597421
646e55405b433fdedc9601dab91f99832b641f87 master~124,"Fix to Stage UI to display numbers on progress bar

Fixes an issue on Stage UI to display numbers on progress bar which are today hidden behind the progress bar div. Please refer to the attached images to see the issue.
![screen shot 2014-03-21 at 4 48 46 pm](https://f.cloud.github.com/assets/563652/2489083/8c127e80-b153-11e3-807c-048ebd45104b.png)
![screen shot 2014-03-21 at 4 49 00 pm](https://f.cloud.github.com/assets/563652/2489084/8c12cf5c-b153-11e3-8747-9d93ff6fceb4.png)

Author: Emtiaz Ahmed <emtiazahmed@gmail.com>

Closes #201 from emtiazahmed/master and squashes the following commits:

a7964fe [Emtiaz Ahmed] Fix to Stage UI to display numbers on progress bar
",1395450353
dab5439a083b5f771d5d5b462d0d517fa8e9aaf2 master~126,"Make SQL keywords case-insensitive

This is a bit of a hack that allows all variations of a keyword, but it still seems to produce valid error messages and such.

Author: Matei Zaharia <matei@databricks.com>

Closes #193 from mateiz/case-insensitive-sql and squashes the following commits:

0ee4ace [Matei Zaharia] Removed unnecessary `+ """"`
e3ed773 [Matei Zaharia] Make SQL keywords case-insensitive
",1395445998
7e17fe69f9c3dc4cac024ea483f5d5f34ee06203 master~128,"Add hive test files to repository.  Remove download script.

This PR removes our test dependence on files hosted at Berkeley by checking the test queries and answers into the repository.  This should also fix the maven Jenkins build.

I realize this is a *giant* commit.  But size wise its actually pretty small.  We are only looking at ~1.2Mb compressed (~30Mb uncompressed).  Given that we already have a ~80Mb file permanently added to the spark code lineage, I do not think that this will change the developer experience significantly.

Furthermore, I think it is good engineering practice to consider such test support files as ""code"", since changes to them would indicate a change in functionality.  These files were only excluded from the initial PR as I wanted the diff to be readable.

Author: Michael Armbrust <michael@databricks.com>

Closes #199 from marmbrus/hiveTestFiles and squashes the following commits:

b9b9b17 [Michael Armbrust] Add hive test files to repository.  Remove download script.
",1395439545
e09139d9ca529a8f983a8b3e2a8158c3f3caa523 remotes/origin/vldb~14,"Fix maven jenkins: Add explicit init for required tables in SQLQuerySuite

Sorry! I added this test at the last minute and failed to run it in maven as well.

Note that, this will probably not be sufficient to actually fix the maven jenkins build, as that does not use the dev/run-tests scripts.  We will need to configure it to also run dev/download-hive-tests.sh.  The other option would be to check in the tests as I suggested in the original PR. (I can do this if we agree its the right thing to do).

Long term it would probably be a good idea to also have maven run some sort of test env setup script so that we can decouple the test environment from the jenkins configuration.

Author: Michael Armbrust <michael@databricks.com>

Closes #191 from marmbrus/fixMaven and squashes the following commits:

3366e37 [Michael Armbrust] Fix maven jenkins: Add explicit init for required tables in SQLQuerySuite
",1395379871
9aadcffabd226557174f3ff566927f873c71672e remotes/origin/vldb~15,"SPARK-1251 Support for optimizing and executing structured queries

This pull request adds support to Spark for working with structured data using a simple SQL dialect, HiveQL and a Scala Query DSL.

*This is being contributed as a new __alpha component__ to Spark and does not modify Spark core or other components.*

The code is broken into three primary components:
 - Catalyst (sql/catalyst) - An implementation-agnostic framework for manipulating trees of relational operators and expressions.
 - Execution (sql/core) - A query planner / execution engine for translating Catalyst’s logical query plans into Spark RDDs.  This component also includes a new public interface, SqlContext, that allows users to execute SQL or structured scala queries against existing RDDs and Parquet files.
 - Hive Metastore Support (sql/hive) - An extension of SqlContext called HiveContext that allows users to write queries using a subset of HiveQL and access data from a Hive Metastore using Hive SerDes.  There are also wrappers that allows users to run queries that include Hive UDFs, UDAFs, and UDTFs.

A more complete design of this new component can be found in [the associated JIRA](https://spark-project.atlassian.net/browse/SPARK-1251).

[An updated version of the Spark documentation, including API Docs for all three sub-components,](http://people.apache.org/~pwendell/catalyst-docs/sql-programming-guide.html) is also available for review.

With this PR comes support for inferring the schema of existing RDDs that contain case classes.  Using this information, developers can now express structured queries that are automatically compiled into RDD operations.

```scala
// Define the schema using a case class.
case class Person(name: String, age: Int)
val people: RDD[Person] =
  sc.textFile(""people.txt"").map(_.split("","")).map(p => Person(p(0), p(1).toInt))

// The following is the same as 'SELECT name FROM people WHERE age >= 10 && age <= 19'
val teenagers = people.where('age >= 10).where('age <= 19).select('name).toRdd
```

RDDs can also be registered as Tables, allowing SQL queries to be written over them.
```scala
people.registerAsTable(""people"")
val teenagers = sql(""SELECT name FROM people WHERE age >= 10 && age <= 19"")
```

The results of queries are themselves RDDs and support standard RDD operations:
```scala
teenagers.map(t => ""Name: "" + t(0)).collect().foreach(println)
```

Finally, with the optional Hive support, users can read and write data located in existing Apache Hive deployments using HiveQL.
```scala
sql(""CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"")
sql(""LOAD DATA LOCAL INPATH 'src/main/resources/kv1.txt' INTO TABLE src"")

// Queries are expressed in HiveQL
sql(""SELECT key, value FROM src"").collect().foreach(println)
```

## Relationship to Shark

Unlike Shark, Spark SQL does not act as a drop in replacement for Hive or the HiveServer. Instead this new feature is intended to make it easier for Spark developers to run queries over structured data, using either SQL or the query DSL. After this sub-project graduates from Alpha status it will likely become a new optimizer/backend for the Shark project.

Author: Michael Armbrust <michael@databricks.com>
Author: Yin Huai <huaiyin.thu@gmail.com>
Author: Reynold Xin <rxin@apache.org>
Author: Lian, Cheng <rhythm.mail@gmail.com>
Author: Andre Schumacher <andre.schumacher@iki.fi>
Author: Yin Huai <huai@cse.ohio-state.edu>
Author: Timothy Chen <tnachen@gmail.com>
Author: Cheng Lian <lian.cs.zju@gmail.com>
Author: Timothy Chen <tnachen@apache.org>
Author: Henry Cook <henry.m.cook+github@gmail.com>
Author: Mark Hamstra <markhamstra@gmail.com>

Closes #146 from marmbrus/catalyst and squashes the following commits:

458bd1b [Michael Armbrust] Update people.txt
0d638c3 [Michael Armbrust] Typo fix from @ash211.
bdab185 [Michael Armbrust] Address another round of comments: * Doc examples can now copy/paste into spark-shell. * SQLContext is serializable * Minor parser bugs fixed * Self-joins of RDDs now handled correctly. * Removed deprecated examples * Removed deprecated parquet docs * Made more of the API private * Copied all the DSLQuery tests and rewrote them as SQLQueryTests
778299a [Michael Armbrust] Fix some old links to spark-project.org
fead0b6 [Michael Armbrust] Create a new RDD type, SchemaRDD, that is now the return type for all SQL operations.  This improves the old API by reducing the number of implicits that are required, and avoids throwing away schema information when returning an RDD to the user.  This change also makes it slightly less verbose to run language integrated queries.
fee847b [Michael Armbrust] Merge remote-tracking branch 'origin/master' into catalyst, integrating changes to serialization for ShuffledRDD.
48a99bc [Michael Armbrust] Address first round of feedback.
461581c [Michael Armbrust] Blacklist test that depends on JVM specific rounding behaviour
adcf1a4 [Henry Cook] Update sql-programming-guide.md
9dffbfa [Michael Armbrust] Style fixes. Add downloading of test cases to jenkins.
6978dd8 [Michael Armbrust] update docs, add apache license
1d0eb63 [Michael Armbrust] update changes with spark core
e5e1d6b [Michael Armbrust] Remove travis configuration.
c2efad6 [Michael Armbrust] First draft of SQL documentation.
013f62a [Michael Armbrust] Fix documentation / code style.
c01470f [Michael Armbrust] Clean up example
2f22454 [Michael Armbrust] WIP: Parquet example.
ce8073b [Michael Armbrust] clean up implicits.
f7d992d [Michael Armbrust] Naming / spelling.
9eb0294 [Michael Armbrust] Bring expressions implicits into SqlContext.
d2d9678 [Michael Armbrust] Make sure hive isn't in the assembly jar.  Create a separate, optional Hive assembly that is used when present.
8b35e0a [Michael Armbrust] address feedback, work on DSL
5d71074 [Michael Armbrust] Merge pull request #62 from AndreSchumacher/parquet_file_fixes
f93aa39 [Andre Schumacher] Better handling of path names in ParquetRelation
1a4bbd9 [Michael Armbrust] Merge pull request #60 from marmbrus/maven
3386e4f [Michael Armbrust] Merge pull request #58 from AndreSchumacher/parquet_fixes
3447c3e [Michael Armbrust] Don't override the metastore / warehouse in non-local/test hive context.
7233a74 [Michael Armbrust] initial support for maven builds
f0ba39e [Michael Armbrust] Merge remote-tracking branch 'origin/master' into maven
7386a9f [Michael Armbrust] Initial example programs using spark sql.
aeaef54 [Andre Schumacher] Removing unnecessary Row copying and reverting some changes to MutableRow
7ca4b4e [Andre Schumacher] Improving checks in Parquet tests
5bacdc0 [Andre Schumacher] Moving towards mutable rows inside ParquetRowSupport
54637ec [Andre Schumacher] First part of second round of code review feedback
c2a658d [Michael Armbrust] Merge pull request #55 from marmbrus/mutableRows
ba28849 [Michael Armbrust] code review comments.
d994333 [Michael Armbrust] Remove copies before shuffle, this required changing the default shuffle serialization.
9049cf0 [Michael Armbrust] Extend MutablePair interface to support easy syntax for in-place updates.  Also add a constructor so that it can be serialized out-of-the-box.
959bdf0 [Michael Armbrust] Don't silently swallow all KryoExceptions, only the one that indicates the end of a stream.
d371393 [Michael Armbrust] Add a framework for dealing with mutable rows to reduce the number of object allocations that occur in the critical path.
c9f8fb3 [Michael Armbrust] Merge pull request #53 from AndreSchumacher/parquet_support
3c3f962 [Michael Armbrust] Fix a bug due to array reuse.  This will need to be revisited after we merge the mutable row PR.
7d0f13e [Michael Armbrust] Update parquet support with master.
9d419a6 [Michael Armbrust] Merge remote-tracking branch 'catalyst/catalystIntegration' into parquet_support
0040ae6 [Andre Schumacher] Feedback from code review
1ce01c7 [Michael Armbrust] Merge pull request #56 from liancheng/unapplySeqForRow
70e489d [Cheng Lian] Fixed a spelling typo
6d315bb [Cheng Lian] Added Row.unapplySeq to extract fields from a Row object.
8d5da5e [Michael Armbrust] modify compute-classpath.sh to include datanucleus jars explicitly
99e61fb [Michael Armbrust] Merge pull request #51 from marmbrus/expressionEval
7b9d142 [Michael Armbrust] Update travis to increase permgen size.
da9afbd [Michael Armbrust] Add byte wrappers for hive UDFS.
6fdefe6 [Michael Armbrust] Port sbt improvements from master.
296fe50 [Michael Armbrust] Address review feedback.
d7fbc3a [Michael Armbrust] Several performance enhancements and simplifications of the expression evaluation framework.
3bda72d [Andre Schumacher] Adding license banner to new files
3ac9eb0 [Andre Schumacher] Rebasing to new main branch
c863bed [Andre Schumacher] Codestyle checks
61e3bfb [Andre Schumacher] Adding WriteToFile operator and rewriting ParquetQuerySuite
3321195 [Andre Schumacher] Fixing one import in ParquetQueryTests.scala
3a0a552 [Andre Schumacher] Reorganizing Parquet table operations
18fdc44 [Andre Schumacher] Reworking Parquet metadata in relation and adding CREATE TABLE AS for Parquet tables
75262ee [Andre Schumacher] Integrating operations on Parquet files into SharkStrategies
f347273 [Andre Schumacher] Adding ParquetMetaData extraction, fixing schema projection
6a6bf98 [Andre Schumacher] Added column projections to ParquetTableScan
0f17d7b [Andre Schumacher] Rewriting ParquetRelation tests with RowWriteSupport
a11e364 [Andre Schumacher] Adding Parquet RowWriteSupport
6ad05b3 [Andre Schumacher] Moving ParquetRelation to spark.sql core
eb0e521 [Andre Schumacher] Fixing package names and other problems that came up after the rebase
99a9209 [Andre Schumacher] Expanding ParquetQueryTests to cover all primitive types
b33e47e [Andre Schumacher] First commit of Parquet import of primitive column types
c334386 [Michael Armbrust] Initial support for generating schema's based on case classes.
608a29e [Michael Armbrust] Add hive as a repl dependency
7413ac2 [Michael Armbrust] make test downloading quieter.
4d57d0e [Michael Armbrust] Fix test execution on travis.
5f2963c [Michael Armbrust] naming and continuous compilation fixes.
f5e7492 [Michael Armbrust] Add Apache license.  Make naming more consistent.
3ac9416 [Michael Armbrust] Merge support for working with schema-ed RDDs using catalyst in as a spark subproject.
2225431 [Michael Armbrust] Merge pull request #48 from marmbrus/minorFixes
d393d2a [Michael Armbrust] Review Comments: Add comment to map that adds a sub query.
24eaa79 [Michael Armbrust] fix > 100 chars
6e04e5b [Michael Armbrust] Add insertIntoTable to the DSL.
df88f01 [Michael Armbrust] add a simple test for aggregation
18a861b [Michael Armbrust] Correctly convert nested products into nested rows when turning scala data into catalyst data.
b922511 [Michael Armbrust] Fix insertion of nested types into hive tables.
5fe7de4 [Michael Armbrust] Move table creation out of rule into a separate function.
a430895 [Michael Armbrust] Planning for logical Repartition operators.
532dd37 [Michael Armbrust] Allow the local warehouse path to be specified.
4905b2b [Michael Armbrust] Add more efficient TopK that avoids global sort for logical Sort => StopAfter.
8c01c24 [Michael Armbrust] Move definition of Row out of execution to top level sql package.
c9116a6 [Michael Armbrust] Add combiner to avoid NPE when spark performs external aggregation.
29effad [Michael Armbrust] Include alias in attributes that are produced by overridden tables.
9990ec7 [Michael Armbrust] Merge pull request #28 from liancheng/columnPruning
f22df3a [Michael Armbrust] Merge pull request #37 from yhuai/SerDe
cf4db59 [Lian, Cheng] Added golden answers for PruningSuite
54f165b [Lian, Cheng] Fixed spelling typo in two golden answer file names
2682f72 [Lian, Cheng] Merge remote-tracking branch 'origin/master' into columnPruning
c5a4fab [Lian, Cheng] Merge branch 'master' into columnPruning
f670c8c [Yin Huai] Throw a NotImplementedError for not supported clauses in a CTAS query.
128a9f8 [Yin Huai] Minor changes.
017872c [Yin Huai] Remove stats20 from whitelist.
a1a4776 [Yin Huai] Update comments.
feb022c [Yin Huai] Partitioning key should be case insensitive.
555fb1d [Yin Huai] Correctly set the extension for a text file.
d00260b [Yin Huai] Strips backticks from partition keys.
334aace [Yin Huai] New golden files.
a40d6d6 [Yin Huai] Loading the static partition specified in a INSERT INTO/OVERWRITE query.
428aff5 [Yin Huai] Distinguish `INSERT INTO` and `INSERT OVERWRITE`.
eea75c5 [Yin Huai] Correctly set codec.
45ffb86 [Yin Huai] Merge remote-tracking branch 'upstream/master' into SerDeNew
e089627 [Yin Huai] Code style.
563bb22 [Yin Huai] Set compression info in FileSinkDesc.
35c9a8a [Michael Armbrust] Merge pull request #46 from marmbrus/reviewFeedback
bdab5ed [Yin Huai] Add a TODO for loading data into partitioned tables.
5495fab [Yin Huai] Remove cloneRecords which is no longer needed.
1596e1b [Yin Huai] Cleanup imports to make IntelliJ happy.
3bb272d [Michael Armbrust] move org.apache.spark.sql package.scala to the correct location.
8506c17 [Michael Armbrust] Address review feedback.
3cb4f2e [Michael Armbrust] Merge pull request #45 from tnachen/master
9ad474d [Michael Armbrust] Merge pull request #44 from marmbrus/sampling
566fd66 [Timothy Chen] Whitelist tests and add support for Binary type
69adf72 [Yin Huai] Set cloneRecords to false.
a9c3188 [Timothy Chen] Fix udaf struct return
346f828 [Yin Huai] Move SharkHadoopWriter to the correct location.
59e37a3 [Yin Huai] Merge remote-tracking branch 'upstream/master' into SerDeNew
ed3a1d1 [Yin Huai] Load data directly into Hive.
7f206b5 [Michael Armbrust] Add support for hive TABLESAMPLE PERCENT.
b6de691 [Michael Armbrust] Merge pull request #43 from liancheng/fixMakefile
1f6260d [Lian, Cheng] Fixed package name and test suite name in Makefile
5ae010f [Michael Armbrust] Merge pull request #42 from markhamstra/non-ascii
678341a [Mark Hamstra] Replaced non-ascii text
887f928 [Yin Huai] Merge remote-tracking branch 'upstream/master' into SerDeNew
1f7d00a [Reynold Xin] Merge pull request #41 from marmbrus/splitComponents
7588a57 [Michael Armbrust] Break into 3 major components and move everything into the org.apache.spark.sql package.
bc9a12c [Michael Armbrust] Move hive test files.
5720d2b [Lian, Cheng] Fixed comment typo
f0c3742 [Lian, Cheng] Refactored PhysicalOperation
f235914 [Lian, Cheng] Test case udf_regex and udf_like need BooleanWritable registered
cf691df [Lian, Cheng] Added the PhysicalOperation to generalize ColumnPrunings
2407a21 [Lian, Cheng] Added optimized logical plan to debugging output
a7ad058 [Michael Armbrust] Merge pull request #40 from marmbrus/includeGoldens
9329820 [Michael Armbrust] add golden answer files to repository
dce0593 [Michael Armbrust] move golden answer to the source code directory.
964368f [Michael Armbrust] Merge pull request #39 from marmbrus/lateralView
7785ee6 [Michael Armbrust] Tighten visibility based on comments.
341116c [Michael Armbrust] address comments.
0e6c1d7 [Reynold Xin] Merge pull request #38 from yhuai/parseDBNameInCTAS
2897deb [Michael Armbrust] fix scaladoc
7123225 [Yin Huai] Correctly parse the db name and table name in INSERT queries.
b376d15 [Michael Armbrust] fix newlines at EOF
5cc367c [Michael Armbrust] use berkeley instead of cloudbees
ff5ea3f [Michael Armbrust] new golden
db92adc [Michael Armbrust] more tests passing. clean up logging.
740febb [Michael Armbrust] Tests for tgfs.
0ce61b0 [Michael Armbrust] Docs for GenericHiveUdtf.
ba8897f [Michael Armbrust] Merge remote-tracking branch 'yin/parseDBNameInCTAS' into lateralView
dd00b7e [Michael Armbrust] initial implementation of generators.
ea76cf9 [Michael Armbrust] Add NoRelation to planner.
bea4b7f [Michael Armbrust] Add SumDistinct.
016b489 [Michael Armbrust] fix typo.
acb9566 [Michael Armbrust] Correctly type attributes of CTAS.
8841eb8 [Michael Armbrust] Rename Transform -> ScriptTransformation.
02ff8e4 [Yin Huai] Correctly parse the db name and table name in a CTAS query.
5e4d9b4 [Michael Armbrust] Merge pull request #35 from marmbrus/smallFixes
5479066 [Reynold Xin] Merge pull request #36 from marmbrus/partialAgg
8017afb [Michael Armbrust] fix copy paste error.
dc6353b [Michael Armbrust] turn off deprecation
cab1a84 [Michael Armbrust] Fix PartialAggregate inheritance.
883006d [Michael Armbrust] improve tests.
32b615b [Michael Armbrust] add override to asPartial.
e1999f9 [Yin Huai] Use Deserializer and Serializer instead of AbstractSerDe.
f94345c [Michael Armbrust] fix doc link
d8cb805 [Michael Armbrust] Implement partial aggregation.
ccdb07a [Michael Armbrust] Fix bug where averages of strings are turned into sums of strings.  Remove a blank line.
b4be6a5 [Michael Armbrust] better logging when applying rules.
67128b8 [Reynold Xin] Merge pull request #30 from marmbrus/complex
cb57459 [Michael Armbrust] blacklist machine specific test.
2f27604 [Michael Armbrust] Address comments / style errors.
389525d [Michael Armbrust] update golden, blacklist mr.
e3c10bd [Michael Armbrust] update whitelist.
44d343c [Michael Armbrust] Merge remote-tracking branch 'databricks/master' into complex
42ec4af [Michael Armbrust] improve complex type support in hive udfs/udafs.
ab5bff3 [Michael Armbrust] Support for get item of map types.
1679554 [Michael Armbrust] add toString for if and IS NOT NULL.
ab9a131 [Michael Armbrust] when UDFs fail they should return null.
25288d0 [Michael Armbrust] Implement [] for arrays and maps.
e7933e9 [Michael Armbrust] fix casting bug when working with fractional expressions.
010accb [Michael Armbrust] add tinyint to metastore type parser.
7a0f543 [Michael Armbrust] Avoid propagating types from unresolved nodes.
ac9d7de [Michael Armbrust] Resolve *s in Transform clauses.
692a477 [Michael Armbrust] Support for wrapping arrays to be written into hive tables.
92e4158 [Reynold Xin] Merge pull request #32 from marmbrus/tooManyProjects
9c06778 [Michael Armbrust] fix serialization issues, add JavaStringObjectInspector.
72a003d [Michael Armbrust] revert regex change
7661b6c [Michael Armbrust] blacklist machines specific tests
aa430e7 [Michael Armbrust] Update .travis.yml
e4def6b [Michael Armbrust] set dataType for HiveGenericUdfs.
5e54aa6 [Michael Armbrust] quotes for struct field names.
bbec500 [Michael Armbrust] update test coverage, new golden
3734a94 [Michael Armbrust] only quote string types.
3f9e519 [Michael Armbrust] use names w/ boolean args
5b3d2c8 [Michael Armbrust] implement distinct.
5b33216 [Michael Armbrust] work on decimal support.
2c6deb3 [Michael Armbrust] improve printing compatibility.
35a70fb [Michael Armbrust] multi-letter field names.
a9388fb [Michael Armbrust] printing for map types.
c3feda7 [Michael Armbrust] use toArray.
c654f19 [Michael Armbrust] Support for list and maps in hive table scan.
cf8d992 [Michael Armbrust] Use built in functions for creating temp directory.
1579eec [Michael Armbrust] Only cast unresolved inserts.
6420c7c [Michael Armbrust] Memoize the ordinal in the GetField expression.
da7ae9d [Michael Armbrust] Add boolean writable that was breaking udf_regexp test.  Not sure how this was passing before...
6709441 [Michael Armbrust] Evaluation for accessing nested fields.
dc6463a [Michael Armbrust] Support for resolving access to nested fields using ""."" notation.
d670e41 [Michael Armbrust] Print nested fields like hive does.
efa7217 [Michael Armbrust] Support for reading structs in HiveTableScan.
9c22b4e [Michael Armbrust] Support for parsing nested types.
82163e3 [Michael Armbrust] special case handling of partitionKeys when casting insert into tables
ea6f37f [Michael Armbrust] fix style.
7845364 [Michael Armbrust] deactivate concurrent test.
b649c20 [Michael Armbrust] fix test logging / caching.
1590568 [Michael Armbrust] add log4j.properties
19bfd74 [Michael Armbrust] store hive output in circular buffer
dfb67aa [Michael Armbrust] add test case
cb775ac [Michael Armbrust] get rid of SharkContext singleton
2de89d0 [Michael Armbrust] Merge pull request #13 from tnachen/master
63003e9 [Michael Armbrust] Fix spacing.
41b41f3 [Michael Armbrust] Only cast unresolved inserts.
6eb5960 [Michael Armbrust] Merge remote-tracking branch 'databricks/master' into udafs
5b7afd8 [Michael Armbrust] Merge pull request #10 from yhuai/exchangeOperator
b1151a8 [Timothy Chen] Fix load data regex
8e0931f [Michael Armbrust] Cast to avoid using deprecated hive API.
e079f2b [Timothy Chen] Add GenericUDAF wrapper and HiveUDAFFunction
45b334b [Yin Huai] fix comments
235cbb4 [Yin Huai] Merge remote-tracking branch 'upstream/master' into exchangeOperator
fc67b50 [Yin Huai] Check for a Sort operator with the global flag set instead of an Exchange operator with a RangePartitioning.
6015f93 [Michael Armbrust] Merge pull request #29 from rxin/style
271e483 [Michael Armbrust] Update build status icon.
d3a3d48 [Michael Armbrust] add testing to travis
807b2d7 [Michael Armbrust] check style and publish docs with travis
d20b565 [Michael Armbrust] fix if style
bce024d [Michael Armbrust] Merge remote-tracking branch 'databricks/master' into style Disable if brace checking as it errors in single line functional cases unlike the style guide.
d91e276 [Michael Armbrust] Remove dependence on HIVE_HOME for running tests.  This was done by moving all the hive query test (from branch-0.12) and data files into src/test/hive.  These are used by default when HIVE_HOME is not set.
f47c2f6 [Yin Huai] set outputPartitioning in BroadcastNestedLoopJoin
41bbee6 [Yin Huai] Merge remote-tracking branch 'upstream/master' into exchangeOperator
7e24436 [Reynold Xin] Removed dependency on JDK 7 (nio.file).
5c1e600 [Reynold Xin] Added hash code implementation for AttributeReference
7213a2c [Reynold Xin] style fix for Hive.scala.
08e4d05 [Reynold Xin] First round of style cleanup.
605255e [Reynold Xin] Added scalastyle checker.
61e729c [Lian, Cheng] Added ColumnPrunings strategy and test cases
2486fb7 [Lian, Cheng] Fixed spelling
8ee41be [Lian, Cheng] Minor refactoring
ebb56fa [Michael Armbrust] add travis config
4c89d6e [Reynold Xin] Merge pull request #27 from marmbrus/moreTests
d4f539a [Michael Armbrust] blacklist mr and user specific tests.
677eb07 [Michael Armbrust] Update test whitelist.
5dab0bc [Michael Armbrust] Merge pull request #26 from liancheng/serdeAndPartitionPruning
c263c84 [Michael Armbrust] Only push predicates into partitioned table scans.
ab77882 [Michael Armbrust] upgrade spark to RC5.
c98ede5 [Lian, Cheng] Response to comments from @marmbrus
83d4520 [Yin Huai] marmbrus's comments
70994a3 [Lian, Cheng] Revert unnecessary Scaladoc changes
9ebff47 [Yin Huai] remove unnecessary .toSeq
e811d1a [Yin Huai] markhamstra's comments
4802f69 [Yin Huai] The outputPartitioning of a UnaryNode inherits its child's outputPartitioning by default. Also, update the logic in AddExchange to avoid unnecessary shuffling operations.
040fbdf [Yin Huai] AddExchange is the only place to add Exchange operators.
9fb357a [Yin Huai] use getSpecifiedDistribution to create Distribution. ClusteredDistribution and OrderedDistribution do not take Nil as inptu expressions.
e9347fc [Michael Armbrust] Remove broken scaladoc links.
99c6707 [Michael Armbrust] upgrade spark
57799ad [Lian, Cheng] Added special treat for HiveVarchar in InsertIntoHiveTable
cb49af0 [Lian, Cheng] Fixed Scaladoc links
4e5e4d4 [Lian, Cheng] Added PreInsertionCasts to do necessary casting before insertion
111ffdc [Lian, Cheng] More comments and minor reformatting
9e0d840 [Lian, Cheng] Added partition pruning optimization
761bbb8 [Lian, Cheng] Generalized BindReferences to run against any query plan
04eb5da [Yin Huai] Merge remote-tracking branch 'upstream/master' into exchangeOperator
9dd3b26 [Michael Armbrust] Fix scaladoc.
6f44cac [Lian, Cheng] Made TableReader & HadoopTableReader private to catalyst
7c92a41 [Lian, Cheng] Added Hive SerDe support
ce5fdd6 [Yin Huai] Merge remote-tracking branch 'upstream/master' into exchangeOperator
2957f31 [Yin Huai] addressed comments on PR
907db68 [Michael Armbrust] Space after while.
04573a0 [Reynold Xin] Merge pull request #24 from marmbrus/binaryCasts
4e50679 [Reynold Xin] Merge pull request #25 from marmbrus/rowOrderingWhile
5bc1dc2 [Yin Huai] Merge remote-tracking branch 'upstream/master' into exchangeOperator
be1fff7 [Michael Armbrust] Replace foreach with while in RowOrdering. Fixes #23
fd084a4 [Michael Armbrust] implement casts binary <=> string.
0b31176 [Michael Armbrust] Merge pull request #22 from rxin/type
548e479 [Yin Huai] merge master into exchangeOperator and fix code style
5b11db0 [Reynold Xin] Added Void to Boolean type widening.
9e3d989 [Reynold Xin] Made HiveTypeCoercion.WidenTypes more clear.
9bb1979 [Reynold Xin] Merge pull request #19 from marmbrus/variadicUnion
a2beb38 [Michael Armbrust] Merge pull request #21 from liancheng/fixIssue20
b20a4d4 [Lian, Cheng] Fix issue #20
6d6cb58 [Michael Armbrust] add source links that point to github to the scala doc.
4285962 [Michael Armbrust] Remove temporary test cases
167162f [Michael Armbrust] more merge errors, cleanup.
e170ccf [Michael Armbrust] Improve documentation and remove some spurious changes that were introduced by the merge.
6377d0b [Michael Armbrust] Drop empty files, fix if ().
c0b0e60 [Michael Armbrust] cleanup broken doc links.
330a88b [Michael Armbrust] Fix bugs in AddExchange.
4f345f2 [Michael Armbrust] Remove SortKey, use RowOrdering.
043e296 [Michael Armbrust] Make physical union nodes variadic.
ece15e1 [Michael Armbrust] update unit tests
5c89d2e [Michael Armbrust] Merge remote-tracking branch 'databricks/master' into exchangeOperator Fix deprecated use of combineValuesByKey. Get rid of test where the answer is dependent on the plan execution width.
9804eb5 [Michael Armbrust] upgrade spark
053a371 [Michael Armbrust] Merge pull request #15 from marmbrus/orderedRow
5ab18be [Michael Armbrust] Merge remote-tracking branch 'databricks/master' into orderedRow
ca2ff68 [Michael Armbrust] Merge pull request #17 from marmbrus/unionTypes
bf9161c [Michael Armbrust] Merge pull request #18 from marmbrus/noSparkAgg
563053f [Michael Armbrust] Address @rxin's comments.
6537c66 [Michael Armbrust] Address @rxin's comments.
2a76fc6 [Michael Armbrust] add notes from @rxin.
685bfa1 [Michael Armbrust] fix spelling
69ed98f [Michael Armbrust] Output a single row for empty Aggregations with no grouping expressions.
7859a86 [Michael Armbrust] Remove SparkAggregate.  Its kinda broken and breaks RDD lineage.
fc22e01 [Michael Armbrust] whitelist newly passing union test.
3f547b8 [Michael Armbrust] Add support for widening types in unions.
53b95f8 [Michael Armbrust] coercion should not occur until children are resolved.
b892e32 [Michael Armbrust] Union is not resolved until the types match up.
95ab382 [Michael Armbrust] Use resolved instead of custom function.  This is better because some nodes override the notion of resolved.
81a109d [Michael Armbrust] fix link.
f143f61 [Michael Armbrust] Implement sampling.  Fixes a flaky test where the JVM notices that RAND as a Comparison method ""violates its general contract!""
6cd442b [Michael Armbrust] Use numPartitions variable, fix grammar.
c800798 [Michael Armbrust] Add build status icon.
0cf5a75 [Michael Armbrust] Merge pull request #16 from marmbrus/filterPushDown
05d3a0d [Michael Armbrust] Refactor to avoid serializing ordering details with every row.
f2fdd77 [Michael Armbrust] fix required distribtion for aggregate.
658866e [Michael Armbrust] Pull back in changes made by @yhuai eliminating CoGroupedLocallyRDD.scala
583a337 [Michael Armbrust] break apart distribution and partitioning.
e8d41a9 [Michael Armbrust] Merge remote-tracking branch 'yin/exchangeOperator' into exchangeOperator
0ff8be7 [Michael Armbrust] Cleanup spurious changes and fix doc links.
73c70de [Yin Huai] add a first set of unit tests for data properties.
fbfa437 [Michael Armbrust] Merge remote-tracking branch 'databricks/master' into filterPushDown Minor doc improvements.
2b9d80f [Yin Huai] initial commit of adding exchange operators to physical plans.
fcbc03b [Michael Armbrust] Fix if ().
7b9080c [Michael Armbrust] Create OrderedRow class to allow ordering to be used by multiple operators.
b4adb0f [Michael Armbrust] Merge pull request #14 from marmbrus/castingAndTypes
b2a1ec5 [Michael Armbrust] add comment on how using numeric implicitly complicates spark serialization.
e286d20 [Michael Armbrust] address code review comments.
80d0681 [Michael Armbrust] fix scaladoc links.
de0c248 [Michael Armbrust] Print the executed plan in SharkQuery toString.
3413e61 [Michael Armbrust] Add mapChildren and withNewChildren methods to TreeNode.
404d552 [Michael Armbrust] Better exception when unbound attributes make it to evaluation.
fb84ae4 [Michael Armbrust] Refactor DataProperty into Distribution.
2abb0bc [Michael Armbrust] better debug messages, use exists.
098dfc4 [Michael Armbrust] Implement Long sorting again.
60f3a9a [Michael Armbrust] More aggregate functions out of the aggregate class to make things more readable.
a1ef62e [Michael Armbrust] Print the executed plan in SharkQuery toString.
dfce426 [Michael Armbrust] Add mapChildren and withNewChildren methods to TreeNode.
037a2ed [Michael Armbrust] Better exception when unbound attributes make it to evaluation.
ec90620 [Michael Armbrust] Support for Sets as arguments to TreeNode classes.
b21f803 [Michael Armbrust] Merge pull request #11 from marmbrus/goldenGen
83adb9d [Yin Huai] add DataProperty
5a26292 [Michael Armbrust] Rules to bring casting more inline with Hive semantics.
f0e0161 [Michael Armbrust] Move numeric types into DataTypes simplifying evaluator.  This can probably also be use for codegen...
6d2924d [Michael Armbrust] add support for If. Not integrated in HiveQL yet.
ccc4dbf [Michael Armbrust] Add optimization rule to simplify casts.
058ec15 [Michael Armbrust] handle more writeables.
ffa9f25 [Michael Armbrust] blacklist some more MR tests.
aa2239c [Michael Armbrust] filter test lines containing Owner:
f71a325 [Michael Armbrust] Update golden jar.
a3003ae [Michael Armbrust] Update makefile to use better sharding support.
568d150 [Michael Armbrust] Updates to white/blacklist.
8351f25 [Michael Armbrust] Add an ignored test to remind us we don't do empty aggregations right.
c4104ec [Michael Armbrust] Numerous improvements to testing infrastructure.  See comments for details.
09c6300 [Michael Armbrust] Add nullability information to StructFields.
5460b2d [Michael Armbrust] load srcpart by default.
3695141 [Michael Armbrust] Lots of parser improvements.
965ac9a [Michael Armbrust] Add expressions that allow access into complex types.
3ba53c9 [Michael Armbrust] Output type suffixes on AttributeReferences.
8777489 [Michael Armbrust] Initial support for operators that allow the user to specify partitioning.
e57f97a [Michael Armbrust] more decimal/null support.
e1440ed [Michael Armbrust] Initial support for function specific type conversions.
1814ed3 [Michael Armbrust] use childrenResolved function.
f2ec57e [Michael Armbrust] Begin supporting decimal.
6924e6e [Michael Armbrust] Handle NullTypes when resolving HiveUDFs
7fcfa8a [Michael Armbrust] Initial support for parsing unspecified partition parameters.
d0124f3 [Michael Armbrust] Correctly type null literals.
b65626e [Michael Armbrust] Initial support for parsing BigDecimal.
a90efda [Michael Armbrust] utility function for outputing string stacktraces.
7102f33 [Michael Armbrust] methods with side-effects should use ().
3ccaef7 [Michael Armbrust] add renaming TODO.
bc282c7 [Michael Armbrust] fix bug in getNodeNumbered
c8e89d5 [Michael Armbrust] memoize inputSet calculation.
6aefa46 [Michael Armbrust] Skip folding literals.
a72e540 [Michael Armbrust] Add IN operator.
04f885b [Michael Armbrust] literals are only non-nullable if they are not null.
35d2948 [Michael Armbrust] correctly order partition and normal attributes in hive relation output.
12fd52d [Michael Armbrust] support for sorting longs.
0606520 [Michael Armbrust] drop old comment.
859200a [Michael Armbrust] support for reading more types from the metastore.
1fedd18 [Michael Armbrust] coercion from null to numeric types
71e902d [Michael Armbrust] fix test cases.
cc06b6c [Michael Armbrust] Merge remote-tracking branch 'databricks/master' into interviewAnswer
8a8b521 [Reynold Xin] Merge pull request #8 from marmbrus/testImprovment
86355a6 [Michael Armbrust] throw error if there are unexpected join clauses.
c5842d2 [Michael Armbrust] don't throw an error when a select clause outputs multiple copies of the same attribute.
0e975ea [Michael Armbrust] parse bucket sampling as percentage sampling
a92919d [Michael Armbrust] add alter view as to native commands
f58d5a5 [Michael Armbrust] support for parsing SELECT DISTINCT
f0faa26 [Michael Armbrust] add sample and distinct operators.
ef7b943 [Michael Armbrust] add metastore support for float
e9f4588 [Michael Armbrust] fix > 100 char.
755b229 [Michael Armbrust] blacklist some ddl tests.
9ae740a [Michael Armbrust] blacklist more tests that require MR.
4cfc11a [Michael Armbrust] more test coverage.
0d9d56a [Michael Armbrust] add more native commands to parser
78d730d [Michael Armbrust] Load src test table on RESET.
8364ec2 [Michael Armbrust] whitelist all possible partition values.
b01468d [Michael Armbrust] support path rewrites when the query begins with a comment.
4c6b454 [Michael Armbrust] add option for recomputing the cached golden answer when tests fail.
4c5fb0f [Michael Armbrust] makefile target for building new whitelist.
4b6fed8 [Michael Armbrust] support for parsing both DESTINATION and INSERT_INTO.
516481c [Michael Armbrust] Ignore requests to explain native commands.
68aa2e6 [Michael Armbrust] Stronger type for Token extractor.
ca4ea26 [Michael Armbrust] Support for parsing UDF(*).
1aafea3 [Michael Armbrust] Configure partition whitelist in TestShark reset.
9627616 [Michael Armbrust] Use current database as default database.
9b02b44 [Michael Armbrust] Fix spelling error. Add failFast mode.
6f64cee [Michael Armbrust] don't line wrap string literal
eafaeed [Michael Armbrust] add type documentation
f54c94c [Michael Armbrust] make golden answers file a test dependency
5362365 [Michael Armbrust] push conditions into join
0d2388b [Michael Armbrust] Point at databricks hosted scaladoc.
73b29cd [Michael Armbrust] fix bad casting
9aa06c5 [Michael Armbrust] Merge pull request #7 from marmbrus/docFixes
7eff191 [Michael Armbrust] link all the expression names.
83227e4 [Michael Armbrust] fix scaladoc list syntax, add docs for some rules
9de6b74 [Michael Armbrust] fix language feature and deprecation warnings.
0b1960a [Michael Armbrust] Fix broken scala doc links / warnings.
b1acb36 [Michael Armbrust] Merge pull request #3 from yhuai/evalauteLiteralsInExpressions
01c00c2 [Michael Armbrust] new golden
5c14857 [Yin Huai] Merge remote-tracking branch 'upstream/master' into evalauteLiteralsInExpressions
b749b51 [Michael Armbrust] Merge pull request #5 from marmbrus/testCaching
66adceb [Michael Armbrust] Merge pull request #6 from marmbrus/joinWork
1a393da [Yin Huai] folded -> foldable
1e964ea [Yin Huai] update
a43d41c [Michael Armbrust] more tests passing!
8ca38d0 [Michael Armbrust] begin support for varchar / binary types.
ab8bbd1 [Michael Armbrust] parsing % operator
c16c8b5 [Michael Armbrust] case insensitive checking for hooks in tests.
3a90a5f [Michael Armbrust] simpler output when running a single test from the commandline.
5332fee [Yin Huai] Merge remote-tracking branch 'upstream/master' into evalauteLiteralsInExpressions
367fb9e [Yin Huai] update
0cd5cc6 [Michael Armbrust] add BIGINT cast parsing
61b266f [Michael Armbrust] comment for eliminate subqueries.
d72a5a2 [Michael Armbrust] add long to literal factory object.
b3bd15f [Michael Armbrust] blacklist more mr requiring tests.
e06fd38 [Michael Armbrust] black list map reduce tests.
8e7ce30 [Michael Armbrust] blacklist some env specific tests.
6250cbd [Michael Armbrust] Do not exit on test failure
b22b220 [Michael Armbrust] also look for cached hive test answers on the classpath.
b6e4899 [Yin Huai] formatting
e75c90d [Reynold Xin] Merge pull request #4 from marmbrus/hive12
5fabbec [Michael Armbrust] ignore partitioned scan test. scan seems to be working but there is some error about the table already existing?
9e190f5 [Michael Armbrust] drop unneeded ()
68b58c1 [Michael Armbrust] drop a few more tests.
b0aa400 [Michael Armbrust] update whitelist.
c99012c [Michael Armbrust] skip tests with hooks
db00ebf [Michael Armbrust] more types for hive udfs
dbc3678 [Michael Armbrust] update ghpages repo
138f53d [Yin Huai] addressed comments and added a space after a space after the defining keyword of every control structure.
6f954ee [Michael Armbrust] export the hadoop classpath when starting sbt, required to invoke hive during tests.
46bf41b [Michael Armbrust] add a makefile for priming the test answer cache in parallel.  usage: ""make -j 8 -i""
8d47ed4 [Yin Huai] comment
2795f05 [Yin Huai] comment
e003728 [Yin Huai] move OptimizerSuite to the package of catalyst.optimizer
2941d3a [Yin Huai] Merge remote-tracking branch 'upstream/master' into evalauteLiteralsInExpressions
0bd1688 [Yin Huai] update
6a7bd75 [Michael Armbrust] fix partition column delimiter configuration.
e942da1 [Michael Armbrust] Begin upgrade to Hive 0.12.0.
b8cd7e3 [Michael Armbrust] Merge pull request #7 from rxin/moreclean
52864da [Reynold Xin] Added executeCollect method to SharkPlan.
f0e1cbf [Reynold Xin] Added resolved lazy val to LogicalPlan.
b367e36 [Reynold Xin] Replaced the use of ??? with UnsupportedOperationException.
38124bd [Yin Huai] formatting
2924468 [Yin Huai] add two tests for testing pre-order and post-order tree traversal, respectively
555d839 [Reynold Xin] More cleaning ...
d48d0e1 [Reynold Xin] Code review feedback.
aa2e694 [Yin Huai] Merge remote-tracking branch 'upstream/master' into evalauteLiteralsInExpressions
5c421ac [Reynold Xin] Imported SharkEnv, SharkContext, and HadoopTableReader to remove Shark dependency.
479e055 [Reynold Xin] A set of minor changes, including: - import order - limit some lines to 100 character wide - inline code comment - more scaladocs - minor spacing (i.e. add a space after if)
da16e45 [Reynold Xin] Merge pull request #3 from rxin/packagename
e36caf5 [Reynold Xin] Renamed Rule.name to Rule.ruleName since name is used too frequently in the code base and is shadowed often by local scope.
72426ed [Reynold Xin] Rename shark2 package to execution.
0892153 [Reynold Xin] Merge pull request #2 from rxin/packagename
e58304a [Reynold Xin] Merge pull request #1 from rxin/gitignore
3f9fee1 [Michael Armbrust] rewrite push filter through join optimization.
c6527f5 [Reynold Xin] Moved the test src files into the catalyst directory.
c9777d8 [Reynold Xin] Put all source files in a catalyst directory.
019ea74 [Reynold Xin] Updated .gitignore to include IntelliJ files.
80ca4be [Timothy Chen] Address comments
0079392 [Michael Armbrust] support for multiple insert commands in a single query
75b5a01 [Michael Armbrust] remove space.
4283400 [Timothy Chen] Add limited predicate push down
e547e50 [Michael Armbrust] implement First.
e77c9b6 [Michael Armbrust] more work on unique join.
c795e06 [Michael Armbrust] improve star expansion
a26494e [Michael Armbrust] allow aliases to have qualifiers
d078333 [Michael Armbrust] remove extra space
a75c023 [Michael Armbrust] implement Coalesce
3a018b6 [Michael Armbrust] fix up docs.
ab6f67d [Michael Armbrust] import the string ""null"" as actual null.
5377c04 [Michael Armbrust] don't call dataType until checking if children are resolved.
191ce3e [Michael Armbrust] analyze rewrite test query.
60b1526 [Michael Armbrust] don't call dataType until checking if children are resolved.
2ab5a32 [Michael Armbrust] stop using uberjar as it has its own set of issues.
e42f75a [Michael Armbrust] Merge remote-tracking branch 'origin/master' into HEAD
c086a35 [Michael Armbrust] docs, spacing
c4060e4 [Michael Armbrust] cleanup
3b85462 [Michael Armbrust] more tests passing
bcfc8c5 [Michael Armbrust] start supporting partition attributes when inserting data.
c944a95 [Michael Armbrust] First aggregate expression.
1e28311 [Michael Armbrust] make tests execute in alpha order again
a287481 [Michael Armbrust] spelling
8492548 [Michael Armbrust] beginning of UNIQUEJOIN parsing.
a6ab6c7 [Michael Armbrust] add !=
4529594 [Michael Armbrust] draft of coalesce
70f253f [Michael Armbrust] more tests passing!
7349e7b [Michael Armbrust] initial support for test thrift table
d3c9305 [Michael Armbrust] fix > 100 char line
93b64b0 [Michael Armbrust] load test tables that are args to ""DESCRIBE""
06b2aba [Michael Armbrust] don't be case sensitive when fixing load paths
6355d0e [Michael Armbrust] match actual return type of count with expected
cda43ab [Michael Armbrust] don't throw an exception when one of the join tables is empty.
fd4b096 [Michael Armbrust] fix casing of null strings as well.
4632695 [Michael Armbrust] support for megastore bigint
67b88cf [Michael Armbrust] more verbose debugging of evaluation return types
c680e0d [Michael Armbrust] Failed string => number conversion should return null.
2326be1 [Michael Armbrust] make getClauses case insensitive.
dac2786 [Michael Armbrust] correctly handle null values when going from string to numeric types.
045ac4b [Yin Huai] Merge remote-tracking branch 'upstream/master' into evalauteLiteralsInExpressions
fb5ddfd [Michael Armbrust] move ViewExamples to examples/
83833e8 [Michael Armbrust] more tests passing!
47c98d6 [Michael Armbrust] add query tests for like and hash.
1724c16 [Michael Armbrust] clear lines that contain last updated times.
cfd6bbc [Michael Armbrust] Quick skipping of tests that we can't even parse.
9b2642b [Michael Armbrust] make the blacklist support regexes
1d50af6 [Michael Armbrust] more datatypes, fix nonserializable instance variables in udfs
910e33e [Michael Armbrust] basic support for building an assembly jar.
d55bb52 [Michael Armbrust] add local warehouse/metastore to gitignore.
495d9dc [Michael Armbrust] Add an expression for when we decide to support LIKE natively instead of using the HIVE udf.
65f4e69 [Michael Armbrust] remove incorrect comments
0831a3c [Michael Armbrust] support for parsing some operator udfs.
6c27aa7 [Michael Armbrust] more cast parsing.
43db061 [Michael Armbrust] significant generalization of hive udf functionality.
3fe24ec [Michael Armbrust] better implementation of 3vl in Evaluate, fix some > 100 char lines.
e5690a6 [Michael Armbrust] add BinaryType
adab892 [Michael Armbrust] Clear out functions that are created during tests when reset is called.
d408021 [Michael Armbrust] support for printing out arrays in the output in the same form as hive (e.g., [e1, e1]).
8d5f504 [Michael Armbrust] Example of schema RDD using scala's dynamic trait, resulting in a more standard ORM style of usage.
21f0d91 [Michael Armbrust] Simple example of schemaRdd with scala filter function.
0daaa0e [Michael Armbrust] Promote booleans that appear in comparisons.
2b70abf [Michael Armbrust] true and false literals.
ef8b0a5 [Michael Armbrust] more tests.
14d070f [Michael Armbrust] add support for correctly extracting partition keys.
0afbe73 [Yin Huai] Merge remote-tracking branch 'upstream/master' into evalauteLiteralsInExpressions
69a0bd4 [Michael Armbrust] promote strings in predicates with number too.
3946e31 [Michael Armbrust] don't build strings unless assertion fails.
90c453d [Michael Armbrust] more tests passing!
6e6417a [Michael Armbrust] correct handling of nulls in boolean logic and sorting.
8000504 [Michael Armbrust] Improve type coercion.
9087152 [Michael Armbrust] fix toString of Not.
58b111c [Michael Armbrust] fix bad scaladoc tag.
d5c05c6 [Michael Armbrust] For now, ignore the big data benchmark tests when the data isn't there.
ac6376d [Michael Armbrust] Split out general shark query execution driver from test harness.
1d0ae1e [Michael Armbrust] Switch from IndexSeq[Any] to Row interface that will allow us unboxed access to primitive types.
d873b2b [Yin Huai] Remove numbers associated with test cases.
8545675 [Yin Huai] Merge remote-tracking branch 'upstream/master' into evalauteLiteralsInExpressions
b34a9eb [Michael Armbrust] Merge branch 'master' into filterPushDown
d1e7b8e [Michael Armbrust] Update README.md
c8b1553 [Michael Armbrust] Update README.md
9307ef9 [Michael Armbrust] update list of passing tests.
934c18c [Michael Armbrust] Filter out non-deterministic lines when comparing test answers.
a045c9c [Michael Armbrust] SparkAggregate doesn't actually support sum right now.
ae0024a [Yin Huai] update
cf80545 [Yin Huai] Merge remote-tracking branch 'origin/evalauteLiteralsInExpressions' into evalauteLiteralsInExpressions
21976ae [Yin Huai] update
b4999fe [Yin Huai] Merge remote-tracking branch 'upstream/filterPushDown' into evalauteLiteralsInExpressions
dedbf0c [Yin Huai] support Boolean literals
eaac9e2 [Yin Huai] explain the limitation of the current EvaluateLiterals
37817b5 [Yin Huai] add a comment to EvaluateLiterals.
468667f [Yin Huai] First draft of literal evaluation in the optimization phase. TreeNode has been extended to support transform in the post order. So, for an expression, we can evaluate literal from the leaf nodes of this expression tree. For an attribute reference in the expression node, we just leave it as is.
b1d1843 [Michael Armbrust] more work on big data benchmark tests.
cc9a957 [Michael Armbrust] support for creating test tables outside of TestShark
7d7fa9f [Michael Armbrust] support for create table as
5f54f03 [Michael Armbrust] parsing for ASC
d42b725 [Michael Armbrust] Sum of strings requires cast
34b30fa [Michael Armbrust] not all attributes need to be bound (e.g. output attributes that are contained in non-leaf operators.)
81659cb [Michael Armbrust] implement transform operator.
5cd76d6 [Michael Armbrust] break up the file based test case code for reuse
1031b65 [Michael Armbrust] support for case insensitive resolution.
320df04 [Michael Armbrust] add snapshot repo for databricks (has shark/spark snapshots)
b6f083e [Michael Armbrust] support for publishing scala doc to github from sbt
d9d18b4 [Michael Armbrust] debug logging implicit.
669089c [Yin Huai] support Boolean literals
ef3321e [Yin Huai] explain the limitation of the current EvaluateLiterals
73a05fd [Yin Huai] add a comment to EvaluateLiterals.
191eb7d [Yin Huai] First draft of literal evaluation in the optimization phase. TreeNode has been extended to support transform in the post order. So, for an expression, we can evaluate literal from the leaf nodes of this expression tree. For an attribute reference in the expression node, we just leave it as is.
80039cc [Yin Huai] Merge pull request #1 from yhuai/master
cbe1ca1 [Yin Huai] add explicit result type to the overloaded sideBySide
5c518e4 [Michael Armbrust] fix bug in test.
b50dd0e [Michael Armbrust] fix return type of overloaded method
05679b7 [Michael Armbrust] download assembly jar for easy compiling during interview.
8c60cc0 [Michael Armbrust] Update README.md
03b9526 [Michael Armbrust] First draft of optimizer tests.
f392755 [Michael Armbrust] Add flatMap to TreeNode
6cbe8d1 [Michael Armbrust] fix bug in side by side, add support for working with unsplit strings
15a53fc [Michael Armbrust] more generic sum calculation and better binding of grouping expressions.
06749d0 [Michael Armbrust] add expression enumerations for query plan operators and recursive version of transform expression.
4b0a888 [Michael Armbrust] implement string comparison and more casts.
356b321 [Michael Armbrust] Update README.md
3776395 [Michael Armbrust] Update README.md
304d17d [Michael Armbrust] Create README.md
b7d8be0 [Michael Armbrust] more tests passing.
b82481f [Michael Armbrust] add todo comment.
02e6dee [Michael Armbrust] add another test that breaks the harness to the blacklist.
cc5efe3 [Michael Armbrust] First draft of broadcast nested loop join with full outer support.
c43a259 [Michael Armbrust] comments
15ff448 [Michael Armbrust] better error message when a dsl test throws an exception
76ec650 [Michael Armbrust] fix join conditions
e10df99 [Michael Armbrust] Create new expr ids for local relations that exist more than once in a query plan.
91573a4 [Michael Armbrust] initial type promotion
e2ef4a5 [Michael Armbrust] logging
e43dc1e [Michael Armbrust] add string => int cast evaluation
f1f7e96 [Michael Armbrust] fix incorrect generation of join keys
2b27230 [Michael Armbrust] add depth based subtree access
0f6279f [Michael Armbrust] broken tests.
389bc0b [Michael Armbrust] support for partitioned columns in output.
12584f4 [Michael Armbrust] better errors for missing clauses. support for matching multiple clauses with the same name.
b67a225 [Michael Armbrust] better errors when types don't match up.
9e74808 [Michael Armbrust] add children resolved.
6d03ce9 [Michael Armbrust] defaults for unresolved relation
2469b00 [Michael Armbrust] skip nodes with unresolved children when doing coersions
be5ae2c [Michael Armbrust] better resolution logging
cb7b5af [Michael Armbrust] views example
420e05b [Michael Armbrust] more tests passing!
6916c63 [Michael Armbrust] Reading from partitioned hive tables.
a1245f9 [Michael Armbrust] more tests passing
956e760 [Michael Armbrust] extended explain
5f14c35 [Michael Armbrust] more test tables supported
175c43e [Michael Armbrust] better errors for parse exceptions
480ade5 [Michael Armbrust] don't use partial cached results.
8a9d21c [Michael Armbrust] fix evaluation
7aee69c [Michael Armbrust] parsing for joins, boolean logic
7fcf480 [Michael Armbrust] test for and logic
3ea9b00 [Michael Armbrust] don't use simpleString if there are no new lines.
6902490 [Michael Armbrust] fix boolean logic evaluation
4d5eba7 [Michael Armbrust] add more dsl for expression arithmetic and boolean logic
8b2a2ee [Michael Armbrust] more tests passing!
ad1f3b4 [Michael Armbrust] toString for null literals
a5c0a1b [Michael Armbrust] more test harness improvements: * regex whitelist * side by side answer comparison (still needs formatting work)
60ec19d [Michael Armbrust] initial support for udfs
c45b440 [Michael Armbrust] support for is (not) null and boolean logic
7f4a1dc [Michael Armbrust] add NoRelation logical operator
72e183b [Michael Armbrust] support for null values in tree node args.
ad596d2 [Michael Armbrust] add sc to Union's otherCopyArgs
e5c9d1a [Michael Armbrust] use nonEmpty
dcc4fe1 [Michael Armbrust] support for src1 test table.
c78b587 [Michael Armbrust] casting.
75c3f3f [Michael Armbrust] add support for logging with scalalogging.
da2c011 [Michael Armbrust] make it more obvious when results are being truncated.
96b73ba [Michael Armbrust] more docs in TestShark
18524fd [Michael Armbrust] add method to SharkSqlQuery for directly executing the same query on hive.
e6d063b [Michael Armbrust] more join tests.
664c1c3 [Michael Armbrust] make parsing of function names case insensitive.
0967d4e [Michael Armbrust] fix hardcoded path to hiveDevHome.
1a6db68 [Michael Armbrust] spelling
7638cb4 [Michael Armbrust] simple join execution with dsl tests.  no hive tests yes.
859d4c9 [Michael Armbrust] better argString printing of nested trees.
fc53615 [Michael Armbrust] add same instance comparisons for tree nodes.
a026e6b [Michael Armbrust] move out hive specific operators
fff4d1c [Michael Armbrust] add simple query execution debugging
e2120ab [Michael Armbrust] sorting for strings
da06eb6 [Michael Armbrust] Parsing for sortby and joins
9eb5c5e [Michael Armbrust] override equality in Attribute references to compare exprId.
8eb2460 [Michael Armbrust] add system property to override whitelist.
88124bb [Michael Armbrust] make strategy evaluation lazy.
74a3a21 [Michael Armbrust] implement outputSet
d25b171 [Michael Armbrust] Add AND and OR expressions
67f0a4a [Michael Armbrust] dsl improvements: string to attribute, subquery, unionAll
12acf0a [Michael Armbrust] add .DS_Store for macs
f7da6ce [Michael Armbrust] add agg with grouping expr in select test
36805b3 [Michael Armbrust] pull out and improve aggregation
75613e1 [Michael Armbrust] better evaluations failure messages.
4789a35 [Michael Armbrust] weaken type since its hard to create pure references.
e89dd36 [Michael Armbrust] no newline for online trees
d0590d4 [Michael Armbrust] include stack trace for catalyst failures.
081c0d9 [Michael Armbrust] more generic computation of agg functions.
31af3a0 [Michael Armbrust] fail when clauses are unhandeled in the parser
ecd45b2 [Michael Armbrust] Add more passing tests.
97d5419 [Michael Armbrust] fix alignment.
565cc13 [Michael Armbrust] make the canary query optional.
a95e65c [Michael Armbrust] support for resolving qualified attribute references.
e1dfa0c [Michael Armbrust] better error reporting for comparison tests when hive works but catalyst fails.
4640a0b [Michael Armbrust] handle test tables when database is specified.
bef12e3 [Michael Armbrust] Add Subquery node and trivial optimizer to remove it after analysis.
fec5158 [Michael Armbrust] add hive / idea files to .gitignore
3f97ffe [Michael Armbrust] Rename Hive => HiveQl
656b836 [Michael Armbrust] Support for parsing select clause aliases.
3ca7414 [Michael Armbrust] StopAfter needs otherCopyArgs.
3ffde66 [Michael Armbrust] When the child of an alias is unresolved it should return an unresolved attribute instead of throwing an exception.
8cbef8a [Michael Armbrust] spelling
aa8c37c [Michael Armbrust] Better toString for SortOrder
1bb8b45 [Michael Armbrust] fix error message for UnresolvedExceptions
a2e0327 [Michael Armbrust] add a bunch of tests.
4a3e1ea [Michael Armbrust] docs and use shark for data loading.
339bb8f [Michael Armbrust] better docs, Not support
1d7b2d9 [Michael Armbrust] Add NaN conversions.
46a2534 [Michael Armbrust] only run canary query on failure.
8996066 [Michael Armbrust] remove protected from makeCopy
53bcf41 [Michael Armbrust] testing improvements: * reset hive vars * delete indexes and tables * delete database * reset to use default database * record tests that pass
04a372a [Michael Armbrust] add a flag for running all tests.
3b2235b [Michael Armbrust] More general implementation of arithmetic.
edd7795 [Michael Armbrust] More testing improvements: * Check that results match for native commands * Ensure explain commands can be planned * Cache hive ""golden"" results
da6c577 [Michael Armbrust] add string <==> file utility functions.
3adf5ca [Michael Armbrust] Initial support for groupBy and count.
7bcd8a4 [Michael Armbrust] Improvements to comparison tests: * Sort answer when query doesn't contain an order by. * Display null values the same as Hive. * Print full query results in easy to read format when they differ.
a52e7c9 [Michael Armbrust] Transform children that are present in sequences of the product.
d66ba7e [Michael Armbrust] drop printlns.
88f2efd [Michael Armbrust] Add sum / count distinct expressions.
05adedc [Michael Armbrust] rewrite relative paths when loading data in TestShark
07784b3 [Michael Armbrust] add support for rewriting paths and running 'set' commands.
b8a9910 [Michael Armbrust] quote tests passing.
8e5e267 [Michael Armbrust] handle aliased select expressions.
4286a96 [Michael Armbrust] drop debugging println
ac34aeb [Michael Armbrust] proof of concept for hive ast transformations.
2238b00 [Michael Armbrust] better error when makeCopy functions fails due to incorrect arguments
ff1eab8 [Michael Armbrust] start trying to make insert into hive table more general.
74a6337 [Michael Armbrust] use fastEquals when doing transformations.
1184a23 [Michael Armbrust] add native test for escapes.
b972b18 [Michael Armbrust] create BaseRelation class
fa6bce9 [Michael Armbrust] implement union
6391a87 [Michael Armbrust] count aggregate.
d47c317 [Michael Armbrust] add unary minus, more tests passing.
c7114e4 [Michael Armbrust] first draft of star expansion.
044c43d [Michael Armbrust] better support for numeric literal parsing.
1d0f072 [Michael Armbrust] use native drop table as it doesn't appear to fail when the ""table"" is actually a view.
61503c5 [Michael Armbrust] add cached toRdd
2036883 [Michael Armbrust] skip explain queries when testing.
ebac4b1 [Michael Armbrust] fix bug in sort reference calculation
ca0dee0 [Michael Armbrust] docs.
1ee0471 [Michael Armbrust] string literal parsing.
357278b [Michael Armbrust] add limit support
9b3e479 [Michael Armbrust] creation of string literals.
02efa30 [Michael Armbrust] alias evaluation
cb68b33 [Michael Armbrust] parsing for random sample in hive ql.
126dd36 [Michael Armbrust] include query plans in failure output
bb59ae9 [Michael Armbrust] doc fixes
7e68286 [Michael Armbrust] fix confusing naming
768bb25 [Michael Armbrust] handle errors in shark query toString
829c3ce [Michael Armbrust] Auto loading of test data on demand. Add reset method to test shark.  Make test shark a singleton to avoid weirdness with the hive megastore.
ad02e41 [Michael Armbrust] comment jdo dependency
7bc89fe [Michael Armbrust] add collect to TreeNode.
438cf74 [Michael Armbrust] create explicit treeString function in addition to toString override. docs.
09679ee [Michael Armbrust] fix bug in TreeNode foreach
2930b27 [Michael Armbrust] more specific name for del query tests.
8842549 [Michael Armbrust] docs.
da81f81 [Michael Armbrust] Implementation and tests for simple AVG query in Hive SQL.
a8969b9 [Michael Armbrust] Factor out hive query comparison test framework.
1a7efb0 [Michael Armbrust] specialize spark aggregate for global aggregations.
a36dd9a [Michael Armbrust] evaluation for other > data types.
cae729b [Michael Armbrust] remove unnecessary lazy vals.
d8e12af [Michael Armbrust] docs
3a60d67 [Michael Armbrust] implement average, placeholder for count
f05c106 [Michael Armbrust] checkAnswer handles single row results.
2730534 [Michael Armbrust] implement inputSet
a9aa79d [Michael Armbrust] debugging for sort exec
8bec3c9 [Michael Armbrust] better tree makeCopy when there are two constructors.
554b4b2 [Michael Armbrust] BoundAttribute pretty printing.
754f5fa [Michael Armbrust] dsl for setting nullability
a206d7a [Michael Armbrust] clean up query tests.
84ad6ef [Michael Armbrust] better sort implementation and tests.
de24923 [Michael Armbrust] add double type.
9611a2c [Michael Armbrust] literal creation for doubles.
7358313 [Michael Armbrust] sort order returns child type.
b544715 [Michael Armbrust] implement eval for rand, and > for doubles
7013bad [Michael Armbrust] asc, desc should work for expressions and unresolved attributes (symbols)
1c1a35e [Michael Armbrust] add simple Rand expression.
3ca51de [Michael Armbrust] add orderBy to dsl
7ae41ab [Michael Armbrust] more literal implicit conversions
b18b675 [Michael Armbrust] First cut at native query tests for shark.
d392e29 [Michael Armbrust] add toRdd implicit conversion for logical plans in TestShark.
5eac895 [Michael Armbrust] better error when descending is specified.
2b16f86 [Michael Armbrust] add todo
e527bb8 [Michael Armbrust] remove arguments to binary predicate constructor as they seem to break serialization
9dde3c8 [Michael Armbrust] add project and filter operations.
ad9037b [Michael Armbrust] Add support for local relations.
6227143 [Michael Armbrust] evaluation of Equals.
7526290 [Michael Armbrust] BoundReference should also be an Attribute.
bd33e26 [Michael Armbrust] more documentation
5de0ea3 [Michael Armbrust] Move all shark specific into a separate package.  Lots of documentation improvements.
0ae292b [Michael Armbrust] implement calculation of sort expressions.
9fd5011 [Michael Armbrust] First cut at expression evaluation.
6259e3a [Michael Armbrust] cleanup
787e5a2 [Michael Armbrust] use fastEquals
f90da36 [Michael Armbrust] better printing of optimization exceptions
b05dd67 [Michael Armbrust] Application of rules to fixed point.
bb2e0db [Michael Armbrust] pretty print for literals.
1ec3287 [Michael Armbrust] Add extractor for IntegerLiterals.
d3a3687 [Michael Armbrust] add fastEquals
2b4935b [Michael Armbrust] set sbt.version explicitly
46dfd7f [Michael Armbrust] first cut at checking answer for HiveCompatability tests.
c79f2fd [Michael Armbrust] insert operator should return an empty rdd.
14c22ec [Michael Armbrust] implement sorting when the sort expression is the first attribute of the input.
ae7b4c3 [Michael Armbrust] remove implicit dependencies.  now compiles without copying things into lib/ manually.
84082f9 [Michael Armbrust] add sbt binaries and scripts
15371a8 [Michael Armbrust] First draft of simple Hive DDL parser.
063bf44 [Michael Armbrust] Periods should end all comments.
e1f7f4c [Michael Armbrust] Remove ""NativePlaceholder"" hack.
ed3633e [Michael Armbrust] start consolidating Hive/Shark specific code. first hive compatibility test case passing!
b34a770 [Michael Armbrust] Add data sink strategy, make strategy application a little more robust.
e7174ec [Michael Armbrust] fix schema, add docs, make helper method protected.
26f410a [Michael Armbrust] physical traits should extend PhysicalPlan.
dc72469 [Michael Armbrust] beginning of hive compatibility testing framework.
0763490 [Michael Armbrust] support for hive native command pass-through.
d8a924f [Michael Armbrust] scaladoc
29a7163 [Michael Armbrust] Insert into hive table physical operator.
633cebc [Michael Armbrust] better error message when there is no appropriate planning strategy.
59ac444 [Michael Armbrust] add unary expression
3aa1b28 [Michael Armbrust] support for table names in the form 'database.tableName'
665f7d0 [Michael Armbrust] add logical nodes for hive data sinks.
64d2923 [Michael Armbrust] Add classes for representing sorts.
f72b7ce [Michael Armbrust] first trivial end to end query execution.
5c7d244 [Michael Armbrust] first draft of references implementation.
7bff274 [Michael Armbrust] point at new shark.
c7cd57f [Michael Armbrust] docs for util function.
910811c [Michael Armbrust] check each item of the sequence
ef21a0b [Michael Armbrust] line up comments.
4b765d5 [Michael Armbrust] docs, drop println
6f9bafd [Michael Armbrust] empty output for unresolved relation to avoid exception in resolution.
a703c49 [Michael Armbrust] this order works better until fixed point is implemented.
ec1d7c0 [Michael Armbrust] Simple attribute resolution.
069df02 [Michael Armbrust] parsing binary predicates
a1cf754 [Michael Armbrust] add joins and equality.
3f5bc98 [Michael Armbrust] add optiq to sbt.
54f3460 [Michael Armbrust] initial optiq parsing.
d9161ce [Michael Armbrust] add join operator
1e423eb [Michael Armbrust] placeholders in LogicalPlan, docs
24ef6fb [Michael Armbrust] toString for alias.
ae7d776 [Michael Armbrust] add nullability changing function
d49dc02 [Michael Armbrust] scaladoc for named exprs
7c45dd7 [Michael Armbrust] pretty printing of trees.
78e34bf [Michael Armbrust] simple git ignore.
7ba19be [Michael Armbrust] First draft of interface to hive metastore.
7e7acf0 [Michael Armbrust] physical placeholder.
1c11136 [Michael Armbrust] first draft of error handling / plans for debugging.
3766a41 [Michael Armbrust] rearrange utility functions.
7fb3d5e [Michael Armbrust] docs and equality improvements.
45da47b [Michael Armbrust] flesh out plans and expressions a little. first cut at named expressions.
002d4d4 [Michael Armbrust] default to no alias.
be25003 [Michael Armbrust] add repl initialization to sbt.
0608a00 [Michael Armbrust] tighten public interface
a1a8b38 [Michael Armbrust] test that ids don't change for no-op transforms.
daa71ca [Michael Armbrust] foreach, maps, and scaladoc
6a158cb [Michael Armbrust] simple transform working.
db0299f [Michael Armbrust] basic analysis of relations minus transform function.
f74c4ee [Michael Armbrust] parsing a simple query.
08e4f57 [Michael Armbrust] upgrade scala include shark.
d3c6404 [Michael Armbrust] initial commit
",1395363800
ca76423e23f5f626c56041cecbc38a93ae1e0298 remotes/origin/vldb~16,"[Hot Fix #42] Do not stop SparkUI if bind() is not called

This is a bug fix for #42 (79d07d66040f206708e14de393ab0b80020ed96a).

In Master, we do not bind() each SparkUI because we do not want to start a server for each finished application. However, when we remove the associated application, we call stop() on the SparkUI, which throws an assertion failure.

This fix ensures we don't call stop() on a SparkUI that was never bind()'ed.

Author: Andrew Or <andrewor14@gmail.com>

Closes #188 from andrewor14/ui-fix and squashes the following commits:

94a925f [Andrew Or] Do not stop SparkUI if bind() is not called
",1395349996
66a03e5fe0167f590d150e099b15902e826a188f remotes/origin/vldb~17,"Principal Component Analysis

# Principal Component Analysis

Computes the top k principal component coefficients for the m-by-n data matrix X. Rows of X correspond to observations and columns correspond to variables. The coefficient matrix is n-by-k. Each column of the coefficients return matrix contains coefficients for one principal component, and the columns are in descending order of component variance. This function centers the data and uses the singular value decomposition (SVD) algorithm.

## Testing
Tests included:
 * All principal components
 * Only top k principal components
 * Dense SVD tests
 * Dense/sparse matrix tests

The results are tested against MATLAB's pca: http://www.mathworks.com/help/stats/pca.html

## Documentation
Added to mllib-guide.md

## Example Usage
Added to examples directory under SparkPCA.scala

Author: Reza Zadeh <rizlar@gmail.com>

Closes #88 from rezazadeh/sparkpca and squashes the following commits:

e298700 [Reza Zadeh] reformat using IDE
3f23271 [Reza Zadeh] documentation and cleanup
b025ab2 [Reza Zadeh] documentation
e2667d4 [Reza Zadeh] assertMatrixApproximatelyEquals
3787bb4 [Reza Zadeh] stylin
c6ecc1f [Reza Zadeh] docs
aa2bbcb [Reza Zadeh] rename sparseToTallSkinnyDense
56975b0 [Reza Zadeh] docs
2df9bde [Reza Zadeh] docs update
8fb0015 [Reza Zadeh] rcond documentation
dbf7797 [Reza Zadeh] correct argument number
a9f1f62 [Reza Zadeh] documentation
4ce6caa [Reza Zadeh] style changes
9a56a02 [Reza Zadeh] use rcond relative to larget svalue
120f796 [Reza Zadeh] housekeeping
156ff78 [Reza Zadeh] string comprehension
2e1cf43 [Reza Zadeh] rename rcond
ea223a6 [Reza Zadeh] many style changes
f4002d7 [Reza Zadeh] more docs
bd53c7a [Reza Zadeh] proper accumulator
a8b5ecf [Reza Zadeh] Don't use for loops
0dc7980 [Reza Zadeh] filter zeros in sparse
6115610 [Reza Zadeh] More documentation
36d51e8 [Reza Zadeh] use JBLAS for UVS^-1 computation
bc4599f [Reza Zadeh] configurable rcond
86f7515 [Reza Zadeh] compute per parition, use while
09726b3 [Reza Zadeh] more style changes
4195e69 [Reza Zadeh] private, accumulator
17002be [Reza Zadeh] style changes
4ba7471 [Reza Zadeh] style change
f4982e6 [Reza Zadeh] Use dense matrix in example
2828d28 [Reza Zadeh] optimizations: normalize once, use inplace ops
72c9fa1 [Reza Zadeh] rename DenseMatrix to TallSkinnyDenseMatrix, lean
f807be9 [Reza Zadeh] fix typo
2d7ccde [Reza Zadeh] Array interface for dense svd and pca
cd290fa [Reza Zadeh] provide RDD[Array[Double]] support
398d123 [Reza Zadeh] style change
55abbfa [Reza Zadeh] docs fix
ef29644 [Reza Zadeh] bad chnage undo
472566e [Reza Zadeh] all files from old pr
555168f [Reza Zadeh] initial files
",1395337160
79d07d66040f206708e14de393ab0b80020ed96a remotes/origin/vldb~21,"[SPARK-1132] Persisting Web UI through refactoring the SparkListener interface

The fleeting nature of the Spark Web UI has long been a problem reported by many users: The existing Web UI disappears as soon as the associated application terminates. This is because SparkUI is tightly coupled with SparkContext, and cannot be instantiated independently from it. To solve this, some state must be saved to persistent storage while the application is still running.

The approach taken by this PR involves persisting the UI state through SparkListenerEvents. This requires a major refactor of the SparkListener interface because existing events (1) maintain deep references, making de/serialization is difficult, and (2) do not encode all the information displayed on the UI. In this design, each existing listener for the UI (e.g. ExecutorsListener) maintains state that can be fully constructed from SparkListenerEvents. This state is then supplied to the parent UI (e.g. ExecutorsUI), which renders the associated page(s) on demand.

This PR introduces two important classes: the **EventLoggingListener**, and the **ReplayListenerBus**. In a live application, SparkUI registers an EventLoggingListener with the SparkContext in addition to the existing listeners. Over the course of the application, this listener serializes and logs all events to persisted storage. Then, after the application has finished, the SparkUI can be revived by replaying all the logged events to the existing UI listeners through the ReplayListenerBus.

This feature is currently integrated with the Master Web UI, which optionally rebuilds a SparkUI from event logs as soon as the corresponding application finishes.

More details can be found in the commit messages, comments within the code, and the [design doc](https://spark-project.atlassian.net/secure/attachment/12900/PersistingSparkWebUI.pdf). Comments and feedback are most welcome.

Author: Andrew Or <andrewor14@gmail.com>
Author: andrewor14 <andrewor14@gmail.com>

Closes #42 from andrewor14/master and squashes the following commits:

e5f14fa [Andrew Or] Merge github.com:apache/spark
a1c5cd9 [Andrew Or] Merge github.com:apache/spark
b8ba817 [Andrew Or] Remove UI from map when removing application in Master
83af656 [Andrew Or] Scraps and pieces (no functionality change)
222adcd [Andrew Or] Merge github.com:apache/spark
124429f [Andrew Or] Clarify LiveListenerBus behavior + Add tests for new behavior
f80bd31 [Andrew Or] Simplify static handler and BlockManager status update logic
9e14f97 [Andrew Or] Moved around functionality + renamed classes per Patrick
6740e49 [Andrew Or] Fix comment nits
650eb12 [Andrew Or] Add unit tests + Fix bugs found through tests
45fd84c [Andrew Or] Remove now deprecated test
c5c2c8f [Andrew Or] Remove list of (TaskInfo, TaskMetrics) from StageInfo
3456090 [Andrew Or] Address Patrick's comments
bf80e3d [Andrew Or] Imports, comments, and code formatting, once again (minor)
ac69ec8 [Andrew Or] Fix test fail
d801d11 [Andrew Or] Merge github.com:apache/spark (major)
dc93915 [Andrew Or] Imports, comments, and code formatting (minor)
77ba283 [Andrew Or] Address Kay's and Patrick's comments
b6eaea7 [Andrew Or] Treating SparkUI as a handler of MasterUI
d59da5f [Andrew Or] Avoid logging all the blocks on each executor
d6e3b4a [Andrew Or] Merge github.com:apache/spark
ca258a4 [Andrew Or] Master UI - add support for reading compressed event logs
176e68e [Andrew Or] Fix deprecated message for JavaSparkContext (minor)
4f69c4a [Andrew Or] Master UI - Rebuild SparkUI on application finish
291b2be [Andrew Or] Correct directory in log message ""INFO: Logging events to <dir>""
1ba3407 [Andrew Or] Add a few configurable options to event logging
e375431 [Andrew Or] Add new constructors for SparkUI
18b256d [Andrew Or] Refactor out event logging and replaying logic from UI
bb4c503 [Andrew Or] Use a more mnemonic path for logging
aef411c [Andrew Or] Fix bug: storage status was not reflected on UI in the local case
03eda0b [Andrew Or] Fix HDFS flush behavior
36b3e5d [Andrew Or] Add HDFS support for event logging
cceff2b [andrewor14] Fix 100 char format fail
2fee310 [Andrew Or] Address Patrick's comments
2981d61 [Andrew Or] Move SparkListenerBus out of DAGScheduler + Clean up
5d2cec1 [Andrew Or] JobLogger: ID -> Id
0503e4b [Andrew Or] Fix PySpark tests + remove sc.clearFiles/clearJars
4d2fb0c [Andrew Or] Fix format fail
faa113e [Andrew Or] General clean up
d47585f [Andrew Or] Clean up FileLogger
472fd8a [Andrew Or] Fix a couple of tests
996d7a2 [Andrew Or] Reflect RDD unpersist on UI
7b2f811 [Andrew Or] Guard against TaskMetrics NPE + Fix tests
d1f4285 [Andrew Or] Migrate from lift-json to json4s-jackson
28019ca [Andrew Or] Merge github.com:apache/spark
bbe3501 [Andrew Or] Embed storage status and RDD info in Task events
6631c02 [Andrew Or] More formatting changes, this time mainly for Json DSL
70e7e7a [Andrew Or] Formatting changes
e9e1c6d [Andrew Or] Move all JSON de/serialization logic to JsonProtocol
d646df6 [Andrew Or] Completely decouple SparkUI from SparkContext
6814da0 [Andrew Or] Explicitly register each UI listener rather than through some magic
64d2ce1 [Andrew Or] Fix BlockManagerUI bug by introducing new event
4273013 [Andrew Or] Add a gateway SparkListener to simplify event logging
904c729 [Andrew Or] Fix another major bug
5ac906d [Andrew Or] Mostly naming, formatting, and code style changes
3fd584e [Andrew Or] Fix two major bugs
f3fc13b [Andrew Or] General refactor
4dfcd22 [Andrew Or] Merge git://git.apache.org/incubator-spark into persist-ui
b3976b0 [Andrew Or] Add functionality of reconstructing a persisted UI from SparkContext
8add36b [Andrew Or] JobProgressUI: Add JSON functionality
d859efc [Andrew Or] BlockManagerUI: Add JSON functionality
c4cd480 [Andrew Or] Also deserialize new events
8a2ebe6 [Andrew Or] Fix bugs for EnvironmentUI and ExecutorsUI
de8a1cd [Andrew Or] Serialize events both to and from JSON (rather than just to)
bf0b2e9 [Andrew Or] ExecutorUI: Serialize events rather than arbitary executor information
bb222b9 [Andrew Or] ExecutorUI: render completely from JSON
dcbd312 [Andrew Or] Add JSON Serializability for all SparkListenerEvent's
10ed49d [Andrew Or] Merge github.com:apache/incubator-spark into persist-ui
8e09306 [Andrew Or] Use JSON for ExecutorsUI
e3ae35f [Andrew Or] Merge github.com:apache/incubator-spark
3ddeb7e [Andrew Or] Also privatize fields
090544a [Andrew Or] Privatize methods
13920c9 [Andrew Or] Update docs
bd5a1d7 [Andrew Or] Typo: phyiscal -> physical
287ef44 [Andrew Or] Avoid reading the entire batch into memory; also simplify streaming logic
3df7005 [Andrew Or] Merge branch 'master' of github.com:andrewor14/incubator-spark
a531d2e [Andrew Or] Relax assumptions on compressors and serializers when batching
164489d [Andrew Or] Relax assumptions on compressors and serializers when batching
",1395260221
ab747d39ddc7c8a314ed2fb26548fc5652af0d74 remotes/origin/vldb~22,"Bugfixes/improvements to scheduler

Move the PR#517 of apache-incubator-spark to the apache-spark

Author: Mridul Muralidharan <mridul@gmail.com>

Closes #159 from mridulm/master and squashes the following commits:

5ff59c2 [Mridul Muralidharan] Change property in suite also
167fad8 [Mridul Muralidharan] Address review comments
9bda70e [Mridul Muralidharan] Address review comments, akwats add to failedExecutors
270d841 [Mridul Muralidharan] Address review comments
fa5d9f1 [Mridul Muralidharan] Bugfixes/improvements to scheduler : PR #517
",1395258415
6112270c94f1a30a461a91f6e56485a5eaec2606 remotes/origin/vldb~23,"SPARK-1203 fix saving to hdfs from yarn

Author: Thomas Graves <tgraves@apache.org>

Closes #173 from tgravescs/SPARK-1203 and squashes the following commits:

4fd5ded [Thomas Graves] adding import
964e3f7 [Thomas Graves] SPARK-1203 fix saving to hdfs from yarn
",1395234560
d55ec86de2e96f7dc9d1dd107daa35c3823791ec remotes/origin/vldb~24,"bugfix: Wrong ""Duration"" in ""Active Stages"" in stages page

If a stage which has completed once loss parts of data, it will be resubmitted. At this time, it appears that stage.completionTime > stage.submissionTime.

Author: shiyun.wxm <shiyun.wxm@taobao.com>

Closes #170 from BlackNiuza/duration_problem and squashes the following commits:

a86d261 [shiyun.wxm] tow space indent
c0d7b24 [shiyun.wxm] change the style
3b072e1 [shiyun.wxm] fix scala style
f20701e [shiyun.wxm] bugfix: ""Duration"" in ""Active Stages"" in stages page
",1395218554
cc2655a237442a71c75d4fade99767df7648e55f remotes/origin/vldb~26,"Fix SPARK-1256: Master web UI and Worker web UI returns a 404 error

Author: witgo <witgo@qq.com>

Closes #150 from witgo/SPARK-1256 and squashes the following commits:

08044a2 [witgo] Merge branch 'master' of https://github.com/apache/spark into SPARK-1256
c99b030 [witgo] Fix SPARK-1256
",1395205067
2fa26ec02fc2251102f89bb67523419fd7dd3757 remotes/origin/vldb~30,"SPARK-1102: Create a saveAsNewAPIHadoopDataset method

https://spark-project.atlassian.net/browse/SPARK-1102

Create a saveAsNewAPIHadoopDataset method

By @mateiz: ""Right now RDDs can only be saved as files using the new Hadoop API, not as ""datasets"" with no filename and just a JobConf. See http://codeforhire.com/2014/02/18/using-spark-with-mongodb/ for an example of how you have to give a bogus filename. For the old Hadoop API, we have saveAsHadoopDataset.""

Author: CodingCat <zhunansjtu@gmail.com>

Closes #12 from CodingCat/SPARK-1102 and squashes the following commits:

6ba0c83 [CodingCat] add test cases for saveAsHadoopDataSet (new&old API)
a8d11ba [CodingCat] style fix.........
95a6929 [CodingCat] code clean
7643c88 [CodingCat] change the parameter type back to Configuration
a8583ee [CodingCat] Create a saveAsNewAPIHadoopDataset method
",1395165978
e3681f26fae7e87321ac991f5a0fb7517415803a remotes/origin/vldb~32,"Spark 1246 add min max to stat counter

Here's the addition of min and max to statscounter.py and min and max methods to rdd.py.

Author: Dan McClary <dan.mcclary@gmail.com>

Closes #144 from dwmclary/SPARK-1246-add-min-max-to-stat-counter and squashes the following commits:

fd3fd4b [Dan McClary] fixed  error, updated test
82cde0e [Dan McClary] flipped incorrectly assigned inf values in StatCounter
5d96799 [Dan McClary] added max and min to StatCounter repr for pyspark
21dd366 [Dan McClary] added max and min to StatCounter output, updated doc
1a97558 [Dan McClary] added max and min to StatCounter output, updated doc
a5c13b0 [Dan McClary] Added min and max to Scala and Java RDD, added min and max to StatCounter
ed67136 [Dan McClary] broke min/max out into separate transaction, added to rdd.py
1e7056d [Dan McClary] added underscore to getBucket
37a7dea [Dan McClary] cleaned up boundaries for histogram -- uses real min/max when buckets are derived
29981f2 [Dan McClary] fixed indentation on doctest comment
eaf89d9 [Dan McClary] added correct doctest for histogram
4916016 [Dan McClary] added histogram method, added max and min to statscounter
",1395128747
dc9654638f1d781ee1e54348fa41436b27793365 remotes/origin/vldb~35,"SPARK-1240: handle the case of empty RDD when takeSample

https://spark-project.atlassian.net/browse/SPARK-1240

It seems that the current implementation does not handle the empty RDD case when run takeSample

In this patch, before calling sample() inside takeSample API, I add a checker for this case and returns an empty Array when it's a empty RDD; also in sample(), I add a checker for the invalid fraction value

In the test case, I also add several lines for this case

Author: CodingCat <zhunansjtu@gmail.com>

Closes #135 from CodingCat/SPARK-1240 and squashes the following commits:

fef57d4 [CodingCat] fix the same problem in PySpark
36db06b [CodingCat] create new test cases for takeSample from an empty red
810948d [CodingCat] further fix
a40e8fb [CodingCat] replace if with require
ad483fd [CodingCat] handle the case with empty RDD when take sample
",1395033299
97e4459e1e4cca8696535e10a91733c15f960107 remotes/origin/vldb~37,"SPARK-1254. Consolidate, order, and harmonize repository declarations in Maven/SBT builds

This suggestion addresses a few minor suboptimalities with how repositories are handled.

1) Use HTTPS consistently to access repos, instead of HTTP

2) Consolidate repository declarations in the parent POM file, in the case of the Maven build, so that their ordering can be controlled to put the fully optional Cloudera repo at the end, after required repos. (This was prompted by the untimely failure of the Cloudera repo this week, which made the Spark build fail. #2 would have prevented that.)

3) Update SBT build to match Maven build in this regard

4) Update SBT build to not refer to Sonatype snapshot repos. This wasn't in Maven, and a build generally would not refer to external snapshots, but I'm not 100% sure on this one.

Author: Sean Owen <sowen@cloudera.com>

Closes #145 from srowen/SPARK-1254 and squashes the following commits:

42f9bfc [Sean Owen] Use HTTPS for repos; consolidate repos in parent in order to put optional Cloudera repo last; harmonize SBT build repos with Maven; remove snapshot repos from SBT build which weren't in Maven
",1394927074
181b130a0c2a0752009fdf5602e5d6d87f5b1212 remotes/origin/vldb~39,"[bugfix] wrong client arg, should use executor-cores

client arg is wrong, it should be executor-cores. it causes executor fail to start when executor-cores is specified

Author: Tianshuo Deng <tdeng@twitter.com>

Closes #138 from tsdeng/bugfix_wrong_client_args and squashes the following commits:

304826d [Tianshuo Deng] wrong client arg, should use executor-cores
",1394767656
698373211ef3cdf841c82d48168cd5dbe00a57b4 remotes/origin/vldb~41,"SPARK-1183. Don't use ""worker"" to mean executor

Author: Sandy Ryza <sandy@cloudera.com>

Closes #120 from sryza/sandy-spark-1183 and squashes the following commits:

5066a4a [Sandy Ryza] Remove ""worker"" in a couple comments
0bd1e46 [Sandy Ryza] Remove --am-class from usage
bfc8fe0 [Sandy Ryza] Remove am-class from doc and fix yarn-alpha
607539f [Sandy Ryza] Address review comments
74d087a [Sandy Ryza] SPARK-1183. Don't use ""worker"" to mean executor
",1394737893
6bd2eaa4a5bcf811c5b85be27c5e50058b5d0c12 remotes/origin/vldb~44,"hot fix for PR105 - change to Java annotation

Author: CodingCat <zhunansjtu@gmail.com>

Closes #133 from CodingCat/SPARK-1160-2 and squashes the following commits:

6607155 [CodingCat] hot fix for PR105 - change to Java annotation
",1394678958
31a704004f9b4ad34f92ae5c95ae6e90d0ab62c7 remotes/origin/vldb~45,"Fix example bug: compile error

Author: jianghan <jianghan@xiaomi.com>

Closes #132 from pooorman/master and squashes the following commits:

54afbe0 [jianghan] Fix example bug: compile error
",1394678772
5d1ec64e7934ad7f922cdab516fa5de690644780 remotes/origin/vldb~48,"Fix #SPARK-1149 Bad partitioners can cause Spark to hang

Author: liguoqiang <liguoqiang@rd.tuan800.com>

Closes #44 from witgo/SPARK-1149 and squashes the following commits:

3dcdcaf [liguoqiang] Merge branch 'master' into SPARK-1149
8425395 [liguoqiang] Merge remote-tracking branch 'upstream/master' into SPARK-1149
3dad595 [liguoqiang] review comment
e3e56aa [liguoqiang] Merge branch 'master' into SPARK-1149
b0d5c07 [liguoqiang] review comment
d0a6005 [liguoqiang] review comment
3395ee7 [liguoqiang] Merge remote-tracking branch 'upstream/master' into SPARK-1149
ac006a3 [liguoqiang] code Formatting
3feb3a8 [liguoqiang] Merge branch 'master' into SPARK-1149
adc443e [liguoqiang] partitions check  bugfix
928e1e3 [liguoqiang] Added a unit test for PairRDDFunctions.lookup with bad partitioner
db6ecc5 [liguoqiang] Merge branch 'master' into SPARK-1149
1e3331e [liguoqiang] Merge branch 'master' into SPARK-1149
3348619 [liguoqiang] Optimize performance for partitions check
61e5a87 [liguoqiang] Merge branch 'master' into SPARK-1149
e68210a [liguoqiang] add partition index check to submitJob
3a65903 [liguoqiang] make the code more readable
6bb725e [liguoqiang] fix #SPARK-1149 Bad partitioners can cause Spark to hang
",1394654404
16788a654246067fd966033b5dc9bc0d4c759b70 remotes/origin/vldb~53,"SPARK-1167: Remove metrics-ganglia from default build due to LGPL issues...

This patch removes Ganglia integration from the default build. It
allows users willing to link against LGPL code to use Ganglia
by adding build flags or linking against a new Spark artifact called
spark-ganglia-lgpl.

This brings Spark in line with the Apache policy on LGPL code
enumerated here:

https://www.apache.org/legal/3party.html#options-optional

Author: Patrick Wendell <pwendell@gmail.com>

Closes #108 from pwendell/ganglia and squashes the following commits:

326712a [Patrick Wendell] Responding to review feedback
5f28ee4 [Patrick Wendell] SPARK-1167: Remove metrics-ganglia from default build due to LGPL issues.
",1394561819
2a5161708f4d2f743c7bd69ed3d98bb7bff46460 remotes/origin/vldb~55,"SPARK-1205: Clean up callSite/origin/generator.

This patch removes the `generator` field and simplifies + documents
the tracking of callsites.

There are two places where we care about call sites, when a job is
run and when an RDD is created. This patch retains both of those
features but does a slight refactoring and renaming to make things
less confusing.

There was another feature of an rdd called the `generator` which was
by default the user class that in which the RDD was created. This is
used exclusively in the JobLogger. It been subsumed by the ability
to name a job group. The job logger can later be refectored to
read the job group directly (will require some work) but for now
this just preserves the default logged value of the user class.
I'm not sure any users ever used the ability to override this.

Author: Patrick Wendell <pwendell@gmail.com>

Closes #106 from pwendell/callsite and squashes the following commits:

fc1d009 [Patrick Wendell] Compile fix
e17fb76 [Patrick Wendell] Review feedback: callSite -> creationSite
62e77ef [Patrick Wendell] Review feedback
576e60b [Patrick Wendell] SPARK-1205: Clean up callSite/origin/generator.
",1394494121
faf4cad1debb76148facc008e0a3308ac96eee7a remotes/origin/vldb~61,"Fix markup errors introduced in #33 (SPARK-1189)

These were causing errors on the configuration page.

Author: Patrick Wendell <pwendell@gmail.com>

Closes #111 from pwendell/master and squashes the following commits:

8467a86 [Patrick Wendell] Fix markup errors introduced in #33 (SPARK-1189)
",1394391426
52834d761b059264214dfc6a1f9c70b8bc7ec089 remotes/origin/vldb~63,"SPARK-929: Fully deprecate usage of SPARK_MEM

(Continued from old repo, prior discussion at https://github.com/apache/incubator-spark/pull/615)

This patch cements our deprecation of the SPARK_MEM environment variable by replacing it with three more specialized variables:
SPARK_DAEMON_MEMORY, SPARK_EXECUTOR_MEMORY, and SPARK_DRIVER_MEMORY

The creation of the latter two variables means that we can safely set driver/job memory without accidentally setting the executor memory. Neither is public.

SPARK_EXECUTOR_MEMORY is only used by the Mesos scheduler (and set within SparkContext). The proper way of configuring executor memory is through the ""spark.executor.memory"" property.

SPARK_DRIVER_MEMORY is the new way of specifying the amount of memory run by jobs launched by spark-class, without possibly affecting executor memory.

Other memory considerations:
- The repl's memory can be set through the ""--drivermem"" command-line option, which really just sets SPARK_DRIVER_MEMORY.
- run-example doesn't use spark-class, so the only way to modify examples' memory is actually an unusual use of SPARK_JAVA_OPTS (which is normally overriden in all cases by spark-class).

This patch also fixes a lurking bug where spark-shell misused spark-class (the first argument is supposed to be the main class name, not java options), as well as a bug in the Windows spark-class2.cmd. I have not yet tested this patch on either Windows or Mesos, however.

Author: Aaron Davidson <aaron@databricks.com>

Closes #99 from aarondav/sparkmem and squashes the following commits:

9df4c68 [Aaron Davidson] SPARK-929: Fully deprecate usage of SPARK_MEM
",1394388519
e59a3b6c415b95e8137f5a154716b12653a8aed0 remotes/origin/vldb~64,"SPARK-1190: Do not initialize log4j if slf4j log4j backend is not being used

Author: Patrick Wendell <pwendell@gmail.com>

Closes #107 from pwendell/logging and squashes the following commits:

be21c11 [Patrick Wendell] Logging fix
",1394323362
0b7b7fd45cd9037d23cb090e62be3ff075214fe7 remotes/origin/vldb~66,"[SPARK-1194] Fix the same-RDD rule for cache replacement

SPARK-1194: https://spark-project.atlassian.net/browse/SPARK-1194

In the current implementation, when selecting candidate blocks to be swapped out, once we find a block from the same RDD that the block to be stored belongs to, cache eviction fails  and aborts.

In this PR, we keep selecting blocks *not* from the RDD that the block to be stored belongs to until either enough free space can be ensured (cache eviction succeeds) or all such blocks are checked (cache eviction fails).

Author: Cheng Lian <lian.cs.zju@gmail.com>

Closes #96 from liancheng/fix-spark-1194 and squashes the following commits:

2524ab9 [Cheng Lian] Added regression test case for SPARK-1194
6e40c22 [Cheng Lian] Remove redundant comments
40cdcb2 [Cheng Lian] Bug fix, and addressed PR comments from @mridulm
62c92ac [Cheng Lian] Fixed SPARK-1194 https://spark-project.atlassian.net/browse/SPARK-1194
",1394263606
dabeb6f160f7ad7df1c54b1b8b069700dd4b74dd remotes/origin/vldb~71,"SPARK-1136: Fix FaultToleranceTest for Docker 0.8.1

This patch allows the FaultToleranceTest to work in newer versions of Docker.
See https://spark-project.atlassian.net/browse/SPARK-1136 for more details.

Besides changing the Docker and FaultToleranceTest internals, this patch also changes the behavior of Master to accept new Workers which share an address with a Worker that we are currently trying to recover. This can only happen when the Worker itself was restarted and got the same IP address/port at the same time as a Master recovery occurs.

Finally, this adds a good bit of ASCII art to the test to make failures, successes, and actions more apparent. This is very much needed.

Author: Aaron Davidson <aaron@databricks.com>

Closes #5 from aarondav/zookeeper and squashes the following commits:

5d7a72a [Aaron Davidson] SPARK-1136: Fix FaultToleranceTest for Docker 0.8.1
",1394216547
328c73d037c17440c2a91a6c88b4258fbefa0c08 remotes/origin/vldb~74,"SPARK-1197. Change yarn-standalone to yarn-cluster and fix up running on YARN docs

This patch changes ""yarn-standalone"" to ""yarn-cluster"" (but still supports the former).  It also cleans up the Running on YARN docs and adds a section on how to view logs.

Author: Sandy Ryza <sandy@cloudera.com>

Closes #95 from sryza/sandy-spark-1197 and squashes the following commits:

563ef3a [Sandy Ryza] Review feedback
6ad06d4 [Sandy Ryza] Change yarn-standalone to yarn-cluster and fix up running on YARN docs
",1394154778
7edbea41b43e0dc11a2de156be220db8b7952d01 remotes/origin/vldb~75,"SPARK-1189: Add Security to Spark - Akka, Http, ConnectionManager, UI use servlets

resubmit pull request.  was https://github.com/apache/incubator-spark/pull/332.

Author: Thomas Graves <tgraves@apache.org>

Closes #33 from tgravescs/security-branch-0.9-with-client-rebase and squashes the following commits:

dfe3918 [Thomas Graves] Fix merge conflict since startUserClass now using runAsUser
05eebed [Thomas Graves] Fix dependency lost in upmerge
d1040ec [Thomas Graves] Fix up various imports
05ff5e0 [Thomas Graves] Fix up imports after upmerging to master
ac046b3 [Thomas Graves] Merge remote-tracking branch 'upstream/master' into security-branch-0.9-with-client-rebase
13733e1 [Thomas Graves] Pass securityManager and SparkConf around where we can. Switch to use sparkConf for reading config whereever possible. Added ConnectionManagerSuite unit tests.
4a57acc [Thomas Graves] Change UI createHandler routines to createServlet since they now return servlets
2f77147 [Thomas Graves] Rework from comments
50dd9f2 [Thomas Graves] fix header in SecurityManager
ecbfb65 [Thomas Graves] Fix spacing and formatting
b514bec [Thomas Graves] Fix reference to config
ed3d1c1 [Thomas Graves] Add security.md
6f7ddf3 [Thomas Graves] Convert SaslClient and SaslServer to scala, change spark.authenticate.ui to spark.ui.acls.enable, and fix up various other things from review comments
2d9e23e [Thomas Graves] Merge remote-tracking branch 'upstream/master' into security-branch-0.9-with-client-rebase_rework
5721c5a [Thomas Graves] update AkkaUtilsSuite test for the actorSelection changes, fix typos based on comments, and remove extra lines I missed in rebase from AkkaUtils
f351763 [Thomas Graves] Add Security to Spark - Akka, Http, ConnectionManager, UI to use servlets
",1394152070
40566e10aae4b21ffc71ea72702b8df118ac5c8e remotes/origin/vldb~76,"SPARK-942: Do not materialize partitions when DISK_ONLY storage level is used

This is a port of a pull request original targeted at incubator-spark: https://github.com/apache/incubator-spark/pull/180

Essentially if a user returns a generative iterator (from a flatMap operation), when trying to persist the data, Spark would first unroll the iterator into an ArrayBuffer, and then try to figure out if it could store the data. In cases where the user provided an iterator that generated more data then available memory, this would case a crash. With this patch, if the user requests a persist with a 'StorageLevel.DISK_ONLY', the iterator will be unrolled as it is inputed into the serializer.

To do this, two changes where made:
1) The type of the 'values' argument in the putValues method of the BlockStore interface was changed from ArrayBuffer to Iterator (and all code interfacing with this method was modified to connect correctly.
2) The JavaSerializer now calls the ObjectOutputStream 'reset' method every 1000 objects. This was done because the ObjectOutputStream caches objects (thus preventing them from being GC'd) to write more compact serialization. If reset is never called, eventually the memory fills up, if it is called too often then the serialization streams become much larger because of redundant class descriptions.

Author: Kyle Ellrott <kellrott@gmail.com>

Closes #50 from kellrott/iterator-to-disk and squashes the following commits:

9ef7cb8 [Kyle Ellrott] Fixing formatting issues.
60e0c57 [Kyle Ellrott] Fixing issues (formatting, variable names, etc.) from review comments
8aa31cd [Kyle Ellrott] Merge ../incubator-spark into iterator-to-disk
33ac390 [Kyle Ellrott] Merge branch 'iterator-to-disk' of github.com:kellrott/incubator-spark into iterator-to-disk
2f684ea [Kyle Ellrott] Refactoring the BlockManager to replace the Either[Either[A,B]] usage. Now using trait 'Values'. Also modified BlockStore.putBytes call to return PutResult, so that it behaves like putValues.
f70d069 [Kyle Ellrott] Adding docs for spark.serializer.objectStreamReset configuration
7ccc74b [Kyle Ellrott] Moving the 'LargeIteratorSuite' to simply test persistance of iterators. It doesn't try to invoke a OOM error any more
16a4cea [Kyle Ellrott] Streamlined the LargeIteratorSuite unit test. It should now run in ~25 seconds. Confirmed that it still crashes an unpatched copy of Spark.
c2fb430 [Kyle Ellrott] Removing more un-needed array-buffer to iterator conversions
627a8b7 [Kyle Ellrott] Wrapping a few long lines
0f28ec7 [Kyle Ellrott] Adding second putValues to BlockStore interface that accepts an ArrayBuffer (rather then an Iterator). This will allow BlockStores to have slightly different behaviors dependent on whether they get an Iterator or ArrayBuffer. In the case of the MemoryStore, it needs to duplicate and cache an Iterator into an ArrayBuffer, but if handed a ArrayBuffer, it can skip the duplication.
656c33e [Kyle Ellrott] Fixing the JavaSerializer to read from the SparkConf rather then the System property.
8644ee8 [Kyle Ellrott] Merge branch 'master' into iterator-to-disk
00c98e0 [Kyle Ellrott] Making the Java ObjectStreamSerializer reset rate configurable by the system variable 'spark.serializer.objectStreamReset', default is not 10000.
40fe1d7 [Kyle Ellrott] Removing rouge space
31fe08e [Kyle Ellrott] Removing un-needed semi-colons
9df0276 [Kyle Ellrott] Added check to make sure that streamed-to-dist RDD actually returns good data in the LargeIteratorSuite
a6424ba [Kyle Ellrott] Wrapping long line
2eeda75 [Kyle Ellrott] Fixing dumb mistake (""||"" instead of ""&&"")
0e6f808 [Kyle Ellrott] Deleting temp output directory when done
95c7f67 [Kyle Ellrott] Simplifying StorageLevel checks
56f71cd [Kyle Ellrott] Merge branch 'master' into iterator-to-disk
44ec35a [Kyle Ellrott] Adding some comments.
5eb2b7e [Kyle Ellrott] Changing the JavaSerializer reset to occur every 1000 objects.
f403826 [Kyle Ellrott] Merge branch 'master' into iterator-to-disk
81d670c [Kyle Ellrott] Adding unit test for straight to disk iterator methods.
d32992f [Kyle Ellrott] Merge remote-tracking branch 'origin/master' into iterator-to-disk
cac1fad [Kyle Ellrott] Fixing MemoryStore, so that it converts incoming iterators to ArrayBuffer objects. This was previously done higher up the stack.
efe1102 [Kyle Ellrott] Changing CacheManager and BlockManager to pass iterators directly to the serializer when a 'DISK_ONLY' persist is called. This is in response to SPARK-942.
",1394146279
3eb009f362993dbe43028419c2d48011111a200d remotes/origin/vldb~78,"SPARK-1156: allow user to login into a cluster without slaves

Reported in https://spark-project.atlassian.net/browse/SPARK-1156

The current spark-ec2 script doesn't allow user to login to a cluster without slaves. One of the issues brought by this behaviour is that when all the worker died, the user cannot even login to the cluster for debugging, etc.

Author: CodingCat <zhunansjtu@gmail.com>

Closes #58 from CodingCat/SPARK-1156 and squashes the following commits:

104af07 [CodingCat] output ERROR to stderr
9a71769 [CodingCat] do not allow user to start 0-slave cluster
24a7c79 [CodingCat] allow user to login into a cluster without slaves
",1394084854
a3da5088195eea7d90b37feee5dd2a372fcd9ace remotes/origin/vldb~81,"SPARK-1171: when executor is removed, we should minus totalCores instead of just freeCores on that executor

https://spark-project.atlassian.net/browse/SPARK-1171

When the executor is removed, the current implementation will only minus the freeCores of that executor. Actually we should minus the totalCores...

Author: CodingCat <zhunansjtu@gmail.com>
Author: Nan Zhu <CodingCat@users.noreply.github.com>

Closes #63 from CodingCat/simplify_CoarseGrainedSchedulerBackend and squashes the following commits:

f6bf93f [Nan Zhu] code clean
19c2bb4 [CodingCat] use copy idiom to reconstruct the workerOffers
43c13e9 [CodingCat] keep WorkerOffer immutable
af470d3 [CodingCat] style fix
0c0e409 [CodingCat] simplify the implementation of CoarseGrainedSchedulerBackend
",1394056828
f65c1f38eb7ed99a578a5430831a4a2c1d774e7a remotes/origin/vldb~92,"SPARK-1173. (#2) Fix typo in Java streaming example.

Companion commit to pull request #64, fix the typo on the Java side of the docs.

Author: Aaron Kimball <aaron@magnify.io>

Closes #65 from kimballa/spark-1173-java-doc-update and squashes the following commits:

8ce11d3 [Aaron Kimball] SPARK-1173. (#2) Fix typo in Java streaming example.
",1393832928
2b53447f325fa7adcfb9c69fd824467bf420af04 remotes/origin/vldb~93,"SPARK-1173. Improve scala streaming docs.

Clarify imports to add implicit conversions to DStream and
fix other small typos in the streaming intro documentation.

Tested by inspecting output via a local jekyll server, c&p'ing the scala commands into a spark terminal.

Author: Aaron Kimball <aaron@magnify.io>

Closes #64 from kimballa/spark-1173-streaming-docs and squashes the following commits:

6fbff0e [Aaron Kimball] SPARK-1173. Improve scala streaming docs.
",1393831607
c3f5e075335a65ea522b2f76716921ec056c52ed remotes/origin/vldb~95,"SPARK-1121: Include avro for yarn-alpha builds

This lets us explicitly include Avro based on a profile for 0.23.X
builds. It makes me sad how convoluted it is to express this logic
in Maven. @tgraves and @sryza curious if this works for you.

I'm also considering just reverting to how it was before. The only
real problem was that Spark advertised a dependency on Avro
even though it only really depends transitively on Avro through
other deps.

Author: Patrick Wendell <pwendell@gmail.com>

Closes #49 from pwendell/avro-build-fix and squashes the following commits:

8d6ee92 [Patrick Wendell] SPARK-1121: Add avro to yarn-alpha profile
",1393802299
fd31adbf27d824f00f62646e13c23f632d1b77d3 remotes/origin/vldb~96,"SPARK-1084.2 (resubmitted)

(Ported from https://github.com/apache/incubator-spark/pull/650 )

This adds one more change though, to fix the scala version warning introduced by json4s recently.

Author: Sean Owen <sowen@cloudera.com>

Closes #32 from srowen/SPARK-1084.2 and squashes the following commits:

9240abd [Sean Owen] Avoid scala version conflict in scalap induced by json4s dependency
1561cec [Sean Owen] Remove ""exclude *"" dependencies that are causing Maven warnings, and that are apparently unneeded anyway
",1393799273
b70823c91d4dff9be81badf6567e25aa1df4c574 remotes/origin/vldb~100,"Update io.netty from 4.0.13 Final to 4.0.17.Final

This update contains a lot of bug fixes and some new perf improvements.
It is also binary compatible with the current 4.0.13.Final

For more information: http://netty.io/news/2014/02/25/4-0-17-Final.html

Author: Binh Nguyen <ngbinh@gmail.com>

Author: Binh Nguyen <ngbinh@gmail.com>

Closes #41 from ngbinh/master and squashes the following commits:

a9498f4 [Binh Nguyen] update io.netty to 4.0.17.Final
",1393750130
3a8b698e961ac05d9d53e2bbf0c2844fcb1010d1 remotes/origin/vldb~103,"[SPARK-1100] prevent Spark from overwriting directory silently

Thanks for Diana Carroll to report this issue (https://spark-project.atlassian.net/browse/SPARK-1100)

the current saveAsTextFile/SequenceFile will overwrite the output directory silently if the directory already exists, this behaviour is not desirable because

overwriting the data silently is not user-friendly

if the partition number of two writing operation changed, then the output directory will contain the results generated by two runnings

My fix includes:

add some new APIs with a flag for users to define whether he/she wants to overwrite the directory:
if the flag is set to true, then the output directory is deleted first and then written into the new data to prevent the output directory contains results from multiple rounds of running;

if the flag is set to false, Spark will throw an exception if the output directory already exists

changed JavaAPI part

default behaviour is overwriting

Two questions

should we deprecate the old APIs without such a flag?

I noticed that Spark Streaming also called these APIs, I thought we don't need to change the related part in streaming? @tdas

Author: CodingCat <zhunansjtu@gmail.com>

Closes #11 from CodingCat/SPARK-1100 and squashes the following commits:

6a4e3a3 [CodingCat] code clean
ef2d43f [CodingCat] add new test cases and code clean
ac63136 [CodingCat] checkOutputSpecs not applicable to FSOutputFormat
ec490e8 [CodingCat] prevent Spark from overwriting directory silently and leaving dirty directory
",1393723674
fe195ae113941766b3921b1e4ec222ed830b5b8f remotes/origin/vldb~104,"[SPARK-1150] fix repo location in create script (re-open)

reopen for https://spark-project.atlassian.net/browse/SPARK-1150

Author: CodingCat <zhunansjtu@gmail.com>

Closes #52 from CodingCat/script_fixes and squashes the following commits:

fc05a71 [CodingCat] fix repo location in create script
",1393723493
ec992e182231da7313d85d10b3d5fd5975c44c8b remotes/origin/vldb~105,"Revert ""[SPARK-1150] fix repo location in create script""

This reverts commit 9aa095711858ce8670e51488f66a3d7c1a821c30.
",1393722938
9aa095711858ce8670e51488f66a3d7c1a821c30 remotes/origin/vldb~106,"[SPARK-1150] fix repo location in create script

https://spark-project.atlassian.net/browse/SPARK-1150

fix the repo location in create_release script

Author: Mark Grover <mark@apache.org>

Closes #48 from CodingCat/script_fixes and squashes the following commits:

01f4bf7 [Mark Grover] Fixing some nitpicks
d2244d4 [Mark Grover] SPARK-676: Abbreviation in SPARK_MEM but not in SPARK_WORKER_MEMORY
",1393719682
4ba3f70a4e385368d0e826ba261f8eb60c25c896 remotes/origin/vldb~108,"SPARK-1151: Update dev merge script to use spark.git instead of incubator-spark

Author: Thomas Graves <tgraves@apache.org>

Closes #47 from tgravescs/fix_merge_script and squashes the following commits:

8209ab1 [Thomas Graves] Update dev merge script to use spark.git instead of incubator-spark
",1393640913
5f419bf9f433e8f057237f1d5bfed9f5f4e9427c remotes/origin/vldb~110,"SPARK-1032. If Yarn app fails before registering, app master stays aroun...

...d long after

This reopens https://github.com/apache/incubator-spark/pull/648 against the new repo.

Author: Sandy Ryza <sandy@cloudera.com>

Closes #28 from sryza/sandy-spark-1032 and squashes the following commits:

5953f50 [Sandy Ryza] SPARK-1032. If Yarn app fails before registering, app master stays around long after
",1393602047
12bbca20657c17d5ebfceaacb37dddc851772675 remotes/origin/vldb~114,"SPARK 1084.1 (resubmitted)

(Ported from https://github.com/apache/incubator-spark/pull/637 )

Author: Sean Owen <sowen@cloudera.com>

Closes #31 from srowen/SPARK-1084.1 and squashes the following commits:

6c4a32c [Sean Owen] Suppress warnings about legitimate unchecked array creations, or change code to avoid it
f35b833 [Sean Owen] Fix two misc javadoc problems
254e8ef [Sean Owen] Fix one new style error introduced in scaladoc warning commit
5b2fce2 [Sean Owen] Fix scaladoc invocation warning, and enable javac warnings properly, with plugin config updates
007762b [Sean Owen] Remove dead scaladoc links
b8ff8cb [Sean Owen] Replace deprecated Ant <tasks> with <target>
",1393528341
345df5f4a9c16a6a87440afa2b09082fc3d224bd remotes/origin/vldb~116,"[SPARK-1089] fix the regression problem on ADD_JARS in 0.9

https://spark-project.atlassian.net/browse/SPARK-1089

copied from JIRA, reported by @ash211

""Using the ADD_JARS environment variable with spark-shell used to add the jar to both the shell and the various workers. Now it only adds to the workers and importing a custom class in the shell is broken.
The workaround is to add custom jars to both ADD_JARS and SPARK_CLASSPATH.
We should fix ADD_JARS so it works properly again.
See various threads on the user list:
https://mail-archives.apache.org/mod_mbox/incubator-spark-user/201402.mbox/%3CCAJbo4neMLiTrnm1XbyqomWmp0m+EUcg4yE-txuRGSVKOb5KLeA@mail.gmail.com%3E
(another one that doesn't appear in the archives yet titled ""ADD_JARS not working on 0.9"")""

The reason of this bug is two-folds

in the current implementation of SparkILoop.scala, the settings.classpath is not set properly when the process() method is invoked

the weird behaviour of Scala 2.10, (I personally thought it is a bug)

if we simply set value of a PathSettings object (like settings.classpath), the isDefault is not set to true (this is a flag showing if the variable is modified), so it makes the PathResolver loads the default CLASSPATH environment variable value to calculated the path (see https://github.com/scala/scala/blob/2.10.x/src/compiler/scala/tools/util/PathResolver.scala#L215)

what we have to do is to manually make this flag set, (https://github.com/CodingCat/incubator-spark/blob/e3991d97ddc33e77645e4559b13bf78b9e68239a/repl/src/main/scala/org/apache/spark/repl/SparkILoop.scala#L884)

Author: CodingCat <zhunansjtu@gmail.com>

Closes #13 from CodingCat/SPARK-1089 and squashes the following commits:

8af81e7 [CodingCat] impose non-null settings
9aa2125 [CodingCat] code cleaning
ce36676 [CodingCat] code cleaning
e045582 [CodingCat] fix the regression problem on ADD_JARS in 0.9
",1393486935
6ccd6c55bdfcf1a4f8f8fd1a9715d4a45ec1703a remotes/origin/vldb~117,"SPARK-1121 Only add avro if the build is for Hadoop 0.23.X and SPARK_YARN is set

Author: Prashant Sharma <prashant.s@imaginea.com>

Closes #6 from ScrapCodes/SPARK-1121/avro-dep-fix and squashes the following commits:

9b29e34 [Prashant Sharma] Review feedback on PR
46ed2ad [Prashant Sharma] SPARK-1121-Only add avro if the build is for Hadoop 0.23.X and SPARK_YARN is set
",1393486849
12738c1aec136acd7f2e3e2f8f2b541db0890630 remotes/origin/vldb~123,"SPARK-1115: Catch depickling errors

This surroungs the complete worker code in a try/except block so we catch any error that arrives. An example would be the depickling failing for some reason

@JoshRosen

Author: Bouke van der Bijl <boukevanderbijl@gmail.com>

Closes #644 from bouk/catch-depickling-errors and squashes the following commits:

f0f67cc [Bouke van der Bijl] Lol indentation
0e4d504 [Bouke van der Bijl] Surround the complete python worker with the try block
",1393455081
c86eec584312072e73754a4f1cbe34d2e1968c77 remotes/origin/vldb~124,"SPARK-1135: fix broken anchors in docs

A recent PR that added Java vs Scala tabs for streaming also
inadvertently added some bad code to a document.ready handler, breaking
our other handler that manages scrolling to anchors correctly with the
floating top bar. As a result the section title ended up always being
hidden below the top bar. This removes the unnecessary JavaScript code.

Author: Matei Zaharia <matei@databricks.com>

Closes #3 from mateiz/doc-links and squashes the following commits:

e2a3488 [Matei Zaharia] SPARK-1135: fix broken anchors in docs
",1393442416
1f4c7f7ecc9d2393663fc4d059e71fe4c70bad84 remotes/origin/vldb~128,"Graph primitives2

Hi guys,

I'm following Joey and Ankur's suggestions to add collectEdges and pickRandomVertex. I'm also adding the tests for collectEdges and refactoring one method getCycleGraph in GraphOpsSuite.scala.

Thank you,

semih

Author: Semih Salihoglu <semihsalihoglu@gmail.com>

Closes #580 from semihsalihoglu/GraphPrimitives2 and squashes the following commits:

937d3ec [Semih Salihoglu] - Fixed the scalastyle errors.
a69a152 [Semih Salihoglu] - Adding collectEdges and pickRandomVertices. - Adding tests for collectEdges. - Refactoring a getCycle utility function for GraphOpsSuite.scala.
41265a6 [Semih Salihoglu] - Adding collectEdges and pickRandomVertex. - Adding tests for collectEdges. - Recycling a getCycle utility test file.
",1393310550
4d880304867b55a4f2138617b30600b7fa013b14 remotes/origin/vldb~130,"For outputformats that are Configurable, call setConf before sending data to them.

[SPARK-1108] This allows us to use, e.g. HBase's TableOutputFormat with PairRDDFunctions.saveAsNewAPIHadoopFile, which otherwise would throw NullPointerException because the output table name hasn't been configured.

Note this bug also affects branch-0.9

Author: Bryn Keller <bryn.keller@intel.com>

Closes #638 from xoltar/SPARK-1108 and squashes the following commits:

7e94e7d [Bryn Keller] Import, comment, and format cleanup per code review
7cbcaa1 [Bryn Keller] For outputformats that are Configurable, call setConf before sending data to them. This allows us to use, e.g. HBase TableOutputFormat, which otherwise would throw NullPointerException because the output table name hasn't been configured
",1393292122
d8d190efd2d08c3894b20f6814b10f9ca2157309 remotes/origin/vldb~131,"Merge pull request #641 from mateiz/spark-1124-master

SPARK-1124: Fix infinite retries of reduce stage when a map stage failed

In the previous code, if you had a failing map stage and then tried to run reduce stages on it repeatedly, the first reduce stage would fail correctly, but the later ones would mistakenly believe that all map outputs are available and start failing infinitely with fetch failures from ""null"". See https://spark-project.atlassian.net/browse/SPARK-1124 for an example.

This PR also cleans up code style slightly where there was a variable named ""s"" and some weird map manipulation.
",1393289937
cd32d5e4dee1291e4509e5965322b7ffe620b1f3 remotes/origin/vldb~131^2~1,"SPARK-1124: Fix infinite retries of reduce stage when a map stage failed

In the previous code, if you had a failing map stage and then tried to
run reduce stages on it repeatedly, the first reduce stage would fail
correctly, but the later ones would mistakenly believe that all map
outputs are available and start failing infinitely with fetch failures
from ""null"".
",1393228112
c0ef3afa82c1eaf58ff5efec961540a74b639fd9 remotes/origin/vldb~132,"SPARK-1071: Tidy logging strategy and use of log4j

Prompted by a recent thread on the mailing list, I tried and failed to see if Spark can be made independent of log4j. There are a few cases where control of the underlying logging is pretty useful, and to do that, you have to bind to a specific logger.

Instead I propose some tidying that leaves Spark's use of log4j, but gets rid of warnings and should still enable downstream users to switch. The idea is to pipe everything (except log4j) through SLF4J, and have Spark use SLF4J directly when logging, and where Spark needs to output info (REPL and tests), bind from SLF4J to log4j.

This leaves the same behavior in Spark. It means that downstream users who want to use something except log4j should:

- Exclude dependencies on log4j, slf4j-log4j12 from Spark
- Include dependency on log4j-over-slf4j
- Include dependency on another logger X, and another slf4j-X
- Recreate any log config that Spark does, that is needed, in the other logger's config

That sounds about right.

Here are the key changes:

- Include the jcl-over-slf4j shim everywhere by depending on it in core.
- Exclude dependencies on commons-logging from third-party libraries.
- Include the jul-to-slf4j shim everywhere by depending on it in core.
- Exclude slf4j-* dependencies from third-party libraries to prevent collision or warnings
- Added missing slf4j-log4j12 binding to GraphX, Bagel module tests

And minor/incidental changes:

- Update to SLF4J 1.7.5, which happily matches Hadoop 2’s version and is a recommended update over 1.7.2
- (Remove a duplicate HBase dependency declaration in SparkBuild.scala)
- (Remove a duplicate mockito dependency declaration that was causing warnings and bugging me)

Author: Sean Owen <sowen@cloudera.com>

Closes #570 from srowen/SPARK-1071 and squashes the following commits:

52eac9f [Sean Owen] Add slf4j-over-log4j12 dependency to core (non-test) and remove it from things that depend on core.
77a7fa9 [Sean Owen] SPARK-1071: Tidy logging strategy and use of log4j
",1393184455
1aa4f8af7220bc311196ef0eef0a4814cd2757d3 remotes/origin/vldb~135,"[SPARK-1055] fix the SCALA_VERSION and SPARK_VERSION in docker file

As reported in https://spark-project.atlassian.net/browse/SPARK-1055

""The used Spark version in the .../base/Dockerfile is stale on 0.8.1 and should be updated to 0.9.x to match the release.""

Author: CodingCat <zhunansjtu@gmail.com>
Author: Nan Zhu <CodingCat@users.noreply.github.com>

Closes #634 from CodingCat/SPARK-1055 and squashes the following commits:

cb7330e [Nan Zhu] Update Dockerfile
adf8259 [CodingCat] fix the SCALA_VERSION and SPARK_VERSION in docker file
",1393112365
3ff077d489af99ad36c9d2389e2afab6465648d4 remotes/origin/vldb~137,"Fixed minor typo in worker.py

Fixed minor typo in worker.py

Author: jyotiska <jyotiska123@gmail.com>

Closes #630 from jyotiska/pyspark_code and squashes the following commits:

ee44201 [jyotiska] typo fixed in worker.py
",1393092590
fefd22f4c3e95d904cb6f4f3fd88b89050907ae9 remotes/origin/vldb~139,"[SPARK-1113] External spilling - fix Int.MaxValue hash code collision bug

The original poster of this bug is @guojc, who opened a PR that preceded this one at https://github.com/apache/incubator-spark/pull/612.

ExternalAppendOnlyMap uses key hash code to order the buffer streams from which spilled files are read back into memory. When a buffer stream is empty, the default hash code for that stream is equal to Int.MaxValue. This is, however, a perfectly legitimate candidate for a key hash code. When reading from a spilled map containing such a key, a hash collision may occur, in which case we attempt to read from an empty stream and throw NoSuchElementException.

The fix is to maintain the invariant that empty buffer streams are never added back to the merge queue to be considered. This guarantees that we never read from an empty buffer stream, ever again.

This PR also includes two new tests for hash collisions.

Author: Andrew Or <andrewor14@gmail.com>

Closes #624 from andrewor14/spilling-bug and squashes the following commits:

9e7263d [Andrew Or] Slightly optimize next()
2037ae2 [Andrew Or] Move a few comments around...
cf95942 [Andrew Or] Remove default value of Int.MaxValue for minKeyHash
c11f03b [Andrew Or] Fix Int.MaxValue hash collision bug in ExternalAppendOnlyMap
21c1a39 [Andrew Or] Add hash collision tests to ExternalAppendOnlyMapSuite
",1393041939
c8a4c9b1f6005815f5a4a331970624d1706b6b13 remotes/origin/vldb~140,"MLLIB-25: Implicit ALS runs out of memory for moderately large numbers of features

There's a step in implicit ALS where the matrix `Yt * Y` is computed. It's computed as the sum of matrices; an f x f matrix is created for each of n user/item rows in a partition. In `ALS.scala:214`:

```
        factors.flatMapValues{ case factorArray =>
          factorArray.map{ vector =>
            val x = new DoubleMatrix(vector)
            x.mmul(x.transpose())
          }
        }.reduceByKeyLocally((a, b) => a.addi(b))
         .values
         .reduce((a, b) => a.addi(b))
```

Completely correct, but there's a subtle but quite large memory problem here. map() is going to create all of these matrices in memory at once, when they don't need to ever all exist at the same time.
For example, if a partition has n = 100000 rows, and f = 200, then this intermediate product requires 32GB of heap. The computation will never work unless you can cough up workers with (more than) that much heap.

Fortunately there's a trivial change that fixes it; just add `.view` in there.

Author: Sean Owen <sowen@cloudera.com>

Closes #629 from srowen/ALSMatrixAllocationOptimization and squashes the following commits:

062cda9 [Sean Owen] Update style per review comments
e9a5d63 [Sean Owen] Avoid unnecessary out of memory situation by not simultaneously allocating lots of matrices
",1393015572
45b15e27a84527abeaa8588b0eb1ade7e831e6ef remotes/origin/vldb~141,"SPARK-1111: URL Validation Throws Error for HDFS URL's

Fixes an error where HDFS URL's cause an exception. Should be merged into master and 0.9.

Author: Patrick Wendell <pwendell@gmail.com>

Closes #625 from pwendell/url-validation and squashes the following commits:

d14bfe3 [Patrick Wendell] SPARK-1111: URL Validation Throws Error for HDFS URL's
",1393009915
9e63f80e75bb6d9bbe6df268908c3219de6852d9 remotes/origin/vldb~144,"MLLIB-22. Support negative implicit input in ALS

I'm back with another less trivial suggestion for ALS:

In ALS for implicit feedback, input values are treated as weights on squared-errors in a loss function (or rather, the weight is a simple function of the input r, like c = 1 + alpha*r). The paper on which it's based assumes that the input is positive. Indeed, if the input is negative, it will create a negative weight on squared-errors, which causes things to go haywire. The optimization will try to make the error in a cell as large possible, and the result is silently bogus.

There is a good use case for negative input values though. Implicit feedback is usually collected from signals of positive interaction like a view or like or buy, but equally, can come from ""not interested"" signals. The natural representation is negative values.

The algorithm can be extended quite simply to provide a sound interpretation of these values: negative values should encourage the factorization to come up with 0 for cells with large negative input values, just as much as positive values encourage it to come up with 1.

The implications for the algorithm are simple:
* the confidence function value must not be negative, and so can become 1 + alpha*|r|
* the matrix P should have a value 1 where the input R is _positive_, not merely where it is non-zero. Actually, that's what the paper already says, it's just that we can't assume P = 1 when a cell in R is specified anymore, since it may be negative

This in turn entails just a few lines of code change in `ALS.scala`:
* `rs(i)` becomes `abs(rs(i))`
* When constructing `userXy(us(i))`, it's implicitly only adding where P is 1. That had been true for any us(i) that is iterated over, before, since these are exactly the ones for which P is 1. But now P is zero where rs(i) <= 0, and should not be added

I think it's a safe change because:
* It doesn't change any existing behavior (unless you're using negative values, in which case results are already borked)
* It's the simplest direct extension of the paper's algorithm
* (I've used it to good effect in production FWIW)

Tests included.

I tweaked minor things en route:
* `ALS.scala` javadoc writes ""R = Xt*Y"" when the paper and rest of code defines it as ""R = X*Yt""
* RMSE in the ALS tests uses a confidence-weighted mean, but the denominator is not actually sum of weights

Excuse my Scala style; I'm sure it needs tweaks.

Author: Sean Owen <sowen@cloudera.com>

Closes #500 from srowen/ALSNegativeImplicitInput and squashes the following commits:

cf902a9 [Sean Owen] Support negative implicit input in ALS
953be1c [Sean Owen] Make weighted RMSE in ALS test actually weighted; adjust comment about R = X*Yt
",1392882293
f9b7d64a4e7dd03be672728335cb72df4be5dbf6 remotes/origin/vldb~145,"MLLIB-24:  url of ""Collaborative Filtering for Implicit Feedback Datasets"" in ALS is invalid now

url of ""Collaborative Filtering for Implicit Feedback Datasets""  is invalid now. A new url is provided. http://research.yahoo.com/files/HuKorenVolinsky-ICDM08.pdf

Author: Chen Chao <crazyjvm@gmail.com>

Closes #619 from CrazyJvm/master and squashes the following commits:

a0b54e4 [Chen Chao] change url to IEEE
9e0e9f0 [Chen Chao] correct spell mistale
fcfab5d [Chen Chao] wrap line to to fit within 100 chars
590d56e [Chen Chao] url error
",1392876395
7b012c93973201a1cbb4fc9a02e322152e5185a9 remotes/origin/vldb~146,"[SPARK-1105] fix site scala version error in docs

https://spark-project.atlassian.net/browse/SPARK-1105

fix site scala version error

Author: CodingCat <zhunansjtu@gmail.com>

Closes #618 from CodingCat/doc_version and squashes the following commits:

39bb8aa [CodingCat] more fixes
65bedb0 [CodingCat] fix site scala version error in doc
",1392854043
b61435c7ff620a05bee65607aed249541ab54b13 remotes/origin/vldb~147,"SPARK-1106: check key name and identity file before launch a cluster

I launched an EC2 cluster without providing a key name and an identity file. The error showed up after two minutes. It would be good to check those options before launch, given the fact that EC2 billing rounds up to hours.

JIRA: https://spark-project.atlassian.net/browse/SPARK-1106

Author: Xiangrui Meng <meng@databricks.com>

Closes #617 from mengxr/ec2 and squashes the following commits:

2dfb316 [Xiangrui Meng] check key name and identity file before launch a cluster
",1392777002
d9bb32a790e76e35f32229082648f97170ffef07 remotes/origin/vldb~148,"Revert ""[SPARK-1105] fix site scala version error in doc""

This reverts commit d99773d5bba674cc1434c86435b6d9b3739314c8.
",1392774407
d99773d5bba674cc1434c86435b6d9b3739314c8 remotes/origin/vldb~149,"[SPARK-1105] fix site scala version error in doc

https://spark-project.atlassian.net/browse/SPARK-1105

fix site scala version error

Author: CodingCat <zhunansjtu@gmail.com>

Closes #616 from CodingCat/doc_version and squashes the following commits:

eafd99a [CodingCat] fix site scala version error in doc
",1392769763
c0795cf481d47425ec92f4fd0780e2e0b3fdda85 remotes/origin/vldb~154,"Worker registration logging fix

Author: Andrew Ash <andrew@andrewash.com>

Closes #608 from ash211/patch-7 and squashes the following commits:

bd85f2a [Andrew Ash] Worker registration logging fix
",1392659515
73cfdcfe71c3fdd4a9c5e71c8568f25371dab9bf remotes/origin/vldb~156,"fix for https://spark-project.atlassian.net/browse/SPARK-1052

Author: Bijay Bisht <bijay.bisht@gmail.com>

Closes #568 from bijaybisht/SPARK-1052 and squashes the following commits:

da70395 [Bijay Bisht] fix for https://spark-project.atlassian.net/browse/SPARK-1052 - comments incorporated
fdb1d94 [Bijay Bisht] fix for https://spark-project.atlassian.net/browse/SPARK-1052

(cherry picked from commit e797c1abd9692f1b7ec290e4c83d31fd106e6b05)
Signed-off-by: Aaron Davidson <aaron@databricks.com>
",1392598443
a3bb86179e452d348f7e8bd3859befb3ff1f4df1 remotes/origin/vldb~161,"Ported hadoopClient jar for < 1.0.1 fix

#522 got messed after i rewrote the branch hadoop_jar_name. So created a new one.

Author: Bijay Bisht <bijay.bisht@gmail.com>

Closes #584 from bijaybisht/hadoop_jar_name_on_0.9.0 and squashes the following commits:

1b6fb3c [Bijay Bisht] Ported hadoopClient jar for < 1.0.1 fix
(cherry picked from commit 8093de1bb319e86dcf0d6d8d97b043a2bc1aa8f2)

Signed-off-by: Patrick Wendell <pwendell@gmail.com>
",1392277345
68b2c0d02dbdca246ca686b871c06af53845d5b5 remotes/origin/vldb~167,"Merge pull request #583 from colorant/zookeeper.

Minor fix for ZooKeeperPersistenceEngine to use configured working dir

Author: Raymond Liu <raymond.liu@intel.com>

Closes #583 and squashes the following commits:

91b0609 [Raymond Liu] Minor fix for ZooKeeperPersistenceEngine to use configured working dir
",1392187188
ba38d9892ec922ff11f204cd4c1b8ddc90f1bd55 remotes/origin/vldb~169,"Merge pull request #577 from hsaputra/fix_simple_streaming_doc.

SPARK-1075 Fix doc in the Spark Streaming custom receiver closing bracket in the class constructor

The closing parentheses in the constructor in the first code block example is reversed:
diff --git a/docs/streaming-custom-receivers.md b/docs/streaming-custom-receivers.md
index 4e27d65..3fb540c 100644
— a/docs/streaming-custom-receivers.md
+++ b/docs/streaming-custom-receivers.md
@@ -14,7 +14,7 @@ This starts with implementing NetworkReceiver(api/streaming/index.html#org.apa
The following is a simple socket text-stream receiver.
{% highlight scala %}
class SocketTextStreamReceiver(host: String, port: Int(
+ class SocketTextStreamReceiver(host: String, port: Int)
extends NetworkReceiverString
{
protected lazy val blocksGenerator: BlockGenerator =

Author: Henry Saputra <henry@platfora.com>

Closes #577 and squashes the following commits:

6508341 [Henry Saputra] SPARK-1075 Fix doc in the Spark Streaming custom receiver.
",1392158782
919bd7f669c61500eee7231298d9880b320eb6f3 remotes/origin/vldb~172,"Merge pull request #567 from ScrapCodes/style2.

SPARK-1058, Fix Style Errors and Add Scala Style to Spark Build. Pt 2

Continuation of PR #557

With this all scala style errors are fixed across the code base !!

The reason for creating a separate PR was to not interrupt an already reviewed and ready to merge PR. Hope this gets reviewed soon and merged too.

Author: Prashant Sharma <prashant.s@imaginea.com>

Closes #567 and squashes the following commits:

3b1ec30 [Prashant Sharma] scala style fixes
",1392013072
94ccf869aacbe99b7ca7a40ca585a759923cb407 remotes/origin/vldb~175,"Merge pull request #569 from pwendell/merge-fixes.

Fixes bug where merges won't close associated pull request.

Previously we added ""Closes #XX"" in the title. Github will sometimes
linbreak the title in a way that causes this to not work. This patch
instead adds the line in the body.

This also makes the commit format more concise for merge commits.
We might consider just dropping those in the future.

Author: Patrick Wendell <pwendell@gmail.com>

Closes #569 and squashes the following commits:

732eba1 [Patrick Wendell] Fixes bug where merges won't close associated pull request.
",1391982867
b69f8b2a01669851c656739b6886efe4cddef31a remotes/origin/vldb~176,"Merge pull request #557 from ScrapCodes/style. Closes #557.

SPARK-1058, Fix Style Errors and Add Scala Style to Spark Build.

Author: Patrick Wendell <pwendell@gmail.com>
Author: Prashant Sharma <scrapcodes@gmail.com>

== Merge branch commits ==

commit 1a8bd1c059b842cb95cc246aaea74a79fec684f4
Author: Prashant Sharma <scrapcodes@gmail.com>
Date:   Sun Feb 9 17:39:07 2014 +0530

    scala style fixes

commit f91709887a8e0b608c5c2b282db19b8a44d53a43
Author: Patrick Wendell <pwendell@gmail.com>
Date:   Fri Jan 24 11:22:53 2014 -0800

    Adding scalastyle snapshot
",1391969359
b6d40b782327188a25ded5b22790552121e5271f remotes/origin/vldb~179,"Merge pull request #560 from pwendell/logging. Closes #560.

[WIP] SPARK-1067: Default log4j initialization causes errors for those not using log4j

To fix this - we add a check when initializing log4j.

Author: Patrick Wendell <pwendell@gmail.com>

== Merge branch commits ==

commit ffdce513877f64b6eed6d36138c3e0003d392889
Author: Patrick Wendell <pwendell@gmail.com>
Date:   Fri Feb 7 15:22:29 2014 -0800

    Logging fix
",1391931331
3a9d82cc9e85accb5c1577cf4718aa44c8d5038c remotes/origin/vldb~185,"Merge pull request #506 from ash211/intersection. Closes #506.

SPARK-1062 Add rdd.intersection(otherRdd) method

Author: Andrew Ash <andrew@andrewash.com>

== Merge branch commits ==

commit 5d9982b171b9572649e9828f37ef0b43f0242912
Author: Andrew Ash <andrew@andrewash.com>
Date:   Thu Feb 6 18:11:45 2014 -0800

    Minor fixes

    - style: (v,null) => (v, null)
    - mention the shuffle in Javadoc

commit b86d02f14e810902719cef893cf6bfa18ff9acb0
Author: Andrew Ash <andrew@andrewash.com>
Date:   Sun Feb 2 13:17:40 2014 -0800

    Overload .intersection() for numPartitions and custom Partitioner

commit bcaa34911fcc6bb5bc5e4f9fe46d1df73cb71c09
Author: Andrew Ash <andrew@andrewash.com>
Date:   Sun Feb 2 13:05:40 2014 -0800

    Better naming of parameters in intersection's filter

commit b10a6af2d793ec6e9a06c798007fac3f6b860d89
Author: Andrew Ash <andrew@andrewash.com>
Date:   Sat Jan 25 23:06:26 2014 -0800

    Follow spark code format conventions of tab => 2 spaces

commit 965256e4304cca514bb36a1a36087711dec535ec
Author: Andrew Ash <andrew@andrewash.com>
Date:   Fri Jan 24 00:28:01 2014 -0800

    Add rdd.intersection(otherRdd) method
",1391755148
0b448df6ac520a7977b1eb51e8c55e33f3fd2da8 remotes/origin/vldb~187,"Merge pull request #450 from kayousterhout/fetch_failures. Closes #450.

Only run ResubmitFailedStages event after a fetch fails

Previously, the ResubmitFailedStages event was called every
200 milliseconds, leading to a lot of unnecessary event processing
and clogged DAGScheduler logs.

Author: Kay Ousterhout <kayousterhout@gmail.com>

== Merge branch commits ==

commit e603784b3a562980e6f1863845097effe2129d3b
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   Wed Feb 5 11:34:41 2014 -0800

    Re-add check for empty set of failed stages

commit d258f0ef50caff4bbb19fb95a6b82186db1935bf
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   Wed Jan 15 23:35:41 2014 -0800

    Only run ResubmitFailedStages event after a fetch fails

    Previously, the ResubmitFailedStages event was called every
    200 milliseconds, leading to a lot of unnecessary event processing
    and clogged DAGScheduler logs.
",1391732124
18ad59e2c6b7bd009e8ba5ebf8fcf99630863029 remotes/origin/vldb~188,"Merge pull request #321 from kayousterhout/ui_kill_fix. Closes #321.

Inform DAG scheduler about all started/finished tasks.

Previously, the DAG scheduler was not always informed
when tasks started and finished. The simplest example here
is for speculated tasks: the DAGScheduler was only told about
the first attempt of a task, meaning that SparkListeners were
also not told about multiple task attempts, so users can't see
what's going on with speculation in the UI.  The DAGScheduler
also wasn't always told about finished tasks, so in the UI, some
tasks will never be shown as finished (this occurs, for example,
if a task set gets killed).

The other problem is that the fairness accounting was wrong
-- the number of running tasks in a pool was decreased when a
task set was considered done, even if all of its tasks hadn't
yet finished.

Author: Kay Ousterhout <kayousterhout@gmail.com>

== Merge branch commits ==

commit c8d547d0f7a17f5a193bef05f5872b9f475675c5
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   Wed Jan 15 16:47:33 2014 -0800

    Addressed Reynold's review comments.

    Always use a TaskEndReason (remove the option), and explicitly
    signal when we don't know the reason. Also, always tell
    DAGScheduler (and associated listeners) about started tasks, even
    when they're speculated.

commit 3fee1e2e3c06b975ff7f95d595448f38cce97a04
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   Wed Jan 8 22:58:13 2014 -0800

    Fixed broken test and improved logging

commit ff12fcaa2567c5d02b75a1d5db35687225bcd46f
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   Sun Dec 29 21:08:20 2013 -0800

    Inform DAG scheduler about all finished tasks.

    Previously, the DAG scheduler was not always informed
    when tasks finished. For example, when a task set was
    aborted, the DAG scheduler was never told when the tasks
    in that task set finished. The DAG scheduler was also
    never told about the completion of speculated tasks.
    This led to confusion with SparkListeners because information
    about the completion of those tasks was never passed on to
    the listeners (so in the UI, for example, some tasks will never
    be shown as finished).

    The other problem is that the fairness accounting was wrong
    -- the number of running tasks in a pool was decreased when a
    task set was considered done, even if all of its tasks hadn't
    yet finished.
",1391731848
084839ba357e03bb56517620123682b50a91cb0b remotes/origin/vldb~190,"Merge pull request #498 from ScrapCodes/python-api. Closes #498.

Python api additions

Author: Prashant Sharma <prashant.s@imaginea.com>

== Merge branch commits ==

commit 8b51591f1a7a79a62c13ee66ff8d83040f7eccd8
Author: Prashant Sharma <prashant.s@imaginea.com>
Date:   Fri Jan 24 11:50:29 2014 +0530

    Josh's and Patricks review comments.

commit d37f9677838e43bef6c18ef61fbf08055ba6d1ca
Author: Prashant Sharma <prashant.s@imaginea.com>
Date:   Thu Jan 23 17:27:17 2014 +0530

    fixed doc tests

commit 27cb54bf5c99b1ea38a73858c291d0a1c43d8b7c
Author: Prashant Sharma <prashant.s@imaginea.com>
Date:   Thu Jan 23 16:48:43 2014 +0530

    Added keys and values methods for PairFunctions in python

commit 4ce76b396fbaefef2386d7a36d611572bdef9b5d
Author: Prashant Sharma <prashant.s@imaginea.com>
Date:   Thu Jan 23 13:51:26 2014 +0530

    Added foreachPartition

commit 05f05341a187cba829ac0e6c2bdf30be49948c89
Author: Prashant Sharma <prashant.s@imaginea.com>
Date:   Thu Jan 23 13:02:59 2014 +0530

    Added coalesce fucntion to python API

commit 6568d2c2fa14845dc56322c0f39ba2e13b3b26dd
Author: Prashant Sharma <prashant.s@imaginea.com>
Date:   Thu Jan 23 12:52:44 2014 +0530

    added repartition function to python API.
",1391727515
79c95527a77af32bd83a968c1a56feb22e441b7d remotes/origin/vldb~191,"Merge pull request #545 from kayousterhout/fix_progress. Closes #545.

Fix off-by-one error with task progress info log.

Author: Kay Ousterhout <kayousterhout@gmail.com>

== Merge branch commits ==

commit 29798fc685c4e7e3eb3bf91c75df7fa8ec94a235
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   Wed Feb 5 13:40:01 2014 -0800

    Fix off-by-one error with task progress info log.
",1391672292
38020961d101e792393855fd00d8e42f40713754 remotes/origin/vldb~192,"Merge pull request #526 from tgravescs/yarn_client_stop_am_fix. Closes #526.

spark on yarn - yarn-client mode doesn't always exit immediately

https://spark-project.atlassian.net/browse/SPARK-1049

If you run in the yarn-client mode but you don't get all the workers you requested right away and then you exit your application, the application master stays around until it gets the number of workers you initially requested. This is a waste of resources.  The AM should exit immediately upon the client going away.

This fix simply checks to see if the driver closed while its waiting for the initial # of workers.

Author: Thomas Graves <tgraves@apache.org>

== Merge branch commits ==

commit 03f40a62584b6bdd094ba91670cd4aa6afe7cd81
Author: Thomas Graves <tgraves@apache.org>
Date:   Fri Jan 31 11:23:10 2014 -0600

    spark on yarn - yarn-client mode doesn't always exit immediately
",1391672227
cc14ba974c8e98c08548a2ccf64c2765f313f649 remotes/origin/vldb~194,"Merge pull request #544 from kayousterhout/fix_test_warnings. Closes #544.

Fixed warnings in test compilation.

This commit fixes two problems: a redundant import, and a
deprecated function.

Author: Kay Ousterhout <kayousterhout@gmail.com>

== Merge branch commits ==

commit da9d2e13ee4102bc58888df0559c65cb26232a82
Author: Kay Ousterhout <kayousterhout@gmail.com>
Date:   Wed Feb 5 11:41:51 2014 -0800

    Fixed warnings in test compilation.

    This commit fixes two problems: a redundant import, and a
    deprecated function.
",1391633064
f7fd80d9a71069cba94294e6b77c0eaeb90e73d7 remotes/origin/vldb~195,"Merge pull request #540 from sslavic/patch-3. Closes #540.

Fix line end character stripping for Windows

LogQuery Spark example would produce unwanted result when run on Windows platform because of different, platform specific trailing line end characters (not only \n but \r too).

This fix makes use of Scala's standard library string functions to properly strip all trailing line end characters, letting Scala handle the platform specific stuff.

Author: Stevo Slavić <sslavic@gmail.com>

== Merge branch commits ==

commit 1e43ba0ea773cc005cf0aef78b6c1755f8e88b27
Author: Stevo Slavić <sslavic@gmail.com>
Date:   Wed Feb 5 14:48:29 2014 +0100

    Fix line end character stripping for Windows

    LogQuery Spark example would produce unwanted result when run on Windows platform because of different, platform specific trailing line end characters (not only \n but \r too).

    This fix makes use of Scala's standard library string functions to properly strip all trailing line end characters, letting Scala handle the platform specific stuff.
",1391624985
23af00f9e0e5108f62cdb9629e3eb4e54bbaa321 remotes/origin/vldb~198,"Merge pull request #528 from mengxr/sample. Closes #528.

 Refactor RDD sampling and add randomSplit to RDD (update)

Replace SampledRDD by PartitionwiseSampledRDD, which accepts a RandomSampler instance as input. The current sample with/without replacement can be easily integrated via BernoulliSampler and PoissonSampler. The benefits are:

1) RDD.randomSplit is implemented in the same way, related to https://github.com/apache/incubator-spark/pull/513
2) Stratified sampling and importance sampling can be implemented in the same manner as well.

Unit tests are included for samplers and RDD.randomSplit.

This should performance better than my previous request where the BernoulliSampler creates many Iterator instances:
https://github.com/apache/incubator-spark/pull/513

Author: Xiangrui Meng <meng@databricks.com>

== Merge branch commits ==

commit e8ce957e5f0a600f2dec057924f4a2ca6adba373
Author: Xiangrui Meng <meng@databricks.com>
Date:   Mon Feb 3 12:21:08 2014 -0800

    more docs to PartitionwiseSampledRDD

commit fbb4586d0478ff638b24bce95f75ff06f713d43b
Author: Xiangrui Meng <meng@databricks.com>
Date:   Mon Feb 3 00:44:23 2014 -0800

    move XORShiftRandom to util.random and use it in BernoulliSampler

commit 987456b0ee8612fd4f73cb8c40967112dc3c4c2d
Author: Xiangrui Meng <meng@databricks.com>
Date:   Sat Feb 1 11:06:59 2014 -0800

    relax assertions in SortingSuite because the RangePartitioner has large variance in this case

commit 3690aae416b2dc9b2f9ba32efa465ba7948477f4
Author: Xiangrui Meng <meng@databricks.com>
Date:   Sat Feb 1 09:56:28 2014 -0800

    test split ratio of RDD.randomSplit

commit 8a410bc933a60c4d63852606f8bbc812e416d6ae
Author: Xiangrui Meng <meng@databricks.com>
Date:   Sat Feb 1 09:25:22 2014 -0800

    add a test to ensure seed distribution and minor style update

commit ce7e866f674c30ab48a9ceb09da846d5362ab4b6
Author: Xiangrui Meng <meng@databricks.com>
Date:   Fri Jan 31 18:06:22 2014 -0800

    minor style change

commit 750912b4d77596ed807d361347bd2b7e3b9b7a74
Author: Xiangrui Meng <meng@databricks.com>
Date:   Fri Jan 31 18:04:54 2014 -0800

    fix some long lines

commit c446a25c38d81db02821f7f194b0ce5ab4ed7ff5
Author: Xiangrui Meng <meng@databricks.com>
Date:   Fri Jan 31 17:59:59 2014 -0800

    add complement to BernoulliSampler and minor style changes

commit dbe2bc2bd888a7bdccb127ee6595840274499403
Author: Xiangrui Meng <meng@databricks.com>
Date:   Fri Jan 31 17:45:08 2014 -0800

    switch to partition-wise sampling for better performance

commit a1fca5232308feb369339eac67864c787455bb23
Merge: ac712e4 cf6128f
Author: Xiangrui Meng <meng@databricks.com>
Date:   Fri Jan 31 16:33:09 2014 -0800

    Merge branch 'sample' of github.com:mengxr/incubator-spark into sample

commit cf6128fb672e8c589615adbd3eaa3cbdb72bd461
Author: Xiangrui Meng <meng@databricks.com>
Date:   Sun Jan 26 14:40:07 2014 -0800

    set SampledRDD deprecated in 1.0

commit f430f847c3df91a3894687c513f23f823f77c255
Author: Xiangrui Meng <meng@databricks.com>
Date:   Sun Jan 26 14:38:59 2014 -0800

    update code style

commit a8b5e2021a9204e318c80a44d00c5c495f1befb6
Author: Xiangrui Meng <meng@databricks.com>
Date:   Sun Jan 26 12:56:27 2014 -0800

    move package random to util.random

commit ab0fa2c4965033737a9e3a9bf0a59cbb0df6a6f5
Author: Xiangrui Meng <meng@databricks.com>
Date:   Sun Jan 26 12:50:35 2014 -0800

    add Apache headers and update code style

commit 985609fe1a55655ad11966e05a93c18c138a403d
Author: Xiangrui Meng <meng@databricks.com>
Date:   Sun Jan 26 11:49:25 2014 -0800

    add new lines

commit b21bddf29850a2c006a868869b8f91960a029322
Author: Xiangrui Meng <meng@databricks.com>
Date:   Sun Jan 26 11:46:35 2014 -0800

    move samplers to random.IndependentRandomSampler and add tests

commit c02dacb4a941618e434cefc129c002915db08be6
Author: Xiangrui Meng <meng@databricks.com>
Date:   Sat Jan 25 15:20:24 2014 -0800

    add RandomSampler

commit 8ff7ba3c5cf1fc338c29ae8b5fa06c222640e89c
Author: Xiangrui Meng <meng@databricks.com>
Date:   Fri Jan 24 13:23:22 2014 -0800

    init impl of IndependentlySampledRDD
",1391461329
0ff38c22205f14770ecca1e66378e7c207ca2d1d remotes/origin/vldb~203,"Merge pull request #494 from tyro89/worker_registration_issue

Issue with failed worker registrations

I've been going through the spark source after having some odd issues with workers dying and not coming back. After some digging (I'm very new to scala and spark) I believe I've found a worker registration issue. It looks to me like a failed registration follows the same code path as a successful registration which end up with workers believing they are connected (since they received a `RegisteredWorker` event) even tho they are not registered on the Master.

This is a quick fix that I hope addresses this issue (assuming I didn't completely miss-read the code and I'm about to look like a silly person :P)

I'm opening this pr now to start a chat with you guys while I do some more testing on my side :)

Author: Erik Selin <erik.selin@jadedpixel.com>

== Merge branch commits ==

commit 973012f8a2dcf1ac1e68a69a2086a1b9a50f401b
Author: Erik Selin <erik.selin@jadedpixel.com>
Date:   Tue Jan 28 23:36:12 2014 -0500

    break logwarning into two lines to respect line character limit.

commit e3754dc5b94730f37e9806974340e6dd93400f85
Author: Erik Selin <erik.selin@jadedpixel.com>
Date:   Tue Jan 28 21:16:21 2014 -0500

    add log warning when worker registration fails due to attempt to re-register on same address.

commit 14baca241fa7823e1213cfc12a3ff2a9b865b1ed
Author: Erik Selin <erik.selin@jadedpixel.com>
Date:   Wed Jan 22 21:23:26 2014 -0500

    address code style comment

commit 71c0d7e6f59cd378d4e24994c21140ab893954ee
Author: Erik Selin <erik.selin@jadedpixel.com>
Date:   Wed Jan 22 16:01:42 2014 -0500

    Make a failed registration not persist, not send a `RegisteredWordker` event and not run `schedule` but rather send a `RegisterWorkerFailed` message to the worker attempting to register.
",1391028294
793020961489e16e924c4531da3a13884d2b9175 remotes/origin/vldb~204,"Merge pull request #497 from tdas/docs-update

Updated Spark Streaming Programming Guide

Here is the updated version of the Spark Streaming Programming Guide. This is still a work in progress, but the major changes are in place. So feedback is most welcome.

In general, I have tried to make the guide to easier to understand even if the reader does not know much about Spark. The updated website is hosted here -

http://www.eecs.berkeley.edu/~tdas/spark_docs/streaming-programming-guide.html

The major changes are:
- Overview illustrates the usecases of Spark Streaming - various input sources and various output sources
- An example right after overview to quickly give an idea of what Spark Streaming program looks like
- Made Java API and examples a first class citizen like Scala by using tabs to show both Scala and Java examples (similar to AMPCamp tutorial's code tabs)
- Highlighted the DStream operations updateStateByKey and transform because of their powerful nature
- Updated driver node failure recovery text to highlight automatic recovery in Spark standalone mode
- Added information about linking and using the external input sources like Kafka and Flume
- In general, reorganized the sections to better show the Basic section and the more advanced sections like Tuning and Recovery.

Todos:
- Links to the docs of external Kafka, Flume, etc
- Illustrate window operation with figure as well as example.

Author: Tathagata Das <tathagata.das1565@gmail.com>

== Merge branch commits ==

commit 18ff10556570b39d672beeb0a32075215cfcc944
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   Tue Jan 28 21:49:30 2014 -0800

    Fixed a lot of broken links.

commit 34a5a6008dac2e107624c7ff0db0824ee5bae45f
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   Tue Jan 28 18:02:28 2014 -0800

    Updated github url to use SPARK_GITHUB_URL variable.

commit f338a60ae8069e0a382d2cb170227e5757cc0b7a
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   Mon Jan 27 22:42:42 2014 -0800

    More updates based on Patrick and Harvey's comments.

commit 89a81ff25726bf6d26163e0dd938290a79582c0f
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   Mon Jan 27 13:08:34 2014 -0800

    Updated docs based on Patricks PR comments.

commit d5b6196b532b5746e019b959a79ea0cc013a8fc3
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   Sun Jan 26 20:15:58 2014 -0800

    Added spark.streaming.unpersist config and info on StreamingListener interface.

commit e3dcb46ab83d7071f611d9b5008ba6bc16c9f951
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   Sun Jan 26 18:41:12 2014 -0800

    Fixed docs on StreamingContext.getOrCreate.

commit 6c29524639463f11eec721e4d17a9d7159f2944b
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   Thu Jan 23 18:49:39 2014 -0800

    Added example and figure for window operations, and links to Kafka and Flume API docs.

commit f06b964a51bb3b21cde2ff8bdea7d9785f6ce3a9
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   Wed Jan 22 22:49:12 2014 -0800

    Fixed missing endhighlight tag in the MLlib guide.

commit 036a7d46187ea3f2a0fb8349ef78f10d6c0b43a9
Merge: eab351d a1cd185
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   Wed Jan 22 22:17:42 2014 -0800

    Merge remote-tracking branch 'apache/master' into docs-update

commit eab351d05c0baef1d4b549e1581310087158d78d
Author: Tathagata Das <tathagata.das1565@gmail.com>
Date:   Wed Jan 22 22:17:15 2014 -0800

    Update Spark Streaming Programming Guide.
",1390974665
f8c742ce274fbae2a9e616d4c97469b6a22069bb remotes/origin/vldb~205,"Merge pull request #523 from JoshRosen/SPARK-1043

Switch from MUTF8 to UTF8 in PySpark serializers.

This fixes SPARK-1043, a bug introduced in 0.9.0 where PySpark couldn't serialize strings > 64kB.

This fix was written by @tyro89 and @bouk in #512. This commit squashes and rebases their pull request in order to fix some merge conflicts.
",1390973420
1381fc72f7a34f690a98ab72cec8ffb61e0e564d remotes/origin/vldb~205^2,"Switch from MUTF8 to UTF8 in PySpark serializers.

This fixes SPARK-1043, a bug introduced in 0.9.0
where PySpark couldn't serialize strings > 64kB.

This fix was written by @tyro89 and @bouk in #512.
This commit squashes and rebases their pull request
in order to fix some merge conflicts.
",1390969208
3d5c03e2305777b8a32f2e196e3b73ab221b3e79 remotes/origin/vldb~207,"Merge pull request #516 from sarutak/master

modified SparkPluginBuild.scala to use https protocol for accessing gith...

We cannot build Spark behind a proxy although we execute sbt with -Dhttp(s).proxyHost -Dhttp(s).proxyPort -Dhttp(s).proxyUser -Dhttp(s).proxyPassword options.
It's because of using git protocol to clone junit_xml_listener.git.
I could build after modifying SparkPluginBuild.scala.

I reported this issue to JIRA.
https://spark-project.atlassian.net/browse/SPARK-1046
",1390868821
c40619d4873f36ffb96a2e6292b32d5b64eab153 remotes/origin/vldb~210,"Merge pull request #504 from JoshRosen/SPARK-1025

Fix PySpark hang when input files are deleted (SPARK-1025)

This pull request addresses [SPARK-1025](https://spark-project.atlassian.net/browse/SPARK-1025), an issue where PySpark could hang if its input files were deleted.
",1390718490
c66a2ef1c2dc9c218069b3ce8c39a49e5b92fc16 remotes/origin/vldb~211,"Merge pull request #511 from JoshRosen/SPARK-1040

Fix ClassCastException in JavaPairRDD.collectAsMap() (SPARK-1040)

This fixes [SPARK-1040](https://spark-project.atlassian.net/browse/SPARK-1040), an issue where JavaPairRDD.collectAsMap() could sometimes fail with ClassCastException.  I applied the same fix to the Spark Streaming Java APIs.  The commit message describes the fix in more detail.

I also increased the verbosity of JUnit test output under SBT to make it easier to verify that the Java tests are actually running.
",1390718167
740e865f40704dc9158a6cf635990580fb6adcac remotes/origin/vldb~211^2,"Fix ClassCastException in JavaPairRDD.collectAsMap() (SPARK-1040)

This fixes an issue where collectAsMap() could
fail when called on a JavaPairRDD that was derived
by transforming a non-JavaPairRDD.

The root problem was that we were creating the
JavaPairRDD's ClassTag by casting a
ClassTag[AnyRef] to a ClassTag[Tuple2[K2, V2]].
To fix this, I cast a ClassTag[Tuple2[_, _]]
instead, since this actually produces a ClassTag
of the appropriate type because ClassTags don't
capture type parameters:

scala> implicitly[ClassTag[Tuple2[_, _]]] == implicitly[ClassTag[Tuple2[Int, Int]]]
res8: Boolean = true

scala> implicitly[ClassTag[AnyRef]].asInstanceOf[ClassTag[Tuple2[Int, Int]]] == implicitly[ClassTag[Tuple2[Int, Int]]]
res9: Boolean = false
",1390696872
4cebb79c9f3067da0c533292de45d7ecf56f2ff2 remotes/origin/vldb~212^2,"Deprecate mapPartitionsWithSplit in PySpark.

Also, replace the last reference to it in the docs.

This fixes SPARK-1026.
",1390536096
3d6e75419330d27435becfdf8cfb0b6d20d56cf8 remotes/origin/vldb~213,"Merge pull request #503 from pwendell/master

Fix bug on read-side of external sort when using Snappy.

This case wasn't handled correctly and this patch fixes it.
",1390535220
ff44732171730fd9e5db005062a45464a3801358 remotes/origin/vldb~214,"Minor fix
",1390533792
cad3002fead89d3c9a8de4fa989e88f367bc0b05 remotes/origin/vldb~216,"Merge pull request #501 from JoshRosen/cartesian-rdd-fixes

Fix two bugs in PySpark cartesian(): SPARK-978 and SPARK-1034

This pull request fixes two bugs in PySpark's `cartesian()` method:

- [SPARK-978](https://spark-project.atlassian.net/browse/SPARK-978): PySpark's cartesian method throws ClassCastException exception
- [SPARK-1034](https://spark-project.atlassian.net/browse/SPARK-1034): Py4JException on PySpark Cartesian Result

The JIRAs have more details describing the fixes.
",1390532914
0213b4032a78d621405105365119677edc663b1b remotes/origin/vldb~213^2,"Fix bug on read-side of external sort when using Snappy.

This case wasn't handled correctly and this patch fixes it.
",1390529095
60e7457266eef18f562ef5cb93d62db1af821fdf remotes/origin/vldb~217^2,"fixed ClassTag in mapPartitions
",1390479036
a1cd185122602c96fb8ae16c0b506702283bf6e2 remotes/origin/vldb~219,"Merge pull request #496 from pwendell/master

Fix bug in worker clean-up in UI

Introduced in d5a96fec (/cc @aarondav).

This should be picked into 0.8 and 0.9 as well. The bug causes old (zombie) workers on a node to not disappear immediately from the UI when a new one registers.
",1390448249
034dce2a7e0537836e15b9bfecc333b28fc76a6a remotes/origin/vldb~220,"Merge pull request #447 from CodingCat/SPARK-1027

fix for SPARK-1027

fix for SPARK-1027  (https://spark-project.atlassian.net/browse/SPARK-1027)

FIXES

1. change sparkhome from String to Option(String) in ApplicationDesc

2. remove sparkhome parameter in LaunchExecutor message

3. adjust involved files
",1390445882
6285513147c78b6bedc9ea21e6f4644b1a71e8f4 remotes/origin/vldb~213^2~1,"Fix bug in worker clean-up in UI

Introduced in d5a96fec. This should be picked into 0.8 and 0.9 as well.
",1390443592
3184facdc5b1e9ded89133f9b1e4985c9ac78c55 remotes/origin/vldb~221,"Merge pull request #495 from srowen/GraphXCommonsMathDependency

Fix graphx Commons Math dependency

`graphx` depends on Commons Math (2.x) in `SVDPlusPlus.scala`. However the module doesn't declare this dependency. It happens to work because it is included by Hadoop artifacts. But, I can tell you this isn't true as of a month or so ago. Building versus recent Hadoop would fail. (That's how we noticed.)

The simple fix is to declare the dependency, as it should be. But it's also worth noting that `commons-math` is the old-ish 2.x line, while `commons-math3` is where newer 3.x releases are. Drop-in replacement, but different artifact and package name. Changing this only usage to `commons-math3` works, tests pass, and isn't surprising that it does, so is probably also worth changing. (A comment in some test code also references `commons-math3`, FWIW.)

It does raise another question though: `mllib` looks like it uses the `jblas` `DoubleMatrix` for general purpose vector/matrix stuff. Should `graphx` really use Commons Math for this? Beyond the tiny scope here but worth asking.
",1390434304
a1238bb5fcab763d32c729ea7ed99cb3c05c896f remotes/origin/vldb~222,"Merge pull request #492 from skicavs/master

fixed job name and usage information for the JavaSparkPi example
",1390429979
5bcfd798117e8617e604c1dd3b5c9b67e83100bb remotes/origin/vldb~224,"Merge pull request #493 from kayousterhout/double_add

Fixed bug where task set managers are added to queue twice

@mateiz can you verify that this is a bug and wasn't intentional? (https://github.com/apache/incubator-spark/commit/90a04dab8d9a2a9a372cea7cdf46cc0fd0f2f76c#diff-7fa4f84a961750c374f2120ca70e96edR551)

This bug leads to a small performance hit because task
set managers will get offered each rejected resource
offer twice, but doesn't lead to any incorrect functionality.

Thanks to @hdc1112 for pointing this out.
",1390428348
19da82c50f1188a1d9949cd7df15b1541f3fd1e9 remotes/origin/vldb~224^2,"Fixed bug where task set managers are added to queue twice

This bug leads to a small performance hit because task
set managers will get offered each rejected resource
offer twice, but doesn't lead to any incorrect functionality.
",1390413132
36f9a64ec9b09309df82865a0caa482b7a3ffce0 remotes/origin/vldb~222^2,"fixed job name and usage information for the JavaSparkPi example
",1390402703
77b986f6616e6f7e0be9e46bb355829686f9845b remotes/origin/vldb~228,"Merge pull request #480 from pwendell/0.9-fixes

Handful of 0.9 fixes

This patch addresses a few fixes for Spark 0.9.0 based on the last release candidate.

@mridulm gets credit for reporting most of the issues here. Many of the fixes here are based on his work in #477 and follow up discussion with him.
",1390291782
c67d3d8beb101fff2ea6397b759dd1bfdf9fcfa5 remotes/origin/vldb~229,"Merge pull request #484 from tdas/run-example-fix

Made run-example respect SPARK_JAVA_OPTS and SPARK_MEM.

bin/run-example scripts was not passing Java properties set through the SPARK_JAVA_OPTS to the example. This is important for examples like Twitter** as the Twitter authentication information must be set through java properties. Hence added the same JAVA_OPTS code in run-example as it is in bin/spark-class script.

Also added SPARK_MEM, in case someone wants to run the example with different amounts of memory. This can be removed if it is not tune with the intended semantics of the run-example scripts.

@matei Please check this soon I want this to go in 0.9-rc4
",1390289675
6b4eed779bd1889543ac2c058745bd0864f02b2a remotes/origin/vldb~230,"Merge pull request #449 from CrazyJvm/master

SPARK-1028 : fix ""set MASTER automatically fails"" bug.

spark-shell intends to set MASTER automatically if we do not provide the option when we start the shell , but there's a problem.
The condition is ""if [[ ""x"" != ""x$SPARK_MASTER_IP"" && ""y"" != ""y$SPARK_MASTER_PORT"" ]];"" we sure will set SPARK_MASTER_IP explicitly, the SPARK_MASTER_PORT option, however, we probably do not set just using spark default port 7077. So if we do not set SPARK_MASTER_PORT, the condition will never be true. We should just use default port if users do not set port explicitly I think.
",1390286145
0367981d47761cdccd8a44fc6fe803079979c5e3 remotes/origin/vldb~231,"Merge pull request #482 from tdas/streaming-example-fix

Added StreamingContext.awaitTermination to streaming examples

StreamingContext.start() currently starts a non-daemon thread which prevents termination of a Spark Streaming program even if main function has exited. Since the expected behavior of a streaming program is to run until explicitly killed, this was sort of fine when spark streaming applications are launched from the command line. However, when launched in Yarn-standalone mode, this did not work as the driver effectively got terminated when the main function exits. So SparkStreaming examples did not work on Yarn.

This addition to the examples ensures that the examples work on Yarn and also ensures that everyone learns that StreamingContext.awaitTermination() being necessary for SparkStreaming programs to wait.

The true bug-fix of making sure all threads by Spark Streaming are daemon threads is left for post-0.9.
",1390285550
f84400e86c459b1ebbe452bc21e821a11cd72c29 remotes/origin/vldb~228^2~4,"Fixing speculation bug
",1390273503
1b299142a8d5feb70677dce993127de466266ff6 remotes/origin/vldb~228^2~6,"Bug fix for reporting of spill output
",1390271640
54867e9566c81d2f60e7a5ffb52a8d2204eb47f8 remotes/origin/vldb~228^2~7,"Minor fixes
",1390271601
29f4b6a2d9f42a727691444312964e59ef9b95ee remotes/origin/vldb~220^2~1,"fix for SPARK-1027

change TestClient & Worker to Some(""xxx"")

kill manager if it is started

remove unnecessary .get when fetch ""SPARK_HOME"" values
",1390204230
f9a95d67365509cdd260858e858e7a9b120c1d1b remotes/origin/vldb~220^2~2,"executor creation failed should not make the worker restart
",1390204230
792d9084e2bc9f778a00a56fa7dcfe4084153aea remotes/origin/vldb~233,"Merge pull request #470 from tgravescs/fix_spark_examples_yarn

Only log error on missing jar to allow spark examples to jar.

Right now to run the spark examples on Yarn you have to use the --addJars option and put the jar in hdfs.  To make that nicer  so the user doesn't have to specify the --addJars option change it to simply log an error instead of throwing.
",1390159991
ceb79a393117d725398e333bddc237613818c333 remotes/origin/vldb~233^2~1,"Only log error on missing jar to allow spark examples to jar.
",1390155418
bf5699543bf69fc850dbc2676caac97fa27818da remotes/origin/vldb~238,"Merge pull request #462 from mateiz/conf-file-fix

Remove Typesafe Config usage and conf files to fix nested property names

With Typesafe Config we had the subtle problem of no longer allowing
nested property names, which are used for a few of our properties:
http://apache-spark-developers-list.1001551.n3.nabble.com/Config-properties-broken-in-master-td208.html

This PR is for branch 0.9 but should be added into master too.
(cherry picked from commit 34e911ce9a9f91f3259189861779032069257852)

Signed-off-by: Patrick Wendell <pwendell@gmail.com>
",1390090800
d749d472b37448edb322bc7208a3db925c9a4fc2 remotes/origin/vldb~240,"Merge pull request #451 from Qiuzhuang/master

Fixed Window spark shell launch script error.

 JIRA SPARK-1029:https://spark-project.atlassian.net/browse/SPARK-1029
",1389943095
4e510b0b0c8a69cfe0ee037b37661caf9bf1d057 remotes/origin/vldb~240^2,"Fixed Window spark shell launch script error.
 JIRA SPARK-1029:https://spark-project.atlassian.net/browse/SPARK-1029
",1389859750
c06a307ca22901839df00d25fe623f6faa6af17e remotes/origin/vldb~242,"Merge pull request #445 from kayousterhout/exec_lost

Fail rather than hanging if a task crashes the JVM.

Prior to this commit, if a task crashes the JVM, the task (and
all other tasks running on that executor) is marked at KILLED rather
than FAILED.  As a result, the TaskSetManager will retry the task
indefinitely rather than failing the job after maxFailures. Eventually,
this makes the job hang, because the Standalone Scheduler removes
the application after 10 works have failed, and then the app is left
in a state where it's disconnected from the master and waiting to reconnect.
This commit fixes that problem by marking tasks as FAILED rather than
killed when an executor is lost.

The downside of this commit is that if task A fails because another
task running on the same executor caused the VM to crash, the failure
will incorrectly be counted as a failure of task A. This should not
be an issue because we typically set maxFailures to 3, and it is
unlikely that a task will be co-located with a JVM-crashing task
multiple times.
",1389858445
8400536456ecff26145244cf74b7c00dd1c7034b remotes/origin/vldb~230^2,fix some format problem.,1389844666
7a0c5b5a2362253e70f9c7d72dab1cad380e52c7 remotes/origin/vldb~230^2~1,"fix ""set MASTER automatically fails"" bug.

spark-shell intends to set MASTER automatically if we do not provide the option when we start the shell , but there's a problem. 
The condition is ""if [[ ""x"" != ""x$SPARK_MASTER_IP"" && ""y"" != ""y$SPARK_MASTER_PORT"" ]];"" we sure will set SPARK_MASTER_IP explicitly, the SPARK_MASTER_PORT option, however, we probably do not set just using spark default port 7077. So if we do not set SPARK_MASTER_PORT, the condition will never be true. We should just use default port if users do not set port explicitly I think.",1389843902
a268d634113536f7aca11af23619b9713b5ef5de remotes/origin/vldb~242^2~1,"Fail rather than hanging if a task crashes the JVM.

Prior to this commit, if a task crashes the JVM, the task (and
all other tasks running on that executor) is marked at KILLED rather
than FAILED.  As a result, the TaskSetManager will retry the task
indefiniteily rather than failing the job after maxFailures. This
commit fixes that problem by marking tasks as FAILED rather than
killed when an executor is lost.

The downside of this commit is that if task A fails because another
task running on the same executor caused the VM to crash, the failure
will incorrectly be counted as a failure of task A. This should not
be an issue because we typically set maxFailures to 3, and it is
unlikely that a task will be co-located with a JVM-crashing task
multiple times.
",1389830620
2a05403a7ced4ecf6084c96f582ee3a24f3cc874 remotes/origin/vldb~247,"Merge pull request #443 from tdas/filestream-fix

Made some classes private[stremaing] and deprecated a method in JavaStreamingContext.

Classes `RawTextHelper`, `RawTextSender` and `RateLimitedOutputStream` are not useful in the streaming API. There are not used by the core functionality and was there as a support classes for an obscure example. One of the classes is RawTextSender has a main function which can be executed using bin/spark-class even if it is made private[streaming]. In future, I will probably completely remove these classes. For the time being, I am just converting them to private[streaming].

Accessing underlying JavaSparkContext in JavaStreamingContext was through `JavaStreamingContext.sc` . This is deprecated and preferred method is `JavaStreamingContext.sparkContext` to keep it consistent with the `StreamingContext.sparkContext`.
",1389822885
139c24ef08e6ffb090975c9808a2cba304eb79e0 remotes/origin/vldb~252,"Merge pull request #435 from tdas/filestream-fix

Fixed the flaky tests by making SparkConf not serializable

SparkConf was being serialized with CoGroupedRDD and Aggregator, which somehow caused OptionalJavaException while being deserialized as part of a ShuffleMapTask. SparkConf should not even be serializable (according to conversation with Matei). This change fixes that.

@mateiz @pwendell
",1389769675
0e15bd7827d5acb5c1ccb071e358338817f95a79 remotes/origin/vldb~247^2~1,"Merge remote-tracking branch 'apache/master' into filestream-fix
",1389766880
1f4718c4805082cb6d6fa5af7c3529c6a79ae4e0 remotes/origin/vldb~247^2~2,"Changed SparkConf to not be serializable. And also fixed unit-test log paths in log4j.properties of external modules.
",1389766814
3a386e238984c48a6ac07974b92647beae1199b3 remotes/origin/vldb~254,"Merge pull request #424 from jegonzal/GraphXProgrammingGuide

Additional edits for clarity in the graphx programming guide.

Added an overview of the Graph and GraphOps functions and fixed numerous typos.
",1389765170
b1b22b7a13c7836c77bbb16df225bf410c87163e remotes/origin/vldb~256^2,"Style fix
",1389736587
8ea2cd56e4a243a834214d04e29502a5fdb539df remotes/origin/vldb~258^2,"Adding fix covering combineCombinersByKey as well
",1389736343
980250b1ee0cdba9cf06ea87c790a2d504bbf03e remotes/origin/vldb~262,"Merge pull request #416 from tdas/filestream-fix

Removed unnecessary DStream operations and updated docs

Removed StreamingContext.registerInputStream and registerOutputStream - they were useless. InputDStream has been made to register itself, and just registering a DStream as output stream cause RDD objects to be created but the RDDs will not be computed at all.. Also made DStream.register() private[streaming] for the same reasons.

Updated docs, specially added package documentation for streaming package.

Also, changed NetworkWordCount's input storage level to use MEMORY_ONLY, replication on the local machine causes warning messages (as replication fails) which is scary for a new user trying out his/her first example.
",1389686737
f8e239e058953e8db88e784439cfd9eca446e606 remotes/origin/vldb~247^2~4,"Merge remote-tracking branch 'apache/master' into filestream-fix

Conflicts:
	streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala
",1389686247
fdaabdc67387524ffb84354f87985f48bd31cf60 remotes/origin/vldb~264,"Merge pull request #380 from mateiz/py-bayes

Add Naive Bayes to Python MLlib, and some API fixes

- Added a Python wrapper for Naive Bayes
- Updated the Scala Naive Bayes to match the style of our other
  algorithms better and in particular make it easier to call from Java
  (added builder pattern, removed default value in train method)
- Updated Python MLlib functions to not require a SparkContext; we can
  get that from the RDD the user gives
- Added a toString method in LabeledPoint
- Made the Python MLlib tests run as part of run-tests as well (before
  they could only be run individually through each file)
",1389683306
4a805aff5e381752afb2bfd579af908d623743ed remotes/origin/vldb~265,"Merge pull request #367 from ankurdave/graphx

GraphX: Unifying Graphs and Tables

GraphX extends Spark's distributed fault-tolerant collections API and interactive console with a new graph API which leverages recent advances in graph systems (e.g., [GraphLab](http://graphlab.org)) to enable users to easily and interactively build, transform, and reason about graph structured data at scale. See http://amplab.github.io/graphx/.

Thanks to @jegonzal, @rxin, @ankurdave, @dcrankshaw, @jianpingjwang, @amatsukawa, @kellrott, and @adamnovak.

Tasks left:
- [x] Graph-level uncache
- [x] Uncache previous iterations in Pregel
- [x] ~~Uncache previous iterations in GraphLab~~ (postponed to post-release)
- [x] - Describe GC issue with GraphLab
- [ ] Write `docs/graphx-programming-guide.md`
- [x] - Mention future Bagel support in docs
- [ ] - Section on caching/uncaching in docs: As with Spark, cache something that is used more than once. In an iterative algorithm, try to cache and force (i.e., materialize) something every iteration, then uncache the cached things that depended on the newly materialized RDD but that won't be referenced again.
- [x] Undo modifications to core collections and instead copy them to org.apache.spark.graphx
- [x] Make Graph serializable to work around capture in Spark shell
- [x] Rename graph -> graphx in package name and subproject
- [x] Remove standalone PageRank
- [x] ~~Fix amplab/graphx#52 by checking `iter.hasNext`~~
",1389682718
76ebdae79866f4721ef39aaccfa89a255d3cea6a remotes/origin/vldb~265^2~4,"Fix bug in GraphLoader.edgeListFile that caused srcId > dstId
",1389680445
c6023bee60cee06b3dd31bb8253da6e07862c13d remotes/origin/vldb~265^2~11,"Fix infinite loop in GraphGenerators.generateRandomEdges

The loop occurred when numEdges < numVertices. This commit fixes it by
allowing generateRandomEdges to generate a multigraph.
",1389675757
a2fee38ee054c7dd6ff5f5d72f036fef54194d53 remotes/origin/vldb~271,"Merge pull request #411 from tdas/filestream-fix

Improved logic of finding new files in FileInputDStream

Earlier, if HDFS has a hiccup and reports a existence of a new file (mod time T sec) at time T + 1 sec, then fileStream could have missed that file. With this change, it should be able to find files that are delayed by up to <batch size> seconds. That is, even if file is reported at T + <batch time> sec, file stream should be able to catch it.

The new logic, at a high level, is as follows. It keeps track of the new files it found in the previous interval and mod time of the oldest of those files (lets call it X). Then in the current interval, it will ignore those files that were seen in the previous interval and those which have mod time older than X. So if a new file gets reported by HDFS that in the current interval, but has mod time in the previous interval, it will be considered. However, if the mod time earlier than the previous interval (that is, earlier than X), they will be ignored. This is the current limitation, and future version would improve this behavior further.

Also reduced line lengths in DStream to <=100 chars.
",1389671126
4c22c55ad6900433014c36f8c025645c3e261c43 remotes/origin/vldb~250^2,"Address comments to fix code formats
",1389667302
1233b3de01be1ff57910786f5f3e2e2a23e228ab remotes/origin/vldb~247^2~6,"Merge remote-tracking branch 'apache/master' into filestream-fix
",1389662959
cfe4a29dcb516ceae5f243ac3b5d0c3a505d7f5a remotes/origin/vldb~265^2~22^2,"Improvements in example code for the programming guide as well as adding serialization support for GraphImpl to address issues with failed closure capture.
",1389662311
ae4b75d94a4a0f2545e6d90d6f9b8f162bf70ded remotes/origin/vldb~265^2~22^2~1,"Add EdgeDirection.Either and use it to fix CC bug

The bug was due to a misunderstanding of the activeSetOpt parameter to
Graph.mapReduceTriplets. Passing EdgeDirection.Both causes
mapReduceTriplets to run only on edges with *both* vertices in the
active set. This commit adds EdgeDirection.Either, which causes
mapReduceTriplets to run on edges with *either* vertex in the active
set. This is what connected components needed.
",1389661383
01c0d72b322544665c51a9066b870fd723dbd3d2 remotes/origin/vldb~272,"Merge pull request #410 from rxin/scaladoc1

Updated JavaStreamingContext to make scaladoc compile.

`sbt/sbt doc` used to fail. This fixed it.
",1389659070
30328c347bb9974f551fea2d3e9b19c6a9cd25a9 remotes/origin/vldb~267^2~1,"Updated JavaStreamingContext to make scaladoc compile.

`sbt/sbt doc` used to fail. This fixed it.
",1389657519
8038da232870fe016e73122a2ef110ac8e56ca1e remotes/origin/vldb~265^2~26,"Merge pull request #2 from jegonzal/GraphXCCIssue

Improving documentation and identifying potential bug in CC calculation.",1389653970
80e4d98dc656e20dacbd8cdbf92d4912673b42ae remotes/origin/vldb~265^2~26^2,"Improving documentation and identifying potential bug in CC calculation.
",1389649216
b93f9d42f21f03163734ef97b2871db945e166da remotes/origin/vldb~273,"Merge pull request #400 from tdas/dstream-move

Moved DStream and PairDSream to org.apache.spark.streaming.dstream

Similar to the package location of `org.apache.spark.rdd.RDD`, `DStream` has been moved from `org.apache.spark.streaming.DStream` to `org.apache.spark.streaming.dstream.DStream`. I know that the package name is a little long, but I think its better to keep it consistent with Spark's structure.

Also fixed persistence of windowed DStream. The RDDs generated generated by windowed DStream are essentially unions of underlying RDDs, and persistent these union RDDs would store numerous copies of the underlying data. Instead setting the persistence level on the windowed DStream is made to set the persistence level of the underlying DStream.
",1389644285
e6ed13f255d70de422711b979447690cdab7423b remotes/origin/vldb~274,"Merge pull request #397 from pwendell/host-port

Remove now un-needed hostPort option

I noticed this was logging some scary error messages in various places. After I looked into it, this is no longer really used. I removed the option and re-wrote the one remaining use case (it was unnecessary there anyways).
",1389594914
0b96d85c2063bd2864b5753496551c6cf2f9047a remotes/origin/vldb~275,"Merge pull request #399 from pwendell/consolidate-off

Disable shuffle file consolidation by default

After running various performance tests for the 0.9 release, this still seems to have performance issues even on XFS. So let's keep this off-by-default for 0.9 and users can experiment with it depending on their disk configurations.
",1389591103
405bfe86ef9c3021358d2ac89192857478861fe0 remotes/origin/vldb~277,"Merge pull request #394 from tdas/error-handling

Better error handling in Spark Streaming and more API cleanup

Earlier errors in jobs generated by Spark Streaming (or in the generation of jobs) could not be caught from the main driver thread (i.e. the thread that called StreamingContext.start()) as it would be thrown in different threads. With this change, after `ssc.start`, one can call `ssc.awaitTermination()` which will be block until the ssc is closed, or there is an exception. This makes it easier to debug.

This change also adds ssc.stop(<stop-spark-context>) where you can stop StreamingContext without stopping the SparkContext.

Also fixes the bug that came up with PRs #393 and #381. MetadataCleaner default value has been changed from 3500 to -1 for normal SparkContext and 3600 when creating a StreamingContext. Also, updated StreamingListenerBus with changes similar to SparkListenerBus in #392.

And changed a lot of protected[streaming] to private[streaming].
",1389585861
034f89aaab1db95e8908432f2445d6841526efcf remotes/origin/vldb~273^2~2,"Fixed persistence logic of WindowedDStream, and fixed default persistence level of input streams.
",1389582147
aa2c993858f87adc249eb9c20a908a125f8f4033 remotes/origin/vldb~277^2,"Merge remote-tracking branch 'apache/master' into error-handling
",1389577066
d1820fef574e8f559d8fba3995e21216033be303 remotes/origin/vldb~273^2~4,"Merge branch 'error-handling' into dstream-move
",1389577009
074f50232fd8d8cf05eb88db0ac6f03f61452810 remotes/origin/vldb~279,"Merge pull request #396 from pwendell/executor-env

Setting load defaults to true in executor

This preserves the behavior in earlier releases. If properties are set for the executors via `spark-env.sh` on the slaves, then they should take precedence over spark defaults. This is useful for if system administrators are setting properties for a standalone cluster, such as shuffle locations.

/cc @andrewor14 who initially reported this issue.
",1389574873
7883b8f5798e3de6f55a1182a5d5775c4aaa783b remotes/origin/vldb~277^2~2,"Fixed bugs to ensure better cleanup of JobScheduler, JobGenerator and NetworkInputTracker upon close.
",1389573847
c5921e5c6184ddc99c12c0b1f2646b6bd74a9e98 remotes/origin/vldb~273^2~6,"Fixed bugs.
",1389517928
5741078c46828f124bb8286919398a6c346b109c remotes/origin/vldb~264^2~3,"Log Python exceptions to stderr as well

This helps in case the exception happened while serializing a record to
be sent to Java, leaving the stream to Java in an inconsistent state
where PythonRDD won't be able to read the error.
",1389514241
18f4889d96b61b59569ec05f64900da1477404d0 remotes/origin/vldb~273^2~7,"Merge remote-tracking branch 'apache/master' into error-handling
",1389512457
f00e949f84df949fbe32c254b592a580b4623811 remotes/origin/vldb~264^2~4,"Added Java unit test, data, and main method for Naive Bayes

Also fixes mains of a few other algorithms to print the final model
",1389508248
9a0dfdf868187fb9a2e1656e4cf5f29d952ce5db remotes/origin/vldb~264^2~6,"Add Naive Bayes to Python MLlib, and some API fixes

- Added a Python wrapper for Naive Bayes
- Updated the Scala Naive Bayes to match the style of our other
  algorithms better and in particular make it easier to call from Java
  (added builder pattern, removed default value in train method)
- Updated Python MLlib functions to not require a SparkContext; we can
  get that from the RDD the user gives
- Added a toString method in LabeledPoint
- Made the Python MLlib tests run as part of run-tests as well (before
  they could only be run individually through each file)
",1389508248
dbc11df4113a61d56431d5122c9b6554c33951cc remotes/origin/vldb~282,"Merge pull request #388 from pwendell/master

Fix UI bug introduced in #244.

The 'duration' field was incorrectly renamed to 'task time' in the table that
lists stages.
",1389492433
409866b35150193a8adf10010fa4ee2d1736f495 remotes/origin/vldb~283,"Merge pull request #393 from pwendell/revert-381

Revert PR 381

This PR missed a bunch of test cases that require ""spark.cleaner.ttl"". I think it is what is causing test failures on Jenkins right now (though it's a bit hard to tell because the DNS for cs.berkeley.edu is down).

I'm submitting this to see if it fixes jeknins. I did try just patching various tests but it was taking a really long time because there are a bunch of them, so for now I'm just seeing if a revert works.
",1389489126
6510f04e4dd99163acdc15ba5fc2bcbd8ff5d338 remotes/origin/vldb~284,"Merge pull request #387 from jerryshao/conf-fix

Fix configure didn't work small problem in ALS
",1389473306
b313e156161d5d05a15c3c7a676716fb622ee313 remotes/origin/vldb~282^2,"Fix UI bug introduced in #244.

The 'duration' field was incorrectly renamed to 'task time' in the table that
lists stages.
",1389466377
34496d6a9fd18ba708b66dcc318c7568608e963f remotes/origin/vldb~265^2~55,"Move Analytics to algorithms and fix doc
",1389427716
0b5ce7af17c96fcfbefe8e2fb750e171bbd9163a remotes/origin/vldb~288,"Merge pull request #386 from pwendell/typo-fix

Small typo fix
",1389425001
1d7bef0c91abe053570e006917185d3b4ae19b0e remotes/origin/vldb~289,"Merge pull request #381 from mateiz/default-ttl

Fix default TTL for metadata cleaner

It seems to have been set to 3500 in a previous commit for debugging, but it should be off by default.
",1389408783
44d6a8e3d803e6d6b78044a4060c54558f90ef57 remotes/origin/vldb~290,"Merge pull request #382 from RongGu/master

Fix a type error in comment lines

Fix a type error in comment lines
",1389405110
08370a52b8be2c07feca614750bba5a4a3d78960 remotes/origin/vldb~288^2,"Small typo fix
",1389404835
f26553102c1995acf2a2ba6b502de4f2dbbd73b3 remotes/origin/vldb~292,"Merge pull request #383 from tdas/driver-test

API for automatic driver recovery for streaming programs and other bug fixes

1. Added Scala and Java API for automatically loading checkpoint if it exists in the provided checkpoint directory.

  Scala API: `StreamingContext.getOrCreate(<checkpoint dir>, <function to create new StreamingContext>)` returns a StreamingContext
  Java API: `JavaStreamingContext.getOrCreate(<checkpoint dir>, <factory obj of type JavaStreamingContextFactory>)`, return a JavaStreamingContext

  See the RecoverableNetworkWordCount below as an example of how to use it.

2. Refactored streaming.Checkpoint*** code to fix bugs and make the DStream metadata checkpoint writing and reading more robust. Specifically, it fixes and improves the logic behind backing up and writing metadata checkpoint files. Also, it ensure that spark.driver.* and spark.hostPort is cleared from SparkConf before being written to checkpoint.

3. Fixed bug in cleaning up of checkpointed RDDs created by DStream. Specifically, this fix ensures that checkpointed RDD's files are not prematurely cleaned up, thus ensuring reliable recovery.

4. TimeStampedHashMap is upgraded to optionally update the timestamp on map.get(key). This allows clearing of data based on access time (i.e., clear records were last accessed before a threshold timestamp).

5. Added caching for file modification time in FileInputDStream using the updated TimeStampedHashMap. Without the caching, enumerating the mod times to find new files can take seconds if there are 1000s of files. This cache is automatically cleared.

This PR is not entirely final as I may make some minor additions - a Java examples, and adding StreamingContext.getOrCreate to unit test.

Edit: Java example to be added later, unit test added.
",1389399944
d37408f39ca3fd94f45b50a65f919f4d7007a533 remotes/origin/vldb~293,"Merge pull request #377 from andrewor14/master

External Sorting for Aggregator and CoGroupedRDDs (Revisited)

(This pull request is re-opened from https://github.com/apache/incubator-spark/pull/303, which was closed because Jenkins / github was misbehaving)

The target issue for this patch is the out-of-memory exceptions triggered by aggregate operations such as reduce, groupBy, join, and cogroup. The existing AppendOnlyMap used by these operations resides purely in memory, and grows with the size of the input data until the amount of allocated memory is exceeded. Under large workloads, this problem is aggravated by the fact that OOM frequently occurs only after a very long (> 1 hour) map phase, in which case the entire job must be restarted.

The solution is to spill the contents of this map to disk once a certain memory threshold is exceeded. This functionality is provided by ExternalAppendOnlyMap, which additionally sorts this buffer before writing it out to disk, and later merges these buffers back in sorted order.

Under normal circumstances in which OOM is not triggered, ExternalAppendOnlyMap is simply a wrapper around AppendOnlyMap and incurs little overhead. Only when the memory usage is expected to exceed the given threshold does ExternalAppendOnlyMap spill to disk.
",1389399901
94776f753f176216ece0c49e07771fa031a45a4e remotes/origin/vldb~290^2,"fix a type error in comment lines
",1389390236
7cef8435d7b6b43a33e8be684c769412186ad6ac remotes/origin/vldb~295,"Merge pull request #371 from tgravescs/yarn_client_addjar_misc_fixes

Yarn client addjar and misc fixes

Fix the addJar functionality in yarn-client mode, add support for the other options supported in yarn-standalone mode, set the application type on yarn in hadoop 2.X, add documentation, change heartbeat interval to be same code as the yarn-standalone so it doesn't take so long to get containers and exit.
",1389389655
7b58f116e5a028336e25a26daae5852c95e56340 remotes/origin/vldb~296,"Merge pull request #384 from pwendell/debug-logs

Make DEBUG-level logs consummable.

Removes two things that caused issues with the debug logs:

(a) Internal polling in the DAGScheduler was polluting the logs.
(b) The Scala REPL logs were really noisy.
",1389386866
e9ed2d9e82e7119d7ce2e520358e32a9582047cf remotes/origin/vldb~296^2,"Make DEBUG-level logs consummable.

Removes two things that caused issues with the debug logs:

(a) Internal polling in the DAGScheduler was polluting the logs.
(b) The Scala REPL logs were really noisy.
",1389378804
9d3d9c8251724712590f3178e69e78ea0b750e9c remotes/origin/vldb~273^2~13^2~2,"Refactored graph checkpoint file reading and writing code to make it cleaner and easily debuggable.
",1389354242
669ba4caa95014f4511f842206c3e506f1a41a7a remotes/origin/vldb~289^2~1,"Fix default TTL for metadata cleaner

It seems to have been set to 3500 in a previous commit for debugging,
but it should be off by default
",1389342096
0ebc97305a0e23a9d2183c335998c800c5d865b7 remotes/origin/vldb~297,"Merge pull request #375 from mateiz/option-fix

Fix bug added when we changed AppDescription.maxCores to an Option

The Scala compiler warned about this -- we were comparing an Option against an integer now.
",1389340729
cf5bd4ab2e9db72d3d9164053523e9e872d85b94 remotes/origin/vldb~225^2~14,"fix example
",1389335981
300eaa994c399a0c991c1e39b4dd864a7aa4bdc6 remotes/origin/vldb~300,"Merge pull request #353 from pwendell/ipython-simplify

Simplify and fix pyspark script.

This patch removes compatibility for IPython < 1.0 but fixes the launch
script and makes it much simpler.

I tested this using the three commands in the PySpark documentation page:

1. IPYTHON=1 ./pyspark
2. IPYTHON_OPTS=""notebook"" ./pyspark
3. IPYTHON_OPTS=""notebook --pylab inline"" ./pyspark

There are two changes:
- We rely on PYTHONSTARTUP env var to start PySpark
- Removed the quotes around $IPYTHON_OPTS... having quotes
  gloms them together as a single argument passed to `exec` which
  seemed to cause ipython to fail (it instead expects them as
  multiple arguments).
",1389328191
4a5558ca9921ce89b3996e9ead13b07123fc7a2d remotes/origin/vldb~273^2~16,"Fixed bugs in reading of checkpoints.
",1389324519
a9d533333da63160c5b7507705b1b6e9c3dbf71e remotes/origin/vldb~302,"Merge pull request #294 from RongGu/master

Bug fixes for updating the RDD block's memory and disk usage information

Bug fixes for updating the RDD block's memory and disk usage information.
From the code context, we can find that the memSize and diskSize here are both always equal to the size of the block. Actually, they never be zero. Thus, the logic here is wrong for recording the block usage in BlockStatus, especially for the blocks which are dropped from memory to ensure space for the new input rdd blocks. I have tested it that this would cause the storage metrics shown in the Storage webpage wrong and misleading. With this patch, the metrics will be okay.
 Finally, Merry Christmas, guys:)
",1389322006
77ca9e1ba845c8cbb1566f803b591f6a826b0f1d remotes/origin/vldb~300^2,"Small fix suggested by josh
",1389321660
d86a85e9cac09bd909a356de9181bd282c905e72 remotes/origin/vldb~303,"Merge pull request #293 from pwendell/standalone-driver

SPARK-998: Support Launching Driver Inside of Standalone Mode

[NOTE: I need to bring the tests up to date with new changes, so for now they will fail]

This patch provides support for launching driver programs inside of a standalone cluster manager. It also supports monitoring and re-launching of driver programs which is useful for long running, recoverable applications such as Spark Streaming jobs. For those jobs, this patch allows a deployment mode which is resilient to the failure of any worker node, failure of a master node (provided a multi-master setup), and even failures of the applicaiton itself, provided they are recoverable on a restart. Driver information, such as the status and logs from a driver, is displayed in the UI

There are a few small TODO's here, but the code is generally feature-complete. They are:
- Bring tests up to date and add test coverage
- Restarting on failure should be optional and maybe off by default.
- See if we can re-use akka connections to facilitate clients behind a firewall

A sensible place to start for review would be to look at the `DriverClient` class which presents users the ability to launch their driver program. I've also added an example program (`DriverSubmissionTest`) that allows you to test this locally and play around with killing workers, etc. Most of the code is devoted to persisting driver state in the cluster manger, exposing it in the UI, and dealing correctly with various types of failures.

Instructions to test locally:
- `sbt/sbt assembly/assembly examples/assembly`
- start a local version of the standalone cluster manager

```
./spark-class org.apache.spark.deploy.client.DriverClient \
  -j -Dspark.test.property=something \
  -e SPARK_TEST_KEY=SOMEVALUE \
  launch spark://10.99.1.14:7077 \
  ../path-to-examples-assembly-jar \
  org.apache.spark.examples.DriverSubmissionTest 1000 some extra options --some-option-here -X 13
```
- Go in the UI and make sure it started correctly, look at the output etc
- Kill workers, the driver program, masters, etc.
",1389321472
c43eb006442bbab14f5cd0898b6ad0f39f273506 remotes/origin/vldb~297^2,"Fix bug added when we changed AppDescription.maxCores to an Option

The Scala compiler warned about this -- we were comparing an Option
against an integer now.
",1389320060
26cdb5f68a83e904e3e9a114790c729ca2eb3040 remotes/origin/vldb~304,"Merge pull request #372 from pwendell/log4j-fix-1

Send logs to stderr by default (instead of stdout).
",1389316594
b5b0de2de53563c43e1c5844a52b4eeeb2542ea5 remotes/origin/vldb~265^2~100,"Start fixing formatting of graphx-programming-guide
",1389302665
c617083e478e3cfbddc4232060aa7b7a0c5812d4 remotes/origin/vldb~295^2~2,"yarn-client addJar fix and misc other
",1389284675
73c724e1237db383737a9d37c1c8d697064ec28e remotes/origin/vldb~307,"Merge pull request #368 from pwendell/sbt-fix

Don't delegate to users `sbt`.

This changes our `sbt/sbt` script to not delegate to the user's `sbt`
even if it is present. If users already have sbt installed and they
want to use their own sbt, we'd expect them to just call sbt directly
from within Spark. We no longer set any enironment variables or anything
from this script, so they should just launch sbt directly on their own.

There are a number of hard-to-debug issues which can come from the
current appraoch. One is if the user is unaware of an existing sbt
installation and now without explanation their build breaks because
they haven't configured options correctly (such as permgen size)
within their sbt (reported by @patmcdonough). Another is if the user has a much older version
of sbt hanging around, in which case some of the older versions
don't acutally work well when newer verisons of sbt are specified
in the build file (reported by @marmbrus). A third is if the user
has done some other modification to their sbt script, such as
setting it to delegate to sbt/sbt in Spark, and this causes
that to break (also reported by @marmbrus).

So to keep things simple let's just avoid this path and
remove it. Any user who already has sbt and wants to build
spark with it should be able to understand easily how to do it.
",1389256339
49cbf48bcc4a3866984c8370da1917dd1c142115 remotes/origin/vldb~307^2,"Small typo fix
",1389255154
4d2e388e6a2df3a8e24f63d7a61074ad8f8c9ee4 remotes/origin/vldb~307^2~1,"Don't delegate to users `sbt`.

This changes our `sbt/sbt` script to not delegate to the user's `sbt`
even if it is present. If users already have sbt installed and they
want to use their own sbt, we'd expect them to just call sbt directly
from within Spark. We no longer set any enironment variables or anything
from this script, so they should just launch sbt directly on their own.

There are a number of hard-to-debug issues which can come from the
current appraoch. One is if the user is unaware of an existing sbt
installation and now without explanation their build breaks because
they haven't configured options correctly (such as permgen size)
within their sbt. Another is if the user has a much older version
of sbt hanging around, in which case some of the older versions
don't acutally work well when newer verisons of sbt are specified
in the build file (reported by @marmbrus). A third is if the user
has done some other modification to their sbt script, such as
setting it to delegate to sbt/sbt in Spark, and this causes
that to break (also reported by @marmbrus).

So to keep things simple let's just avoid this path and
remove it. Any user who already has sbt and wants to build
spark with it should be able to understand easily how to do it.
",1389254213
dceedb466032eca38f943cb97a8dca27496ca914 remotes/origin/vldb~308,"Merge pull request #364 from pwendell/fix

Fixing config option ""retained_stages"" => ""retainedStages"".

This is a very esoteric option and it's out of sync with the style we use.
So it seems fitting to fix it for 0.9.0.
",1389251968
112c0a1776bbc866a1026a9579c6f72f293414c4 remotes/origin/vldb~308^2,"Fixing config option ""retained_stages"" => ""retainedStages"".

This is a very esoteric option and it's out of sync with the style we use.
So it seems fitting to fix it for 0.9.0.
",1389244576
04d83fc37f9eef89c20331c85291a0a169f75e6d remotes/origin/vldb~309,"Merge pull request #360 from witgo/master

fix make-distribution.sh show version: command not found
",1389210937
cf4aaf92d672e03f5d53078cdfe6ea60ba0c1c0c remotes/origin/vldb~309^2,"fix make-distribution.sh show version: command not found
",1389198893
a17cc602ac79b22457ed457023493fe82e9d39df remotes/origin/vldb~273^2~19,"More bug fixes.
",1389183125
bb6a39a6872fb8e7118c5a402b7b9d67459e9a64 remotes/origin/vldb~314,"Merge pull request #322 from falaki/MLLibDocumentationImprovement

SPARK-1009 Updated MLlib docs to show how to use it in Python

In addition added detailed examples for regression, clustering and recommendation algorithms in a separate Scala section. Fixed a few minor issues with existing documentation.
",1389162738
d75dc428dae72add407aec2c134a25537e754303 remotes/origin/vldb~320,"Merge pull request #350 from mateiz/standalone-limit

Add way to limit default # of cores used by apps in standalone mode

Also documents the spark.deploy.spreadOut option, and fixes a config option that had a dash in its name.
",1389159003
82a1d38aea3b10930a2659b9c0e7ad2fb2c2ab4a remotes/origin/vldb~300^2~1,"Simplify and fix pyspark script.

This patch removes compatibility for IPython < 1.0 but fixes the launch
script and makes it much simpler.

I tested this using the three commands in the PySpark documentation page:

1. IPYTHON=1 ./pyspark
2. IPYTHON_OPTS=""notebook"" ./pyspark
3. IPYTHON_OPTS=""notebook --pylab inline"" ./pyspark

There are two changes:
- We rely on PYTHONSTARTUP env var to start PySpark
- Removed the quotes around $IPYTHON_OPTS... having quotes
  gloms them together as a single argument passed to `exec` which
  seemed to cause ipython to fail (it instead expects them as
  multiple arguments).
",1389146125
b2e690f839e7ee47f405135d35170173386c5d13 remotes/origin/vldb~322,"Merge pull request #328 from falaki/MatrixFactorizationModel-fix

SPARK-1012: DAGScheduler Exception Fix

Added a predict method to MatrixFactorizationModel to enable bulk prediction. This method takes and RDD[(Int, Int)] of users and products and return an RDD with a Rating element per each element in the input RDD.

Also added python bindings to the new bulk prediction methods to address SPARK-1011 issue.

This is ready to be merged now.
",1389142628
6ccf8ce705f13b5c6993675ee034db64363caa31 remotes/origin/vldb~323,"Merge pull request #351 from pwendell/maven-fix

Add log4j exclusion rule to maven.

To make this work I had to rename the defaults file. Otherwise
maven's pattern matching rules included it when trying to match
other log4j.properties files.

I also fixed a bug in the existing maven build where two
<transformers> tags were present in assembly/pom.xml
such that one overwrote the other.
",1389138554
3a8beb46cb53cf6807f39cca54b1efdbbc303f41 remotes/origin/vldb~322^2,"Merge branch 'master' into MatrixFactorizationModel-fix
",1389136962
e688e11206401850a13a87d7db52941cc716f88a remotes/origin/vldb~323^2,"Add log4j exclusion rule to maven.

To make this work I had to rename the defaults file. Otherwise
maven's pattern matching rules included it when trying to match
other log4j.properties files.

I also fixed a bug in the existing maven build where two
<transformers> tags were present in assembly/pom.xml
such that one overwrote the other.
",1389128184
7d5fa175ca9cd2260c7bcd18c201bc087d4f62c3 remotes/origin/vldb~324,"Merge pull request #337 from yinxusen/mllib-16-bugfix

Mllib 16 bugfix

Bug fix: https://spark-project.atlassian.net/browse/MLLIB-16

Hi, I fixed the bug and added a test suite for `GradientDescent`. There are 2 checks in the test case. First, the final loss must be lower than the initial one. Second, the trend of loss sequence should be decreasing, i.e., at least 80% iterations have lower losses than their prior iterations.

Thanks!
",1389123094
a862cafacf555373b5fdbafb4c9c4d712b191648 remotes/origin/vldb~329,"Merge pull request #331 from holdenk/master

Add a script to download sbt if not present on the system

As per the discussion on the dev mailing list this script will use the system sbt if present or otherwise attempt to install the sbt launcher. The fall back error message in the event it fails instructs the user to install sbt. While the URLs it fetches from aren't controlled by the spark project directly, they are stable and the current authoritative sources.
",1389082700
c729fa7c8ed733a778a7201ed17bf74f3e132845 remotes/origin/vldb~328^2,"formatting related fixes suggested by Patrick.
",1389080296
b72cceba2727586c1e1f89c58b66417628e1afa7 remotes/origin/vldb~303^2~9,"Some doc fixes
",1389074753
7210257ba3038d5e22d4b60fe9c3113dc45c3dff remotes/origin/vldb~265^2~110,"Merge pull request #128 from adamnovak/master

Fix failing ""sbt/sbt publish-local"" by adding a no-argument PrimitiveKeyOpenHashMap constructor

Closes #78.",1389061544
da4694a0d85433832fda1dd917fc698a08e65907 remotes/origin/vldb~313^2~1,"Minor typo fix for yarn client
",1389061450
e4d6057b6692ca5f071819b4ec6eb5240a0a16bc remotes/origin/vldb~331,"Merge pull request #343 from pwendell/build-fix

Fix test breaking downstream builds

This wasn't detected in the pull-request-builder because it manually sets SPARK_HOME. I'm going to change that (it should't do this) to make it like the other builds.
",1389049014
fa8ce3fdd7f91add0e8ead6e48f4d69e67c604b9 remotes/origin/vldb~265^2~110^2,"Changing org.apache.spark.util.collection.PrimitiveKeyOpenHashMap to have a real no-argument constructor, instead of a one-argument constructor with a default value. The lack of a real no-argument constructor was causing ""sbt/sbt publish-local"" to fail thusly:

```
[error] /pod/home/anovak/build/graphx/core/src/main/scala/org/apache/spark/storage/ShuffleBlockManager.scala:172: not enough arguments for constructor PrimitiveKeyOpenHashMap: (initialCapacity: Int)(implicit evidence$3: ClassManifest[Int], implicit evidence$4: ClassManifest[Int])org.apache.spark.util.collection.PrimitiveKeyOpenHashMap[Int,Int]
[error]     private val mapIdToIndex = new PrimitiveKeyOpenHashMap[Int, Int]()
[error]                                ^
[info] No documentation generated with unsucessful compiler run
[error] one error found
[error] (core/compile:doc) Scaladoc generation failed
[error] Total time: 67 s, completed Jan 6, 2014 2:20:51 PM
```

In theory a no-argument constructor ought not to differ from one with a single argument that has a default value, but in practice there seems to be an issue.
",1389048735
150089dae12bbba693db4edbfcea360b443637df remotes/origin/vldb~314^2~1,"Added proper evaluation example for collaborative filtering and fixed typo
",1389040997
93bf96205da5d2629c5ac3c6b1d6ce0690688415 remotes/origin/vldb~332,"Merge pull request #340 from ScrapCodes/sbt-fixes

Made java options to be applied during tests so that they become self explanatory.
",1389037361
33fcb91e81f761237dd48a8947bb43f24f7a263a remotes/origin/vldb~335,"Merge pull request #342 from tgravescs/fix_maven_protobuf

Change protobuf version for yarn alpha back to 2.4.1

The maven build for yarn-alpha uses the wrong protobuf version and hence the generated assembly jar doesn't work with Hadoop 0.23.  Removing the setting for the yarn-alpha profile since the default protobuf version is 2.4.1 at the top of the pom file.
",1389035963
357083c29f59bc7df593911ac2c5735e4ee9de74 remotes/origin/vldb~336,"Merge pull request #330 from tgravescs/fix_addjars_null_handling

Fix handling of empty SPARK_EXAMPLES_JAR

Currently if SPARK_EXAMPLES_JAR is left unset you get a null pointer exception when running the examples (atleast on spark on yarn).  The null now gets turned into a string of ""null"" when its put into the SparkConf so addJar no longer properly ignores it. This fixes that so that it can be left unset.
",1389032944
d0fd3b9ad238294346eb3465c489eabd41fb2380 remotes/origin/vldb~316^2~3,"Changed JavaStreamingContextWith*** to ***Function in streaming.api.java.*** package. Also fixed packages of Flume and MQTT tests.
",1389001673
a2e7e0497484554f86bd71e93705eb0422b1512b remotes/origin/vldb~337,"Merge pull request #333 from pwendell/logging-silence

Quiet ERROR-level Akka Logs

This fixes an issue I've seen where akka logs a bunch of things at ERROR level when connecting to a standalone cluster, even in the normal case. I noticed that even when lifecycle logging was disabled, the netty code inside of akka still logged away via akka's EndpointWriter class. There are also some other log streams that I think are new in akka 2.2.1 that I've disabled.

Finally, I added some better logging to the standalone client. This makes it more clear when a connection failure occurs what is going on. Previously it never explicitly said if a connection attempt had failed.

The commit messages here have some more detail.
",1388990256
a72107284ae4d8b6c7c47ded31c6784732028603 remotes/origin/vldb~324^2~1,"fix logistic loss bug
",1388982617
eb24684748da5dc2495fc4afe6da58edb463294b remotes/origin/vldb~317^2,"Fixed test suite compilation errors
",1388978819
5b0986a1d675f0f9d7d14b3d48fdadcb4f7055b7 remotes/origin/vldb~338,"Merge pull request #334 from pwendell/examples-fix

Removing SPARK_EXAMPLES_JAR in the code

This re-writes all of the examples to use the `SparkContext.jarOfClass` mechanism for loading the examples jar. This necessary for environments like YARN and the Standalone mode where example programs will be submit from inside the cluster rather than at the client using `./spark-example`.

This still leaves SPARK_EXAMPLES_JAR in place in the shell scripts for setting up the classpath if `./spark-example` is run.
",1388978709
5c152e3e219a44f97d9df38ba00cdc4adbf4d873 remotes/origin/vldb~317^2~1,"Fixed several compilation errors in test suites
",1388975945
23947945913cafb4f6549167c53a3cdd4a09fef0 remotes/origin/vldb~273^2~22,"Merge branch 'filestream-fix' into driver-test

Conflicts:
	streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala
",1388975033
8e88db3ca56e6a56668b029e39c8e96b86d4dd5e remotes/origin/vldb~273^2~23,"Bug fixes to the DriverRunner and minor changes here and there.
",1388974916
746148bc18d5e25ea93f5ff17a6cb4da9b671b75 remotes/origin/vldb~225^2~17,"fix docs to use SparseMatrix
",1388973837
94fdcda89638498f127abf3bb5231064182b4945 remotes/origin/vldb~337^2~1,"Provide logging when attempts to connect to the master fail.

Without these it's a bit less clear what's going on for the user.

One thing I realize when doing this is that akka itself actually retries
the initial association. So the retry we currently have is redundant with
akka's.
",1388963761
aaaa6731845495743aff4cc9bd64a54b9aa36c27 remotes/origin/vldb~337^2~2,"Quite akka when remote lifecycle logging is disabled.

I noticed when connecting to a standalone cluster Spark gives a bunch
of Akka ERROR logs that make it seem like something is failing.

This patch does two things:

1. Akka dead letter logging is turned on/off according to the existing
   lifecycle spark property.
2. We explicitly silence akka's EndpointWriter log in log4j. This is necessary
   because for some reason that log doesn't pick up on the lifecycle
   logging settings. After a few hours of debugging this was the only solution
   I found that worked.
",1388963759
e68cdb1b822a169fa16e1c9aca24ea0eef9988c5 remotes/origin/vldb~265^2~112,"Merge pull request #124 from jianpingjwang/master

refactor and bug fix",1388871962
8bfcce1ad81348a5eac3e3d332ddc293380c041a remotes/origin/vldb~225^2~21,"fix tests
",1388865162
8b5be0675245e206943574b8c6f6b77018b3561a remotes/origin/vldb~314^2~2,"Added table of contents and minor fixes
",1388795913
94f2fffa23436ed66a24c705f88dabe59bf54037 remotes/origin/vldb~341^2~3,"fixed review comments
",1388740417
f442afc22ef2e8a10ea22d5a7b392d41a1c7fdf8 remotes/origin/vldb~344^2,"fix docs for yarn
",1388729675
18b3633e54a8c902083f15e64983abca1eb0de7e remotes/origin/vldb~344^2~1,"minor fix for loginfo
",1388722478
498a5f0a1c6e82a33c2ad8c48b68bbdb8da57a95 remotes/origin/vldb~345,"Merge pull request #323 from tgravescs/sparkconf_yarn_fix

fix spark on yarn after the sparkConf changes

This fixes it so that spark on yarn now compiles and works after the sparkConf changes.

There are also other issues I discovered along the way that are broken:
- mvn builds for yarn don't assemble correctly
- unset SPARK_EXAMPLES_JAR isn't handled properly anymore
- I'm pretty sure spark.conf doesn't actually work as its not distributed with yarn

those things can be fixed in separate pr unless others disagree.
",1388718400
81989e26647ede54e19ef8058846e1bd42c0bfb5 remotes/origin/vldb~314^2~3,"Commented the last part of collaborative filtering examples that lead to errors
",1388708533
0475ca8f81b6b8f21fdb841922cd9ab51cfc8cc3 remotes/origin/vldb~346,"Merge pull request #320 from kayousterhout/erroneous_failed_msg

Remove erroneous FAILED state for killed tasks.

Currently, when tasks are killed, the Executor first sends a
status update for the task with a ""KILLED"" state, and then
sends a second status update with a ""FAILED"" state saying that
the task failed due to an exception. The second FAILED state is
misleading/unncessary, and occurs due to a NonLocalReturnControl
Exception that gets thrown due to the way we kill tasks. This
commit eliminates that problem.

I'm not at all sure that this is the best way to fix this problem,
so alternate suggestions welcome. @rxin guessing you're the right
person to look at this.
",1388704628
fced7885cb6cd09761578f960540d739bcbb465a remotes/origin/vldb~345^2,"fix yarn-client
",1388704276
588a1695f4b0b7763ecfa8ea56e371783810dd68 remotes/origin/vldb~347,"Merge pull request #297 from tdas/window-improvement

Improvements to DStream window ops and refactoring of Spark's CheckpointSuite

- Added a new RDD - PartitionerAwareUnionRDD. Using this RDD, one can take multiple RDDs partitioned by the same partitioner and unify them into a single RDD while preserving the partitioner. So m RDDs with p partitions each will be unified to a single RDD with p partitions and the same partitioner. The preferred location for each partition of the unified RDD will be the most common preferred location of the corresponding partitions of the parent RDDs. For example, location of partition 0 of the unified RDD will be where most of partition 0 of the parent RDDs are located.
- Improved the performance of DStream's reduceByKeyAndWindow and groupByKeyAndWindow. Both these operations work by doing per-batch reduceByKey/groupByKey and then using PartitionerAwareUnionRDD to union the RDDs across the window. This eliminates a shuffle related to the window operation, which can reduce batch processing time by 30-40% for simple workloads.
- Fixed bugs and simplified Spark's CheckpointSuite. Some of the tests were incorrect and unreliable. Added missing tests for ZippedRDD. I can go into greater detail if necessary.
- Added mapSideCombine option to combineByKeyAndWindow.
",1388697654
5e67cdc801baf8f08b8229472cb16a00e52ddd00 remotes/origin/vldb~348^2,"Merge pull request #319 from kayousterhout/remove_error_method

Removed redundant TaskSetManager.error() function.

This function was leftover from a while ago, and now just
passes all calls through to the abort() function, so this
commit deletes it.
",1388696188
a1b438d94de10506dc7dcac54eb331ee2c0479aa remotes/origin/vldb~346^2,"Remove erroneous FAILED state for killed tasks.

Currently, when tasks are killed, the Executor first sends a
status update for the task with a ""KILLED"" state, and then
sends a second status update with a ""FAILED"" state saying that
the task failed due to an exception. The second FAILED state is
misleading/unncessary, and occurs due to a NonLocalReturnControl
Exception that gets thrown due to the way we kill tasks. This
commit eliminates that problem.
",1388694886
5a3c00c9581f81522a32c0b5d21ba81498c2d9c3 remotes/origin/vldb~348^2^2,"Removed redundant TaskSetManager.error() function.

This function was leftover from a while ago, and now just
passes all calls through to the abort() function, so this
commit deletes it.
",1388690038
e617ae2dad20950e5358c15fa1290d52ca03a874 remotes/origin/vldb~225^2~29,"fix error message
",1388656298
3713f8129a618a633a7aca8c944960c3e7ac9d3b remotes/origin/vldb~350,"Merge pull request #309 from mateiz/conf2

SPARK-544. Migrate configuration to a SparkConf class

This is still a work in progress based on Prashant and Evan's code. So far I've done the following:

- Got rid of global SparkContext.globalConf
- Passed SparkConf to serializers and compression codecs
- Made SparkConf public instead of private[spark]
- Improved API of SparkContext and SparkConf
- Switched executor environment vars to be passed through SparkConf
- Fixed some places that were still using system properties
- Fixed some tests, though others are still failing

This still fails several tests in core, repl and streaming, likely due to properties not being set or cleared correctly (some of the tests run fine in isolation). But the API at least is hopefully ready for review. Unfortunately there was a lot of global stuff before due to a ""SparkContext.globalConf"" method that let you set a ""default"" configuration of sorts, which meant I had to make some pretty big changes.
",1388640552
e2c68642c64345434e2034082cf9b299491e9e9f remotes/origin/vldb~350^2~2,"Miscellaneous fixes from code review.

Also replaced SparkConf.getOrElse with just a ""get"" that takes a default
value, and added getInt, getLong, etc to make code that uses this
simpler later on.
",1388631819
c1d928a897f8daed5d7e74f4af476b67046f348d remotes/origin/vldb~351,"Merge pull request #312 from pwendell/log4j-fix-2

SPARK-1008: Logging improvments

1. Adds a default log4j file that gets loaded if users haven't specified a log4j file.
2. Isolates use of the tools assembly jar. I found this produced SLF4J warnings
   after building with SBT (and I've seen similar warnings on the mailing list).
",1388624628
f8d245bdfc703eefa4fd34795739a1a851031f5b remotes/origin/vldb~351^2,"Merge remote-tracking branch 'apache-github/master' into log4j-fix-2

Conflicts:
	streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala
",1388621451
2f2524fd11876a19e315f994020877564a8a0df8 remotes/origin/vldb~265^2~114^2,"Addressing issue in compute where compute is invoked instead of iterator on the parent RDD.
",1388554671
42bcfb2bb2b532dc12e13d3cfc1b4556bbb2c43c remotes/origin/vldb~350^2~5,"Fix two compile errors introduced in merge
",1388532383
55b7e2fdffc6c3537da69152a3d02d5be599fa1b remotes/origin/vldb~355,"Merge pull request #289 from tdas/filestream-fix

Bug fixes for file input stream and checkpointing

- Fixed bugs in the file input stream that led the stream to fail due to transient HDFS errors (listing files when a background thread it deleting fails caused errors, etc.)
- Updated Spark's CheckpointRDD and Streaming's CheckpointWriter to use SparkContext.hadoopConfiguration, to allow checkpoints to be written to any HDFS compatible store requiring special configuration.
- Changed the API of SparkContext.setCheckpointDir() - eliminated the unnecessary 'useExisting' parameter. Now SparkContext will always create a unique subdirectory within the user specified checkpoint directory. This is to ensure that previous checkpoint files are not accidentally overwritten.
- Fixed bug where setting checkpoint directory as a relative local path caused the checkpointing to fail.
",1388513571
61e6671f5abbbd0a96cc7359ea6302b84e6e9248 remotes/origin/vldb~265^2~112^2~1,fix test bug,1388498462
779c66ae4ee681f9cf8ab85cd48f4761ee49e031 remotes/origin/vldb~265^2~112^2~5,refactor and fix bugs,1388480345
4abb0c57ab43de9461518e255dfec5e113d88298 remotes/origin/vldb~351^2~2,"Tiny typo fix
",1388477103
3c254f2eec87fbb9de4589816102245e01b0d42c remotes/origin/vldb~351^2~4,"Minor fixes
",1388476533
347fafe4fccc9345ed0ffa6c7863bc233c079b43 remotes/origin/vldb~268^2~26,"Fix CheckpointSuite test fail
",1388437833
2b71ab97c4787e7f82d026e37b41f3a5767b4e89 remotes/origin/vldb~268^2~28,"Merge pull request from aarondav: Utilize DiskBlockManager pathway for temp file writing

This gives us a couple advantages:

- Uses spark.local.dir and randomly selects a directory/disk.
- Ensure files are deleted on normal DiskBlockManager cleanup.
- Availability of same stats as usual DiskBlockObjectWriter (currenty unused).

Also enable basic cleanup when iterator is fully drained.
Still requires cleanup for operations that fail or don't go through all elements.
",1388430090
50e3b8ec4c8150f1cfc6b92f8871f520adf2cfda remotes/origin/vldb~356,"Merge pull request #308 from kayousterhout/stage_naming

Changed naming of StageCompleted event to be consistent

The rest of the SparkListener events are named with ""SparkListener""
as the prefix of the name; this commit renames the StageCompleted
event to SparkListenerStageCompleted for consistency.
",1388418266
29fe6bdaa29193c9dbf3a8fbd05094f3d812d4e5 remotes/origin/vldb~265^2~112^2~7,refactor and bug fix,1388418075
11540b798d622f3883cb40b20cc30ea7d894790a remotes/origin/vldb~350^2~10,"Added tests for SparkConf and fixed a bug

Typesafe Config caches system properties the first time it's invoked
by default, ignoring later changes unless you do something special
",1388360646
20631348d198ba52059f278c1b415c3a80a95b81 remotes/origin/vldb~350^2~18,"Fix other failing tests
",1388290678
a8f316386a429c6d73e3e3824ea6eb28b0381cb5 remotes/origin/vldb~350^2~20,"Fix CheckpointSuite test failures
",1388284003
578bd1fc28513eb84002c604000250f5cff9b815 remotes/origin/vldb~350^2~21,"Fix test failures due to setting / clearing clock type in Streaming
",1388283666
642029e7f43322f84abe4f7f36bb0b1b95d8101d remotes/origin/vldb~350^2~24,"Various fixes to configuration code

- Got rid of global SparkContext.globalConf
- Pass SparkConf to serializers and compression codecs
- Made SparkConf public instead of private[spark]
- Improved API of SparkContext and SparkConf
- Switched executor environment vars to be passed through SparkConf
- Fixed some places that were still using system properties
- Fixed some tests, though others are still failing

This still fails several tests in core, repl and streaming, likely due
to properties not being set or cleared correctly (some of the tests run
fine in isolation).
",1388268795
7375047d516c5aa69221611f5f7b0f1d367039af remotes/origin/vldb~359,"Merge pull request #304 from kayousterhout/remove_unused

Removed unused failed and causeOfFailure variables (in TaskSetManager)
",1388265906
ad3dfd153196497fefe6c1925e0a495a4373f1c5 remotes/origin/vldb~360,"Merge pull request #307 from kayousterhout/other_failure

Removed unused OtherFailure TaskEndReason.

The OtherFailure TaskEndReason was added by @mateiz 3 years ago in this commit: https://github.com/apache/incubator-spark/commit/24a1e7f8380bfd8d4fbdda688482a451bd6ea215

Unless I am missing something, it doesn't seem to have been used then, and is not used now, so seems safe for deletion.
",1388200214
b4619e509bc3e06baa3b031ef2c1981d3bf02cbd remotes/origin/vldb~356^2~1,"Changed naming of StageCompleted event to be consistent

The rest of the SparkListener events are named with ""SparkListener""
as the prefix of the name; this commit renames the StageCompleted
event to SparkListenerStageCompleted for consistency.
",1388195120
19672dca32e41b63dce88b682690ac256b536c8f remotes/origin/vldb~362,"Merge pull request #305 from kayousterhout/line_spacing

Fixed >100char lines in DAGScheduler.scala

There's no changed functionality here -- only line spacing and one grammatical fix in a comment.
",1388180230
0c71ffe924a158608b1760477b883e4818d53af4 remotes/origin/vldb~362^2,"Style fixes as per Reynold's review
",1388175578
2c5bade4ee6db747cbc7b0884094ad443834e3b1 remotes/origin/vldb~354^2,"Fix failed unit tests

    Also clean up a bit.
",1388172270
baaabcedc9225519c728ea34619b2c824aa9ac89 remotes/origin/vldb~359^2,"Removed unused failed and causeOfFailure variables
",1388171556
5c1b4f64052e8fae0d942def4d6085a971faee4e remotes/origin/vldb~273^2~33,"Minor fixes
",1388097579
bacc65cf28b9f95b129e9adede43f684f2c5ced3 remotes/origin/vldb~355^2~4,"Removed slack time in file stream and added better handling of exceptions due to failures due FileNotFound exceptions.
",1388053126
b662c88a24b853542846db538863e04f4862bc20 remotes/origin/vldb~366^2," fix this import order
",1388044173
c344ed04c7d65d64e87bb50ad6eba57534945398 remotes/origin/vldb~367,"Merge pull request #283 from tmyklebu/master

Python bindings for mllib

This pull request contains Python bindings for the regression, clustering, classification, and recommendation tools in mllib.

For each 'train' frontend exposed, there is a Scala stub in PythonMLLibAPI.scala and a Python stub in mllib.py.  The Python stub serialises the input RDD and any vector/matrix arguments into a mutually-understood format and calls the Scala stub.  The Scala stub deserialises the RDD and the vector/matrix arguments, calls the appropriate 'train' function, serialises the resulting model, and returns the serialised model.

ALSModel is slightly different since a MatrixFactorizationModel has RDDs inside.  The Scala stub returns a handle to a Scala MatrixFactorizationModel; prediction is done by calling the Scala predict method.

I have tested these bindings on an x86_64 machine running Linux.  There is a risk that these bindings may fail on some choose-your-own-endian platform if Python's endian differs from java.nio.ByteBuffer's idea of the native byte order.
",1388039466
94479673eb0ea839d5f6b6bd43c5abf75af7b9eb remotes/origin/vldb~347^2~5,"Fixed bug in PartitionAwareUnionRDD
",1388016465
0af4b4f3e86791dc47673be832e77e51b8a8ebcc remotes/origin/vldb~302^2~4,"Bug fixes for updating the RDD block's memory and disk usage information
",1387973221
18ad419b521bab4c5ffd8761652905fdc116a163 remotes/origin/vldb~273^2~39,"Small fix from rebase
",1387963358
55f833803a8e5755eb01b99b976752e5c6bf14e2 remotes/origin/vldb~273^2~40,"Minor bug fix
",1387963165
02208a175c76be111eeb66dc19c7499a6656a067 remotes/origin/vldb~367^2~2,"Initial weights in Scala are ones; do that too.  Also fix some errors.
",1387950828
85a344b4f0cd149c6e6f06f8b942c34146b302be remotes/origin/vldb~370,"Merge pull request #127 from kayousterhout/consolidate_schedulers

Deduplicate Local and Cluster schedulers.

The code in LocalScheduler/LocalTaskSetManager was nearly identical
to the code in ClusterScheduler/ClusterTaskSetManager. The redundancy
made making updating the schedulers unnecessarily painful and error-
prone. This commit combines the two into a single TaskScheduler/
TaskSetManager.

Unfortunately the diff makes this change look much more invasive than it is -- TaskScheduler.scala is only superficially changed (names updated, overrides removed) from the old ClusterScheduler.scala, and the same with
TaskSetManager.scala.

Thanks @rxin for suggesting this change!
",1387931706
9115a5de62dcb832569727773112a4688ef63f03 remotes/origin/vldb~354^2~2,"Remove import * and fix some formatting
",1387925970
c2dd6bcd6eda2c5a741138e9a984c40d2635ca33 remotes/origin/vldb~371,"Merge pull request #279 from aarondav/shuffle-cleanup0

Clean up shuffle files once their metadata is gone

Previously, we would only clean the in-memory metadata for consolidated shuffle files.

Additionally, fixes a bug where the Metadata Cleaner was ignoring type-specific TTLs.
",1387924607
d4dfab503a9222b5acf5c4bf69b91c16f298e4aa remotes/origin/vldb~355^2~5,"Fixed Python API for sc.setCheckpointDir. Also other fixes based on Reynold's comments on PR 289.
",1387922473
2402180b32d530319d0526490afa3cfafc5c36b8 remotes/origin/vldb~367^2~8,"Fix error message ugliness.
",1387919913
9f79fd89dc84cda7ebeb98a0b43c8e982fefa787 remotes/origin/vldb~355^2~6,"Merge branch 'apache-master' into filestream-fix
",1387913897
0af7f84c8eb631cd2e427b692f407ec2d37dad64 remotes/origin/vldb~355^2~7,"Minor formatting fixes.
",1387849636
b31e91f927356c50d24286ba70f00fa8f6527e2f remotes/origin/vldb~355^2~9,"Merge branch 'scheduler-update' into filestream-fix
",1387843155
19d1d58b67a767b227e009ab8723efaa7087dd07 remotes/origin/vldb~355^2~10,"Fixed bug in file stream that prevented some files from being read
correctly.
",1387842523
f9771690a698b6ce5d29eb36b38bbeb498d1af0d remotes/origin/vldb~374^2~1,"Minor formatting fixes.
",1387827146
e7b62cbfbfdb8fda880548bce4249672c6a0a851 remotes/origin/vldb~355^2~11,"Updated CheckpointWriter and FileInputDStream to be robust against failed FileSystem objects. Refactored JobGenerator to use actor so that all updating of DStream's metadata is single threaded.
",1387766976
d91ec6f8ea3ab6683af92eddfedd9ea6c0710f00 remotes/origin/vldb~355^2~12,"Merge branch 'scheduler-update' into filestream-fix
",1387754615
44e4205ac579a9a4dfb2f6041d34caea568059ce remotes/origin/vldb~265^2~116,"Merge pull request #116 from jianpingjwang/master

remove unused variables and fix a bug",1387741458
b7bfae1afecad0ae79d5d040d2e02e390c272efb remotes/origin/vldb~370^2~1,"Correctly merged in maxTaskFailures fix
",1387726484
b8ae096a40eb0f83aac889deb061a9484effd9aa remotes/origin/vldb~370^2~2,"Fix build error in test
",1387697328
4797c227ff7aafcc1e4dcbbaa5281b55361484e6 remotes/origin/vldb~265^2~117,"Merge pull request #118 from ankurdave/VertexPartitionSuite

Test VertexPartition and fix bugs",1387575039
32508e20d468fcb72fb89e6ae23c9fdd6475f0c8 remotes/origin/vldb~265^2~117^2,"Test VertexPartition and fix bugs
",1387573206
343d8977aa7d53f381b014778fb60106f9cbcabb remotes/origin/vldb~265^2~116^2~5,remove unused variable and fix a bug,1387529844
a69465b1fa7250d036e1585543c225b6340e4790 remotes/origin/vldb~265^2~119^2~1,"Split VertexRDD tests; fix #114
",1387513950
0647ec97573dc267c7a6b4679fb938b4dfa4fbb6 remotes/origin/vldb~371^2,"Clean up shuffle files once their metadata is gone

Previously, we would only clean the in-memory metadata for consolidated
shuffle files.

Additionally, fixes a bug where the Metadata Cleaner was ignoring type-
specific TTLs.
",1387496448
984c5824876e0daceb8a74af57593926faf727ce remotes/origin/vldb~355^2~13,"Merge branch 'scheduler-update' into filestream-fix

Conflicts:
	core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala
	streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala
	streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala
	streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala
	streaming/src/test/scala/org/apache/spark/streaming/CheckpointSuite.scala
",1387480848
bf20591a006b9d2fdd9a674d637f5e929fd065a2 remotes/origin/vldb~367^2~24,"Incorporate most of Josh's style suggestions.  I don't want to deal with the type and length checking errors until we've got at least one working stub that we're all happy with.
",1387442457
d8d3f3e60d919c788e63f18a70f11b96c9f6daa0 remotes/origin/vldb~380,"Merge pull request #183 from aarondav/spark-959

[SPARK-959] Explicitly depend on org.eclipse.jetty.orbit jar

Without this, in some cases, Ivy attempts to download the wrong file and fails, stopping the whole build. See [bug](https://spark-project.atlassian.net/browse/SPARK-959) for more details.

Note that this may not be the best solution, as I do not understand the root cause of why this only happens for some people. However, it is reported to work.
",1387440403
eaf6a269b123e1eca1f1a3cb9e210a9b37ae4a27 remotes/origin/vldb~380^2,"[SPARK-959] Explicitly depend on org.eclipse.jetty.orbit jar

Without this, in some cases, Ivy attempts to download the wrong file
and fails, stopping the whole build. See bug for more details.

(This is probably also the beginning of the slow death of our
recently prettified dependencies. Form follow function.)
",1387438651
bfba5323bedca3aa021e3cd9267d88379e767a92 remotes/origin/vldb~381,"Merge pull request #247 from aarondav/minor

Increase spark.akka.askTimeout default to 30 seconds

In experimental clusters we've observed that a 10 second timeout was insufficient, despite having a low number of nodes and relatively small workload (16 nodes, <1.5 TB data). This would cause an entire job to fail at the beginning of the reduce phase.
There is no particular reason for this value to be small as a timeout should only occur in an exceptional situation.

Also centralized the reading of spark.akka.askTimeout to AkkaUtils (surely this can later be cleaned up to use Typesafe).

Finally, deleted some lurking implicits. If anyone can think of a reason they should still be there, please let me know.
",1387434141
293a0af5a1def95e47d9188f42957083f5adf3b8 remotes/origin/vldb~381^2,"In experimental clusters we've observed that a 10 second timeout was insufficient,
despite having a low number of nodes and relatively small workload (16 nodes, <1.5 TB data).
This would cause an entire job to fail at the beginning of the reduce phase.
There is no particular reason for this value to be small as a timeout should only occur
in an exceptional situation.

Also centralized the reading of spark.akka.askTimeout to AkkaUtils (surely this can later
be cleaned up to use Typesafe).

Finally, deleted some lurking implicits. If anyone can think of a reason they should still
be there, please let me know.
",1387431749
10c0ffa1eb23c6190f460be224ea555573458980 remotes/origin/vldb~385^2,"One other fix
",1387260655
ceb013f8b97051ee96c65a8da7489a2b251ef799 remotes/origin/vldb~385^2~4,"Remove trailing slashes from repository specifications.

The correct format is to not have a trailing slash.

For me this caused non-deterministic failures due to issues fetching
certain artifacts. The issue was that some of the maven caches would
fail to fetch the artifact (due to the way that the artifact
path was concatenated with the repository) and this short-circuited
the download process in a silent way. Here is what the log output
looked like:

    Downloading: http://repo.maven.apache.org/maven2/org/spark-project/akka/akka-remote_2.10/2.2.3-shaded-protobuf/akka-remote_2.10-2.2.3-shaded-protobuf.pom
    [WARNING] The POM for org.spark-project.akka:akka-remote_2.10:jar:2.2.3-shaded-protobuf is missing, no dependency information available

This was pretty brutal to debug since there was no error message
anywhere and the path *looks* correct as reported by the Maven log.
",1387259631
964a3b6971716823f473f672611951d1e489a552 remotes/origin/vldb~386,"Merge pull request #270 from ewencp/really-force-ssh-pseudo-tty-master

Force pseudo-tty allocation in spark-ec2 script.

ssh commands need the -t argument repeated twice if there is no local
tty, e.g. if the process running spark-ec2 uses nohup and the parent
process exits.

Without this change, if you run the script this way (e.g. using nohup from a cron job), it will fail setting up the nodes because some of the ssh commands complain about missing ttys and then fail.

(This version is for the master branch. I've filed a separate request for the 0.8 since changes to the script caused the patches to be different.)
",1387236231
883e034aebe61a25631497b4e299a8f2e3389b00 remotes/origin/vldb~387,"Merge pull request #245 from gregakespret/task-maxfailures-fix

Fix for spark.task.maxFailures not enforced correctly.

Docs at http://spark.incubator.apache.org/docs/latest/configuration.html say:

```
spark.task.maxFailures

Number of individual task failures before giving up on the job. Should be greater than or equal to 1. Number of allowed retries = this value - 1.
```

Previous implementation worked incorrectly. When for example `spark.task.maxFailures` was set to 1, the job was aborted only after the second task failure, not after the first one.
",1387232162
d2ced6d58c5e8aea23f909c2fc4ac11aa1b55607 remotes/origin/vldb~389,"Merge pull request #256 from MLnick/master

Fix 'IPYTHON=1 ./pyspark' throwing ValueError

This fixes an annoying issue where running ```IPYTHON=1 ./pyspark``` resulted in:

```
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 0.8.0
      /_/

Using Python version 2.7.5 (default, Jun 20 2013 11:06:30)
Spark context avaiable as sc.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/local/lib/python2.7/site-packages/IPython/utils/py3compat.pyc in execfile(fname, *where)
    202             else:
    203                 filename = fname
--> 204             __builtin__.execfile(filename, *where)

/Users/Nick/workspace/scala/spark-0.8.0-incubating-bin-hadoop1/python/pyspark/shell.py in <module>()
     30 add_files = os.environ.get(""ADD_FILES"").split(',') if os.environ.get(""ADD_FILES"") != None else None
     31
---> 32 sc = SparkContext(os.environ.get(""MASTER"", ""local""), ""PySparkShell"", pyFiles=add_files)
     33
     34 print """"""Welcome to

/Users/Nick/workspace/scala/spark-0.8.0-incubating-bin-hadoop1/python/pyspark/context.pyc in __init__(self, master, jobName, sparkHome, pyFiles, environment, batchSize)
     70         with SparkContext._lock:
     71             if SparkContext._active_spark_context:
---> 72                 raise ValueError(""Cannot run multiple SparkContexts at once"")
     73             else:
     74                 SparkContext._active_spark_context = self

ValueError: Cannot run multiple SparkContexts at once
```

The issue arises since previously IPython didn't seem to respect ```$PYTHONSTARTUP```, but since at least 1.0.0 it has. Technically this might break for older versions of IPython, but most users should be able to upgrade IPython to at least 1.0.0 (and should be encouraged to do so :).

New behaviour:
```
Nicks-MacBook-Pro:incubator-spark-mlnick Nick$ IPYTHON=1 ./pyspark
Python 2.7.5 (default, Jun 20 2013, 11:06:30)
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 1.1.0 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/Users/Nick/workspace/scala/incubator-spark-mlnick/tools/target/scala-2.9.3/spark-tools-assembly-0.9.0-incubating-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/Users/Nick/workspace/scala/incubator-spark-mlnick/assembly/target/scala-2.9.3/spark-assembly-0.9.0-incubating-SNAPSHOT-hadoop1.0.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
13/12/12 13:08:15 WARN Utils: Your hostname, Nicks-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.4 instead (on interface en0)
13/12/12 13:08:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
13/12/12 13:08:15 INFO Slf4jEventHandler: Slf4jEventHandler started
13/12/12 13:08:15 INFO SparkEnv: Registering BlockManagerMaster
13/12/12 13:08:15 INFO DiskBlockManager: Created local directory at /var/folders/_l/06wxljt13wqgm7r08jlc44_r0000gn/T/spark-local-20131212130815-0e76
13/12/12 13:08:15 INFO MemoryStore: MemoryStore started with capacity 326.7 MB.
13/12/12 13:08:15 INFO ConnectionManager: Bound socket to port 53732 with id = ConnectionManagerId(10.0.0.4,53732)
13/12/12 13:08:15 INFO BlockManagerMaster: Trying to register BlockManager
13/12/12 13:08:15 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 10.0.0.4:53732 with 326.7 MB RAM
13/12/12 13:08:15 INFO BlockManagerMaster: Registered BlockManager
13/12/12 13:08:15 INFO HttpBroadcast: Broadcast server started at http://10.0.0.4:53733
13/12/12 13:08:15 INFO SparkEnv: Registering MapOutputTracker
13/12/12 13:08:15 INFO HttpFileServer: HTTP File server directory is /var/folders/_l/06wxljt13wqgm7r08jlc44_r0000gn/T/spark-8f40e897-8211-4628-a7a8-755562d5244c
13/12/12 13:08:16 INFO SparkUI: Started Spark Web UI at http://10.0.0.4:4040
2013-12-12 13:08:16.337 java[56801:4003] Unable to load realm info from SCDynamicStore
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 0.9.0-SNAPSHOT
      /_/

Using Python version 2.7.5 (default, Jun 20 2013 11:06:30)
Spark context avaiable as sc.
```
",1387145494
ab85f88fd77a883684cec0a58222d983b9bd18fd remotes/origin/vldb~391,"Merge pull request #264 from shivaram/spark-class-fix

Use CoarseGrainedExecutorBackend in spark-class
",1387140512
8a56c1ff671f23e17ad86c15ec49e7817dbeb75f remotes/origin/vldb~265^2~127,"Merge pull request #84 from amatsukawa/graphlab_enhancements

GraphLab bug fix & set start vertex",1387067364
7dbd3bf8252125e7ea3726802b160af8a0aafae2 remotes/origin/vldb~265^2~124^2~3,"Remove debug printing from PregelSuite
",1387064723
59f625b7454d882c06461222c3e305910fff9594 remotes/origin/vldb~265^2~124^2~6,"Revert ""Add debug logging to Pregel""

This reverts commit e62013cdd92137507a96b6a5b347a1d239209587.
",1387063704
ee5c69e481c4a8c883d3992b9ab5a7e8f87a0b41 remotes/origin/vldb~265^2~124^2~7,"Fix bug in VertexPartition.isActive

This took me ~5 hours to find!
",1387063703
4d3bba3a1336bb2762e050ed515de0fac3add252 remotes/origin/vldb~265^2~124^2~8,"Add debug logging to Pregel
",1387063703
7a8952e9bbf282c261c4d27ab3462b7d28ca575c remotes/origin/vldb~265^2~124^2~13,"Replace skipStale with activeSetOpt in mrTriplets (fails Pregel)
",1387063703
77b92748ad0224f55df9e2dbb80aa98f1597a49a remotes/origin/vldb~265^2~124^2~14,"Replace update with innerJoin (has a bug)

There is a conflict between vertices that didn't change so are not moved
but still need to run, and vertices that were deleted by the innerJoin
so should not run.
",1387063703
dc7214790072d57cab45326e4777452cc5740495 remotes/origin/vldb~265^2~124^2~21,"Fix bug in interaction of incr. view maint., skipStale, and join rewrite
",1387063702
4f4f0add2496c5478b0538c0dc861dba4419459c remotes/origin/vldb~265^2~124^2~23,"Fix bug in mapVertices and outerJoinVertices
",1387063702
d00cc8092bf9c09fffedafbe6d040e2f7bc1da5a remotes/origin/vldb~265^2~125^2~1,"Fix argument bug and closure capture
",1387062237
97ac06018206b593600594605be241d0cd706e08 remotes/origin/vldb~394,"Merge pull request #259 from pwendell/scala-2.10

Migration to Scala 2.10

== Below description was written by Prashant Sharma ==

This PR migrates spark to scala 2.10.

Summary of changes apart from scala 2.10 migration:
(has no implications for user.)
1. Migrated Akka to 2.2.3.

Does not use remote death watch for it has a bug, where it tries to send message to dead node infinitely.

Uses an indestructible actorsystem which tolerates errors only on executors.

(Might be useful for user.)
4. New configuration settings introduced:

System.getProperty(""spark.akka.heartbeat.pauses"", ""600"")
System.getProperty(""spark.akka.failure-detector.threshold"", ""300.0"")
System.getProperty(""spark.akka.heartbeat.interval"", ""1000"")

Defaults for these are fairly large to only disable Failure detector that comes with akka. The reason for doing so is we have our own failure detector like mechanism in place and then this is just an overhead on top of that + it leads to a lot of false positives. But with these properties it is possible to enable them. A good use case for enabling it could be when someone wants spark to be sensitive (in a controllable manner ofc.) to GC pauses/Network lags and quickly evict executors that experienced it. More information is included in configuration.md

Once we have the SPARK-544 merged, I had like to deprecate atleast these akka properties and may be others too.

This PR is duplicate of #221(where all the discussion happened.) for that one pointed to master this one points to scala-2.10 branch.
",1387009365
7ac944fc27805e2a76285ce3a31f3b2ecf4a7b78 remotes/origin/vldb~394^2,"Merge pull request #262 from pwendell/mvn-fix

Fix maven build issues in 2.10 branch

Found some issues when locally testing maven.
",1387005728
6e8a96c7e7652b3d1fc709b3c3ccc5f90ffeb623 remotes/origin/vldb~394^2^2,"Fix maven build issues in 2.10 branch
",1387005248
2e89398e44b3103598a4fae0b09368ed9fbda9c2 remotes/origin/vldb~394^2~4,"Merge pull request #254 from ScrapCodes/scala-2.10

Scala 2.10 migration

This PR migrates spark to scala 2.10.

Summary of changes apart from scala 2.10 migration:
(has no implications for user.)
1. Migrated Akka to 2.2.3.

Does not use remote death watch for it has a bug, where it tries to send message to dead node infinitely.

Uses an indestructible actorsystem which tolerates errors only on executors.

(Might be useful for user.)
4. New configuration settings introduced:

System.getProperty(""spark.akka.heartbeat.pauses"", ""600"")
System.getProperty(""spark.akka.failure-detector.threshold"", ""300.0"")
System.getProperty(""spark.akka.heartbeat.interval"", ""1000"")

Defaults for these are fairly large to only disable Failure detector that comes with akka. The reason for doing so is we have our own failure detector like mechanism in place and then this is just an overhead on top of that + it leads to a lot of false positives. But with these properties it is possible to enable them. A good use case for enabling it could be when someone wants spark to be sensitive (in a controllable manner ofc.) to GC pauses/Network lags and quickly evict executors that experienced it. More information is included in configuration.md

Once we have the SPARK-544 merged, I had like to deprecate atleast these akka properties and may be others too.

This PR is duplicate of #221(where all the discussion happened.) for that one pointed to master this one points to scala-2.10 branch.
",1386832253
ce6ca4ea6177432aef694f6213e36c75ac213bf4 remotes/origin/standalone-pagerank~3,"Merge pull request #97 from dcrankshaw/fix-rddtop

Added BoundedPriorityQueue kryo registrator. Fixes top issue.",1386829854
12483d4ae641d4f3d9153e763fbd8aab0931ef9b remotes/origin/standalone-pagerank~3^2,"Added BoundedPriorityQueue kryo registrator. Fixes top issue.
",1386824181
5e9ce83d682d6198cda4631faf11cb53fcccf07f remotes/origin/vldb~355^2~14,"Fixed multiple file stream and checkpointing bugs.
- Made file stream more robust to transient failures.
- Changed Spark.setCheckpointDir API to not have the second
  'useExisting' parameter. Spark will always create a unique directory
  for checkpointing underneath the directory provide to the funtion.
- Fixed bug wrt local relative paths as checkpoint directory.
- Made DStream and RDD checkpointing use
  SparkContext.hadoopConfiguration, so that more HDFS compatible
  filesystems are supported for checkpointing.
",1386799296
f4c73df5c981476c4ca431613d8b4a827eddf653 remotes/origin/vldb~394^2~4^2~1,"Merge branch 'akka-bug-fix' of github.com:ScrapCodes/incubator-spark into akka-bug-fix
",1386737564
603af51bb5257744ce0db28e7f10db6a2ba899ec remotes/origin/vldb~394^2~4^2~2,"Merge branch 'master' into akka-bug-fix

Conflicts:
	core/pom.xml
	core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala
	pom.xml
	project/SparkBuild.scala
	streaming/pom.xml
	yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala
",1386737513
17db6a9041d5e83d7b6fe47f9c36758d0613fcd6 remotes/origin/vldb~394^2~4^2~3,"Style fixes and addressed review comments at #221
",1386656236
c1201f47e0d44e92da42adb23d27f60d9d494642 remotes/origin/vldb~394^2~4^2~4,"fixed yarn build
",1386574250
d992ec6d9be30e624c8edb2a50c193ac3cfbab7a remotes/origin/vldb~397,"Merge pull request #195 from dhardy92/fix_DebScriptPackage

[Deb] fix package of Spark classes adding org.apache prefix in scripts embeded in .deb
",1386564560
0428145ed419c63d689078387d80584c7105b0b7 remotes/origin/vldb~392^2~3,"Small fix
",1386484391
f466f79b88cc0766d87de4ea5bdb27d826be4487 remotes/origin/vldb~400,"Merge pull request #239 from aarondav/nit

Correct spellling error in configuration.md
",1386445912
3abfbfb104465fbb4636c093580351c499e9dd81 remotes/origin/standalone-pagerank~4,"Merge pull request #92 from ankurdave/rdd-names

Set RDD names for easy debugging",1386444259
84d0e1a3344fb6afbc8c3562018a4b8767002051 remotes/origin/standalone-pagerank~4^2,"Set RDD names for easy debugging
",1386417945
cb6ac8aafb5491f137146db8e8937cbefc7ffdb1 remotes/origin/vldb~400^2,"Correct spellling error in configuration.md
",1386409201
10c3c0c6524d0cf6c59b6f2227bf316cdeb7d06c remotes/origin/vldb~401,"Merge pull request #237 from pwendell/formatting-fix

Formatting fix

This is a single-line change. The diff appears larger here due to github being out of sync.
",1386390585
7a1d1c93b824a5b38c4ddeb16900dc6173482c83 remotes/origin/vldb~401^2,"Minor formatting fix in config file
",1386390502
e5d5728b72e58046cc175ab06b5f1c7be4957711 remotes/origin/vldb~403,"Merge pull request #235 from pwendell/master

Minor doc fixes and updating README
",1386389696
bb6e25c663a0fa96552994bcdda2049e9b621db7 remotes/origin/vldb~403^2,"Minor doc fixes and updating README
",1386380548
e0392343a026d632bac0df0ad4f399fce742c151 remotes/origin/vldb~405,"Merge pull request #190 from markhamstra/Stages4Jobs

stageId <--> jobId mapping in DAGScheduler

Okay, I think this one is ready to go -- or at least it's ready for review and discussion.  It's a carry-over of https://github.com/mesos/spark/pull/842 with updates for the newer job cancellation functionality.  The prior discussion still applies.  I've actually changed the job cancellation flow a bit: Instead of ``cancelTasks`` going to the TaskScheduler and then ``taskSetFailed`` coming back to the DAGScheduler (resulting in ``abortStage`` there), the DAGScheduler now takes care of figuring out which stages should be cancelled, tells the TaskScheduler to cancel tasks for those stages, then does the cleanup within the DAGScheduler directly without the need for any further prompting by the TaskScheduler.

I know of three outstanding issues, each of which can and should, I believe, be handled in follow-up pull requests:

1) https://spark-project.atlassian.net/browse/SPARK-960
2) JobLogger should be re-factored to eliminate duplication
3) Related to 2), the WebUI should also become a consumer of the DAGScheduler's new understanding of the relationship between jobs and stages so that it can display progress indication and the like grouped by job.  Right now, some of this information is just being sent out as part of ``SparkListenerJobStart`` messages, but more or different job <--> stage information may need to be exported from the DAGScheduler to meet listeners needs.

Except for the eventQueue -> Actor commit, the rest can be cherry-picked almost cleanly into branch-0.8.  A little merging is needed in MapOutputTracker and the DAGScheduler.  Merged versions of those files are in https://github.com/markhamstra/incubator-spark/tree/aba2b40ce04ee9b7b9ea260abb6f09e050142d43

Note that between the recent Actor change in the DAGScheduler and the cleaning up of DAGScheduler data structures on job completion in this PR, some races have been introduced into the DAGSchedulerSuite.  Those tests usually pass, and I don't think that better-behaved code that doesn't directly inspect DAGScheduler data structures should be seeing any problems, but I'll work on fixing DAGSchedulerSuite as either an addition to this PR or as a separate request.

UPDATE: Fixed the race that I introduced.  Created a JIRA issue (SPARK-965) for the one that was introduced with the switch to eventProcessorActor in the DAGScheduler.
",1386359399
41721b1494fe33c184374925bada3981296dee69 remotes/origin/sigmod~14,"Fixed a bug in VTableReplicated that we only process the first block.
",1386319872
c9cd2af71eae4126536db790ceaffe0587da7d89 remotes/origin/vldb~394^2~4^2~7^2,"Merge branch 'wip-scala-2.10' into akka-bug-fix
",1386316935
15168d6c4dab8f5debf6d3935035fbdb923b157b remotes/origin/sigmod~16,"Fixed a bug in VTableReplicated that we are always broadcasting all the vertices.
",1386314753
1c8500efc0a718b08a15f98c5c0cb23174498b29 remotes/origin/standalone-pagerank~6,"Merge pull request #88 from amplab/varenc

Fixed a bug that variable encoding doesn't work for ints that use all 64 bits.",1386289544
4f80dd22bd4068ca82a6b96436bc0fdd97d8e9be remotes/origin/standalone-pagerank~6^2,"Fixed a bug that variable encoding doesn't work for ints that use all 64 bits.
",1386289177
72b696156c8662cae2cef4b943520b4be86148ea remotes/origin/vldb~411,"Merge pull request #199 from harveyfeng/yarn-2.2

Hadoop 2.2 migration

Includes support for the YARN API stabilized in the Hadoop 2.2 release, and a few style patches.

Short description for each set of commits:

a98f5a0 - ""Misc style changes in the 'yarn' package""
a67ebf4 - ""A few more style fixes in the 'yarn' package""
Both of these are some minor style changes, such as fixing lines over 100 chars, to the existing YARN code.

ab8652f - ""Add a 'new-yarn' directory ... ""
Copies everything from `SPARK_HOME/yarn` to `SPARK_HOME/new-yarn`. No actual code changes here.

4f1c3fa - ""Hadoop 2.2 YARN API migration ...""
API patches to code in the `SPARK_HOME/new-yarn` directory. There are a few more small style changes mixed in, too.
Based on @colorant's Hadoop 2.2 support for the scala-2.10 branch in #141.

a1a1c62 - ""Add optional Hadoop 2.2 settings in sbt build ... ""
If Spark should be built against Hadoop 2.2, then:
a) the `org.apache.spark.deploy.yarn` package will be compiled from the `new-yarn` directory.
b) Protobuf v2.5 will be used as a Spark dependency, since Hadoop 2.2 depends on it. Also, Spark will be built against a version of Akka v2.0.5 that's built against Protobuf 2.5, named `akka-2.0.5-protobuf-2.5`. The patched Akka is here: https://github.com/harveyfeng/akka/tree/2.0.5-protobuf-2.5, and was published to local Ivy during testing.

There's also a new boolean environment variable, `SPARK_IS_NEW_HADOOP`, that users can manually set if their `SPARK_HADOOP_VERSION` specification does not start with `2.2`, which is how the build file tries to detect a 2.2 version. Not sure if this is necessary or done in the best way, though...
",1386228784
e0347ba6c74fba51d2ac5c0a8becb3dd6d5c4fd0 remotes/origin/standalone-pagerank~9,"Merge pull request #83 from ankurdave/fix-tests

Fix compile errors in GraphSuite and SerializerSuite",1386207486
92e96f727ee944e7ed17b84aa8a57106907a9881 remotes/origin/standalone-pagerank~9^2~1,"Fix compile errors in GraphSuite and SerializerSuite
",1386206992
182f9baeed8e4cc62ca14ae04413394477a7ccfb remotes/origin/vldb~412,"Merge pull request #227 from pwendell/master

Fix small bug in web UI and minor clean-up.

There was a bug where sorting order didn't work correctly for write time metrics.

I also cleaned up some earlier code that fixed the same issue for read and
write bytes.
",1386201127
380b90b9b360db9cb6a4edc1312704afe11eb31d remotes/origin/vldb~412^2,"Fix small bug in web UI and minor clean-up.

There was a bug where sorting order didn't work correctly for write time metrics.

I also cleaned up some earlier code that fixed the same issue for read and
write bytes.
",1386196908
8a3475aed66617772f4e98e9f774b109756eb391 remotes/origin/vldb~416,"Merge pull request #218 from JoshRosen/spark-970-pyspark-unicode-error

Fix UnicodeEncodeError in PySpark saveAsTextFile() (SPARK-970)

This fixes [SPARK-970](https://spark-project.atlassian.net/browse/SPARK-970), an issue where PySpark's saveAsTextFile() could throw UnicodeEncodeError when called on an RDD of Unicode strings.

Please merge this into master and branch-0.8.
",1386109300
1b6e450771ded9b4852efab362afe3e10f7631ee remotes/origin/vldb~411^2~1,"Use published ""org.spark-project.akka-*"" in sbt build for Hadoop-2.2 dependencies.

This also includes:
-Change `isNewYarn` to `isNewHadoop`, since the protobuf-2.5 dependency is from Hadoop-2.2 itself.
-Regexp bugix

Credits to @alig for this patch.",1386059313
740922f25d5f81617fbe02c7bcd1610d6426bbef remotes/origin/vldb~418,"Merge pull request #219 from sundeepn/schedulerexception

Scheduler quits when newStage fails

The current scheduler thread does not handle exceptions from newStage stage while launching new jobs. The thread fails on any exception that gets triggered at that level, leaving the cluster hanging with no schduler.
",1385930818
60e23a58b288dae3c87da28e1506323b1d88ee9e remotes/origin/vldb~419,"Merge pull request #216 from liancheng/fix-spark-966

Bugfix: SPARK-965 & SPARK-966

SPARK-965: https://spark-project.atlassian.net/browse/SPARK-965
SPARK-966: https://spark-project.atlassian.net/browse/SPARK-966

* Add back `DAGScheduler.start()`, `eventProcessActor` is created and started here.

  Notice that function is only called by `SparkContext`.

* Cancel the scheduled stage resubmission task when stopping `eventProcessActor`

* Add a new `DAGSchedulerEvent` `ResubmitFailedStages`

  This event message is sent by the scheduled stage resubmission task to `eventProcessActor`.  In this way, `DAGScheduler.resubmitFailedStages()` is guaranteed to be executed from the same thread that runs `DAGScheduler.processEvent()`.

  Please refer to discussion in [SPARK-966](https://spark-project.atlassian.net/browse/SPARK-966) for details.
",1385883529
4d53830eb79174cfd9641f6342727bc980d5c3e0 remotes/origin/vldb~418^2~1,"Scheduler quits when createStage fails.

The current scheduler thread does not handle exceptions from createStage stage while launching new jobs. The thread fails on any exception that gets triggered at that level, leaving the cluster hanging with no schduler.
",1385857092
34ee81415eacc990e3e709daa833318868aff763 remotes/origin/standalone-pagerank~10^2~5,"Merged Ankur's pull request #80 and fixed subgraph.
",1385853030
3292cb0f9c1b0b6efbf4a7596dfddfc1a54c004f remotes/origin/standalone-pagerank~10^2~9^2,"Revert ""Fix join error by caching vTable in mapReduceTriplets""

This reverts commit dee1318d3d4e37489dc2b0bb373857cc13b4d533, which is
unnecessary due to 7528e6d5f15fd0e01a206f60a6db218858cac4d3.
",1385849132
dee1318d3d4e37489dc2b0bb373857cc13b4d533 remotes/origin/standalone-pagerank~10^2~13,"Fix join error by caching vTable in mapReduceTriplets
",1385847439
1bc83ca79187979f58385d3f28236111217174e0 remotes/origin/vldb~394^2~4^2~11,"Changed defaults for akka to almost disable failure detector.
",1385712665
18def5d6f20b33c946f9b8b2cea8cfb6848dcc34 remotes/origin/vldb~419^2~2,"Bugfix: SPARK-965 & SPARK-966

SPARK-965: https://spark-project.atlassian.net/browse/SPARK-965
SPARK-966: https://spark-project.atlassian.net/browse/SPARK-966

* Add back DAGScheduler.start(), eventProcessActor is created and started here.

  Notice that function is only called by SparkContext.

* Cancel the scheduled stage resubmission task when stopping eventProcessActor

* Add a new DAGSchedulerEvent ResubmitFailedStages

  This event message is sent by the scheduled stage resubmission task to eventProcessActor.  In this way, DAGScheduler.resubmitFailedStages is guaranteed to be executed from the same thread that runs DAGScheduler.processEvent.

  Please refer to discussion in SPARK-966 for details.
",1385631966
743a31a7ca4421cbbd5b615b773997a06a7ab4ee remotes/origin/vldb~420,"Merge pull request #210 from haitaoyao/http-timeout

add http timeout for httpbroadcast

While pulling task bytecode from HttpBroadcast server, there's no timeout value set. This may cause spark executor code hang and other task in the same executor process wait for the lock. I have encountered the issue in my cluster. Here's the stacktrace I captured  : https://gist.github.com/haitaoyao/7655830

So add a time out value to ensure the task fail fast.
",1385605479
9e896be375a5c0270bbdf45a2532e59bcb813efa remotes/origin/standalone-pagerank~10^2~6^2,"Test edge filtering in subgraph (test fails)
",1385510335
137294e2abeacda85bdf9036f5b09ab2a96d9cdf remotes/origin/standalone-pagerank~10^2~6^2~1,"Test GraphImpl.subgraph and fix bug
",1385508767
0e2109ddb2f27d8a6a9f125206674273b03d1f5e remotes/origin/vldb~426,"Merge pull request #204 from rxin/hash

OpenHashSet fixes

Incorporated ideas from pull request #200.
- Use Murmur Hash 3 finalization step to scramble the bits of HashCode
  instead of the simpler version in java.util.HashMap; the latter one
  had trouble with ranges of consecutive integers. Murmur Hash 3 is used
  by fastutil.
- Don't check keys for equality when re-inserting due to growing the
  table; the keys will already be unique.
- Remember the grow threshold instead of recomputing it on each insert

Also added unit tests for size estimation for specialized hash sets and maps.
",1385441317
489862a65766d30278c186d280c6286937c81155 remotes/origin/vldb~394^2~4^2~19,"Remote death watch has a funny bug.
https://gist.github.com/ScrapCodes/4805fd84906e40b7b03d
",1385382602
77929cfeed95905106f5b3891e8de1b1c312d119 remotes/origin/vldb~394^2~4^2~20,"Fine tuning defaults for akka and restored tracking of dissassociated events, for they are delivered when a remote TCP socket is closed. Also made transport failure heartbeats larger interval for it is mostly not needed. As we are using remote death watch instead.
",1385369001
6af03edcf15e517f69598c4e974cca69b63904fa tags/pre_edgerdd~1,"Merge pull request #76 from dcrankshaw/fix_partitioners

Actually use partitioner command line args in Analytics.",1385340157
859d62dc2a37f56f8c85138df069a359e7fabb45 remotes/origin/vldb~431,"Merge pull request #151 from russellcardullo/add-graphite-sink

Add graphite sink for metrics

This adds a metrics sink for graphite.  The sink must
be configured with the host and port of a graphite node
and optionally may be configured with a prefix that will
be prepended to all metrics that are sent to graphite.
",1385338791
972171b9d93b07e8511a2da3a33f897ba033484b remotes/origin/vldb~433,"Merge pull request #197 from aarondav/patrick-fix

Fix 'timeWriting' stat for shuffle files

Due to concurrent git branches, changes from shuffle file consolidation patch
caused the shuffle write timing patch to no longer actually measure the time,
since it requires time be measured after the stream has been closed.
",1385337046
718cc803f7e0600c9ab265022eb6027926a38010 remotes/origin/vldb~434,"Merge pull request #200 from mateiz/hash-fix

AppendOnlyMap fixes

- Chose a more random reshuffling step for values returned by Object.hashCode to avoid some long chaining that was happening for consecutive integers (e.g. `sc.makeRDD(1 to 100000000, 100).map(t => (t, t)).reduceByKey(_ + _).count`)
- Some other small optimizations throughout (see commit comments)
",1385262122
a67ebf43776d9b66c077c48f2c9d1976791ca4e8 remotes/origin/vldb~411^2~7,"A few more style fixes in `yarn` package.
",1385255310
f20093c3afa68439b1c9010de189d497df787c2a remotes/origin/vldb~437,"Merge pull request #196 from pwendell/master

TimeTrackingOutputStream should pass on calls to close() and flush().

Without this fix you get a huge number of open files when running shuffles.
",1385086333
53b94ef2f5179bdbebe70883b2593b569518e77e remotes/origin/vldb~437^2,"TimeTrackingOutputStream should pass on calls to close() and flush().

Without this fix you get a huge number of open shuffles after running
shuffles.
",1385083215
92c7cc00fbf265a16115dc885f84c618d457389c remotes/origin/vldb~397^2,"[Deb] fix package of Spark classes adding org.apache prefix in scripts embeded in .deb
",1385056296
b12b2ccde8683a091ed2ac15113c5609184c95e6 tags/pre_edgerdd~3^2~9,"Addressing bug in open hash set where getPos on a full open hash set could loop forever.
",1384923780
f568912f85f58ae152db90f199c1f3a002f270c1 remotes/origin/vldb~440,"Merge pull request #181 from BlackNiuza/fix_tasks_number

correct number of tasks in ExecutorsUI

Index `a` is not `execId` here
",1384906291
aa638ed9c140174a47df082ed5631ffe8e624ee6 remotes/origin/vldb~441,"Merge pull request #189 from tgravescs/sparkYarnErrorHandling

Impove Spark on Yarn Error handling

Improve cli error handling and only allow a certain number of worker failures before failing the application.  This will help prevent users from doing foolish things and their jobs running forever.  For instance using 32 bit java but trying to allocate 8G containers. This loops forever without this change, now it errors out after a certain number of retries.  The number of tries is configurable.  Also increase the frequency we ping the RM to increase speed at which we get containers if they die. The Yarn MR app defaults to pinging the RM every 1 seconds, so the default of 5 seconds here is fine. But that is configurable as well in case people want to change it.

I do want to make sure there aren't any cases that calling stopExecutors in CoarseGrainedSchedulerBackend would cause problems?  I couldn't think of any and testing on standalone cluster as well as yarn.
",1384905944
55925805fcef6ca782e1f42848a20133865b9412 remotes/origin/vldb~442,"Merge pull request #187 from aarondav/example-bcast-test

Enable the Broadcast examples to work in a cluster setting

Since they rely on println to display results, we need to first collect those results to the driver to have them actually display locally.

This issue came up on the mailing lists [here](http://mail-archives.apache.org/mod_mbox/incubator-spark-user/201311.mbox/%3C2013111909591557147628%40ict.ac.cn%3E).
",1384905841
26f616d73a3441cec749335913890b8c721de9b1 remotes/origin/vldb~443^2,"Merge pull request #3 from aarondav/pv-test

Add PrimitiveVectorSuite and fix bug in resize()",1384741096
85763f4942afc095595dc32c853d077bdbf49644 remotes/origin/vldb~443^2^2,"Add PrimitiveVectorSuite and fix bug in resize()
",1384741011
2b0a6e7d9210ed828395243027c7001f7dae77a4 remotes/origin/vldb~370^2~6,"Fixed error message in ClusterScheduler to be consistent with the old LocalScheduler
",1384569268
96e0fb46309698b685c811a65bd8e1a691389994 remotes/origin/vldb~445,"Merge pull request #173 from kayousterhout/scheduler_hang

Fix bug where scheduler could hang after task failure.

When a task fails, we need to call reviveOffers() so that the
task can be rescheduled on a different machine. In the current code,
the state in ClusterTaskSetManager indicating which tasks are
pending may be updated after revive offers is called (there's a
race condition here), so when revive offers is called, the task set
manager does not yet realize that there are failed tasks that need
to be relaunched.

This isn't currently unit tested but will be once my pull request for
merging the cluster and local schedulers goes in -- at which point
many more of the unit tests will exercise the code paths through
the cluster scheduler (currently the failure test suite uses the local
scheduler, which is why we didn't see this bug before).
",1384496968
dfd40e9f6f87ff1f205944997cdbbb6bb7f0312c remotes/origin/vldb~446,"Merge pull request #175 from kayousterhout/no_retry_not_serializable

Don't retry tasks when they fail due to a NotSerializableException

As with my previous pull request, this will be unit tested once the Cluster and Local schedulers get merged.
",1384487090
d4cd32330e1e4ac83b38bc922a9d3fd85f85f606 remotes/origin/vldb~394^2~4^2~22^2~1,"Some fixes for previous master merge commits
",1384482151
29c88e408ecc3416104530756561fee482393913 remotes/origin/vldb~446^2,"Don't retry tasks when they fail due to a NotSerializableException
",1384470919
b4546ba9e694529c359b7ca5c26829ead2c07f1a remotes/origin/vldb~445^2,"Fix bug where scheduler could hang after task failure.

When a task fails, we need to call reviveOffers() so that the
task can be rescheduled on a different machine. In the current code,
the state in ClusterTaskSetManager indicating which tasks are
pending may be updated after revive offers is called (there's a
race condition here), so when revive offers is called, the task set
manager does not yet realize that there are failed tasks that need
to be relaunched.
",1384466103
2b807e4f2f853a9b1e8cba5147d182e7b05022bc remotes/origin/vldb~370^2~9,"Fix bug where scheduler could hang after task failure.

When a task fails, we need to call reviveOffers() so that the
task can be rescheduled on a different machine. In the current code,
the state in ClusterTaskSetManager indicating which tasks are
pending may be updated after revive offers is called (there's a
race condition here), so when revive offers is called, the task set
manager does not yet realize that there are failed tasks that need
to be relaunched.
",1384464791
1a4cfbea334c7b0dae287eab4c3131c8f4b8a992 remotes/origin/vldb~448,"Merge pull request #169 from kayousterhout/mesos_fix

Don't ignore spark.cores.max when using Mesos Coarse mode

totalCoresAcquired is decremented but never incremented, causing Spark to effectively ignore spark.cores.max in coarse grained Mesos mode.
",1384453931
2c39d809d635f175a0f5aa2a88d508973b81cb57 remotes/origin/import~3,"Merge pull request #69 from jegonzal/MissingVertices

Addressing issue in Graph creation",1384414081
46f9c6b858cf9737b7d46b22b75bfc847244331b remotes/origin/vldb~370^2~11,"Fixed naming issues and added back ability to specify max task failures.
",1384391534
9290e5bcd2c8e4d8bbf1d0ce1ac09bbf62ece4e0 remotes/origin/vldb~452,"Merge pull request #165 from NathanHowell/kerberos-master

spark-assembly.jar fails to authenticate with YARN ResourceManager

The META-INF/services/ sbt MergeStrategy was discarding support for Kerberos, among others. This pull request changes to a merge strategy similar to sbt-assembly's default. I've also included an update to sbt-assembly 0.9.2, a minor fix to it's zip file handling.
",1384390124
68e5ad58b7e7e3e1b42852de8d0fdf9e9b9c1a14 remotes/origin/vldb~370^2~13,"Extracted TaskScheduler interface.

Also changed the default maximum number of task failures to be
0 when running in local mode.
",1384381970
266eb01ce8a71a1d187575e3199546985aa7b967 remotes/origin/MissingVertices~2,"Addressing issue in Graph creation where a graph created with a vertex set that does not span all of the vertices in the edges will crash on triplet construction.
",1384368325
a81fcb749dc55512e040adbd02a763b768e2aa3d remotes/origin/MissingVertices~3,"Merge pull request #68 from jegonzal/BitSetSetUntilBug

Addressing bug in BitSet.setUntil(ind)",1384368061
f0ef75c7a41c417096398d55c08785d1884c4d85 remotes/origin/MissingVertices~3^2,"Addressing bug in BitSet.setUntil(ind) where if invoked with a multiple of 64 could lead to an index out of bounds error.
",1384367723
f49ea28d25728e19e56b140a2f374631c94153bc remotes/origin/vldb~454,"Merge pull request #137 from tgravescs/sparkYarnJarsHdfsRebase

Allow spark on yarn to be run from HDFS.

Allows the spark.jar, app.jar, and log4j.properties to be put into hdfs.  Allows you to specify the files on a different hdfs cluster and it will copy them over. It makes sure permissions are correct and makes sure to put things into public distributed cache so they can be reused amongst users if their permissions are appropriate.  Also add a bit of error handling for missing arguments.
",1384312419
882d069189f26bef344c318db9ec6cdc1c7b66f7 tags/pre_edgerdd~6^2,"Fixed the bug in variable encoding for longs.
",1384311003
b8bf04a085162478a64ca5d7be15d1af2f6a930e remotes/origin/vldb~456,"Merge pull request #160 from xiajunluan/JIRA-923

Fix bug JIRA-923

Fix column sort issue in UI for JIRA-923.
https://spark-project.atlassian.net/browse/SPARK-923

Conflicts:
	core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala
	core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala
",1384301990
23146a67052889797d6761388cbc19ae6bfe6e21 remotes/origin/vldb~452^2~1,"spark-assembly.jar fails to authenticate with YARN ResourceManager

sbt-assembly is setup to pick the first META-INF/services/org.apache.hadoop.security.SecurityInfo file instead of merging them. This causes Kerberos authentication to fail, this manifests itself in the ""info:null"" debug log statement:

    DEBUG SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB info:null
    DEBUG SaslRpcClient: Get kerberos info proto:interface org.apache.hadoop.yarn.api.ApplicationClientProtocolPB info:null
    ERROR UserGroupInformation: PriviledgedActionException as:foo@BAR (auth:KERBEROS) cause:org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
    DEBUG UserGroupInformation: PrivilegedAction as:foo@BAR (auth:KERBEROS) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:583)
    WARN Client: Exception encountered while connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
    ERROR UserGroupInformation: PriviledgedActionException as:foo@BAR (auth:KERBEROS) cause:java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]

This previously would just contain a single class:

$ unzip -c assembly/target/scala-2.10/spark-assembly-0.9.0-incubating-SNAPSHOT-hadoop2.2.0.jar META-INF/services/org.apache.hadoop.security.SecurityInfo
Archive:  assembly/target/scala-2.10/spark-assembly-0.9.0-incubating-SNAPSHOT-hadoop2.2.0.jar
  inflating: META-INF/services/org.apache.hadoop.security.SecurityInfo

    org.apache.hadoop.security.AnnotatedSecurityInfo

And now has the full list of classes:

$ unzip -c assembly/target/scala-2.10/spark-assembly-0.9.0-incubating-SNAPSHOT-hadoop2.2.0.jar META-INF/services/org.apache.hadoop.security.SecurityInfoArchive:  assembly/target/scala-2.10/spark-assembly-0.9.0-incubating-SNAPSHOT-hadoop2.2.0.jar
  inflating: META-INF/services/org.apache.hadoop.security.SecurityInfo

    org.apache.hadoop.security.AnnotatedSecurityInfo
    org.apache.hadoop.mapreduce.v2.app.MRClientSecurityInfo
    org.apache.hadoop.mapreduce.v2.security.client.ClientHSSecurityInfo
    org.apache.hadoop.yarn.security.client.ClientRMSecurityInfo
    org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo
    org.apache.hadoop.yarn.security.SchedulerSecurityInfo
    org.apache.hadoop.yarn.security.admin.AdminSecurityInfo
    org.apache.hadoop.yarn.server.RMNMSecurityInfoClass
",1384291670
dfd1ebc2d1e5c34a5979648e571302ae81a178f5 remotes/origin/vldb~457,"Merge pull request #164 from tdas/kafka-fix

Made block generator thread safe to fix Kafka bug.

This is a very important bug fix. Data can and was being lost in the kafka due to this.
",1384276205
7ccbbdacb9406d67b5acf2a489d6551900babdc9 remotes/origin/vldb~457^2,"Made block generator thread safe to fix Kafka bug.
",1384243845
8d8056da14d3a7eac39f4cf14970467729350018 tags/pre_edgerdd~5^2~8,"Fixed issue with canonical edge partitioner.
",1384242023
e13da05424866785dcbe37754113a2a8aa63f706 remotes/origin/vldb~456^2,"fix format error
",1384168545
b3208063afe7190efd6e54a41b7af28d15c46358 remotes/origin/vldb~456^2~2,"Fix bug JIRA-923
",1384155550
cbb7f04aef2220ece93dea9f3fa98b5db5f270d6 remotes/origin/vldb~421^2~3,"Add custom serializer support to PySpark.

For now, this only adds MarshalSerializer, but it lays the groundwork
for other supporting custom serializers.  Many of these mechanisms
can also be used to support deserialization of different data formats
sent by Java, such as data encoded by MsgPack.

This also fixes a bug in SparkContext.union().
",1384130738
0e813cd483eb4cc612404f8602e635b29295efc3 remotes/origin/rxin~1,"Fix the hanging bug.
",1384068577
3efc0195625977335914f0a18cf32bd4e9b1d6d4 remotes/origin/vldb~460,"Merge pull request #147 from JoshRosen/fix-java-api-completeness-checker

Add spark-tools assembly to spark-class'ss classpath

This commit adds an assembly for `spark-tools` and adds it to `spark-class`'s classpath, allowing the JavaAPICompletenessChecker to be run against Spark 0.8+ with

    ./spark-class org.apache.spark.tools.JavaAPICompletenessChecker

Previously, this tool was run through the `run` script.  I chose to add this to `run-example` because I didn't want to duplicate code in a `run-tool` script.
",1384048429
8af99f2356ed19fe43d722ada02f8802cbd46d40 remotes/origin/vldb~463,"Merge pull request #149 from tgravescs/fixSecureHdfsAccess

Fix secure hdfs access for spark on yarn

https://github.com/apache/incubator-spark/pull/23 broke secure hdfs access. Not sure if it works with secure hdfs on standalone. Fixing it at least for spark on yarn.

The broadcasting of jobconf change also broke secure hdfs access as it didn't take into account things calling the getPartitions before sparkContext is initialized. The DAGScheduler does this as it tries to getShuffleMapStage.
",1384033680
ef85a51f85c9720bc091367a0d4f80e7ed6b9778 remotes/origin/vldb~431^2~1,"Add graphite sink for metrics

This adds a metrics sink for graphite.  The sink must
be configured with the host and port of a graphite node
and optionally may be configured with a prefix that will
be prepended to all metrics that are sent to graphite.
",1383957363
6083e4350f13123b258d4c3b73421e219c58c82b remotes/origin/rxin~2^2,"Adding unit tests to reproduce error.
",1383953970
e523f0d2fbe9ec285e724cae86eff844678ed9f1 remotes/origin/rxin~2^2~2,"merged and debugged
",1383884389
ca66f5d5a287873d930a3bdc36d40d8e776ca25a remotes/origin/vldb~466^2,"fix formatting
",1383830639
be7e8da98ad04d66b61cd7fc8af7ae61a649d71c remotes/origin/vldb~467,"Merge pull request #23 from jerryshao/multi-user

Add Spark multi-user support for standalone mode and Mesos

This PR add multi-user support for Spark both standalone mode and Mesos (coarse and fine grained ) mode, user can specify the user name who submit app through environment variable `SPARK_USER` or use default one. Executor will communicate with Hadoop using  specified user name.

Also I fixed one bug in JobLogger when different user wrote job log to specified folder which has no right file  permission.

I separate previous [PR750](https://github.com/mesos/spark/pull/750) into two PRs, in this PR I only solve multi-user support problem. I will try to solve security auth problem in subsequent PR because security auth is a complicated problem especially for Shark Server like long-run app (both Kerberos TGT and HDFS delegation token should be renewed or re-created through app's run time).
",1383808967
951024feeadcf73b50c3c80ec9e75c7e2214a7a4 remotes/origin/vldb~469,"Merge pull request #145 from aarondav/sls-fix

Attempt to fix SparkListenerSuite breakage

Could not reproduce locally, but this test could've been flaky if the build machine was too fast, due to typo. (index 0 is intentionally slowed down to ensure total time is >= 1 ms)

This should be merged into branch-0.8 as well.
",1383759374
80e98d2bd75336aa493da070df99a634b84dac3a remotes/origin/vldb~469^2,"Attempt to fix SparkListenerSuite breakage

Could not reproduce locally, but this test could've been flaky if the
build machine was too fast.
",1383753815
0b26a392dfd3a33a0c6db55ed9392bde2b23e61a remotes/origin/vldb~473,"Merge pull request #128 from shimingfei/joblogger-doc

add javadoc to JobLogger, and some small fix

against Spark-941

add javadoc to JobLogger, output more info for RDD, modify recordStageDepGraph to avoid output duplicate stage dependency information

(cherry picked from commit 518cf22eb2436d019e4f7087a38080ad4a20df58)
Signed-off-by: Reynold Xin <rxin@apache.org>
",1383618126
a0bb569a818f6ce66c192a3f5782ff56cf58b1d3 remotes/origin/vldb~474^2~4,"use OpenHashMap, remove monotonicity requirement, fix failure bug
",1383543296
7d44dec9bd7c4bbfb8daf4843a0968797e009bea remotes/origin/vldb~474^2~8,"Fix weird bug with specialized PrimitiveVector
",1383543283
b5dc3393a586099229fcd293d96f909e596a11e6 remotes/origin/vldb~475,"Merge pull request #70 from rxin/hash1

Fast, memory-efficient hash set, hash table implementations optimized for primitive data types.

This pull request adds two hash table implementations optimized for primitive data types. For primitive types, the new hash tables are much faster than the current Spark AppendOnlyMap (3X faster - note that the current AppendOnlyMap is already much better than the Java map) while uses much less space (1/4 of the space).

Details:

This PR first adds a open hash set implementation (OpenHashSet) optimized for primitive types (using Scala's specialization feature). This OpenHashSet is designed to serve as building blocks for more advanced structures. It is currently used to build the following two hash tables, but can be used in the future to build multi-valued hash tables as well (GraphX has this use case). Note that there are some peculiarities in the code for working around some Scala compiler bugs.

Building on top of OpenHashSet, this PR adds two different hash tables implementations:
1. OpenHashSet: for nullable keys, optional specialization for primitive values
2. PrimitiveKeyOpenHashMap: for primitive keys that are not nullable, and optional specialization for primitive values

I tested the update speed of these two implementations using the changeValue function (which is what Aggregator and cogroup would use). Runtime relative to AppendOnlyMap for inserting 10 million items:

Int to Int: ~30%
java.lang.Integer to java.lang.Integer: ~100%
Int to java.lang.Integer: ~50%
java.lang.Integer to Int: ~85%
",1383540195
1e9543b567b81cf3207984402269d230c10e713e remotes/origin/vldb~475^2~1,"Fixed a bug that uses twice amount of memory for the primitive arrays due to a scala compiler bug.
Also addressed Matei's code review comment.
",1383459541
41ead7a74533ffdd208a4ba2f7cd38945b4343ec remotes/origin/vldb~476,"Merge pull request #133 from Mistobaan/link_fix

update default github
",1383428510
e7c7b804b529342d6de1c86ee02bc78da81279bc remotes/origin/vldb~478,"Merge pull request #132 from Mistobaan/doc_fix

fix persistent-hdfs
",1383353890
3f89354c45fce179e6cc8e7a4e8294694d24ae18 remotes/origin/vldb~478^2,"fix persistent-hdfs
",1383353257
fb64828b0b573f3a77938592f168af7aa3a2b6c5 remotes/origin/vldb~370^2~14,"Cleaned up imports and fixed test bug
",1383288176
a3ce484a2c01eee05a272715d53cdba7569bad5f remotes/origin/joey_mat_arrays~9^2~1,"Adding additional type constraints to VertexSetRDD to help diagnose issues with recent benchmarks.
",1383192141
8f1098a3f0de8c9b2eb9ede91a1b01da10a525ea remotes/origin/vldb~480,"Merge pull request #117 from stephenh/avoid_concurrent_modification_exception

Handle ConcurrentModificationExceptions in SparkContext init.

System.getProperties.toMap will fail-fast when concurrently modified,
and it seems like some other thread started by SparkContext does
a System.setProperty during it's initialization.

Handle this by just looping on ConcurrentModificationException, which
seems the safest, since the non-fail-fast methods (Hastable.entrySet)
have undefined behavior under concurrent modification.
",1383189108
a124658e53a5abeda00a2582385a294c8e452d21 remotes/origin/vldb~370^2~15,"Fixed most issues with unit tests
",1383186578
5e91495f5c718c837b5a5af2268f6faad00d357f remotes/origin/vldb~370^2~16,"Deduplicate Local and Cluster schedulers.

The code in LocalScheduler/LocalTaskSetManager was nearly identical
to the code in ClusterScheduler/ClusterTaskSetManager. The redundancy
made making updating the schedulers unnecessarily painful and error-
prone. This commit combines the two into a single TaskScheduler/
TaskSetManager.
",1383184114
dc9ce16f6b695295c24062a76fd10f574d278cac remotes/origin/vldb~481,"Merge pull request #126 from kayousterhout/local_fix

Fixed incorrect log message in local scheduler

This change is especially relevant at the moment, because some users are seeing this failure, and the log message is misleading/incorrect (because for the tests, the max failures is set to 0, not 4)
",1383177716
33de11c51dd2dbcbbf1801c54d9ce5ffaa324657 remotes/origin/vldb~482,"Merge pull request #124 from tgravescs/sparkHadoopUtilFix

Pull SparkHadoopUtil out of SparkEnv (jira SPARK-886)

Having the logic to initialize the correct SparkHadoopUtil in SparkEnv prevents it from being used until after the SparkContext is initialized.   This causes issues like https://spark-project.atlassian.net/browse/SPARK-886.  It also makes it hard to use in singleton objects.  For instance I want to use it in the security code.
",1383177507
e1099f4d89362cc907cae0c64cf518dc99567099 tags/pre_edgerdd~5^2~2^2~9^2,"Fixed issue with canonical edge partitioner.
",1383170601
09f3b677cb7cce08882ea030e9af5798a63046ba remotes/origin/vldb~480^2,"Avoid match errors when filtering for spark.hadoop settings.
",1383154179
e5e0ebdb1190a256e51dbf1265c6957f0cd56a29 remotes/origin/vldb~482^2~2,"fix sparkhdfs lr test
",1383095565
38ec0baf5c9033a9e9e9bb015d95357d8176e022 remotes/origin/joey_mat_arrays~11^2~3,"fixing a typo in the VertexSetRDD docs
",1383089275
ede329336d2d072a91898a75c8ae2bdc7d0f671d remotes/origin/joey_mat_arrays~11^2~6,"Fixing a scaladoc bug in graph generators.
",1383083412
d4df4749a8738628aa79802c4fb2805d02f47bb8 remotes/origin/vldb~487,"Merge pull request #115 from aarondav/shuffle-fix

Eliminate extra memory usage when shuffle file consolidation is disabled

Otherwise, we see SPARK-946 even when shuffle file consolidation is disabled.
Fixing SPARK-946 is still forthcoming.
",1382926281
a6ae2b48320d367be5fede60687331ce0d563d00 remotes/origin/vldb~480^2~2,"Handle ConcurrentModificationExceptions in SparkContext init.

System.getProperties.toMap will fail-fast when concurrently modified,
and it seems like some other thread started by SparkContext does
a System.setProperty during it's initialization.

Handle this by just looping on ConcurrentModificationException, which
seems the safest, since the non-fail-fast methods (Hastable.entrySet)
have undefined behavior under concurrent modification.
",1382900912
00e73833cce88d5f77eaadd741a9a4046a4f17a3 remotes/origin/joey_mat_arrays~13^2~5,"Fixing a bug in reverse edge direction.
",1382825430
e018f2d0ae826b682dda30c1ecfb0c51ad7edf58 remotes/origin/vldb~488,"Merge pull request #113 from pwendell/master

Improve error message when multiple assembly jars are present.

This can happen easily if building different hadoop versions. Right now it gives a class not found exception.
",1382812755
4ba32678e04dc687a9f574eeeb1450e4d291ae1f remotes/origin/vldb~437^2~1,"Adding improved error message when multiple assembly jars are present.

This can happen easily if building different hadoop versions.
",1382752875
eef261c89286ddbcdcc03684c1a5d0b94d6da321 remotes/origin/vldb~490^2,"fixing comments on PR
",1382744913
85e2cab6f68cbcb66bb348298463e2e86591d92d remotes/origin/vldb~492,"Merge pull request #111 from kayousterhout/ui_name

Properly display the name of a stage in the UI.

This fixes a bug introduced by the fix for SPARK-940, which
changed the UI to display the RDD name rather than the stage
name. As a result, no name for the stage was shown when
using the Spark shell, which meant that there was no way to
click on the stage to see more details (e.g., the running
tasks). This commit changes the UI back to using the
stage name.

@pwendell -- let me know if this change was intentional
",1382737566
a9c8d83aafe4990d3298b395715a98c107dcd667 remotes/origin/vldb~492^2,"Properly display the name of a stage in the UI.

This fixes a bug introduced by the fix for SPARK-940, which
changed the UI to display the RDD name rather than the stage
name. As a result, no name for the stage was shown when
using the Spark shell, which meant that there was no way to
click on the stage to see more details (e.g., the running
tasks). This commit changes the UI back to using the
stage name.
",1382727609
ab35ec4f0f6c6892ad6457e58b1d95c9224ab5b8 remotes/origin/vldb~493,"Merge pull request #110 from pwendell/master

Exclude jopt from kafka dependency.

Kafka uses an older version of jopt that causes bad conflicts with the version
used by spark-perf. It's not easy to remove this downstream because of the way
that spark-perf uses Spark (by including a spark assembly as an unmanaged jar).
This fixes the problem at its source by just never including it.
",1382721378
af4a529f6efae7d0f7e5915f0221f31ea755a374 remotes/origin/vldb~437^2~2,"Exclude jopt from kafka dependency.

Kafka uses an older version of jopt that causes bad conflicts with the version
used by spark-perf. It's not easy to remove this downstream because of the way
that spark-perf uses Spark (by including a spark assembly as an unmanaged jar).
This fixes the problem at its source by just never including it.
",1382718030
ad5f579cbfb3f0c9834cc5fdd26ad0745f5f8abf remotes/origin/vldb~437^2~3,"Style fixes
",1382678333
e5f6d5697b43ac89a50fb791f4b284409e75b1f4 remotes/origin/vldb~437^2~4,"Spacing fix
",1382677686
a351fd4aeda7d137e4cece705e25b51d7634ca63 remotes/origin/vldb~437^2~5,"Small spacing fix
",1382674590
e962a6e6ee8d8ef9d1245d85616fe50554f7f689 remotes/origin/vldb~491^2~1,"Fixed accidental bug.
",1382653046
5429d62dfa16305eb23d67dfe38172803c80db65 remotes/origin/vldb~394^2~5,"Merge pull request #107 from ScrapCodes/scala-2.10

Updating to latest akka 2.2.3, which fixes our only failing test Driver Suite.
",1382638555
c77ca1fed994b5a23c745dc1f6a837739b4a2138 remotes/origin/vldb~394^2~5^2,"Updating to latest akka 2.2.3, which fixes our only failing Driver Suite
",1382611300
c4b187d1db2069abefc54cda011b0d96f60be6a6 remotes/origin/vldb~497,"Merge pull request #105 from pwendell/doc-fix

Fixing broken links in programming guide

Unfortunately these are broken in 0.8.0.
",1382590578
a098438c4824891fb44e9119e038e6e796c69741 remotes/origin/vldb~498,"Merge pull request #103 from JoshRosen/unpersist-fix

Add unpersist() to JavaDoubleRDD and JavaPairRDD.

This fixes a minor inconsistency where [unpersist() was only available on JavaRDD](https://mail-archives.apache.org/mod_mbox/incubator-spark-user/201310.mbox/%3CCE8D8748.68C0%25YannLuppo%40livenation.com%3E) and not JavaPairRDD / JavaDoubleRDD.   I also added support for the new optional `blocking` argument added in 0.8.

Please merge this into branch-0.8, too.
",1382576588
c30624dcbb6f6999f4e4f592ac4379a18f169927 remotes/origin/joey_mat_arrays~14^2~1,"Adding dynamic pregel, fixing bugs in PageRank, and adding basic analytics unit tests.
",1382513145
72d2e1dd777696640f64aaf92fecab64c6387df0 remotes/origin/vldb~491^2~6,"Fixed bug in Java transformWith, added more Java testcases for transform and transformWith, added missing variations of Java join and cogroup, updated various Scala and Java API docs.
",1382510151
0bd92ed8d07712b7d8bb06378d877eb9643ba05a remotes/origin/joey_mat_arrays~14^2~2,"Fixing a bug in pregel where the initial vertex-program results are lost.
",1382494251
9dfcf53a08645227b8722f07068466c7b7909721 remotes/origin/vldb~502,"Merge pull request #100 from JoshRosen/spark-902

Remove redundant Java Function call() definitions

This should fix [SPARK-902](https://spark-project.atlassian.net/browse/SPARK-902), an issue where some Java API Function classes could cause AbstractMethodErrors when user code is compiled using the Eclipse compiler.

Thanks to @MartinWeindel for diagnosing this problem.

(This PR subsumes #30).
",1382482902
768eb9c9629f0e3c24477acf87ad15938ddd75ce remotes/origin/vldb~502^2,"Remove redundant Java Function call() definitions

This should fix SPARK-902, an issue where some
Java API Function classes could cause
AbstractMethodErrors when user code is compiled
using the Eclipse compiler.

Thanks to @MartinWeindel for diagnosing this
problem.

(This PR subsumes / closes #30)
",1382477212
317a9eb1ceb165a74493c452a6c5fc0f9b5e2760 remotes/origin/vldb~501^2~1,"Pass self to SparkContext._ensure_initialized.

The constructor for SparkContext should pass in self so that we track
the current context and produce errors if another one is created. Add
a doctest to make sure creating multiple contexts triggers the
exception.
",1382466409
2fa3c4c49c2b3bbc29e27a1f4c3bd7521944a45c remotes/origin/vldb~504^2~2,"Fix for Spark-870.

This patch fixes a bug where the Spark UI didn't display the correct number of total
tasks if the number of tasks in a Stage doesn't equal the number of RDD partitions.

It also cleans up the listener API a bit by embedding this information in the
StageInfo class rather than passing it seperately.
",1382464825
48952d67e6dde25faaba241b9deba737b83a5372 remotes/origin/vldb~509,"Merge pull request #87 from aarondav/shuffle-base

Basic shuffle file consolidation

The Spark shuffle phase can produce a large number of files, as one file is created
per mapper per reducer. For large or repeated jobs, this often produces millions of
shuffle files, which sees extremely degredaded performance from the OS file system.
This patch seeks to reduce that burden by combining multipe shuffle files into one.

This PR draws upon the work of @jason-dai in https://github.com/mesos/spark/pull/669.
However, it simplifies the design in order to get the majority of the gain with less
overall intellectual and code burden. The vast majority of code in this pull request
is a refactor to allow the insertion of a clean layer of indirection between logical
block ids and physical files. This, I feel, provides some design clarity in addition
to enabling shuffle file consolidation.

The main goal is to produce one shuffle file per reducer per active mapper thread.
This allows us to isolate the mappers (simplifying the failure modes), while still
allowing us to reduce the number of mappers tremendously for large tasks. In order
to accomplish this, we simply create a new set of shuffle files for every parallel
task, and return the files to a pool which will be given out to the next run task.

I have run some ad hoc query testing on 5 m1.xlarge EC2 nodes with 2g of executor memory and the following microbenchmark:

    scala> val nums = sc.parallelize(1 to 1000, 1000).flatMap(x => (1 to 1e6.toInt))
    scala> def time(x: => Unit) = { val now = System.currentTimeMillis; x; System.currentTimeMillis - now }
    scala> (1 to 8).map(_ => time(nums.map(x => (x % 100000, 2000, x)).reduceByKey(_ + _).count) / 1000.0)

For this particular workload, with 1000 mappers and 2000 reducers, I saw the old method running at around 15 minutes, with the consolidated shuffle files running at around 4 minutes. There was a very sharp increase in running time for the non-consolidated version after around 1 million total shuffle files. Below this threshold, however, there wasn't a significant difference between the two.

Better performance measurement of this patch is warranted, and I plan on doing so in the near future as part of a general investigation of our shuffle file bottlenecks and performance.
",1382420700
39d2e9b293768b6eeb291313ee94f3e02a0ad522 remotes/origin/vldb~511,"Merge pull request #94 from aarondav/mesos-fix

Fix mesos urls

This was a bug I introduced in https://github.com/apache/incubator-spark/pull/71.
Previously, we explicitly removed the mesos:// part; with #71, this no longer occurs.
",1382407128
0071f0899c2931610823f16adcf1f71457edebcc remotes/origin/vldb~511^2,"Fix mesos urls

This was a bug I introduced in https://github.com/apache/incubator-spark/pull/71
Previously, we explicitly removed the mesos:// part; with PR 71, this no longer occured.
",1382396174
947fceaa73a21ddc4263b98913ebf11aa71f5ba1 remotes/origin/vldb~509^2~3,"Close shuffle writers during failure & remove executorId from TaskContext
",1382334430
5b9380e0173b3d3d13235ae912e9ccc2a974b98b remotes/origin/vldb~514,"Merge pull request #89 from rxin/executor

Don't setup the uncaught exception handler in local mode.

This avoids unit test failures for Spark streaming.

    java.util.concurrent.RejectedExecutionException: Task org.apache.spark.streaming.JobManager$JobHandler@38cf728d rejected from java.util.concurrent.ThreadPoolExecutor@3b69a41e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 14]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
	at org.apache.spark.streaming.JobManager.runJob(JobManager.scala:54)
	at org.apache.spark.streaming.Scheduler$$anonfun$generateJobs$2.apply(Scheduler.scala:108)
	at org.apache.spark.streaming.Scheduler$$anonfun$generateJobs$2.apply(Scheduler.scala:108)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.streaming.Scheduler.generateJobs(Scheduler.scala:108)
	at org.apache.spark.streaming.Scheduler$$anonfun$1.apply$mcVJ$sp(Scheduler.scala:41)
	at org.apache.spark.streaming.util.RecurringTimer.org$apache$spark$streaming$util$RecurringTimer$$loop(RecurringTimer.scala:66)
	at org.apache.spark.streaming.util.RecurringTimer$$anon$1.run(RecurringTimer.scala:34)
",1382328231
640f253a6572208efa24a36a6442ac08a266cf24 remotes/origin/vldb~506^2~1,Fix test failures in local mode due to updateEpoch,1382305745
7414805e4efdfa893805389f001634a989ae3bda remotes/origin/vldb~514^2,"Don't setup the uncaught exception handler in local mode.

This avoids unit test failures for Spark streaming.
",1382299428
38b8048f291dd42ee996e75bd1b6d33aa24b1a5e remotes/origin/vldb~509^2~6,"Fix compiler errors

Whoops. Last-second changes require testing too, it seems.
",1382292216
e4abb75d70aa08377829f635fe6135d94e28f434 remotes/origin/vldb~518,"Merge pull request #85 from rxin/clean

Moved the top level spark package object from spark to org.apache.spark

This is a pretty annoying documentation bug ...
",1382287117
136b9b3a3ed358bc04b28e8d62657d56d55c2c3e remotes/origin/vldb~509^2~7,"Basic shuffle file consolidation

The Spark shuffle phase can produce a large number of files, as one file is created
per mapper per reducer. For large or repeated jobs, this often produces millions of
shuffle files, which sees extremely degredaded performance from the OS file system.
This patch seeks to reduce that burden by combining multipe shuffle files into one.

This PR draws upon the work of Jason Dai in https://github.com/mesos/spark/pull/669.
However, it simplifies the design in order to get the majority of the gain with less
overall intellectual and code burden. The vast majority of code in this pull request
is a refactor to allow the insertion of a clean layer of indirection between logical
block ids and physical files. This, I feel, provides some design clarity in addition
to enabling shuffle file consolidation.

The main goal is to produce one shuffle file per reducer per active mapper thread.
This allows us to isolate the mappers (simplifying the failure modes), while still
allowing us to reduce the number of mappers tremendously for large tasks. In order
to accomplish this, we simply create a new set of shuffle files for every parallel
task, and return the files to a pool which will be given out to the next run task.
",1382263106
dbc8c9868aac33faa45ab54935df3613e31d4ff9 remotes/origin/joey_mat_arrays~15^2~1,"Fixing bug in VertexSetRDD that breaks Graph tests.
",1382165046
e5316d0685c41a40e54a064cf271f3d62df6c8e8 remotes/origin/vldb~524,"Merge pull request #68 from mosharaf/master

Faster and stable/reliable broadcast

HttpBroadcast is noticeably slow, but the alternatives (TreeBroadcast or BitTorrentBroadcast) are notoriously unreliable. The main problem with them is they try to manage the memory for the pieces of a broadcast themselves. Right now, the BroadcastManager does not know which machines the tasks reading from a broadcast variable is running and when they have finished. Consequently, we try to guess and often guess wrong, which blows up the memory usage and kills/hangs jobs.

This very simple implementation solves the problem by not trying to manage the intermediate pieces; instead, it offloads that duty to the BlockManager which is quite good at juggling blocks. Otherwise, it is very similar to the BitTorrentBroadcast implementation (without fancy optimizations). And it runs much faster than HttpBroadcast we have right now.

I've been using this for another project for last couple of weeks, and just today did some benchmarking against the Http one. The following shows the improvements for increasing broadcast size for cold runs. Each line represent the number of receivers.
![fix-bc-first](https://f.cloud.github.com/assets/232966/1349342/ffa149e4-36e7-11e3-9fa6-c74555829356.png)

After the first broadcast is over, i.e., after JVM is wormed up and for HttpBroadcast the server is already running (I think), the following are the improvements for warm runs.
![fix-bc-succ](https://f.cloud.github.com/assets/232966/1349352/5a948bae-36e8-11e3-98ce-34f19ebd33e0.jpg)
The curves are not as nice as the cold runs, but the improvements are obvious, specially for larger broadcasts and more receivers.

Depending on how it goes, we should deprecate and/or remove old TreeBroadcast and BitTorrentBroadcast implementations, and hopefully, SPARK-889 will not be necessary any more.
",1382153456
8d528af829dc989d4701c08fd90d230c15df7f7e remotes/origin/vldb~525,"Merge pull request #71 from aarondav/scdefaults

Spark shell exits if it cannot create SparkContext

Mainly, this occurs if you provide a messed up MASTER url (one that doesn't match one
of our regexes). Previously, we would default to Mesos, fail, and then start the shell
anyway, except that any Spark command would fail. Simply exiting seems clearer.
",1382153050
74737264c4a9b2a9a99bf3aa77928f6960bad78c remotes/origin/vldb~525^2,"Spark shell exits if it cannot create SparkContext

Mainly, this occurs if you provide a messed up MASTER url (one that doesn't match one
of our regexes). Previously, we would default to Mesos, fail, and then start the shell
anyway, except that any Spark command would fail.
",1382061079
fc26e5b8320556b9edb93741391b759813b4079b remotes/origin/vldb~526,"Merge pull request #69 from KarthikTunga/master

Fix for issue SPARK-627. Implementing --config argument in the scripts.

This code fix is for issue SPARK-627. I added code to consider --config arguments in the scripts. In case the  <conf-dir> is not a directory the scripts exit. I removed the --hosts argument. It can be achieved by giving a different config directory. Let me know if an explicit --hosts argument is required.
",1382041267
f9973cae3aff39c29a2cdad5b54b7674d1126132 remotes/origin/vldb~528,"Merge pull request #65 from tgravescs/fixYarn

Fix yarn build

Fix the yarn build after renaming StandAloneX to CoarseGrainedX from pull request 34.
",1381964321
28e9c2abc0884d096fc3be1e2d1f9ee18ffc3261 remotes/origin/vldb~529,"Merge pull request #63 from pwendell/master

Fixing spark streaming example and a bug in examples build.

- Examples assembly included a log4j.properties which clobbered Spark's
- Example had an error where some classes weren't serializable
- Did some other clean-up in this example
",1381906796
35befe07bb79411f453cf05d482ff051fa827b47 remotes/origin/vldb~521^2~1,"Fixing spark streaming example and a bug in examples build.

- Examples assembly included a log4j.properties which clobbered Spark's
- Example had an error where some classes weren't serializable
- Did some other clean-up in this example
",1381902943
983b83f24da9f7f0251833b409cdcd37acae3d39 remotes/origin/vldb~533,"Merge pull request #61 from kayousterhout/daemon_thread

Unified daemon thread pools

As requested by @mateiz in an earlier pull request, this refactors various daemon thread pools to use a set of methods in utils.scala, and also changes the thread-pool-creation methods in utils.scala to use named thread pools for improved debugging.
",1381888966
9058f261fe6ba7c7889808e124ec2bd8f37f99f5 remotes/origin/alpha~10,"Addressing issue where statistics are not computed correctly
",1381883949
194bb03d1637f731535f964e1d1661d218380162 remotes/origin/alpha~12,"Resolved closure capture issues by addressing capture through implicit variables.
",1381875041
f95a2be04569a63763fa6740ee212b658f0808ab remotes/origin/vldb~532^2,"Fixed build error after merging in master
",1381873897
3249e0e90dd9a7b422f561c42407b6a2b3feab17 remotes/origin/vldb~534,"Merge pull request #59 from rxin/warning

Bump up logging level to warning for failed tasks.
",1381871553
345e1e94ccdc35a7d652b91cf0e54fa3aa7d313f remotes/origin/alpha~14,"Still trying to resolve issues with capture.
",1381870898
b64337ec40fd82d6082052d49a534dc5542663b6 remotes/origin/alpha~15,"Trying to resolve issues with closure capture.
",1381867337
f41feb7b338b5fdd60260f5ce7cba94202102194 remotes/origin/vldb~534^2,"Bump up logging level to warning for failed tasks.
",1381818932
bff223454aa128a673ae835f6104460a6a70dbc6 remotes/origin/alpha~22,"trying to address issues with GraphImpl being caught in closures.
",1381728430
543a54dffa2fce817112452d76a65ea61cefb8f9 remotes/origin/benchmarks~3,"Tried to fix some indenting
",1381532869
fb25f323003e947b1f6939bf29fd2cfe6569ea46 remotes/origin/vldb~540,"Merge pull request #53 from witgo/master

Add a zookeeper compile dependency to fix build in maven

 Add a zookeeper compile dependency to fix build in maven
",1381531483
d6ead478094b3eb02823e7e8b06a16c3989c7f0c remotes/origin/vldb~541,"Merge pull request #32 from mridulm/master

Address review comments, move to incubator spark

Also includes a small fix to speculative execution.

&lt;edit> Continued from https://github.com/mesos/spark/pull/914 &lt;/edit>
",1381531381
fc60c412ab099d4c71a67349a91cf5993006f481 remotes/origin/vldb~540^2,Add a zookeeper compile dependency to fix build in maven,1381480307
80cdbf4f49cdb07bfd765d3fdd1d16d5aec2e60a remotes/origin/vldb~536^2~11,"Switched to use daemon thread in executor and fixed a bug in job cancellation for fair scheduler.
",1381470048
c71499b7795564e1d16495c59273ecc027070fc5 remotes/origin/vldb~543,"Merge pull request #19 from aarondav/master-zk

Standalone Scheduler fault tolerance using ZooKeeper

This patch implements full distributed fault tolerance for standalone scheduler Masters.
There is only one master Leader at a time, which is actively serving scheduling
requests. If this Leader crashes, another master will eventually be elected, reconstruct
the state from the first Master, and continue serving scheduling requests.

Leader election is performed using the ZooKeeper leader election pattern. We try to minimize
the use of ZooKeeper and the assumptions about ZooKeeper's behavior, so there is a layer of
retries and session monitoring on top of the ZooKeeper client.

Master failover follows directly from the single-node Master recovery via the file
system (patch d5a96fe), save that the Master state is stored in ZooKeeper instead.

Configuration:
By default, no recovery mechanism is enabled (spark.deploy.recoveryMode = NONE).
By setting spark.deploy.recoveryMode to ZOOKEEPER and setting spark.deploy.zookeeper.url
to an appropriate ZooKeeper URL, ZooKeeper recovery mode is enabled.
By setting spark.deploy.recoveryMode to FILESYSTEM and setting spark.deploy.recoveryDirectory
to an appropriate directory accessible by the Master, we will keep the behavior of from d5a96fe.

Additionally, places where a Master could be specificied by a spark:// url can now take
comma-delimited lists to specify backup masters. Note that this is only used for registration
of NEW Workers and application Clients. Once a Worker or Client has registered with the
Master Leader, it is ""in the system"" and will never need to register again.
",1381450602
320418f7c8b42d4ce781b32c9ee47a9b54550b89 remotes/origin/vldb~545,"Merge pull request #49 from mateiz/kryo-fix-2

Fix Chill serialization of Range objects

It used to write out each element one by one, creating very large objects.
",1381362930
215238cb399d46c83fafa64b3c98e0ebec21adb9 remotes/origin/vldb~546,"Merge pull request #50 from kayousterhout/SPARK-908

Fix race condition in SparkListenerSuite (fixes SPARK-908).
",1381362584
36966f65df2947a78d32d731f6b004d015ff011e remotes/origin/vldb~546^2,"Style fixes
",1381358194
a34a4e8174b5f285a327d7ff30ac9f3ff0db7689 remotes/origin/vldb~546^2~2,"Fix race condition in SparkListenerSuite (fixes SPARK-908).
",1381356473
7d50f9f87baeb1f4b8d77d669d25649b97dd1d57 remotes/origin/vldb~394^2~7,"Merge pull request #35 from MartinWeindel/scala-2.10

Fixing inconsistencies and warnings on Scala 2.10 branch

- scala 2.10 requires Java 1.6
- using newest Scala release: 2.10.3
- resolved maven-scala-plugin warning
- fixed various compiler warnings
",1381339962
e67d5b962a2adddc073cfc9c99be9012fbb69838 remotes/origin/vldb~552,"Merge pull request #43 from mateiz/kryo-fix

Don't allocate Kryo buffers unless needed

I noticed that the Kryo serializer could be slower than the Java one by 2-3x on small shuffles because it spend a lot of time initializing Kryo Input and Output objects. This is because our default buffer size for them is very large. Since the serializer is often used on streams, I made the initialization lazy for that, and used a smaller buffer (auto-managed by Kryo) for input.
",1381298258
f7628e40330a1721e3d69ccb2390dfa75f839768 remotes/origin/vldb~550^2,"remove those futile suffixes like number/count
",1381279001
4ea8ee468fb1f50fce56853a5127a89efc45b706 remotes/origin/vldb~543^2~2,"Add docs for standalone scheduler fault tolerance

Also fix a couple HTML/Markdown issues in other files.
",1381267111
b0f5f4d441119662c09572de697b2d9943f703ef remotes/origin/vldb~551^2~1,"Bumping up test matrix size to eliminate random failures
",1381139062
d585613ee22ed9292a69e35cedb0d5965f4c906f remotes/origin/vldb~556,"Merge pull request #37 from pwendell/merge-0.8

merge in remaining changes from `branch-0.8`

This merges in the following changes from `branch-0.8`:

- The scala version is included in the published maven artifact names
- A unit tests which had non-deterministic failures is ignored (see SPARK-908)
- A minor documentation change shows the short version instead of the full version
- Moving the kafka jar to be ""provided""
- Changing the default spark ec2 version.
- Some spacing changes caused by Maven's release plugin

Note that I've squashed this into a single commit rather than pull in the branch-0.8 history. There are a bunch of release/revert commits there that make the history super ugly.
",1381039025
e09f4a9601b18921c309903737d309eab5c6d891 remotes/origin/vldb~394^2~7^2,fixed some warnings,1381007303
c6ceaeae50c7b5d69bb9897af32f537bfbde152d remotes/origin/vldb~551^2~3,"Style fix using 'if' rather than 'match' on boolean
",1380887573
1ee60d3b3417559ae4e6638731c4051dca0c2018 remotes/origin/benchmarks~10^2~1,"Fixed bug in sampleLogNormal
",1380847597
232765f7b26d933caa14e0e1bc0e4937dae90523 remotes/origin/vldb~561,"Merge pull request #26 from Du-Li/master

fixed a wildcard bug in make-distribution.sh; ask sbt to check local
maven repo in project/SparkBuild.scala

(1) fixed a wildcard bug in make-distribution.sh:
with the wildcard * in quotes, this cp command failed. it worked after
moving the wildcard out quotes.

(2) ask sbt to check local maven repo in SparkBuild.scala:
To build Spark (0.9.0-SNAPSHOT) with the HEAD of mesos (0.15.0), I must
do ""make maven-install"" under mesos/build, which publishes the java .jar
file under ~/.m2. However, when building Spark (after pointing mesos to
version 0.15.0), sbt uses ivy which by default only checks ~/.ivy2. This
change is to tell sbt to also check ~/.m2.
",1380826848
e597ea34a6df1733addefbb25d06cf9a25ff4f53 remotes/origin/vldb~564,"Merge pull request #10 from kayousterhout/results_through-bm

Send Task results through the block manager when larger than Akka frame size (fixes SPARK-669).

This change requires adding an extra failure mode: tasks can complete
successfully, but the result gets lost or flushed from the block manager
before it's been fetched.

This change also moves the deserialization of tasks into a separate thread, so it's no longer part of the DAG scheduler's tight loop. This should improve scheduler throughput, particularly when tasks are sending back large results.

Thanks Josh for writing the original version of this patch!

This is duplicated from the mesos/spark repo: https://github.com/mesos/spark/pull/835
",1380773664
0d19f00e9e92b47053822b49ec6de502b5df6241 remotes/origin/vldb~561^2~1,"fixed a bug of using wildcard in quotes
",1380667326
0dcad2edcbcc1f3f12a339110e85c8b1a48af156 remotes/origin/vldb~564^2,"Added additional unit test for repeated task failures
",1380608775
dea4677c887a515e7b2a3ef52dd65e69b15c60c3 remotes/origin/vldb~564^2~1,"Fixed compilation errors and broken test.
",1380604021
f549ea33d3d5a584f5d9965bb8e56462a1d6528e remotes/origin/vldb~543^2~10,"Standalone Scheduler fault tolerance using ZooKeeper

This patch implements full distributed fault tolerance for standalone scheduler Masters.
There is only one master Leader at a time, which is actively serving scheduling
requests. If this Leader crashes, another master will eventually be elected, reconstruct
the state from the first Master, and continue serving scheduling requests.

Leader election is performed using the ZooKeeper leader election pattern. We try to minimize
the use of ZooKeeper and the assumptions about ZooKeeper's behavior, so there is a layer of
retries and session monitoring on top of the ZooKeeper client.

Master failover follows directly from the single-node Master recovery via the file
system (patch 194ba4b8), save that the Master state is stored in ZooKeeper instead.

Configuration:
By default, no recovery mechanism is enabled (spark.deploy.recoveryMode = NONE).
By setting spark.deploy.recoveryMode to ZOOKEEPER and setting spark.deploy.zookeeper.url
to an appropriate ZooKeeper URL, ZooKeeper recovery mode is enabled.
By setting spark.deploy.recoveryMode to FILESYSTEM and setting spark.deploy.recoveryDirectory
to an appropriate directory accessible by the Master, we will keep the behavior of from 194ba4b8.

Additionally, places where a Master could be specificied by a spark:// url can now take
comma-delimited lists to specify backup masters. Note that this is only used for registration
of NEW Workers and application Clients. Once a Worker or Client has registered with the
Master Leader, it is ""in the system"" and will never need to register again.

Forthcoming:
Documentation, tests (! - only ad hoc testing has been performed so far)
I do not intend for this commit to be merged until tests are added, but this patch should
still be mostly reviewable until then.
",1380233063
d5a96feccb15dd290b282af9e2f94479c8e4554e remotes/origin/vldb~543^2~11,"Standalone Scheduler fault recovery

Implements a basic form of Standalone Scheduler fault recovery. In particular,
this allows faults to be manually recovered from by means of restarting the
Master process on the same machine. This is the majority of the code necessary
for general fault tolerance, which will first elect a leader and then recover
the Master state.

In order to enable fault recovery, the Master will persist a small amount of state related
to the registration of Workers and Applications to disk. If the Master is started and
sees that this state is still around, it will enter Recovery mode, during which time it
will not schedule any new Executors on Workers (but it does accept the registration of
new Clients and Workers).

At this point, the Master attempts to reconnect to all Workers and Client applications
that were registered at the time of failure. After confirming either the existence
or nonexistence of all such nodes (within a certain timeout), the Master will exit
Recovery mode and resume normal scheduling.
",1380232775
13eced723f222095ea4b52c4f6cb078cae66342e remotes/origin/vldb~566,"Merge pull request #16 from pwendell/master

Bug fix in master build
",1380230299
70a0b993d4f1a72a1b38f9a17fcf269acc1b394f remotes/origin/vldb~567,"Merge pull request #14 from kayousterhout/untangle_scheduler

Improved organization of scheduling packages.

This commit does not change any code -- only file organization.
Please let me know if there was some masterminded strategy behind
the existing organization that I failed to understand!

There are two components of this change:
(1) Moving files out of the cluster package, and down
a level to the scheduling package. These files are all used by
the local scheduler in addition to the cluster scheduler(s), so
should not be in the cluster package. As a result of this change,
none of the files in the local package reference files in the
cluster package.

(2) Moving the mesos package to within the cluster package.
The mesos scheduling code is for a cluster, and represents a
specific case of cluster scheduling (the Mesos-related classes
often subclass cluster scheduling classes). Thus, the most logical
place for it seems to be within the cluster package.

The one thing about the scheduling code that seems a little funny to me
is the naming of the SchedulerBackends.  The StandaloneSchedulerBackend
is not just for Standalone mode, but instead is used by Mesos coarse grained
mode and Yarn, and the backend that *is* just for Standalone mode is instead called SparkDeploySchedulerBackend. I didn't change this because I wasn't sure if there
was a reason for this naming that I'm just not aware of.
",1380229914
e2ff59af728939b173cc12fa3368208a227fbaa2 remotes/origin/vldb~566^2,"Bug fix in master build
",1380226011
560ee5c9bba3f9fde380c831d0c6701343b2fecf remotes/origin/vldb~570,"Merge pull request #7 from wannabeast/memorystore-fixes

some minor fixes to MemoryStore

This is a repeat of #5, moved to its own branch in my repo.

This makes all updates to   on ; it skips on synchronizing the reads where it can get away with it.
",1380220054
e8b1ee04fcb4cd7b666d3148d6d5ff148551ce72 remotes/origin/vldb~343^2~6^2~2^2~1,"fix paths and change spark to use APP_MEM as application driver memory instead of SPARK_MEM, user should add application jars to SPARK_CLASSPATH

Signed-off-by: shane-huang <shengsheng.huang@intel.com>
",1380186527
604dc409969923d1e175e2d77fb0ddcac09c9d31 remotes/origin/vldb~394^2~9^2~1,"Sync with master and some build fixes
",1380175802
7ff4c2d399e1497966689cbe13edf2cd2a9a29b1 remotes/origin/vldb~394^2~9^2~2,"fixed maven build for scala 2.10
",1380172704
14098037630196d2672431539503f27be67be480 remotes/origin/vldb~343^2~6^2~2^2~2,"fix path

Signed-off-by: shane-huang <shengsheng.huang@intel.com>
",1380171010
c75eb14fe52f6789430983471974e5ddf73aacbf remotes/origin/vldb~564^2~4,"Send Task results through the block manager when larger than Akka frame size.

This change requires adding an extra failure mode: tasks can complete
successfully, but the result gets lost or flushed from the block manager
before it's been fetched.
",1379910048
77e9da1f34a0b9e556d7c0bbd4aeaa5c635b881d remotes/origin/vldb~576^2^2,"Change Exception to NoSuchElementException and minor style fix
",1379839808
85024acd2ed9d6b2a03422214865ebb48b2094b8 remotes/origin/vldb~576^2^2~1,"Remove infix style and others
",1379830855
a2ea069a5f2ed83268109deade456dc0fc9b79ee remotes/origin/vldb~576^2~1,"Merge pull request #937 from jerryshao/localProperties-fix

Fix PR926 local properties issues in Spark Streaming like scenarios",1379829882
f06f2da2cb2fd6bd25b57e5f5fd6f8e5d37ab1a3 remotes/origin/vldb~576^2~2,"Merge pull request #941 from ilikerps/master

Add ""org.apache."" prefix to packages in spark-class",1379828614
aa0c29f74779bc5af70250c7481dbd7052ee39cf remotes/origin/vldb~576^2~1^2,"Add barrier for local properties unit test and fix some styles
",1379814791
8933f9e98e8d39717477afa0bb7ffbc6872e05b9 remotes/origin/vldb~576^2~2^2,"Add ""org.apache."" prefix to packages in spark-class

Lacking this, the if/case statements never trigger on Spark 0.8.0+.
",1379730428
026dba6abaaf6314a79ce873bb38b73a9b7fd1a7 remotes/origin/vldb~576^2~3^2,"After unit tests, clear port properties unconditionally

In MapOutputTrackerSuite, the ""remote fetch"" test sets spark.driver.port
and spark.hostPort, assuming that they will be cleared by
LocalSparkContext. However, the test never sets sc, so it remains null,
causing LocalSparkContext to skip clearing these properties. Subsequent
tests therefore fail with java.net.BindException: ""Address already in
use"".

This commit makes LocalSparkContext clear the properties even if sc is
null.
",1379653523
68ad33a12704ae0414e2dc7de0d60b59365d9f91 remotes/origin/benchmarks~15,"Merge pull request #2 from ankurdave/package-fixes

Package fixes (spark.graph -> org.apache.spark.graph)",1379651427
cd7222c3dd2211ce790fa52110db911b862bb63b remotes/origin/vldb~576^2~4,"Merge pull request #938 from ilikerps/master

Fix issue with spark_ec2 seeing empty security groups",1379625684
f589ce771a53dcbfda5f62fe0ac77164688ecc76 remotes/origin/vldb~576^2~4^2,"Fix issue with spark_ec2 seeing empty security groups

Under unknown, but occasional, circumstances, reservation.groups is empty
despite reservation.instances each having groups. This means that the
spark_ec2 get_existing_clusters() method would fail to find any instances.
To fix it, we simply use the instances' groups as the source of truth.

Note that this is actually just a revival of PR #827, now that the issue
has been reproduced.
",1379624966
9f8190c17d19d9149f20fc3c879dee9e6ab7b8ec remotes/origin/vldb~536^2~30,"Fixed a bug for zero partition in JobWaiter.
",1379569355
ffa5f8e11db26dd616e85b9d941de3590ca3643e remotes/origin/vldb~576^2~1^2~1,"Fix issue when local properties pass from parent to child thread
",1379496804
1cb42e6b2d3380f9d9a78b5c7e5959a47e0309ff remotes/origin/vldb~536^2~34,"Properly handle job failure when the job gets killed.
",1379394645
2aff7989ab617f33052098498119886c40794774 remotes/origin/benchmarks~18^2,"Merge pull request #933 from jey/yarn-typo-fix

Fix typo in Maven build docs",1379279104
e86d1d4a52147fe52feeda74ca3558f6bc109285 remotes/origin/vldb~568^2,"Clarify error messages on SSH failure
",1378936782
8432f27fbed968995a2aa223907e9f16d99e536f remotes/origin/benchmarks~18^2~10,"Merge pull request #923 from haoyuan/master

fix run-example script",1378880393
d40f1403f38a406676f6999c7eb889f68c9dfe1d remotes/origin/benchmarks~18^2~11,"Merge pull request #921 from pwendell/master

Fix HDFS access bug with assembly build.",1378879529
56b94078481786a0b0bbcbe3971ee4ef0326d694 remotes/origin/benchmarks~18^2~10^2~1,"fix run-example script
",1378879389
0c1985b153a2dc2c891ae61c1ee67506926384ae remotes/origin/benchmarks~18^2~11^2,"Fix HDFS access bug with assembly build.

Due to this change in HDFS:
https://issues.apache.org/jira/browse/HADOOP-7549

there is a bug when using the new assembly builds. The symptom is that any HDFS access
results in an exception saying ""No filesystem for scheme 'hdfs'"". This adds a merge
strategy in the assembly build which fixes the problem.
",1378875913
6fcfefcb278ffb9d54452d3336cb74b97480fbf5 remotes/origin/vldb~394^2~9^2~8,"Few more fixes to tests broken during merge
",1378790867
c81377b9eda431f9298045d613d0e46301b08c63 remotes/origin/benchmarks~18^2~14,"Merge pull request #915 from ooyala/master

Get rid of / improve ugly NPE when Utils.deleteRecursively() fails",1378782979
61d2a010e1b2d25616c378185606e6ba7c2b916b remotes/origin/benchmarks~18^2~15,"Merge pull request #916 from mateiz/mkdist-fix

Fix copy issue in https://github.com/mesos/spark/pull/899",1378776061
fdb8b0eec3e423e601de8c79658c4644717f4e05 remotes/origin/benchmarks~18^2~14^2,"Style fix: put body of if within curly braces
",1378762172
f5a8afa6c38c6d3a3bf170e609468ff1d610f3ea remotes/origin/benchmarks~18^2~15^2,"Fix copy issue in https://github.com/mesos/spark/pull/899
",1378759676
27726079e4e6931c071de77e91f991cb1b249d02 remotes/origin/benchmarks~18^2~14^2~1,"Print out more friendly error if listFiles() fails

listFiles() could return null if the I/O fails, and this currently results in an ugly NPE which is hard to diagnose.
",1378756692
bf984e27457fb64baa1c1b32bbc2d7dd645c98c4 remotes/origin/benchmarks~18^2~19,"Merge pull request #890 from mridulm/master

Fix hash bug",1378709424
e9d4f44a7adfaf55d0fd312b81350638310c341d remotes/origin/benchmarks~18^2~20,"Merge pull request #909 from mateiz/exec-id-fix

Fix an instance where full standalone mode executor IDs were passed to",1378708608
f9b7f58de20fed0447f7d77499ec89ad6a188a2d remotes/origin/benchmarks~18^2~20^2,"Fix an instance where full standalone mode executor IDs were passed to
StandaloneSchedulerBackend instead of the smaller IDs used within Spark
(that lack the application name).

This was reported by ClearStory in
https://github.com/clearstorydata/spark/pull/9.

Also fixed some messages that said slave instead of executor.
",1378690070
170b3869ee8c854b66ce75901efc2c1a9e96ff99 remotes/origin/benchmarks~18^2~22^2~1,"Fix unit test failure due to changed default
",1378687887
4a7813a2479e2413275c23050afd242af1b7a1ba remotes/origin/benchmarks~18^2~30,"Merge pull request #903 from rxin/resulttask

Fixed the bug that ResultTask was not properly deserializing outputId.",1378587144
afe46ba36ee2d477d6270b4833d68d297f0dab47 remotes/origin/benchmarks~18^2~31,"Merge pull request #892 from jey/fix-yarn-assembly

YARN build fixes",1378564131
210eae26f4121d772210ffa0494689c911771456 remotes/origin/benchmarks~18^2~30^2,"Fixed the bug that ResultTask was not properly deserializing outputId.
",1378562387
1e15feb5a314e7180328b9208054966e040eb2ad remotes/origin/benchmarks~18^2~33,"Hot fix to resolve the compilation error caused by SPARK-821.
",1378478645
7c15e3c5de1282c070560e63203790b71e1c6f0d remotes/origin/benchmarks~18^2~35^2,"Fix bug SPARK-864
",1378367771
b3a82b7df38e4c3e9cd12ca60aa18fa10c8d84c8 remotes/origin/benchmarks~18^2~19^2~1,"Fix hash bug - caused failure after 35k stages, sigh
",1378258345
c9bc8af3d17d153bc182dcddc6611b1fa87ffbbf remotes/origin/vldb~575^2,"Removed repetative import; fixes hidden definition compiler warning.
",1378247120
c592a3c9b9b1e7cb810bd81555c009c642f9d12a remotes/origin/benchmarks~18^2~38,"Minor spacing fix
",1378244351
41c1b5b9a0d8155c94cf1eb4c34d2b2db41b0d8f remotes/origin/benchmarks~18^2~39^2,"Update based on review comments. Change function to prependBaseUri and fix formatting.
",1378237611
36d8fca2cca7cb3f8c490631b0f6699a119e1230 remotes/origin/benchmarks~18^2~40^2^2~1,Reynold's comment fixed,1378175469
d3dd48f8b3a34817114c23d6e667c4952a5da5ca remotes/origin/benchmarks~18^2~42,"Merge pull request #887 from mateiz/misc-fixes

Miscellaneous fixes for 0.8",1378165494
141f54279e538e36e0506eb37b51df90dfa27358 remotes/origin/benchmarks~18^2~44^2~1,"Further fixes to get PySpark to work on Windows
",1378084769
3c520fea7782fd24b2e30347938af9769c72c4ea remotes/origin/benchmarks~18^2~45,"Merge pull request #884 from mateiz/win-fixes

Run script fixes for Windows after package & assembly change",1378081615
3db404a43a90a9cca37090381857dc955496385a remotes/origin/benchmarks~18^2~45^2,"Run script fixes for Windows after package & assembly change
",1378079157
5b4dea21439e86b61447bdb1613b2ddff9ffba9f remotes/origin/benchmarks~18^2~46^2~1,"More fixes
",1378069996
2c5a4b89ee034b7933b258cfc37bc6d91a06c186 remotes/origin/benchmarks~18^2~50,"Small fixes to README
",1377997685
7862c4a3c8b900db81f5c2af157bd4564d814bd9 remotes/origin/benchmarks~18^2~51^2,"Another fix suggested by Patrick
",1377996107
2ee6a7e32aaf6dbe81ea79bee1228bb09e48dd1a remotes/origin/benchmarks~18^2~49^2~1,"Print output from spark-daemon only when it fails to launch
",1377995467
96452eea56c289e25a711ca54c812723a5059739 remotes/origin/benchmarks~18^2~39^2~3,"fix up minor things
",1377896671
9e17e456d2f39bb48200d634315525c7e2458d7e remotes/origin/benchmarks~18^2~56,"Merge pull request #875 from shivaram/build-fix

Fix broken build by removing addIntercept",1377847373
15287766281195a019a400fe11b41e96c6edc362 remotes/origin/benchmarks~18^2~60,"Merge pull request #874 from jerryshao/fix-report-bug

Fix removed block zero size log reporting",1377837047
afcade3ca8fb4c2158675dec311434379ba1d4b7 remotes/origin/benchmarks~18^2~62,"Merge pull request #873 from pwendell/master

Hot fix for command runner",1377746140
2fc9a028f2b3ebd0ab993672ae88d96b8868316c remotes/origin/benchmarks~18^2~62^2~1,"Hot fix for command runner
",1377741786
742c44eae693d2bde76259043c962e416691258c remotes/origin/benchmarks~18^2~53^2,"Don't send SIGINT to Py4J gateway subprocess.

This addresses SPARK-885, a usability issue where PySpark's
Java gateway process would be killed if the user hit ctrl-c.

Note that SIGINT still won't cancel the running s

This fix is based on http://stackoverflow.com/questions/5045771
",1377733184
baa84e7e4c5e0afc8bc3b177379311d309c00cd2 remotes/origin/benchmarks~18^2~63,"Merge pull request #865 from tgravescs/fixtmpdir

Spark on Yarn should use yarn approved directories for spark.local.dir and tmp",1377719086
63dc635de6e7a31095b3d246899a657c665e4ed7 remotes/origin/benchmarks~18^2~64^2,"fix typos
",1377554780
b8c50a0642cf74c25fd70cc1e7d1be95ddafc5d8 remotes/origin/benchmarks~18^2~57^2,"Center & scale variables in Ridge, Lasso.
Also add a unit test that checks if ridge regression lowers
cross-validation error.
",1377494667
d282c1ebbbe1aebbd409c06efedf95fb77833c35 remotes/origin/benchmarks~18^2~68,"Merge pull request #860 from jey/sbt-ide-fixes

Fix IDE project generation under SBT",1377282020
215c13dd41d8500835ef00624a0b4ced2253554e remotes/origin/benchmarks~18^2~70,"Fix code style and a nondeterministic RDD issue in ALS
",1377213226
9ac3d62cacae34e742c70e7006ffaf7e21880802 remotes/origin/benchmarks~18^2~72,"Merge pull request #856 from jey/sbt-fix-hadoop-0.23.9

Re-add removed dependency to fix build under Hadoop 0.23.9",1377211870
c02585ea130045ef27e579172ac2acc71bc8da63 remotes/origin/benchmarks~18^2~67^2~2,"Make initial connection failure message less daunting.

Right now it seems like something has gone wrong when this message is printed out.
Instead, this is a normal condition. So I changed the message a bit.
",1377125145
c0942a710f25c9c690761b0814a07deacd4df595 remotes/origin/benchmarks~18^2~75^2~3,"Bug in test fixed
",1377040565
f1c853d76dce4fbc34f580be0a3ae15cc5be9c80 remotes/origin/benchmarks~18^2~75^2~8,"fixed Matei's comments
",1377040564
33a0f59354197d667a97c600e2bb8fefe50c181b remotes/origin/benchmarks~18^2~75^2~13,"Added error messages to the tests to make failed tests less cryptic
",1377040417
abcefb3858aac373bd8898c3e998375d5f26b803 remotes/origin/benchmarks~18^2~75^2~14,"fixed matei's comments
",1377040417
f24861b60a9ddef1369a0a6816f6922575940656 remotes/origin/benchmarks~18^2~75^2~20,"Fix bug in tests
",1377040416
8cae72e94ed91db3212d9403431e9b126bfd286b remotes/origin/benchmarks~18^2~83,"Merge pull request #828 from mateiz/sched-improvements

Scheduler fixes and improvements",1376980804
efeb14298139fce415f20f94b1e303bf8abd19a4 remotes/origin/benchmarks~18^2~84,"Merge pull request #849 from mateiz/web-fixes

Small fixes to web UI",1376965430
498a26189b197bdaf4be47e6a8baca7b97fe9064 remotes/origin/benchmarks~18^2~84^2~1,"Small fixes to web UI:

- Use SPARK_PUBLIC_DNS environment variable if set (for EC2)
- Use a non-ephemeral port (3030 instead of 33000) by default
- Updated test to use non-ephemeral port too
",1376961469
6f6944c8079bffdd088ddb0a84fbf83356e294ea remotes/origin/benchmarks~18^2~81^2,"Update SBT build to use simpler fix for Hadoop 0.23.9
",1376940793
8ac3d1e2636ec71ab9a14bed68f138e3a365603e remotes/origin/benchmarks~18^2~83^2,"Added unit tests for ClusterTaskSetManager, and fix a bug found with
resetting locality level after a non-local launch
",1376880667
222c8971285190761354456c2fe07f5c31edf330 remotes/origin/benchmarks~18^2~83^2~3,"Comment cleanup (via Kay) and some debug messages
",1376880667
cf39d45d14e0256bbd3ffe206c14997f02429cb3 remotes/origin/benchmarks~18^2~83^2~4,"More scheduling fixes:

- Added periodic revival of offers in StandaloneSchedulerBackend

- Replaced task scheduling aggression with multi-level delay scheduling
  in ClusterTaskSetManager

- Fixed ZippedRDD preferred locations because they can't currently be
  process-local

- Fixed some uses of hostPort
",1376880667
90a04dab8d9a2a9a372cea7cdf46cc0fd0f2f76c remotes/origin/benchmarks~18^2~83^2~5,"Initial work towards scheduler refactoring:

- Replace use of hostPort vs host in Task.preferredLocations with a
  TaskLocation class that contains either an executorId and a host or
  just a host. This is part of a bigger effort to eliminate hostPort
  based data structures and just use executorID, since the hostPort vs
  host stuff is confusing (and not checkable with static typing, leading
  to ugly debug code), and hostPorts are not provided by Mesos.

- Replaced most hostPort-based data structures and fields as above.

- Simplified ClusterTaskSetManager to deal with preferred locations in a
  more concise way and generally be more concise.

- Updated the way ClusterTaskSetManager handles racks: instead of
  enqueueing a task to a separate queue for all the hosts in the rack,
  which would create lots of large queues, have one queue per rack name.

- Removed non-local fallback stuff in ClusterScheduler that tried to
  launch less-local tasks on a node once the local ones were all
  assigned. This change didn't work because many cluster schedulers send
  offers for just one node at a time (even the standalone and YARN ones
  do so as nodes join the cluster one by one). Thus, lots of non-local
  tasks would be assigned even though a node with locality for them
  would be able to receive tasks just a short time later.

- Renamed MapOutputTracker ""generations"" to ""epochs"".
",1376880666
630281bf7671ffec65a4672361d3d9570cfe7a39 remotes/origin/benchmarks~16^2~8,"Corrected all indexed RDD tests.

There appears to be an issue with subtract by key tests that needs to be investigated further.
",1376871405
07fe910669b2ec15b6b5c1e5186df5036d05b9b1 remotes/origin/benchmarks~18^2~57^2~1,"Fixing typos in Java tests, and addressing alignment issues.
",1376863393
c69c48947d5102c81a9425cb380d861c3903685c remotes/origin/benchmarks~18^2~91,"Merge pull request #843 from Reinvigorate/bug-879

fixing typo in conf/slaves",1376625309
a5193a8fac78f8860ae1dc3969e9a8528a1d292c remotes/origin/benchmarks~18^2~91^2,"fixing typo
",1376621578
3bb6e019d4eee5cc80b6b506951ce9efc4f21ed2 remotes/origin/benchmarks~16^2~12,"adding better error handling when indexing an RDD
",1376602188
839f2d4f3f7f39615c1c840b0d7c9394da6a2e64 remotes/origin/benchmarks~18^2~98,"Merge pull request #822 from pwendell/ui-features

Adding GC Stats to TaskMetrics (and three small fixes)",1376522243
63446f9208876b482c2aea4acc42a6713eb94f55 remotes/origin/benchmarks~18^2~99,"Merge pull request #826 from kayousterhout/ui_fix

Fixed 2 bugs in executor UI (incl. SPARK-877)",1376464627
a88aa5e6ed98d212b25a534566e417401da9cc7d remotes/origin/benchmarks~18^2~99^2,"Fixed 2 bugs in executor UI.

1) UI crashed if the executor UI was loaded before any tasks started.
2) The total tasks was incorrectly reported due to using string (rather
than int) arithmetic.
",1376462698
3f14cbab05f732f129b2da4efb19c746949ad0ab remotes/origin/benchmarks~18^2~100,"Merge pull request #825 from shivaram/maven-repl-fix

Set SPARK_CLASSPATH for maven repl tests",1376449791
f0382007dc7268c603db3f0227e2ca01dc5e7b71 remotes/origin/benchmarks~18^2~98^2~5,"Bug fix for display of shuffle read/write metrics.

This fixes an error where empty cells are missing if a given task
has no shuffle read/write.
",1376436355
1f79d21f33fbdd5e546d219decaed3474f42b10a remotes/origin/benchmarks~18^2~103,"Merge pull request #818 from kayousterhout/killed_fix

Properly account for killed tasks.",1376432634
1beb843a6f94644387dba36e92f4850e57de9194 remotes/origin/benchmarks~18^2~103^2,"Reuse the set of failed states rather than creating a new object each time
",1376429260
ecc9bfe37740988622bc41efd8f9776a5eca7873 remotes/origin/benchmarks~18^2~108^2,"Create SparkContext in beforeAll for MLLib tests
This overcomes test failures that occur using Maven
",1376265840
95c62ca3060c89a44aa19aaab1fc9a9fff5a1196 remotes/origin/benchmarks~16^2~23,"Merge pull request #804 from apivovarov/master

Fixed path to JavaALS.java and JavaKMeans.java, fixed hadoop2-yarn profi...",1376242252
2d97cc46af9bceb8d483a55d780f0a01cf286b7a remotes/origin/benchmarks~16^2~23^2,"Fixed path to JavaALS.java and JavaKMeans.java, fixed hadoop2-yarn profile
",1376201090
71c63de22fea19fcd3f6a847dfd3d7b6ab597eac remotes/origin/benchmarks~16^2~27,"Merge pull request #795 from mridulm/master

Fix bug reported in PR 791 : a race condition in ConnectionManager and Connection",1376155280
cd247ba5bb54afa332519826028ab68a4f73849e remotes/origin/benchmarks~16^2~31,"Merge pull request #786 from shivaram/mllib-java

Java fixes, tests and examples for ALS, KMeans",1376106073
7810a76512a0fe07a81549e7d0cb01463bbefeac remotes/origin/benchmarks~16^2~28^2~1,"Only print event queue full error message once
",1376097648
44ca8629d800944daf0f58fb83bcd3ba894455eb remotes/origin/benchmarks~16^2~28^2~2,"Style fix: removing unnecessary return type
",1376094170
29b79714f94cda0db0b56746b29cf60830d076ad remotes/origin/benchmarks~16^2~28^2~3,"Style fixes based on code review
",1376091994
81e1d4a7d19cefe4bbf0fe6ec54b4f894d7b21d0 remotes/origin/benchmarks~16^2~28^2~4,"Refactored SparkListener to process all events asynchronously.

This commit fixes issues where SparkListeners that take a while to
process events slow the DAGScheduler.

This commit also fixes a bug in the UI where if a user goes to a
web page of a stage that does not exist, they can create a memory
leak (granted, this is not an issue at small scale -- probably only
an issue if someone actively tried to DOS the UI).
",1376080061
b09d4b79e83330c96c161ea4eb9af284f0a835e6 remotes/origin/benchmarks~16^2~32,"Merge pull request #799 from woggle/sync-fix

Remove extra synchronization in ResultTask",1376079428
d1e1c1b24d8dd8284952a08dbda138dc9e233a9a remotes/origin/benchmarks~16^2~37^2,"Add test for Kryo with WrappedArray (which was failing in Chill 0.3.0)
",1375994051
dc47084f4ee173fbd11e8e633ca7955c3259af88 remotes/origin/benchmarks~16^2~27^2~1,"Attempt to fix bug reported in PR 791 : a race condition in ConnectionManager and Connection
",1375980567
88049a214df8ee0b155e5c4b894cf32bab7bafc5 remotes/origin/benchmarks~16^2~28^2~5,"Fixed 3 bugs that caused UI to crash (including SPARK-810).

One bug caused the UI to crash if you try to look at a job's status
before any of the tasks have finished.

The second bug was a concurrency issue where two different threads
(the scheduling thread and a UI thread) could be reading/updating
the data structures in JobProgressListener concurrently.

The third bug mis-used an Option, also causing the UI to crash
under certain conditions.
",1375942165
706394b37051a35db22a2df8d0aa35f875d234cb remotes/origin/benchmarks~16^2~38^2~2,"Bumping font size to 14px and fixing sytle issue in progress bars
",1375936024
5133e4bebd47d8ae089f967689ecab551c2c5844 remotes/origin/benchmarks~16^2~39,"Merge pull request #790 from kayousterhout/fix_throughput

Fixed issue in UI that decreased scheduler throughput by 5x or more",1375915845
b88e26248e0e7f0308a14e870677da9ce16a8735 remotes/origin/benchmarks~16^2~39^2,"Fixed issue in UI that limited scheduler throughput.

Removal of items from ArrayBuffers in the UI code was slow and
significantly impacted scheduler throughput. This commit
improves scheduler throughput by 5x.
",1375911725
6caec3f44193a459a2dd10b0393e391979795039 remotes/origin/benchmarks~16^2~31^2~1,"Add a test case for random initialization.
Also workaround a bug where double[][] class cast fails
",1375832147
471fbadd0c8cb8d310e3e1dd0e694e357ff1233e remotes/origin/benchmarks~16^2~31^2~2,"Java examples, tests for KMeans and ALS
- Changes ALS to accept RDD[Rating] instead of (Int, Int, Double) making it
  easier to call from Java
- Renames class methods from `train` to `run` to enable static methods to be
  called from Java.
- Add unit tests which check if both static / class methods can be called.
- Also add examples which port the main() function in ALS, KMeans to the
  examples project.

Couple of minor changes to existing code:
- Add a toJavaRDD method in RDD to convert scala RDD to java RDD easily
- Workaround a bug where using double[] from Java leads to class cast exception in
  KMeans init
",1375829026
42942fc1a917c8ec31f045e7aecfd99bcb0961f6 remotes/origin/benchmarks~19^2~4^2~1,"In the process of bringing the GraphLab api back and fixing the analytics toolkit
",1375815571
bf7033f3ebf9315ccf9aba09a6e702c3a671fd8d remotes/origin/benchmarks~19^2~8^2,"fixing formatting, style, and input
",1375763184
95025afdecdf5b21ab3d73c6786be9f3c6d579ca remotes/origin/benchmarks~16^2~42^2~6,"Made most small fixes for SPARK-849 except for table sort, task progress overlay
",1375733096
8c8947e2b66169dddb828b801ffaa43cc400b8a5 remotes/origin/benchmarks~19^2~8^2~1,"fixing formatting
",1375726938
d93d5fcaacc22f837c861bddbb54928b8121bdfb remotes/origin/benchmarks~19^2~10^2~1^2,"SPARK-850: Give better error message on the console
",1375722543
39e4fda76f704f61924b10618a6f22bd93d8000f remotes/origin/benchmarks~16^2~44^2~1,"[HOTFIX] Extend thread safety for SparkEnv.get()

A ThreadLocal SparkEnv.env is facing various situations leading to
NullPointerExceptions, where SparkEnv.env set in one thread is not
gettable in another thread, but often assumed to be available.

See, e.g., https://groups.google.com/forum/#!topic/spark-developers/GLx8yunSj0A

This hotfixes SparkEnv.env to return either (a) the ThreadLocal
value if non-null, or (b) the previously set value in any thread.

This approach preserves SparkEnv.set() thread safety needed by
RDD.compute() and possibly other places. A refactoring that
parameterizes SparkEnv should be addressed subsequently.

On branch adatao-global-SparkEnv
Changes to be committed:

	modified:   core/src/main/scala/spark/SparkEnv.scala
",1375693794
b4905c383bf24143966d8b2d0ae69ef9ee7c3b0e remotes/origin/benchmarks~16^2~33^2~3,"Log the launch command for Spark daemons

For debugging and analysis purposes, it's nice to have the exact command
used to launch Spark contained within the logs. This adds the necessary
hooks to make that possible.
",1375487899
12d9c82c9b2a5040a2d1e5ab23ab44b41cbe1807 remotes/origin/benchmarks~19^2~13^2~2,"Small style fix
",1375395952
5faac7f4f3678d424ea1e40a109996f3caba2142 remotes/origin/benchmarks~19^2~14^2~3,"Minor style fixes
",1375394442
d58502a1562bbfb1bb4e517ebcc8239efd639297 remotes/origin/benchmarks~19^2~16^2,"fix bug of spark ""SubmitStage"" listener as unit test error
",1375370501
d29ee3689be8b8353d4f2440ac4e0237e98be056 remotes/origin/benchmarks~16^2~41^2,"Merge fixes merge commit hasn't picked
",1375341686
09cd67bf989739b1c72f8513454756da383b195b remotes/origin/benchmarks~16^2~38^2~14,"Changed bootstrap colors, fixed logpaging buttons
",1375312733
39c75f3033eb0b6d3de3517f681bb932efad0caa remotes/origin/benchmarks~19^2~22,"Merge pull request #757 from BlackNiuza/result_task_generation

Bug fix: SPARK-837",1375311156
4ba4c3fe1a7053d1a7414bda9e218d76af8c97fc remotes/origin/benchmarks~19^2~25,"Merge pull request #759 from mateiz/split-fix

Use the Char version of split() instead of the String one in MLUtils",1375301689
43394b9a6d37bb3e4b5ebce9db2160e2a35bb279 remotes/origin/benchmarks~16^2~41^2~14,"fixing formatting
",1375226021
1bca91633eaff0c44d60682622f57eebca09c4ff remotes/origin/benchmarks~16^2~41^2~16,"+ bug fixes;

test added

Conflicts:

	core/src/test/scala/spark/KryoSerializerSuite.scala
",1375207451
b95732632b5b06d494ebd9e539af136ab3b8490e remotes/origin/benchmarks~19^2~35,"Do not inherit master's PYTHONPATH on workers.

This fixes SPARK-832, an issue where PySpark
would not work when the master and workers used
different SPARK_HOME paths.

This change may potentially break code that relied
on the master's PYTHONPATH being used on workers.
To have custom PYTHONPATH additions used on the
workers, users should set a custom PYTHONPATH in
spark-env.sh rather than setting it in the shell.
",1375160937
1e1ffb192a412d19d368fbf1f32de6f3dffbbec7 remotes/origin/benchmarks~19^2~37,"Merge pull request #745 from shivaram/loss-update-fix

Remove duplicate loss history in Gradient Descent",1375151179
07da72b45190f7db9daa2c6bd33577d28e19e659 remotes/origin/benchmarks~19^2~37^2~1,"Remove duplicate loss history and clarify why.
Also some minor style fixes.
",1375140317
fe7298b587b89abffefab2febac4e3861ca2c1c4 remotes/origin/benchmarks~19^2~40,"Merge pull request #741 from pwendell/usability

Fix two small usability issues",1375131660
2b2630ba3c621f4121da8e76fe6fcfa69d3eb74c remotes/origin/benchmarks~19^2~41^2,"Style fix

Lines shortened to < 100 characters
",1375114969
75f375730025788a5982146d97bf3df9ef69ab23 remotes/origin/benchmarks~19^2~41^2~3,"Fix rounding error in LogisticRegression.scala
",1375114796
feba7ee540fca28872957120e5e39b9e36466953 remotes/origin/benchmarks~19^2~44,"SPARK-815. Python parallelize() should split lists before batching

One unfortunate consequence of this fix is that we materialize any
collections that are given to us as generators, but this seems necessary
to get reasonable behavior on small collections. We could add a
batchSize parameter later to bypass auto-computation of batch size if
this becomes a problem (e.g. if users really want to parallelize big
generators nicely)
",1375080703
72ff62a37c7310bab02f0231e91d3ba4d423217a remotes/origin/benchmarks~19^2~49,"Two fixes to IPython support:

- Don't attempt to run worker processes with ipython (that can cause
  some crashes as ipython prints things to standard out)
- Allow passing some IPYTHON_OPTS to launch things like the notebook
",1375064593
f11ad72d4ee2c6821749e1bf95c46d3f2c2cd860 remotes/origin/benchmarks~19^2~42^2,"Some fixes to Python examples (style and package name for LR)
",1374973942
077f2dad226b4f817cb50ad5c07702e78506a698 remotes/origin/benchmarks~19^2~39^2~1,"Fixed outdated bugs
",1374968376
0c391feb73610e56beb542bbf66c56efab01bada remotes/origin/benchmarks~19^2~20^2~3,"Maximum task failures configurable
",1374903283
bd4cc52e309667296ef60557e17e8a7263c51fd8 remotes/origin/benchmarks~19^2~39^2~5,"Made metrics Option instead of Some, fixed NullPointerException
",1374884598
cb366774c84462f33cbaf3acca0c084ca13bcb53 remotes/origin/benchmarks~19^2~51,"Merge pull request #738 from harsha2010/pruning

Fix bug in Partition Pruning.",1374883170
e56aa75de0f3c00e9942f0863c0fb8c57aab5321 remotes/origin/benchmarks~19^2~55^2,"fix wrapping
",1374728889
c258718606a2960649dde0a4925fcf385d617c37 remotes/origin/benchmarks~19^2~56,"Fix Maven build errors after previous commits
",1374707552
93c6015f82d4d27a6f09686b1e849be1cbbd0615 remotes/origin/benchmarks~19^2~58^2~3,"Shows task status and running tasks on Stage Page: fixes SPARK-804 and 811
",1374688382
e9ac88754d4c5d58aedd4de8768787300b15eada remotes/origin/benchmarks~19^2~60^2~11,"Remove twice add Source bug and code clean
",1374649067
6a31b7191d5a8203563a2a4e600210c67439abf5 remotes/origin/benchmarks~19^2~63,"Small bug fix
",1374621624
abc78cd3318fb7bc69d10fd5422d20b299a8d7d8 remotes/origin/benchmarks~19^2~64^2,"Modifies instead of copies HashSets, fixes comment style
",1374619636
8ae1436981664539568f742b71a65b28dc3d3364 remotes/origin/benchmarks~19^2~68^2,"Merge pull request #722 from JoshRosen/spark-825

Fix bug: DoubleRDDFunctions.sampleStdev() computed non-sample stdev()",1374534184
85c4d7bf3bf0969f58ebcda6ca68719972ff0c46 remotes/origin/benchmarks~19^2~67^2~2,"Shows number of complete/total/failed tasks (bug: failed tasks assigned to null executor)
",1374528947
f649dabb4a3a57cb25a852808297fb718cbfedd4 remotes/origin/benchmarks~19^2~68^2^2,"Fix bug: DoubleRDDFunctions.sampleStdev() computed non-sample stdev().

Update JavaDoubleRDD to add new methods and docs.

Fixes SPARK-825.
",1374524508
8901f379c98d19d3145f0cb2ae678d1b0060a311 remotes/origin/benchmarks~19^2~67^2~3,"Fixed memory used/remaining/total bug
",1374512283
4530e8a9bfe35b6d562876b1fb66e534ff5c286d remotes/origin/benchmarks~19^2~74^2~1,"fix typo.
",1374249865
aa6f83289b87f38481dbae60ad91d2ac78ccea46 remotes/origin/benchmarks~19^2~74^2~2,"A better fix for giving local jars unde Yarn mode.
",1374243928
a613628c5078cf41feb973d0ee8a06eb69615bcf remotes/origin/benchmarks~19^2~74^2~3,"Do not copy local jars given to SparkContext in yarn mode since the Context is not running on local. This bug causes failure when jars can not be found. Example codes (such as spark.examples.SparkPi) can not work without this fix under yarn mode.
",1374224352
009c79e5a57c2ee27d249b46b06deaef4db088c6 remotes/origin/benchmarks~19^2~77,"Merge pull request #715 from viirya/dev1

fix a bug in build process that pulls in two versions of ASM.",1374172912
3aad45265324537e51cab79db3f09d9310339f77 remotes/origin/benchmarks~19^2~77^2,"fix a bug in build process that pulls in two versionf of ASM.
",1374172186
84fa20c2a135f54745ddde9abb4f5e60af8856d1 remotes/origin/benchmarks~19^2~75^2~3,"Allow initial weight vectors in LogisticRegression.
Also move LogisticGradient to the LogisticRegression file and fix the
unit tests log path.
",1374095045
f347cc3f659d4414a21de26feadcbe23a130e622 remotes/origin/benchmarks~19^2~86,"Fix deprecation warning and style issues
",1373997210
119c98c1becf9f0038d6bb946545cd65006bd367 remotes/origin/vldb~394^2~11^2~1,"code formatting, The warning related to scope exit and enter is not worth fixing as it only affects debugging scopes and nothing else.
",1373967093
8a8a8f2de225310410ebe4e5d9c1e9b1ee8ffcf2 remotes/origin/benchmarks~19^2~79^2~1,"Merge pull request #705 from rxin/errormessages

Throw a more meaningful message when runJob is called to launch tasks on non-existent partitions.",1373954961
e3d3e6f0ab34f0fe083ef9feb31b9e9fd257519f remotes/origin/benchmarks~19^2~41^2~16,"Merge pull request #702 from karenfeng/ui-fixes

Adds app name in HTML page titles on job web UI",1373925584
0d78b6d9cd11fc12c546f25fa857ba8b285c062d remotes/origin/benchmarks~19^2~79^2~2^2~2^2,"Links to job UI from standalone deploy cluster web UI: fixes SPARK-802
",1373921258
b2aaa1199e7ecd8e1b2a9ddd8356b6393edafe6b remotes/origin/benchmarks~19^2~41^2~16^2,"Adds app name in HTML page titles on job web UI: fixes SPARK-806
",1373913882
a1e56a43b3bc7ed66cbf5d5cee9d5bc12b029f0d remotes/origin/vldb~394^2~12^2~3,"Fixed compilation issues as Map is by default immutable.Map in scala-2.10
",1373867898
c7877d5e16e5441a9891d5eda67a8a062278b103 remotes/origin/benchmarks~19^2~41^2~19^2,"Merge pull request #689 from BlackNiuza/application_status

Bug fix: SPARK-796",1373831893
b91a218cea5a7ab4037675667922fc06bfec6fbf remotes/origin/benchmarks~19^2~90,"Cosmetic fixes to web UI
",1373787093
e271fde10b342216e33f4f45af73c5d103215cf2 remotes/origin/benchmarks~19^2~92,"Fixed a delay scheduling bug in the YARN branch, found by Patrick
",1373783069
ddb97f0fdf16442afaa9cab656376267e4044510 remotes/origin/benchmarks~19^2~87^2~1,"Add `Environment` tab to SparkUI.

This adds a tab which displays system property and classpath information. This
can be useful in debugging various types of issues such as:

1. Extra/incorrect Hadoop jars being included in the classpath
2. Spark launching with a different JRE version than intended
3. Spark system properties not being set to intended values
4. User added jars that conflict with Spark jars
",1373757280
89e8549f539f81b88e7bff93c898620d7aad0b63 remotes/origin/benchmarks~19^2~51^2~4,"Merge pull request #698 from Reinvigorate/sm-deps-change

changing com.google.code.findbugs maven coordinates",1373757068
c4d5b01e44c5f289d67670185a14ff4ccd9537db remotes/origin/benchmarks~19^2~51^2~4^2,"changing com.google.code.findbugs maven coordinates
",1373748983
08150f19abcf08f2a18305080b08736fb8a33a12 remotes/origin/benchmarks~19^2~96^2,"Minor style fix
",1373682755
6855338e1400638188358a7d7926eb86f668c160 remotes/origin/benchmarks~19^2~96^2~1,"Show block locations in Web UI.

This fixes SPARK-769. Support is added for enumerating the locations of blocks
in the UI. There is also some minor cleanup in StorageUtils.
",1373682632
8dccee16af14284eff0b9dbddcde6c8303abb474 remotes/origin/benchmarks~18^2~67^2~13,"Bug fix
",1373586627
9ed036ccdbb1c7aa3279998f3177c1be6b01e16b remotes/origin/benchmarks~19^2~97^2~5,"Replaced logPageLength with byteLength to prevent buffer shrink bug
",1373585633
13809d363c0a5b3212751561e0ecc09c601caf8a remotes/origin/benchmarks~18^2~67^2~15,"Small fix
",1373582735
0ecc33f0c80733ff88518157be543ec458a76bdb remotes/origin/benchmarks~19^2~97^2~12,"Added byte range, page title with log name, previous/next bytes buttons, initialization to end of log, large default buffer, buggy back to master link
",1373567158
69ae7ea227e52c743652e5560ac7666187c0de6a remotes/origin/vldb~394^2~12^2~9,"Removed some unnecessary code and fixed dependencies
",1373547618
24196c91f0bd560c08399eed3cb1903248ca497a remotes/origin/benchmarks~19^2~97^2~14,"Changed buffer to 10,000 bytes, created scrollbar for fixed-height log
",1373495272
620a6974c6603f1c0e5a7cea8f0387a5d18f2e5e remotes/origin/benchmarks~19^2~97^2~21,"Allows for larger files, refactors lastNBytes, removes old Log column, fixes imports, uses map
",1373476853
c1d44be80580f0fad6bb1805bbcf74a34f536d8c remotes/origin/benchmarks~19^2~41^2~19^2^2~3,"Bug fix: SPARK-796
",1373354308
638927b78e5b67e71d0c7360a2f36841d13dc107 remotes/origin/benchmarks~19^2~104,"Merge pull request #683 from shivaram/sbt-test-fix

Remove some stack traces from sbt test output",1373349530
bf4c9a5e0fca2dfc960120a7f3c5fab0b87e3850 remotes/origin/benchmarks~19^2~105^2,"renamed with labeled prefix
",1373319462
4af0d63cb14db902cbd1dbdeeb68f1fcec4b2e97 remotes/origin/benchmarks~19^2~104^2,"Remove akka LogLevel fix as we no longer use spray
",1373218963
f78f8d0b416ef4d88883d8f32382661f4c2ac52d remotes/origin/benchmarks~19^2~106^2~1,"fix formatting and use Vector instead of List to maintain order
",1373154413
32b9d21a97d1c93f174551000d06cc429f317827 remotes/origin/benchmarks~19^2~108^2,"Fix occasional failure in UI listener.

If a task fails before the metrics are initialized, it remains possible
that the metrics field will be `None`. This patch accounts for that possbility
by keeping metrics as an `Option` at all times.
",1373154002
7ba7fa110bec1047d7b67b1bbb580ead80555311 remotes/origin/benchmarks~19^2~115,"Merge pull request #674 from liancheng/master

Bug fix: SPARK-789",1373136308
37abe84212c6dad6fb87a3b47666d6a3c14c1f66 remotes/origin/benchmarks~19^2~87^2~2,"Tracking some task metrics even during failures.
",1373127599
e063e29af8dd1d98edc51722dc97720750090e39 remotes/origin/benchmarks~19^2~117,"Merge pull request #680 from tdas/master

Fixed major performance bug in Network Receiver",1373086492
399bd65ef5f780c2796f6facf9fac8fd9ec2c2f4 remotes/origin/benchmarks~19^2~118^2~4,"Fixed compile error due to merge
",1373048826
e7d49388e3df2cd3abb8fedc8a097831d4f33e3e remotes/origin/benchmarks~19^2~118^2~8,"Added unit test for K-means, and fixed some bugs
",1373048119
cffe3340c5346ab7c21d82cd9c456e9ad17e2e32 remotes/origin/benchmarks~19^2~118^2~10,"Fix logistic regression test failure and test suite cleanup
",1373048026
3c046a6eca182d720b58f5cdfd17b6201664e716 remotes/origin/benchmarks~19^2~118^2~17,"Some small fixes to ALS.
",1373048026
4dc13bf5be713443f22ca9c27530fb1d3cf48993 remotes/origin/benchmarks~19^2~118^2~33,"Revert back to closed form CV error
",1373048025
6ad85d0918460188172ffc1b23b3a2035f13dbcb remotes/origin/benchmarks~19^2~119,"Merge pull request #677 from jerryshao/fix_stage_clean

Clean StageToInfos periodically when spark.cleaner.ttl is enabled",1372998749
c0c3155c3c0e37ba31cbeeccb21654bdf5ec0cfc remotes/origin/benchmarks~19^2~115^2,"Bug fix: SPARK-789

https://spark-project.atlassian.net/browse/SPARK-789
",1372956850
94238aae57475030f6e88102a83c7809c5835494 remotes/origin/benchmarks~19^2~26^2~8^2,"fix dependencies
",1372874918
30b90342410a196d99d25113fbc1e37ce2a8200f remotes/origin/benchmarks~19^2~87^2~8,"Fixing bug where logs aren't shown
",1372711681
2943edf8eece20b21ff568b401cc5e8323ce9c07 remotes/origin/benchmarks~22,"Fixed another bug ..
",1372663470
ec31e68d5df259e6df001529235d8c906ff02a6f remotes/origin/benchmarks~19^2~122,"Fixed PySpark perf regression by not using socket.makefile(), and improved
debuggability by letting ""print"" statements show up in the executor's stderr

Conflicts:
	core/src/main/scala/spark/api/python/PythonRDD.scala
",1372659991
3296d132b6ce042843de6e7384800e089b49e5fa remotes/origin/benchmarks~19^2~123,"Fix performance bug with new Python code not using buffered streams
",1372659943
0791581346517c8fa55540703f667f30abba73a0 remotes/origin/benchmarks~23,"More bug fixes
",1372658860
e721ff7e5a2d738750c40e95d6ba53898eaa7051 remotes/origin/benchmarks~19^2~87^2~12,"Allowing details for failed stages
",1372530390
50ca17635a904f9496ccf996cd2f90325168bb9b remotes/origin/benchmarks~19^2~131,"Merge pull request #664 from pwendell/test-fix

Removing incorrect test statement",1372397092
92a4c2a5f6946bfae2136c52a22899db196f5799 remotes/origin/benchmarks~19^2~87^2~19,"Fixing bug in local scheduler time recording
",1372361586
15b00914c53f1f4f00a3313968f68a8f032e7cb7 remotes/origin/benchmarks~19^2~141,"Some fixes to the launch-java-directly change:

- Split SPARK_JAVA_OPTS into multiple command-line arguments if it
  contains spaces; this splitting follows quoting rules in bash
- Add the Scala JARs to the classpath if they're not in the CLASSPATH
  variable because the ExecutorRunner is launched with ""scala"" (this can
  happen when using local-cluster URLs in spark-shell)
",1372195047
f5e32ed13a7820fdc38f56dd9a19c160800fd652 remotes/origin/benchmarks~19^2~144,"Merge pull request #661 from mesos/streaming

Kafka fixes and DStream.count fix for master",1372177017
c3d11d0d57739ec3f08783f71bf4d0efdec3d627 remotes/origin/benchmarks~19^2~94^2~6,"Get rid of debugging statements
",1372143720
48c7e373c62b2e8cf48157ceb0d92c38c3a40652 remotes/origin/benchmarks~19^2~144^2~1,"Minor formatting fixes
",1372140664
575aff6b718f9447abd6dde40fd72c66b40774a8 remotes/origin/benchmarks~19^2~144^2~4,"Merge pull request #567 from Reinvigorate/sm-count-fix

Fixing count() in Spark Streaming",1372134950
42157027f247035e9ae41efe899e27c0942f5cd8 remotes/origin/benchmarks~19^2~87^2~27,"A few bug fixes and a unit test
",1372116305
93e8ed85aa1fbcb6428934b30d01f2b4090538b9 remotes/origin/benchmarks~19^2~87^2~30,"Work around for initalization issue
",1372104678
3e61beff7b41217a40afdccd1e413d9b90fe6e54 remotes/origin/benchmarks~19^2~148,"Merge pull request #648 from shivaram/netty-dbg

Shuffle fixes and cleanup",1371943367
3b7ebdeeb86868ff1ebd269c2ed832012f22e0d1 remotes/origin/benchmarks~19^2~87^2~33,"Handling entirely failed stages
",1371922297
7c5ff733ee1d3729b4b26f7c5542ca00c4d64139 remotes/origin/vldb~582^2~2,"PySpark daemon: fix deadlock, improve error handling
",1371831256
62c4781400dd908c2fccdcebf0dc816ff0cb8ed4 remotes/origin/vldb~582^2~4,"Add tests and fixes for Python daemon shutdown
",1371831256
ae7a5da6b31f5bf64f713b3d9bff6e441d8615b4 remotes/origin/vldb~585,"Fix some dependency issues in SBT build (same will be needed for Maven):

- Exclude a version of ASM 3.x that comes from HBase
- Don't use a special ASF repo for HBase
- Update SLF4J version
- Add sbt-dependency-graph plugin so we can easily find dependency trees
",1371746686
71030ba3ebb0a2bc371f51383aaf11e6c2dcfc05 remotes/origin/vldb~584^2,"Merge pull request #654 from lyogavin/enhance_pipe

fix typo and coding style in #638",1371680463
bad51c7cb4e15b68ce49ac4886e4631b9fb7e308 remotes/origin/benchmarks~19^2~112^2~1,"upmerge with latest mesos/spark master and fix hbase compile with hadoop2-yarn profile
",1371670753
0a2a9bce1e83e891334985c29176c6426b8b1751 remotes/origin/vldb~584^2^2,"fix typo and coding style
",1371591013
db42451a52ed5b0f228c87ddeb07c118e9d56ef6 remotes/origin/vldb~591,"Merge pull request #643 from adatao/master

Bug fix: Zero-length partitions result in NaN for overall mean & variance",1371507996
1044a95c9fa10292162322d74542e36384f68880 remotes/origin/vldb~394^2~14,"Merge pull request #652 from ScrapCodes/scala-2.10

Fixed maven build without netty fix",1371265464
6f28067f8d2fbb371b3614ff7712aa82278dcb11 remotes/origin/vldb~394^2~14^2,"Fixed maven build without netty fix
",1371224001
0e94b734bed4f8c7663ae3213dccd01e2c0fa003 remotes/origin/vldb~600,"Merge pull request #625 from stephenh/fix-start-slave

Fix start-slave not passing instance number to spark-daemon.",1370894431
190ec617997d621c11ed1aab662a6e3a06815d2f remotes/origin/vldb~593^2~1,"change code style and debug info
",1370849222
5b5b5aedbf01c84e49da884f361c1ca87f410628 remotes/origin/vldb~394^2~16,"Fixed a few test issues due to Akka 2.1, as well as SBT memory.

Unfortunately, in Akka 2.1, ActorSystem.awaitTermination hangs for
remote actors, and Akka also leaves a non-daemon Netty thread even when
run in daemon mode. Thus I had to comment out some of the calls to
awaitTermination, and we still have one failing test.
",1370678964
b58a29295b2e610cadf1cac44438337ce9b51537 remotes/origin/vldb~602,"Small formatting and style fixes
",1370670688
c8fc423bc2d6f9ed571bd0ad8c4c446650c72548 remotes/origin/vldb~603,"Merge pull request #631 from jerryshao/master

Fix block manager UI display issue when enable spark.cleaner.ttl",1370670198
c9ca0a4a588b4c7dc553b155336ae5b95aa9ddd4 remotes/origin/vldb~604,"Small code style fix to SchedulingAlgorithm.scala
",1370670044
1ae60bcb3673118c9838ecc6afac1d015fe79def remotes/origin/vldb~605,"Merge pull request #634 from xiajunluan/master

[Spark-753] Fix ClusterSchedulSuite unit test failed ",1370669946
d65d3dd8f083a826d7c8de0da8ae4680b14254d6 remotes/origin/benchmarks~18^2~67^2~25,"Fixing syntax error
",1370639761
9d359043574f6801ba15ec9d016eba0f00ac2349 remotes/origin/vldb~591^2~3,"In the current code, when both partitions happen to have zero-length, the return mean will be NaN.
Consequently, the result of mean after reducing over all partitions will also be NaN,
which is not correct if there are partitions with non-zero length. This patch fixes this issue.
",1370409167
fff3728552ccd2fc02fc4b7b750a9e3698869718 remotes/origin/vldb~606,"Merge pull request #640 from pwendell/timeout-update

Fixing bug in BlockManager timeout",1370387390
061fd3ae369e744f076e21044de26a00982a408f remotes/origin/vldb~606^2,"Fixing bug in BlockManager timeout
",1370386964
606bb1b450064a2b909e4275ce45325dbbef4eca remotes/origin/vldb~605^2,"Fix schedulingAlgorithm bugs for unit test
",1370226563
81c2adc15c9e232846d4ad0adf14d007039409fa remotes/origin/vldb~587^2~4,"Removing infix call
",1370157675
91aca9224936da84b16ea789cb81914579a0db03 remotes/origin/benchmarks~19^2~148^2~4,"Another round of Netty fixes.
1. Avoid race condition between stop and copier completion
2. Handle socket exceptions by reporting them and filling in a failed
FetchResult
",1370067698
926f41cc522def181c167b71dc919a0759c5d3f6 remotes/origin/vldb~603^2,"fix block manager UI display issue when enable spark.cleaner.ttl
",1369963972
ef77bb73c66ce938e409cbbac32b67badaa5c57d remotes/origin/vldb~609,"Merge pull request #627 from shivaram/master

Netty and shuffle  bug fixes",1369950606
b79b10a6d60a7f1f199e6bddd1243a05c57526ad remotes/origin/vldb~609^2~2,"Flush serializer to fix zero-size kryo blocks bug.
Also convert the local-cluster test case to check for non-zero block sizes
",1369813975
fbc1ab346867d5c81dc59e4c8d85aeda2f516ce2 remotes/origin/vldb~609^2~3,"Couple of Netty fixes
a. Fix the port number by reading it from the bound channel
b. Fix the shutdown sequence to make sure we actually block on the channel
c. Fix the unit test to use two JVMs.
",1369783636
26962c9340ac92b11d43e87200e699471d0b6330 remotes/origin/vldb~617^2,"Automatically configure Netty port. This makes unit tests using
local-cluster pass. Previously they were failing because Netty was
trying to bind to the same port for all processes.

Pair programmed with @shivaram.
",1369438773
69161f9cbceebd68ec593ebb1e587535029c8fa2 remotes/origin/vldb~618,"Merge pull request #622 from rxin/master

bug fix: Shuffle block iterator is ignoring the shuffle serializer setting.",1369431733
6ea085169d8ba2d09ca9236273d65238b8411f04 remotes/origin/vldb~618^2,"Fixed the bug that shuffle serializer is ignored by the new shuffle
block iterators for local blocks. Also added a unit test for that.
",1369429717
cda2b150412314c47c2c24883111bfc441c3a3a2 remotes/origin/vldb~614^2,"Use ec2-metadata in start-slave.sh.

PR #419 applied the same change, but only to start-master.sh,
so some workers were still starting their web UI's using internal
addresses.

This should finally fix SPARK-613.
",1369425906
bd3ea8f2a66de5ddc12dc1b2273e675d0abb8393 remotes/origin/vldb~613^2,"fix CheckpointRDD getPreferredLocations java.io.FileNotFoundException
",1369376779
ecd6d75c6a88232c40070baed3dd67bdf77f0c69 remotes/origin/vldb~593^2~4,"fix bug of unit tests
",1369090163
6c27c38451a9f7b7f870949d2838bc8f78f09892 remotes/origin/vldb~619^2~1,"Merge pull request #615 from rxin/build-fix

Maven build fix & two other small changes",1368750836
43644a293f5faec088530cf3a84d3680f2a103af remotes/origin/vldb~619^2~1^2,"Only check for repl classes if the user is running the repl. Otherwise,
check for core classes in run. This fixed the problem that core tests
depend on whether repl module is compiled or not.
",1368739898
c6e2770bfe940a4f4f26f75c9ba228faea7316f0 remotes/origin/vldb~593^2~7,"Fix ClusterScheduler bug to avoid allocating tasks to same slave
",1368738638
72b9c4cb6ec4080eb8751e5e040f180272ac82a6 remotes/origin/vldb~639^2~1^2~1,"Small fix
",1368341630
f25282def5826fab6caabff28c82c57a7f3fdcb8 remotes/origin/benchmarks~19^2~144^2~3^2,"fixing kafkaStream Java API and adding test
",1368228868
3632980b1b61dbb9ab9a3ab3d92fb415cb7173b9 remotes/origin/benchmarks~19^2~144^2~3^2~1,"fixing indentation
",1368222866
63e1999f6057bd397b49efe432ad74c0015a101b remotes/origin/vldb~607^2~1,"Merge pull request #606 from markhamstra/foreachPartition_fix

Actually use the cleaned closure in foreachPartition",1368219243
012c9e5ab072239e07202abe4775b434be6e32b9 remotes/origin/vldb~588^2~2,"Revert ""Merge pull request #596 from esjewett/master"" because the
dependency on hbase introduces netty-3.2.2 which conflicts with
netty-3.5.3 already in Spark. This caused multiple test failures.

This reverts commit 0f1b7a06e1f6782711170234f105f1b277e3b04c, reversing
changes made to aacca1b8a85bd073ce185a06d6470b070761b2f4.
",1368134401
0ab818d50812f312596170b5e42aa76d2ff59d15 remotes/origin/vldb~634^2,"fix linebreak
",1368085139
f6c965a98b2852d4f9b3cb3c00216cf750ff9738 remotes/origin/benchmarks~18^2~67^2~32,"Changing spark version and availability zone fix
",1368074761
8388e8dd7ab5e55ea67b329d9359ba2147d796b0 remotes/origin/vldb~632^2~3,"Minor style fix in DiskStore...
",1367977235
90577ada691f43be875c9d2ff4aaef962727f34d remotes/origin/vldb~632^2~8,"Merge branch 'shuffle-performance-fix-0.7' of github.com:shane-huang/spark into shufflemerge

Conflicts:
	core/src/main/scala/spark/storage/BlockManager.scala
	core/src/main/scala/spark/storage/DiskStore.scala
	project/SparkBuild.scala
",1367967379
7af92f248bdb977383696436f76362ff94e81a21 remotes/origin/vldb~642,"Merge pull request #597 from JoshRosen/webui-fixes

Two minor bug fixes for Spark Web UI",1367731757
6fe9d4e61e30622abdbf4877daf5653d7339e4e8 remotes/origin/vldb~646,"Merge pull request #592 from woggling/localdir-fix

Don't accept generated local directory names that can't be created",1367555636
538ee755b41585c638935a93ec838b635149f659 remotes/origin/vldb~647,"Merge pull request #581 from jerryshao/master

fix [SPARK-740] block manage UI throws exception when enabling Spark Streaming",1367510502
dfde9ce9dde0a151d42f7aecb826b40a4c98b459 remotes/origin/vldb~639^2^2~6,"comment out debug versions of checkHost, etc from Utils - which were used to test
",1367460693
d960e7e0f83385d8f43129d53c189b3036936daf remotes/origin/vldb~639^2^2~10,"a) Add support for hyper local scheduling - specific to a host + port - before trying host local scheduling.

b) Add some fixes to test code to ensure it passes (and fixes some other issues).

c) Fix bug in task scheduling which incorrectly used availableCores instead of all cores on the node.
",1367420040
aa8fe1a20923d4f86f04398d2cf70de14b04ab50 remotes/origin/vldb~646^2~1,"Merge pull request #586 from mridulm/master

Pull request to address issues Reynold Xin reported",1367386218
dd7bef31472e8c7dedc93bc1519be5900784c736 remotes/origin/vldb~638^2~4,"Two minor fixes according to Ryan LeCompte's review.
",1367359352
e46d547ccd43c0fb3a79a30a7c43a78afba6f93f remotes/origin/vldb~653^2,"Fix issues reported by Reynold
",1367318756
540be6b1544d26c7db79ec84a98fc6696c7c6434 remotes/origin/vldb~646^2~2^2~1,"Modified version of the fix which just removes all per-record tracking.
",1367260327
0f45347c7b7243dbf54569f057a3605f96d614af remotes/origin/vldb~655,"More unit test fixes
",1367213367
f6ee9a87285402ae32dda24bfa1793a4ae12429f remotes/origin/vldb~658,"Merge pull request #583 from mridulm/master

Fix issues with streaming test cases after yarn branch merge",1367188564
430c531464a5372237c97394f8f4b6ec344394c0 remotes/origin/vldb~653^2~1,"Remove debug statements
",1367175270
3a89a76b874298853cf47510ab33e863abf117d7 remotes/origin/vldb~653^2~2,"Make log message more descriptive to aid in debugging
",1367174052
afee9024430ef79cc0840a5e5788b60c8c53f9d2 remotes/origin/vldb~653^2~5^2,"Attempt to fix streaming test failures after yarn branch merge
",1367168205
6e6b5204ea015fc7cc2c3e16e0032be3074413be remotes/origin/vldb~661,"Create an empty directory when checkpointing a 0-partition RDD (fixes a
test failure on Hadoop 2.0)
",1366875757
3b594a4e3b94de49a09dc679a30d857e3f41df69 remotes/origin/vldb~664^2,"Do not add signature files - results in validation errors when using assembled file
",1366778905
dd515ca3ee011cbf9b6941bb45afc55fd905fda8 remotes/origin/vldb~664^2~1,"Attempt at fixing merge conflict
",1366775657
d09db1c051d255157f38f400fe9301fa438c5f41 remotes/origin/vldb~653^2~6,"concurrentRestrictions fails for this PR - but works for master, probably some version change
",1366775129
a402b23bcd9a9470c5fa38bf46f150b51d43eac9 remotes/origin/vldb~653^2~18,"Fudge order of classpath - so that our jars take precedence over what is in CLASSPATH variable. Sounds logical, hope there is no issue cos of it
",1366158120
ad80f68eb5d153d7f666447966755efce186d021 remotes/origin/vldb~653^2~20,"remove spurious debug statements
",1366130734
323ab8ff3b822af28276e1460db0f9c73d6d6409 remotes/origin/vldb~653^2~22,"Scala does not prevent variable shadowing ! Sick error due to it ...
",1366112110
b493f55a4fe43c83061a361eef029edbac50c006 remotes/origin/vldb~632^2~8^2,"fix a bug in netty Block Fetcher

Signed-off-by: shane-huang <shengsheng.huang@intel.com>
",1366077661
dd2b64ec97ad241b6f171cac0dbb1841b185675a remotes/origin/vldb~653^2~24,"Fix bug with atomic update
",1366062564
5540ab8243a8488e30a21e1d4bb1720f1a9a555f remotes/origin/vldb~653^2~25,"Use hostname instead of hostport for executor, fix creation of workdir
",1366061263
b42d68c8ce9f63513969297b65f4b5a2b06e6078 remotes/origin/benchmarks~19^2~144^2~4^2~1,"fixing Spark Streaming count() so that 0 will be emitted when there is nothing to count
",1366052095
19652a44be81f3b8fbbb9ecc4987dcd933d2eca9 remotes/origin/vldb~653^2~27,"Fix issue with FileSuite failing
",1366033596
54b3d45b816f26a9d3509c1f8bea70c6d99d3de0 remotes/origin/vldb~653^2~28,"Checkpoint commit - compiles and passes a lot of tests - not all though, looking into FileSuite issues
",1366030610
d90d2af1036e909f81cf77c85bfe589993c4f9f3 remotes/origin/vldb~653^2~29,"Checkpoint commit - compiles and passes a lot of tests - not all though, looking into FileSuite issues
",1366029731
c35d530bcfea2e1764863eb9f47a794d8fa001af remotes/origin/vldb~670,"Fix compile error
",1365871392
271a4f3bb6ed4a8b2ff012e10115154e7437a376 remotes/origin/vldb~676,"Merge pull request #555 from holdenk/master

Retry failed ssh commands in the ec2 python script.",1365570292
8ac9efba5a435443be9abf8ebbe867806d42c9db remotes/origin/benchmarks~19^2~144^2~5,"Merge pull request #527 from Reinvigorate/sm-kafka-cleanup

KafkaInputDStream fixes and improvements",1365540650
054feb6448578de5542f9ef54d4cc88f706c22f5 remotes/origin/vldb~680,"Fixed a bug with zip
",1365383721
df47b40b764e25cbd10ce49d7152e1d33f51a263 remotes/origin/vldb~632^2~8^2~1,"Shuffle Performance fix: Use netty embeded OIO file server instead of ConnectionManager
Shuffle Performance Optimization: do not send 0-byte block requests to reduce network messages
change reference from io.Source to scala.io.Source to avoid looking into io.netty package

Signed-off-by: shane-huang <shengsheng.huang@intel.com>
",1365316632
ff2130a0ad17388036b66fcdf2b1848e208fa0f8 remotes/origin/vldb~676^2~1,"Retry failed ssh commands. This is especially useful during system startup when the hosts may not have yet come on-line but can be useful at other points for people with flakey connections
",1365233750
d40c1d51229070660cb0c2f0d8fd04954c9d8ab7 remotes/origin/benchmarks~39,"Added unit test and fix a partitioner problem.
",1365147230
d510045d8c156beafaa3e4832fac739ff5309240 remotes/origin/benchmarks~47,"fixing the the silly bug in the dynamic pagerank gather function
",1365128646
db45cf3a497ffdde17a4964d8c31da6e995ad03a remotes/origin/benchmarks~52,"Fixing several bugs in mapReduceNeighborhood.  First a map is used instead of a foreach which for mysterious reasons meant that the map never seems to be executed?  Switching to a foreach causes a null pointer exception since the body of the foreach did not properly initialize the temporary EdgeWithVertex data structure.
",1365115022
93eca18a62cc34fee044c5e5e4b01a53353cdd27 remotes/origin/benchmarks~53,"fixing a silly bug whereby the pagerank equation was implemented incorrectly (divided by degree of dst instead of source).
",1365114907
b5d78307ca79b6c8c0f2733750e546ea5b0b0bb1 remotes/origin/vldb~689,"Merge pull request #551 from jey/python-bugfixes

Python bugfixes",1364955825
2b373dd07a7b3f2906607d910c869e3290ca9d05 remotes/origin/vldb~593^2~11,"add properties default value null to fix sbt/sbt test errors
",1364875874
9831bc1a09fa7fd56f55c3df4f448bc37b3d7d9a remotes/origin/vldb~691,"Merge pull request #539 from cgrothaus/fix-webui-workdirpath

Bugfix: WorkerWebUI must respect workDirPath from Worker",1364620582
cad507adaf22f27393067b4d3da4f718cd294196 remotes/origin/vldb~693,"Merge pull request #547 from jey/maven-streaming-tests-initialization-fix

Move streaming test initialization into 'before' blocks",1364620392
329ef34c2e04d28c2ad150cf6674d6e86d7511ce remotes/origin/benchmarks~19^2~144^2~5^2,"fixing autooffset.reset behavior when set to 'largest'
",1364363775
bc36ee4fbbe3ad3b7e15fbdba53de42a29b81a9f remotes/origin/vldb~696,"Merge pull request #543 from holdenk/master

Re-enable deprecation warnings and fix deprecated warning.",1364335513
445f387ef4213b5b63f28cbc011236c2aba1d1c9 remotes/origin/vldb~691^2,"Bugfix: WorkerWebUI must respect workDirPath from Worker
",1363946920
d9f34e505d88daa6e3665b40ab70dab41e277c9d remotes/origin/vldb~394^2~27^2~7,"Ctrl-D hang bug fixed!
",1363718884
432a227320e505a1790d6fb22463ab3eba4fc830 remotes/origin/vldb~394^2~27^2~8,"fixed autocompletion apparent hang due to logging
",1363676348
9784fc1fcd88dc11dda6cf5a6e44e49c49f1143a remotes/origin/vldb~699^2~1,"fix wayward comma in doc comment
",1363472702
cdbfd1e196b6bfada7c0161305705914a0bbf917 remotes/origin/vldb~708,"Merge pull request #516 from squito/fix_local_metrics

Fix local metrics",1363385608
f9fa2add5c7a7d8b0b4802e7c5234e2dc1d5a292 remotes/origin/vldb~709,"Merge pull request #530 from mbautin/master-update-log4j-and-make-compile-in-IntelliJ

Add a log4j compile dependency to fix build in IntelliJ",1363385563
7fd2708edaa863701ed8032e395e255df399d898 remotes/origin/vldb~709^2,"Add a log4j compile dependency to fix build in IntelliJ

Also rename parent project to spark-parent (otherwise it shows up as
""parent"" in IntelliJ, which is very confusing).
",1363372911
d06928321194b11e082986cd2bb2737d9bc3b698 remotes/origin/benchmarks~19^2~144^2~5^2~3,"fixing memory leak in kafka MessageHandler
",1363326333
1c3d98197b120e2a81f59bd9315d3892ef4d24ca remotes/origin/benchmarks~19^2~144^2~6,"Merge pull request #517 from Reinvigorate/sm-build-fixes

Build fixes for streaming /w SBT",1363227828
91a9d093bdc1d8d0bc26a70fa11d1e9dc3e82820 remotes/origin/vldb~717,"Merge pull request #512 from patelh/fix-kryo-serializer

Fix reference bug in Kryo serializer, add test, update version",1362955703
b0983c5762b583c186a3b64606fa2625af962940 remotes/origin/vldb~718^2,"Notify standalone deploy client of application death.

Usually, this isn't necessary since the application will be removed
as a result of the deploy client disconnecting, but occassionally, the
standalone deploy master removes an application otherwise.

Also mark applications as FAILED instead of FINISHED when they are
killed as a result of their executors failing too many times.
",1362857385
664e5fd24b64d5062b461a9e4f0b72cec106cdbc remotes/origin/vldb~717^2,"Fix reference bug in Kryo serializer, add test, update version
",1362723371
94b3db136d557bd1501b0d440b51e143369c77c4 remotes/origin/vldb~724,"Merge pull request #508 from markhamstra/TestServerInUse

Avoid bind failure in InputStreamsSuite",1362291232
b40907310266be1be5db5f773bc9bcbf2813c090 remotes/origin/vldb~724^2,"Instead of failing to bind to a fixed, already-in-use port, let the OS choose an available port for TestServer.
",1362179107
4223be3aa475fe938daeec161ca8fbd00fd9c5fa remotes/origin/vldb~736^2,"Merge pull request #503 from pwendell/bug-fix

createNewSparkContext should use sparkHome/jars/environment.",1361850185
284ba90958df2d6efc08e3f8381bb9ef09f8b322 remotes/origin/vldb~736^2^2,"createNewSparkContext should use sparkHome/jars/environment.

This fixes a bug introduced by Matei's recent change.
",1361850052
c45d6f1248de6dc1481ed021f43b8e81d301207e remotes/origin/vldb~740,"Merge pull request #501 from tdas/master

Fixed bug in BlockManager and added a testcase",1361842531
c02e064938059133570547b686d655f2a543964e remotes/origin/vldb~740^2,"Fixed replication bug in BlockManager
",1361842066
4d480ec59e8cf268054ed805abcd1e84eca17b41 remotes/origin/vldb~741,"Fixed something that was reported as a compile error in ScalaDoc.

For some reason, ScalaDoc complained about no such constructor for
StreamingContext; it doesn't seem like an actual Scala error but it
prevented sbt publish and from working because docs weren't built.
",1361836423
271a40f0e435664be047e7c1506766b6789fcb78 remotes/origin/vldb~745^2,"Merge pull request #499 from pwendell/streaming-docs

Some changes to streaming failure docs.",1361832452
50ce0516e6bce2a528a28e4719713802923f68ab remotes/origin/vldb~745^2^2,"Some changes to streaming failure docs.

TD gave me the go-ahead to just make these changes:
- Define stateful dstream
- Some minor wording fixes
",1361831919
6b87ef7c8625260e6b99cd6c6ebd16761efd5cda remotes/origin/vldb~750,"Fix compile error
",1361829676
fb7625059837b124da1e31bd126f5278eef68bf9 remotes/origin/vldb~758,"Merge pull request #497 from ScrapCodes/dep-resolution-fix

Moving akka dependency resolver to shared.",1361780212
32f1a2d540796dc8e0c9ba3a32cb1cf098362288 remotes/origin/vldb~759^2~1,"Merge pull request #493 from tdas/streaming

Streaming bug fixes",1361745979
c1a040db3a82fcd9b5da8a1af3a79d19ace99005 remotes/origin/vldb~759^2~1^2~4,"Fixed bugs in examples.
",1361732430
c8a788692185326c001233bb249d2ed046cd7319 remotes/origin/vldb~765^2~1,"Detect when SendingConnections drop by trying to read them.

Comment fix
",1361578312
d4d7993bf5106545ae1056fb6e8d7e2601f60535 remotes/origin/vldb~766,"Several fixes to the work to log when no resources can be used by a job.

Fixed some of the messages as well as code style.
",1361577097
f33662c133d628d51718dec070a9096888330c58 remotes/origin/vldb~767,"Merge remote-tracking branch 'pwendell/starvation-check'

Also fixed a bug where master was offering executors on dead workers

Conflicts:
	core/src/main/scala/spark/deploy/master/Master.scala
",1361575661
334ab9244113e4b792fd51697ef80ab0d3b3de25 remotes/origin/vldb~759^2~5^2~1,"Fixed bug in CheckpointSuite
",1361384796
4e5b09664cdf95effff61c042b6243107355b55c remotes/origin/vldb~759^2~4^2~1,"fixes corresponding to review feedback at pull request #479
",1361367892
991d3342fed1cf5626142bc90872f79618ea94c8 remotes/origin/vldb~759^2~6,"Merge pull request #486 from ScrapCodes/akka-example-bug-fix

A bug fix to AkkaWordCount example.",1361354796
05dc385649277836962a512a83195083990a7134 remotes/origin/vldb~759^2~6^2,"A bug fix post merge, following changes to AkkaUtils
",1361354292
8a992226bd6289cfc11c417e0a17edff7d4a4a87 remotes/origin/vldb~771,"Merge pull request #484 from andyk/master

Fixes a broken link in documentation to issue tracker",1361322444
ecd137a72da189c52b92a1286b004740706bd936 remotes/origin/vldb~771^2,"Fixes link to issue tracker in documentation page ""Contributing to Spark"".
",1361321882
d0588bd6d7da3ba5adaba24303ad8616bdc2484f remotes/origin/vldb~770^2~1,"Catch/log errors deleting temp dirs
",1361307846
8b9c673fce1c733c7fcd8b978e84f943be9e9e35 remotes/origin/vldb~759^2~8,"Merge pull request #476 from tdas/streaming

Major modifications to fix driver fault-tolerance with file input stream",1361272030
f98c7da23ef66812b8b4888230ee98c07f09af23 remotes/origin/vldb~759^2~8^2~2^2~1^2~1,"Many changes to ensure better 2nd recovery if 2nd failure happens while
recovering from 1st failure
- Made the scheduler to checkpoint after clearing old metadata which
  ensures that a new checkpoint is written as soon as at least one batch
  gets computed  while recovering from a failure. This ensures that if
  there is a 2nd failure while recovering from 1st failure, the system
  start 2nd recovery from a newer checkpoint.
- Modified Checkpoint writer to write checkpoint in a different thread.
- Added a check to make sure that compute for InputDStreams gets called
  only for strictly increasing times.
- Changed implementation of slice to call getOrCompute on parent DStream
  in time-increasing order.
- Added testcase to test slice.
- Fixed testGroupByKeyAndWindow testcase in JavaAPISuite to verify
  results with expected output in an order-independent manner.
",1361142401
4b8402e900c803e64b8a4e2094fd845ccfc9df36 remotes/origin/vldb~759^2~8^2~2^2~1^2~3,"Moved Java streaming examples to examples/src/main/java/spark/streaming/... and fixed logging in NetworkInputTracker to highlight errors when receiver deregisters/shuts down.
",1360894237
e8663e0fe5550d3ba0046ac2ba464524aef8d0cd remotes/origin/vldb~784,"Merge pull request #461 from JoshRosen/fix/issue-tracker-link

Update issue tracker link in contributing guide",1360809737
39addd380363c0371e935fae50983fe87158c1ac remotes/origin/vldb~759^2~8^2~2^2~1^2~8,"Changed scheduler and file input stream to fix bugs in the driver fault tolerance. Added MasterFailureTest to rigorously test master fault tolerance with file input stream.
",1360786665
fd7e414bd0eab4f8d82e225d9981d2eba036e756 remotes/origin/vldb~785,"Merge pull request #464 from pwendell/java-type-fix

SPARK-694: All references to [K, V] in JavaDStreamLike should be changed to [K2, V2] ",1360639145
bfeed4725df47b578cb0e735e1ffbc836acde673 remotes/origin/vldb~786,"Merge pull request #465 from pwendell/java-sort-fix

SPARK-696: sortByKey should use 'ascending' parameter",1360635792
582d31dff99c161a51e15497db983a4b5a6d4cdb remotes/origin/vldb~787,"Formatting fixes
",1360617894
e9fb25426ea0b6dbe4c946243a2ac0836b031c58 remotes/origin/vldb~768^2,"Remove hack workaround for SPARK-668.

Renaming the type paramters solves this problem (see SPARK-694).

I tried this fix earlier, but it didn't work because I didn't run
`sbt/sbt clean` first.
",1360610360
04786d07391c4052d6dc42ff0828a79a37bbbfdf remotes/origin/vldb~785^2~1,"small fix
",1360605949
314d87a038d84c4ae9a6471ea19a5431153ea604 remotes/origin/vldb~785^2~4,"Indentation fix
",1360605817
da8afbc77e5796d45686034db5560f18c057d3c9 remotes/origin/vldb~789,"Some bug and formatting fixes to FT

Conflicts:
	core/src/main/scala/spark/scheduler/cluster/SparkDeploySchedulerBackend.scala
	core/src/main/scala/spark/scheduler/cluster/StandaloneSchedulerBackend.scala
",1360565018
1b47fa275236657bea358f5c95d89f568c439395 remotes/origin/vldb~790,"Detect hard crashes of workers using a heartbeat mechanism.

Also fixes some issues in the rest of the code with detecting workers this way.

Conflicts:
	core/src/main/scala/spark/deploy/master/Master.scala
	core/src/main/scala/spark/deploy/worker/Worker.scala
	core/src/main/scala/spark/scheduler/cluster/SparkDeploySchedulerBackend.scala
	core/src/main/scala/spark/scheduler/cluster/StandaloneClusterMessage.scala
	core/src/main/scala/spark/scheduler/cluster/StandaloneSchedulerBackend.scala
",1360564108
0b788b760bc3b2d4d986acb9f6f04592aca9be26 remotes/origin/vldb~793,"Update Windows scripts to launch daemons with less RAM and fix a few
other issues

Conflicts:
	run2.cmd
",1360561909
fd90daf850a922fe33c3638b18304d827953e2cb remotes/origin/vldb~759^2~8^2~2^2~1^2~9,"Fixed bugs in FileInputDStream and Scheduler that occasionally failed to reprocess old files after recovering from master failure. Completely modified spark.streaming.FailureTest to test multiple master failures using file input stream.
",1360554522
16baea62bce62987158acce0595a0916c25b32b2 remotes/origin/vldb~759^2~8^2~2^2~1^2~10,"Fixed bug in CheckpointRDD to prevent exception when the original RDD had zero splits.
",1360552489
131b56afd0ec20b92502e11acda77c6594380471 remotes/origin/vldb~784^2,Update issue tracker link in contributing guide.,1360531711
2ed791fd7fa193ea7e10d70e1c1b0787d584b0fd remotes/origin/vldb~767^2,"Minor fixes
",1360476038
04e828f7c13b83eff84cd2e57a57a821898b5e51 remotes/origin/vldb~722^2~23,"general fixes to Distribution, plus some tests
",1360379256
03eefbb2005e9e88fd79eecef3fc612e9f2ee623 remotes/origin/vldb~802,"Merge pull request #451 from stephenh/fixdeathpactexception

Handle Terminated to avoid endless DeathPactExceptions.",1360117674
870b2aaf5d1398704f69a5b1a8be30de522b284c remotes/origin/vldb~802^2,"Merge branch 'master' into fixdeathpactexception

Conflicts:
	core/src/main/scala/spark/deploy/worker/Worker.scala
",1360117629
0e19093fd89ec9740f98cdcffd1ec09f4faf2490 remotes/origin/vldb~802^2~1,"Handle Terminated to avoid endless DeathPactExceptions.

Credit to Roland Kuhn, Akka's tech lead, for pointing out this
various obvious fix, but StandaloneExecutorBackend.preStart's
catch block would never (ever) get hit, because all of the
operation's in preStart are async.

So, the System.exit in the catch block was skipped, and instead
Akka was sending Terminated messages which, since we didn't
handle, it turned into DeathPactException, which started
a postRestart/preStart infinite loop.
",1360112280
f6ec547ea7b56ee607a4c2a69206f8952318eaf1 remotes/origin/vldb~805,"Small fix to test for distinct
",1360012494
aa4ee1e9e5485c1b96474e704c76225a2b8a7da9 remotes/origin/vldb~806,"Fix failing test
",1360004791
f7b4e428be75d189b9ae50c4302c08f3c49e0161 remotes/origin/vldb~807,"Merge pull request #445 from JoshRosen/pyspark_fixes

Fix exit status in PySpark unit tests; fix/optimize PySpark's RDD.take()",1359956196
d5daaab381358dbe61f51134dbb3d06f44044c6d remotes/origin/vldb~811,"Merge pull request #442 from stephenh/fixsystemnames

Fix createActorSystem not actually using the systemName parameter.",1359877126
2415c18f48fc28d88f29b88c312f98054f530f20 remotes/origin/vldb~807^2~2,"Fix reporting of PySpark doctest failures.
",1359873851
28e0cb9f312b7fb1b0236fd15ba0dd2f423e826d remotes/origin/vldb~811^2,"Fix createActorSystem not actually using the systemName parameter.

This meant all system names were ""spark"", which worked, but didn't
lead to the most intuitive log output.

This fixes createActorSystem to use the passed system name, and
refactors Master/Worker to encapsulate their system/actor names
instead of having the clients guess at them.

Note that the driver system name, ""spark"", is left as is, and is
still repeated a few times, but that seems like a separate issue.
",1359789097
9cc6ff9c4e7eec2d62261fc166ad2ebade148752 remotes/origin/vldb~822^2,"Do not launch JavaGateways on workers (SPARK-674).

The problem was that the gateway was being initialized whenever the
pyspark.context module was loaded.  The fix uses lazy initialization
that occurs only when SparkContext instances are actually constructed.

I also made the gateway and jvm variables private.

This change results in ~3-4x performance improvement when running the
PySpark unit tests.
",1359745990
5ce5efec104364ea6e97965ab5757db7d66f355e remotes/origin/vldb~824,"Merge pull request #435 from JoshRosen/pyspark_stdout_fix

Fix stdout redirection in PySpark.",1359707527
f127f2ae76692b189d86b5a47293579d5657c6d5 remotes/origin/vldb~809^2~2,"fixup merge (master -> driver renaming)
",1359706849
39ab83e9577a5449fb0d6ef944dffc0d7cd00b4a remotes/origin/vldb~825^2,"Small fix from last commit
",1359697972
3190483b98a62b646311dff199417823efacbb47 remotes/origin/vldb~761^2~8,"bug fix for javadoc
",1359613431
d54b10b6ad25ebf29f8ca7f438131ed6a9af24ab remotes/origin/benchmarks~76,"Merge remote-tracking branch 'stephenh/removefailedjob'

Conflicts:
	core/src/main/scala/spark/deploy/master/Master.scala
",1359511949
ccb67ff2cae366973a1a2e7eac57db4e861a4ca7 remotes/origin/benchmarks~77,"Merge pull request #425 from stephenh/toDebugString

Add RDD.toDebugString.",1359485058
951cfd9ba2a9239a777f156f10af820e9df49606 remotes/origin/benchmarks~77^2~2,"Add JavaRDDLike.toDebugString().
",1359439337
b45857c965219e2d26f35adb2ea3a2b831fdb77f remotes/origin/benchmarks~77^2~3,"Add RDD.toDebugString.

Original idea by Nathan Kronenfeld.
",1359439016
ad4232b4dadc6290d3c4696d3cc007d3f01cb236 remotes/origin/benchmarks~88^2~1,"Fix deadlock in BlockManager reregistration triggered by failed updates.
",1359253838
a1d9d1767d821c1e25e485e32d9356b12aba6a01 remotes/origin/benchmarks~87^2~1,"fixup  1cadaa1, changed api of map
",1359137126
b6fc6e67521e8a9a5291693cce3dc766da244395 remotes/origin/benchmarks~92^2,"SPARK-541: Adding a warning for invalid Master URL

Right now Spark silently parses master URL's which do not match any
known regex as a Mesos URL. The Mesos error message when an invalid URL gets
passed is really confusing, so this warns the user when the implicit
conversion is happening.
",1359066683
1dd82743e09789f8fdae2f5628545c0cb9f79245 remotes/origin/benchmarks~97,"Fix compile error due to cherry-pick
",1358975247
4d77d554e1088d6953d1fb1dd634979648293464 remotes/origin/benchmarks~101,"Merge pull request #394 from JoshRosen/add_file_fix

Add SparkFiles.get() API to access files added through addFile().",1358972190
b47d054cfc5ef45b92a1c970388722ffa0283e66 remotes/origin/benchmarks~93^2,"Remove use of abc.ABCMeta due to cloudpickle issue.

cloudpickle runs into issues while pickling subclasses of AccumulatorParam,
which may be related to this Python issue:

    http://bugs.python.org/issue7689

This seems hard to fix and the ABCMeta wasn't necessary, so I removed it.
",1358970447
ae2ed2947d43860c74a8d40767e289ca78073977 remotes/origin/benchmarks~101^2,"Allow PySpark's SparkFiles to be used from driver

Fix minor documentation formatting issues.
",1358967530
666ce431aa03239d580a8c78b3a2f34a851eb413 remotes/origin/vldb~759^2~9^2~4,"Added support for rescheduling unprocessed batches on master failure.
",1358939736
79d55700ce2559051ac61cc2fb72a67fd7035926 remotes/origin/benchmarks~100^2,"One more fix. Made even default constructor of BlockManagerId private to prevent such problems in the future.
",1358935029
0b506dd2ecec909cd514143389d0846db2d194ed remotes/origin/benchmarks~96^2~3,"Add tests of various node failure scenarios.
",1358933895
d209b6b7641059610f734414ea05e0494b5510b0 remotes/origin/benchmarks~96^2~4,"Extra debugging from hostLost()
",1358933714
05f13280de92cf33b776b2e46bb0443b97cd248b remotes/origin/benchmarks~105,"Merge pull request #404 from mbautin/master-fix-avro-dependency-in-repl

Add an Avro dependency to REPL to make it compile with Hadoop 2",1358913592
fad2b82fc8fb49f2171af10cf7e408d8b8dd7349 remotes/origin/vldb~759^2~9^2~5,"Added support for saving input files of FileInputDStream to graph checkpoints. Modified 'file input stream with checkpoint' testcase to test recovery of pre-master-failure input files.
",1358907000
35168d9c89904f0dc0bb470c1799f5ca3b04221f remotes/origin/benchmarks~101^2~1,Fix sys.path bug in PySpark SparkContext.addPyFile,1358906051
e353886a8ca6179f25b4176d7a62b5d04ce79276 remotes/origin/benchmarks~96^2~6,"Use generation numbers for fetch failure tracking
",1358843011
7d3e359f2c463681cf0128da2c6692beb13dade9 remotes/origin/benchmarks~109,"Merge pull request #395 from stephenh/fixrecursion

Remove unneeded/now-broken saveAsNewAPIHadoopFile overload.",1358820691
4d34c7fc3ecd7a4d035005f84c01e6990c0c345e remotes/origin/benchmarks~114,"Fix compile error caused by cherry-pick
",1358796828
5bf73df7f08b17719711a5f05f0b3390b4951272 remotes/origin/benchmarks~118,"oops, fix stupid compile error
",1358796093
c0b9ceb8c3d56c6d6f6f6b5925c87abad06be646 remotes/origin/benchmarks~122,"Log remote lifecycle events in Akka for easier debugging
",1358756633
6e3754bf4759ab3e1e1be978b6b84e6f17742106 remotes/origin/benchmarks~124,"Add Maven build file for streaming, and fix some issues in SBT file

As part of this, changed our Scala 2.9.2 Kafka library to be available
as a local Maven repository, following the example in
(http://blog.dub.podval.org/2010/01/maven-in-project-repository.html)
",1358738544
17035db159e191a11cd86882c97078581073deb2 remotes/origin/benchmarks~128,"Add __repr__ to Accumulator; fix bug in sc.accumulator
",1358711937
2a8c2a67909c4878ea24ec94f203287e55dd3782 remotes/origin/benchmarks~130,"Minor formatting fixes
",1358706293
922c5ec069285116219df72d926cf7ff71d50cda remotes/origin/benchmarks~132,"Merge pull request #385 from pwendell/ec2-guide-fix

Clarifying log directory in EC2 guide",1358705138
fd6e51deec83f01be3db41e84255329eedbe15da remotes/origin/vldb~394^2~28^2,"Fixed the failing test.
",1358697778
33bad85bb9143d41bc5de2068f7e8a8c39928225 remotes/origin/benchmarks~72^2~3^2,"Fixed streaming testsuite bugs
",1358682671
214345ceace634ec9cc83c4c85b233b699e0d219 remotes/origin/benchmarks~72^2~3^2~2,"Fixed issue https://spark-project.atlassian.net/browse/STREAMING-29, along with updates to doc comments in SparkContext.checkpoint().
",1358668217
6fba7683c29b64a33a6daa28cc56bb5d20574314 remotes/origin/benchmarks~72^2~4^2~3,"Small doc fix
",1358477184
70ba994d6d6f9e62269168e6a8a61ffce736a4d2 remotes/origin/benchmarks~72^2~4^2~5,"Import fixup
",1358476919
2a872335c5c7b5481c927272447e4a344ef59dda remotes/origin/benchmarks~72^2~4^2~11,"Bug fix and test cleanup
",1358476918
a512df551f85086a6ec363744542e74749c6b560 remotes/origin/benchmarks~131^2~1,"Fixed index error missing first argument
",1358431527
eded21925ab549330d0337138fa1f81ae192e3e9 remotes/origin/benchmarks~72^2~5,"Merge pull request #375 from tdas/streaming

Important bug fixes",1358233600
8ad6220bd376b04084604cf49b4537c97a16257d remotes/origin/benchmarks~72^2~4^2~13,"Bugfix
",1358204196
ae5290f4a2fbeb51f5dc6e7add38f9c012ab7311 remotes/origin/benchmarks~72^2~4^2~15,"Bug fix
",1358199092
d182a57cae6455804773db23d9498d2dcdd02172 remotes/origin/benchmarks~72^2~4^2~17,"Two changes:

- Updating countByX() types based on bug fix
- Porting new documentation to Java
",1358186635
3461cd99b7b680be9c9dc263382b42f30c9edd7d remotes/origin/benchmarks~72^2~4^2~19,"Flume example and bug fix
",1358185356
c2537057f9ed8723d2c33a1636edf9c9547cdc66 remotes/origin/benchmarks~72^2~4^2~22,"Fixing issue with <Long> types
",1358185356
131be5d62ef6b770de5106eb268a45bca385b599 remotes/origin/benchmarks~72^2~5^2~1,"Fixed bug in RDD checkpointing.
",1358162905
f90f794cde479f4de425e9be0158a136a57666a2 remotes/origin/benchmarks~72^2~6^2,"Minor name fix
",1358141157
6cc8592f26553525e11213830b596fc397243439 remotes/origin/benchmarks~72^2~6^2~1,"Fixed bug
",1358140849
44b3e41f2ede20c30bc540439d705e9ff8075ee1 remotes/origin/benchmarks~144,"Merge pull request #367 from ericzhang-cn/master

fix spelling mistake in LocalLR example",1358062637
ba06e9c97cc3f8723ffdc3895182c529d3bb2fb3 remotes/origin/benchmarks~144^2,"Update examples/src/main/scala/spark/examples/LocalLR.scala

fix spelling mistake",1358062391
3ef82496f43cb2240f4a36b4778a4870ad4f8b17 remotes/origin/benchmarks~146,"Merge pull request #363 from heuermh/quick-start-fix

add repositories section to quick-start SimpleJob pom.xml",1358036110
248995c535713671366f9a6cd7e34ddccf00bb44 remotes/origin/benchmarks~149^2~2,"Merge pull request #356 from shane-huang/master

Fix an issue in ConnectionManager where sendMessage may create too many unnecessary connections",1357869143
b15e8512793475eaeda7225a259db8aacd600741 remotes/origin/benchmarks~121^2~5,"Check for AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY environment variables.

For custom properties, use ""spark.hadoop.*"" as a prefix instead of just ""hadoop.*"".
",1357836941
1a64432ba50904c3933d8a9539a619fc94b3b30b remotes/origin/benchmarks~143^2~2,"Indicate success/failure in PySpark test script.
",1357792236
e4cb72da8a5428c6b9097e92ddbdf4ceee087b85 remotes/origin/benchmarks~149^2~2^2~1,"Fix an issue in ConnectionManager where sendingMessage may create too many unnecessary SendingConnections.
",1357656058
a37adfa67bac51b2630c6e1673f8607a87273402 remotes/origin/benchmarks~153,"Merge pull request #354 from shivaram/ibm-jdk-fixes

Fixes to build and test spark on IBM JVM",1357609023
aed368a970bbaee4bdf297ba3f6f1b0fa131452c remotes/origin/benchmarks~147^2~8,"Update Hadoop dependency to 1.0.3 as 0.20 has Sun specific dependencies. Also
fix SequenceFileRDDFunctions to pick the right type conversion across Hadoop
versions
",1357603053
e60514d79e753f38db06f8d3df20e113ba4dc11a remotes/origin/benchmarks~72^2~9^2~1,"Fixed bug
",1357600576
c438faeac42ea3dbbd7d3c3c6873611bca88c69e remotes/origin/benchmarks~72^2~7^2~6^2,"Merge pull request #10 from radlab/datahandler-fix

Several code-quality improvements to DataHandler.",1357175232
96a6ff0b09f276fb38656bb753592b1deeff5dd1 remotes/origin/benchmarks~72^2~7^2~6^2^2~1,"Merge branch 'dev-merge' into datahandler-fix

Conflicts:
	streaming/src/main/scala/spark/streaming/dstream/DataHandler.scala
",1357164495
3dc87dd923578f20f2c6945be7d8c03797e76237 remotes/origin/benchmarks~72^2~7^2~7,"Fixed compilation bug in RDDSuite created during merge for mesos/master.
",1357087084
170e451fbdd308ae77065bd9c0f2bd278abf0cb7 remotes/origin/benchmarks~143^2~7,Minor documentation and style fixes for PySpark.,1357077134
feadaf72f44e7c66521c03171592671d4c441bda remotes/origin/benchmarks~160^2~2,"Mark key as not loading in CacheTracker even when compute() fails
",1357055840
099898b43955d99351ec94d4a373de854bf7edf7 remotes/origin/benchmarks~143^2~9,"Port LR example to PySpark using numpy.

This version of the example crashes after the first iteration with
""OverflowError: math range error"" because Python's math.exp()
behaves differently than Scala's; see SPARK-646.
",1356832828
26186e2d259f3aa2db9c8594097fd342107ce147 remotes/origin/benchmarks~143^2~13,Use batching in pyspark parallelize(); fix cartesian(),1356824097
3f74f729a190924b7634e08a381232af36aeb328 remotes/origin/benchmarks~162,"Merge pull request #345 from JoshRosen/fix/add-file

Fix deletion of files in current working directory by clearFiles()",1356822093
6ee1ff2663cf1f776dd33e448548a8ddcf974dc6 remotes/origin/benchmarks~143^2~14,"Fix bug in pyspark.serializers.batch; add .gitignore.
",1356819934
7ec3595de28d53839cb3a45e940ec16f81ffdf45 remotes/origin/benchmarks~143^2~16,"Fix bug (introduced by batching) in PySpark take()
",1356762076
f1bf4f0385a8e5da14a1d4b01bbbea17b98c4aa3 remotes/origin/benchmarks~162^2~3,"Skip deletion of files in clearFiles().

This fixes an issue where Spark could delete
original files in the current working directory
that were added to the job using addFile().

There was also the potential for addFile() to
overwrite local files, which is addressed by
changing Utils.fetchFile() to log a warning
instead of overwriting a file with new contents.

This is a short-term fix; a better long-term
solution would be to remove the dependence on
storing files in the current working directory,
since we can't change the cwd from Java.
",1356742857
0bc0a60d3001dd231e13057a838d4b6550e5a2b9 remotes/origin/benchmarks~72^2~7^2~13,"Modifications to make sure LocalScheduler terminate cleanly without errors when SparkContext is shutdown, to minimize spurious exception during master failure tests.
",1356651453
1dca0c51804b9c94709ec9cc0544b8dfb7afe59f remotes/origin/benchmarks~143^2~22,"Remove debug output from PythonPartitioner.
",1356574986
84587a9bf3c734be151251b97ac5af48eb03f4d9 remotes/origin/benchmarks~163,"Merge pull request #343 from markhamstra/spark-601

lookup() needn't fail when there is no partitioner",1356391685
fe777eb77dee3c5bc5a7a332098d27f517ad3fe4 remotes/origin/benchmarks~72^2~7^2~15^2~1,"Fixed bugs in CheckpointRDD and spark.CheckpointSuite.
",1356039567
68c52d80ecd5dd173f755bedc813fdc1a52100aa remotes/origin/benchmarks~169^2~1,"Moved BlockManager's IdGenerator into BlockManager object. Removed some
excessive debug messages.
",1355959643
1e6e154d6d8220149f100cf518a50d97d84186b6 remotes/origin/benchmarks~172,"Merge pull request #338 from tomdz/repl-pom-fix

Fixed repl maven build",1355868209
1948f46093d2934284daeae06cc2891541c39e68 remotes/origin/benchmarks~177^2,"Use spark-env.sh to configure standalone master.  See SPARK-638.

Also fixed a typo in the standalone mode documentation.
",1355448000
7c9e3d1c2105b694bedcfe10e554dbadd2760eb5 remotes/origin/benchmarks~169^2~2^2,"Return success or failure in BlockStore.remove().
",1355440947
fa28f25619d6712e5f920f498ec03085ea208b4d remotes/origin/benchmarks~72^2~7^2~15^2~6,"Fixed bug in UnionRDD and CoGroupedRDD
",1355263183
f97ce3ae14ed05b3e5d3e6cd137ee5164813634e remotes/origin/benchmarks~180^2,"SPARK-626: Making security group deletion optional, handling retried
when deleting security groups fails, fixing bug when using all zones but
only 1 slave.",1355251701
2a87d816a24c62215d682e3a7af65489c0d6e708 remotes/origin/benchmarks~72^2~7^2~17,"Added clear property to JavaAPISuite to remove port binding errors.
",1355219083
746afc2e6513d5f32f261ec0dbf2823f78a5e960 remotes/origin/benchmarks~72^2~7^2~15^2~7,"Bunch of bug fixes related to checkpointing in RDDs. RDDCheckpointData object is used to lock all serialization and dependency changes for checkpointing. ResultTask converted to Externalizable and serialized RDD is cached like ShuffleMapTask.
",1355211397
1f3a75ae9e518c003d84fa38a54583ecd841ffdc remotes/origin/benchmarks~72^2~7^2~15^2~8,"Modified checkpoint testsuite to more comprehensively test checkpointing of various RDDs. Fixed checkpoint bug (splits referring to parent RDDs or parent splits) in UnionRDD and CoalescedRDD. Fixed bug in testing ShuffledRDD. Removed unnecessary and useless map-side combining step for narrow dependencies in CoGroupedRDD. Removed unncessary WeakReference stuff from many other RDDs.
",1354916752
d21ca010ac14890065e559bab80f56830bb533a7 remotes/origin/benchmarks~186^2~7,"Add block manager heart beats.

Renames old message called 'HeartBeat' to 'BlockUpdate'.

The BlockManager periodically sends a heart beat message to the master.
If the manager is currently not registered. The master responds to the
heart beat by indicating whether the BlockManager is currently registered
with the master. Additionally, the master now also responds to block
updates by indicating whether the BlockManager in question is registered.
When the BlockManager detects (by heart beat or failed block update)
that it stopped being registered, it reregisters and sends block
updates for all its blocks.
",1354779320
ddf6cd012c8860e6f66d4988f14a568f2c7edeaa remotes/origin/benchmarks~191,"Merge pull request #316 from JoshRosen/fix/ec2-web-ui-links

Use external addresses in standalone web UI when running on EC2",1354664067
b4dba55f78b0dfda728cf69c9c17e4863010d28d remotes/origin/benchmarks~72^2~7^2~22^2~2,"Made RDD checkpoint not create a new thread. Fixed bug in detecting when spark.cleaner.delay is insufficient.
",1354413785
8d3713c22180e9a7f190549c2b94ac058c07130a remotes/origin/benchmarks~192,"Merge pull request #314 from pwendell/quickstart-bugfix

Adding multi-jar constructor in quickstart",1354172368
84e584fa8c931e436f55a69d70d18c3a129d0c6a remotes/origin/benchmarks~189^2,"Code review feedback fix
",1354160766
d5e7aad039603a8a02d11f9ebda001422ca4c341 remotes/origin/benchmarks~72^2~7^2~22^2~9,"Bug fixes
",1354091815
c97ebf64377e853ab7c616a103869a4417f25954 remotes/origin/benchmarks~72^2~7^2~22^2~13,"Fixed bug in the number of splits in RDD after checkpointing. Modified reduceByKeyAndWindow (naive) computation from window+reduceByKey to reduceByKey+window+reduceByKey.
",1353367327
10c1abcb6ac42b248818fa585a9ad49c2fa4851a remotes/origin/benchmarks~72^2~7^2~25^2~1,"Fixed checkpointing bug in CoGroupedRDD. CoGroupSplits kept around the RDD splits of its parent RDDs, thus checkpointing its parents did not release the references to the parent splits.
",1353202020
6adc7c965f35ede8fb09452e278b2f17981ff600 remotes/origin/benchmarks~199,"Doc fix
",1353127742
12c24e786c9f2eec02131a2bc7a5bb463797aa2a remotes/origin/benchmarks~189^2~8^2,"Set default uncaught exception handler to exit.

Among other things, should prevent OutOfMemoryErrors in some daemon threads
(such as the network manager) from causing a spark executor to enter a state
where it cannot make progress but does not report an error.
",1353125551
32442ee1e109d834d2359506f0161df8df8caf03 remotes/origin/benchmarks~189^2~6^2~1,"Giving the Spark EC2 script the ability to launch instances spread
across multiple availability zones in order to make the cluster more
resilient to failure
",1353115528
26fec8f0b850e7eb0b6cfe63770f2e68cd50441b remotes/origin/benchmarks~72^2~7^2~26^2,"Fixed bug in MappedValuesRDD, and set default graph checkpoint interval to be batch duration.
",1352833557
8a25d530edfa3abcdbe2effcd6bfbe484ac40acb remotes/origin/benchmarks~72^2~7^2~26^2~2,"Optimized checkpoint writing by reusing FileSystem object. Fixed bug in updating of checkpoint data in DStream where the checkpointed RDDs, upon recovery, were not recognized as checkpointed RDDs and therefore deleted from HDFS. Made InputStreamsSuite more robust to timing delays.
",1352801788
ae61ebaee64fad117155d65bcdfc8520bda0e6b4 remotes/origin/benchmarks~72^2~7^2~28^2~2,"Fixed bugs in RawNetworkInputDStream and in its examples. Made the ReducedWindowedDStream persist RDDs to MEMOERY_SER_ONLY by default. Removed unncessary examples. Added streaming-env.sh.template to add recommended setting for streaming.
",1352756716
46222dc56db4a521bd613bd3fac5b91868bb339e remotes/origin/benchmarks~72^2~7^2~28^2~3^2,"Fixed bug in FileInputDStream that allowed it to miss new files. Added tests in the InputStreamsSuite to test checkpointing of file and network streams.
",1352668809
04e9e9d93c512f856116bc2c99c35dfb48b4adee remotes/origin/benchmarks~72^2~7^2~28^2~3^2~1,"Refactored BlockManagerMaster (not BlockManagerMasterActor) to simplify the code and fix live lock problem in unlimited attempts to contact the master. Also added testcases in the BlockManagerSuite to test BlockManagerMaster methods getPeers and getLocations.
",1352652861
d0f0fc8c1eea2d7b4fa3220ff68feb9686269810 remotes/origin/benchmarks~204,"Merge pull request #302 from tdas/blockmanager-fix

Blockmanager fix",1352521640
9915989bfa242a6f82a7b847ad25e434067da5cf remotes/origin/benchmarks~204^2,"Incorporated Matei's suggestions. Tested with 5 producer(consumer) threads each doing 50k puts (gets), took 15 minutes to run, no errors or deadlocks.
",1352504775
cc2a65f54715ff0990d5873d50eec0dedf64d409 remotes/origin/benchmarks~72^2~7^2~32^2,"Fixed bug in InputStreamsSuite
",1352373477
809b2bb1fe92c8ce733ce082c5f6e31316e05a61 remotes/origin/benchmarks~207,"fix bug in getting slave id out of mesos
",1352363668
bb1bce79240da22c2677d9f8159683cdf73158c2 remotes/origin/benchmarks~208,"Various fixes to standalone mode and web UI:

- Don't report a job as finishing multiple times
- Don't show state of workers as LOADING when they're running
- Show start and finish times in web UI
- Sort web UI tables by ID and time by default
",1352335793
fc3d0b602a08fdd182c2138506d1cd9952631f95 remotes/origin/benchmarks~72^2~7^2~32^2~1,"Added FailureTestsuite for testing multiple, repeated master failures.
",1352251411
395167f2b2a1906cde23b1f3ddc2808514bce47b remotes/origin/benchmarks~72^2~7^2~34^2,"Made more bug fixes for checkpointing.
",1352160710
72b2303f99bd652fc4bdaa929f37731a7ba8f640 remotes/origin/benchmarks~72^2~7^2~34^2~1,"Fixed major bugs in checkpointing.
",1352144496
dfce7e74a792b3c6e55952873f07f47f7bf2d099 remotes/origin/benchmarks~210,"Merge pull request #298 from JoshRosen/fix/ec2-existing-cluster-check

Fix check for existing instances during spark-ec2 launch",1351992926
3fb5c9ee24302edf02df130bd0dfd0463cf6c0a4 remotes/origin/benchmarks~72^2~7^2~37,"Fixed serialization bug in countByWindow, added countByKey and countByKeyAndWindow, and added testcases for them.
",1351883545
590e4aa9cb18648de5ddc943f03489273596b16c remotes/origin/benchmarks~211,"Merge pull request #296 from shivaram/block-manager-fix

Remove unnecessary hash-map put in MemoryStore",1351796063
4a47d1a47642da6ed893e1769144c25b8011bae9 remotes/origin/benchmarks~212,"Merge pull request #297 from JoshRosen/fix/ec2-spot-instances

Cancel spot instance requests when exiting spark-ec2",1351794678
2ccf3b665280bf5b0919e3801d028126cb070dbd remotes/origin/benchmarks~143^2~26,"Fix PySpark hash partitioning bug.

A Java array's hashCode is based on its object
identify, not its elements, so this was causing
serialized keys to be hashed incorrectly.

This commit adds a PySpark-specific workaround
and adds more tests.",1351488628
e782187b4af3b2ffe83e67fee7c783b5dfcd09e5 remotes/origin/benchmarks~214,"Don't throw an error in the block manager when a block is cached on the master due to
a locally computed operation

Conflicts:

	core/src/main/scala/spark/storage/BlockManagerMaster.scala
",1351236825
ed71df46cddc9a4f1363b937c10bfa2a928e564c remotes/origin/benchmarks~72^2~7^2~41,"Minor fixes.
",1351122580
7849216bba414b2b2a63a4b093bea8f6397384d9 remotes/origin/benchmarks~218,"Merge pull request #286 from JoshRosen/ec2-error-handling

Allow EC2 script to stop/destroy cluster after master/slave failures",1351052143
c4a2b6f636040bacd3d4b443e65cc33dafd0aa7e remotes/origin/benchmarks~72^2~7^2~50,"Fixed some bugs in tests for forgetting RDDs, and made sure that use of manual clock leads to a zeroTime of 0 in the DStreams (more intuitive).
",1350841285
365a4c1e688daa64447529170d1d3ccbd0eafe7e remotes/origin/benchmarks~218^2,"Allow EC2 script to stop/destroy cluster after master/slave failures.
",1350581810
7855bacd26f01fb0c7e282e45907e23dd0a56cab remotes/origin/benchmarks~232^2,"Merge pull request #278 from pwendell/quickstart-fix

Adding dependency repos in quickstart example",1350240744
b08708e6fcb59a09b36c5b8e3e7a4aa98f7ad050 remotes/origin/benchmarks~72^2~7^2~54,"Fixed bugs in the streaming testsuites.
",1350187344
4be12d97ec4a6ca0acaf324799156e219732a11e remotes/origin/benchmarks~236,"Some doc fixes, including showing version number in nav bar again
",1350180311
23015ccac045dd0e2c95c8625ee354984a8d594c remotes/origin/benchmarks~247^2,"Merge pull request #271 from shivaram/block-manager-npe-fix

Change block manager to accept a ArrayBuffer ",1350077788
066e979e9e9de18eb3f4e985ce45bf60aab37f16 remotes/origin/benchmarks~247^2~3,"Merge pull request #267 from dennybritz/dev

Fixed bug when fetching Jar dependencies.",1349919143
3ed172ea5909499ded54b49d9a9c82ce9c35943f remotes/origin/benchmarks~247^2~2^2~1,"Adding code for publishing to Sonatype.

By default - I'm leaving this commented out. This is because
there is a bug in the PGP signing plugin which causes it to active
even duing a publish-local. So we'll just uncomment when we decide
to publish.
",1349915129
d3f095f9043fe8c3a55e797b49e2244ee15b58ee remotes/origin/benchmarks~247^2~3^2,"Fixed bug when fetching Jar dependencies.

Instead of checking currentFiles check currentJars.
",1349910593
bc0bc672d02e8f5f12cd1e14863db36c42acff96 remotes/origin/benchmarks~250,"Updates to documentation:

- Edited quick start and tuning guide to simplify them a little
- Simplified top menu bar
- Made private a SparkContext constructor parameter that was left as
  public
- Various small fixes
",1349818223
1231eb12e675fec47bc2d3139041b1c178a08c37 remotes/origin/benchmarks~255,"Merge pull request #259 from mosharaf/bc-fix-dev

Synchronization bug fix in broadcast implementations",1349740468
edc67bfba8b7875bb751f1a8c84af7135a1d3d39 remotes/origin/benchmarks~255^2,"Merge branch 'dev' into bc-fix-dev
",1349738353
88152e216498ffab4aba818b7ada3fcff7d1f272 remotes/origin/benchmarks~257,"Merge pull request #254 from pwendell/quickstart-fix

Removing one link in quickstart",1349719709
280286b6cdd78426fdbb88390ac41cacc6a92485 remotes/origin/benchmarks~258,"Merge pull request #255 from andyk/fix-nav-gap

Removes the annoying small gap above the nav menu dropdown boxes",1349714703
d72db3d7dc5980847ebb2db8e0298877d045fb53 remotes/origin/benchmarks~262,"Merge pull request #250 from rxin/dev

Fixed a bug in addFile that if the file is specified as ""file:///"", the symlink is created incorrectly for local mode.",1349596613
80f59e17e2dc7b65026a5a16f2dff65e01964212 remotes/origin/benchmarks~262^2,"Fixed a bug in addFile that if the file is specified as ""file:///"", the
symlink is created wrong for local mode.
",1349596478
f930fe5d817c0f7ce6c0059ecd65169897dfc547 remotes/origin/benchmarks~263,"Improve error message
",1349595276
a3bf0ce57f5333ee16c672355bb2431ab72cd44b remotes/origin/benchmarks~264,"Don't crash on ask timeout exceptions in deploy.Client.stop() (fixes a crash in tests)
",1349594741
ce915cadee1de8e265f090b7be2f6e70d1b4062e remotes/origin/benchmarks~270,"Made run script add test-classes onto the classpath only if SPARK_TESTING is set; fixes #216
",1349583556
716e10ca32ecb470da086290ac7414360f6e7d0a remotes/origin/benchmarks~278,"Minor formatting fixes
",1349499786
1620d59ea44c3315870f3865b6e7cfd06428e839 remotes/origin/benchmarks~222^2~7^2^2,"Merge pull request #246 from shivaram/size-estimator-fix-master

Fix SizeEstimator tests to work with String classes in JDK 6 and 7",1349458110
66d7066d4f2820230fc0bccd639ed7091b7336a4 remotes/origin/benchmarks~222^2~8,"Let the reducer retry if a fetch fails before reading all records
",1349394077
119e50c7b9e50a388648ca9434ee1ace5c22867c remotes/origin/benchmarks~255^2~1,"Conflict fixed
",1349241939
802aa8aef90fe7d2f0c859c68f12361db286bf20 remotes/origin/benchmarks~307,"Some bug fixes and logging fixes for broadcast.
",1349130042
2314132d57878152a84325f86ea320bdbc7cca31 remotes/origin/benchmarks~314,"Added a (failing) test for LRU with MEMORY_AND_DISK.
",1349070736
83143f9a5f92ca5c341332c809f0adf7e58885b6 remotes/origin/benchmarks~316,"Fixed several bugs that caused weird behavior with files in spark-shell:

- SizeEstimator was following through a ClassLoader field of Hadoop
  JobConfs, which referenced the whole interpreter, Scala compiler, etc.
  Chaos ensued, giving an estimated size in the tens of gigabytes.
- Broadcast variables in local mode were only stored as MEMORY_ONLY and
  never made accessible over a server, so they fell out of the cache when
  they were deemed too large and couldn't be reloaded.
",1349065179
3d24281fbf2cea38e75767305968c08a942538d2 remotes/origin/benchmarks~222^2~10,"Backport sampling fixes from dev (suggested by Henry Milner)
",1348980952
2f11e3c285499880b9d800fdd65ea9ad1c82b4af remotes/origin/benchmarks~319^2~1,"Merge pull request #227 from JoshRosen/fix/distinct_numsplits

Allow controlling number of splits in distinct().",1348901844
9fc78f8f2907828c448fd53155ec01203212321d remotes/origin/benchmarks~330^2,"Fixing some whitespace issues
",1348873550
c387e40fb1d9ecfccba1ea5869bffe0b2934c80b remotes/origin/benchmarks~330^2~2,"Log message which records RDD origin

This adds tracking to determine the ""origin"" of an RDD. Origin is defined by
the boundary between the user's code and the spark code, during an RDD's
instantiation. It is meant to help users understand where a Spark RDD is
coming from in their code.

This patch also logs origin data when stages are submitted to the scheduler.

Finally, it adds a new log message to fix an inconsitency in the way that
dependent stages (those missing parents) and independent stages (those
without) are logged during submission.
",1348872706
915ab970b70211f023fb2f2ff792121a831de2a9 remotes/origin/benchmarks~222^2~11,"Make error reporting less scary if we can't look up UseCompressedOops
",1348869157
2a8bfbca00a1701bfe22f5b0967c2d95c088c277 remotes/origin/benchmarks~332,"Fixed a bug where isLocal was set to false when using local[K]
",1348869054
4a138403efd876f177fe2ba43c1bd115b2858919 remotes/origin/benchmarks~333,"Fix a bug in JAR fetcher that made it always fetch the JAR
",1348806726
7bcb08cef5e6438ce8c8efa3da3a8f94f2a1fbf9 remotes/origin/benchmarks~335,"Renamed storage levels to something cleaner; fixes #223.
",1348793459
a4093f75637910524f501d36b268584006455d9b remotes/origin/benchmarks~338,"Minor doc fixes
",1348726935
d51d5e0582c0605deae7497cd95a055698dc9383 remotes/origin/benchmarks~345,"Doc fixes
",1348642744
051785c7e67b7ba0f2f0b5e078753d3f4f380961 remotes/origin/benchmarks~348,"Several fixes to sampling issues pointed out by Henry Milner:

- takeSample was biased towards earlier partitions
- There were some range errors in takeSample
- SampledRDDs with replacement didn't produce appropriate counts
  across partitions (we took exactly frac of each one)
",1348634818
6eeb379cf86b25975456369cc3de50a41a648b69 remotes/origin/benchmarks~357,"Fix some test issues
",1348526398
33fb373e69b5cd437257d5260fb90db7bcec8285 remotes/origin/benchmarks~363,"Merge pull request #215 from dennybritz/dev

HTTP File server fixes",1348274368
afb7ccc83820be287abbd6328b3294367fab2c72 remotes/origin/benchmarks~363^2,"HTTP File server fixes.
",1348250293
a642051ade11d10d7ab26a44bdf7acc743fefc97 remotes/origin/benchmarks~365,"Fixed a performance bug in BlockManager that was creating garbage when
returning deserialized, in-memory RDDs.
",1348206141
8feb5caacd87d72dc15a45ec3c5aea3592de80cb remotes/origin/benchmarks~366,"Fixed an issue with ordering of classloader setup that was causing Java deserializer to break
",1348204399
c94e9cc54aebddc20cc4ab13ca106781c7298642 remotes/origin/benchmarks~367^2,Add Java Programming Guide; fix broken doc links.,1347853606
9b4cd1648b6c2467a63109ba817d7e7a0c46ffb9 remotes/origin/benchmarks~379,"Fix bugs with Connection's shutdown callback failing to get its address
",1347486854
4d3471dd077e9e9c9038707eb5ba3fb8539c05e0 remotes/origin/benchmarks~382^2~4,"Fix serialization bugs and added local cluster tests
",1347316798
f8f125eebbc247c509eee8d71e9932fe5b380da4 remotes/origin/benchmarks~222^2~14^2~1,"fix stupid typo and add documentation
",1347051526
b5750726ff3306e0ea5741141f6cae0eb3449902 remotes/origin/benchmarks~72^2~7^2~56^2~3,"Fixed bugs in streaming Scheduler and optimized QueueInputDStream.
",1347048981
4a7bde6865cf22af060f20a9619c516b811c80f2 remotes/origin/benchmarks~72^2~7^2~58^2,"Fixed bugs and added testcases for naive reduceByKeyAndWindow.
",1346983619
babb7e3ce2a5eda793f87b42839cc20d14cb94cf remotes/origin/benchmarks~72^2~7^2~58^2~2,"Re-implemented ReducedWindowedDSteam to simplify and fix bugs. Added slice operator to DStream. Also, refactored DStream testsuites and added tests for reduceByKeyAndWindow.
",1346934509
9ef90c95f4947e47f7c44f952ff8d294e0932a73 remotes/origin/benchmarks~72^2~7^2~61,"Bug fix
",1346892226
96a1f2277d1c7e0ff970eafcb3ca53cd20b1b89c remotes/origin/benchmarks~72^2~7^2~69^2,"fix the compile error in TopKWordCountRaw.scala
",1346807014
a842c63044b5b1105228dc7aca21a1c31da90338 remotes/origin/benchmarks~389,"Minor formatting fixes
",1346714640
83dad56334e73c477e9b62715df14b0c798220e3 remotes/origin/benchmarks~72^2~7^2~72^2~3,"Further fixes to raw text sender, plus an app that uses it
",1346528725
f84d2bbe55aaf3ef7a6631b9018a573aa5729ff7 remotes/origin/benchmarks~72^2~7^2~72^2~4,"Bug fixes to RateLimitedOutputStream
",1346484675
c42e7ac2822f697a355650a70379d9e4ce2022c0 remotes/origin/benchmarks~72^2~7^2~72^2~6,"More block manager fixes
",1346473871
389fb4cc54f0c433f3afd56913229aa9fb4bf2fd remotes/origin/benchmarks~391,"End runJob() with a SparkException when a task fails too many times in
one of the cluster schedulers.
",1346460463
51fb13dd164c29e6bf97c2e3642b07a7f416ddaa remotes/origin/benchmarks~72^2~7^2~72^2~7,"Bug fix
",1346452571
ce42a463750f37d9c1c6a2c982d3b947039233cd remotes/origin/benchmarks~72^2~7^2~72^2~8,"Bug fix
",1346452535
607b8fffcd5d3d3e7d34361c747580b8cfb296e2 remotes/origin/benchmarks~222^2~15,"End runJob with a SparkException when a Mesos task fails too many times
",1346438412
113277549c5ee1bcd58c7cebc365d28d92b74b4a remotes/origin/benchmarks~72^2~7^2~72^2~11,"Really fixed the replication-3 issue. The problem was a few buffers not being rewound.
",1346391575
31ffe8d5284a4cda5eb8d21d3ea042cc5a2dc8b7 remotes/origin/benchmarks~255^2~2,"Synchronization bug fix in broadcast implementations
",1346390803
38835325454ff3afe2d410583ead37314e1ff49c remotes/origin/benchmarks~255^2~3,"Bug fix. Fixed log messages. Updated BroadcastTest example to have iterations.
",1346388180
a480dec6b26f759cf60eac2a9260484eeafc508d remotes/origin/benchmarks~392,"Deserialize multi-get results in the caller's thread. This fixes an
issue with shared buffers in the KryoSerializer.
",1346382066
1b3e3352ebfed40881d534cd3096d4b6428c24d4 remotes/origin/benchmarks~72^2~7^2~72^2~13,"Deserialize multi-get results in the caller's thread. This fixes an
issue with shared buffers with the KryoSerializer.
",1346374765
4db3a967669a53de4c4b79b4c0b70daa5accb682 remotes/origin/benchmarks~72^2~7^2~75,"Made minor changes to reduce compilation errors in Eclipse. Twirl stuff still does not compile in Eclipse.
",1346270641
1f8085b8d097f060a3939eaff5be1f58111ec224 remotes/origin/benchmarks~72^2~7^2~76,"Compile fixes
",1346210456
bf2e9cb08ea236a776e21721c8b1929901cdeff1 remotes/origin/benchmarks~395,"Fault tolerance and block store fixes discovered through streaming tests.
",1346134070
b4a2214218eeb9ebd95b59d88c2212fe717efd9e remotes/origin/benchmarks~72^2~7^2~77^2~2,"More fault tolerance fixes to catch lost tasks
",1346132969
414367850982c4f8fc5e63cc94caa422eb736db5 remotes/origin/benchmarks~143^2~33,"Fix minor bugs in Python API examples.
",1346052287
117e3f8c8602c1303fa0e31840d85d1a7a6e3d9d remotes/origin/benchmarks~72^2~7^2~77^2~5,"Fix a bug that was causing FetchFailedException not to be thrown
",1346035976
b08ff710af9b6592e3b43308ec4598bd3e6da084 remotes/origin/benchmarks~72^2~7^2~77^2~11^2,"Added sliding word count, and some fixes to reduce window DStream
",1346024450
06ef7c3d1bf8446d4d6ef8f3a055dd1e6d32ca3a remotes/origin/benchmarks~72^2~7^2~77^2~13,"Less debug info
",1346023760
51453eb87bb554a213b9c7ede546659251b195e0 remotes/origin/benchmarks~397,"Merge pull request #179 from JoshRosen/fix/sparklr-caching

Cache points in SparkLR example",1346020370
deedb9e7b722d9fb37c89d3ef82b6bb2d470dcbe remotes/origin/benchmarks~399,"Fix further issues with tests and broadcast.

The broadcast fix is to store values as MEMORY_ONLY_DESER instead of
MEMORY_ONLY, which will save substantial time on serialization.
",1345779109
59b831b9d1161a1c2d4312bb6711d8694983b5d6 remotes/origin/benchmarks~400,"Fixed test failures due to broadcast not stopping correctly
",1345777195
e463e7a333577b4e4b693268fba7f4df9f362426 remotes/origin/benchmarks~411,"Merge pull request #167 from JoshRosen/piped-rdd-fixes

Detect non-zero exit status from PipedRDD process",1344585402
59c22fb444e7a03e2272be1d76e16960083d161d remotes/origin/benchmarks~411^2,"Print exit status in PipedRDD failure exception.
",1344584036
1c5ae3edf2913447d3554e7d48c3c4f117159cfb remotes/origin/benchmarks~222^2~21,"Merge pull request #151 from dennybritz/fix/examples_jar

Examples ship to to cluster",1344127181
63c2020f93ab321041271b3136fe93aa63c610e2 remotes/origin/benchmarks~222^2~21^2~2,"Merge branch 'master' into fix/examples_jar
",1344124518
24b7eb872c26af08f5c9e8c0ce6b81f49ee35a65 remotes/origin/benchmarks~255^2~12,"Bug fixed. Broadcast now works with BlockManager.
",1344065248
62898b631fe6584ff93e62948765f7101304ebfa remotes/origin/benchmarks~416,"Made range partition balance tests more aggressive.

This is because we pull out such a large sample (10x the number of
partitions) that we should expect pretty good balance. The tests are
also deterministic so there's no worry about them failing irreproducibly.
",1344026808
abca69937871508727e87eb9fd26a20ad056a8f1 remotes/origin/benchmarks~222^2~22,"Made range partition balance tests more aggressive.

This is because we pull out such a large sample (10x the number of
partitions) that we should expect pretty good balance. The tests are
also deterministic so there's no worry about them failing irreproducibly.
",1344026657
d05c0f97ca9b66843539a64bde7cef97b4d92bfc remotes/origin/benchmarks~419,"Logging Throwables in Info and Debug

Logging Throwables in logInfo and logDebug instead of swallowing them.

Conflicts:

	core/src/main/scala/spark/Logging.scala
",1344026421
c0d5bd6553e87ff95373e2c52c5580df2a47c6a0 remotes/origin/benchmarks~222^2~25,"Merge pull request #164 from HarveyFeng/master

Bug fix in RangePartitioner for partitioning when sorting in descending order.",1344022037
53008c2d8a8b28f4204eaafa89f0e8087dc11466 remotes/origin/benchmarks~414^2~3,"Settings variables and bugfix for stop script.
",1343948379
43b81eb2719c4666b7869d7d0290f2ee83daeafa remotes/origin/benchmarks~72^2~7^2~80,"Renamed RDS to DStream, plus minor style fixes
",1343930751
b980eabd86f5f3045db802b506e853eb9d7b15d5 remotes/origin/benchmarks~420^2~1,"Merge pull request #161 from JoshRosen/fix/assembly-akka-config

Use sbt mergeStrategy for reference.conf files.",1343929070
650d11817eb15c1c2a8dc322b72c753df88bf8d3 remotes/origin/benchmarks~72^2~7^2~82,"Added a WordCount for external data and fixed bugs in file streams
",1343920183
4a9c58913d0a9bd51811dedca934a2a0adbe8d37 remotes/origin/benchmarks~222^2~26,"Merge pull request #157 from paulcavallaro/log-throwables

Logging Throwables in Info and Debug",1343913966
545165e8153bb516d4364f9b3df1440c6b44c01b remotes/origin/benchmarks~414^2~6,"Merge pull request #158 from JoshRosen/fix/assembly-akka-config

Merge Akka reference.conf files in sbt assembly task",1343713020
509b721d12c909d9298127637a9391bfef6e91b4 remotes/origin/benchmarks~414^2~6^2,"Fix Akka configuration in assembly jar.

This resolves an issue where running Spark from
the assembly jar would cause a ""No configuration
setting found for key 'akka.version'"" exception.

This solution is from the Akka Team Blog:

http://letitcrash.com/post/21025950392/",1343696713
3ee2530c0c40f7670151f55c05232728a12c23e2 remotes/origin/benchmarks~414^2~7,"Merge branch 'block-manager-fix' into dev
",1343681926
f471c82558347216985a8b7015309c4cce099cc4 remotes/origin/benchmarks~428,"Various reorganization and formatting fixes
",1343672641
e3952f31de5995fb8e334c2626f5b6e7e22b187f remotes/origin/benchmarks~222^2~26^2,"Logging Throwables in Info and Debug

Logging Throwables in logInfo and logDebug instead of swallowing them.
",1343670069
3b392c67dbeb7b2267015ffbeb2aac70dfc01870 remotes/origin/benchmarks~432,"fix up scaladoc, naming of type parameters
",1343531761
f6f917bd00a1cae5e99f7888e46e3f232959f280 remotes/origin/benchmarks~440,"Add a sleep to prevent a failing test.

The BlockManager's put seems to be slightly asynchronous, which can
cause it to fail this test by not removing stuff from the cache before
we put the next value. We should probably change the semantics of put()
in this case but it's hard right now. It will also be hard for
asynchronously replicated puts.
",1343433576
b51d733a5783ef29077951e842882bb002a4139e remotes/origin/benchmarks~443,"Fixed Java union methods having same erasure.

Changed union() methods on lists to take a separate ""first element""
argument in order to differentiate them to the compiler, because Java 7
considered it an error to have them all take Lists parameterized with
different types.
",1343417007
d1b7f41671feb6e17e98383b1770757b4941cc3b remotes/origin/benchmarks~414^2~8^2~3,"Fixed bug in BoundedMemoryCache.
",1343404845
435d129bec024512e14b95592f1f3b6e58322350 remotes/origin/benchmarks~414^2~8^2~4,"Fixed bugs in block dropping code of MemoryStore and changed synchronized HashMap to ConcurrentHashMap in BlockManager.
",1343383346
600e99728dc983e520e0a1b82b3f46e50f1dfac5 remotes/origin/benchmarks~447,"Fix a bug where an input path was added to a Hadoop job configuration twice
",1343085379
da4298135cf809b1005e4f92bf8d93cdd6c7f08d remotes/origin/benchmarks~222^2~28,"Merge pull request #152 from dennybritz/fix/testbeforeafter

Always destroy SparkContext in after block for the unit tests.",1343085204
6f44c0db74cc065c676d4d8341da76d86d74365e remotes/origin/benchmarks~222^2~30,"Fix a bug where an input path was added to a Hadoop job configuration twice
",1342933108
d1759c02905758d3dc960626418bc2c58eaf0a55 remotes/origin/benchmarks~222^2~29^2,"Merge pull request #149 from dennybritz/serfix

Instantiating custom serializer using user's classpath",1342932890
01dce3f569e0085dae2d0e4bc5c9b2bef5bd3120 remotes/origin/benchmarks~444^2~6,"Add Java API

Add distinct() method to RDD.

Fix bug in DoubleRDDFunctions.
",1342658069
e4dbaf653fa5ee110d0889b1efa5412250d8682f remotes/origin/benchmarks~222^2~21^2~3,"syntax errors
",1342639080
913d42c6a0c97121c0d2972dbb5769fd1edfca1d remotes/origin/benchmarks~222^2~27^2~2,"fix up scaladoc, naming of type parameters
",1342488315
e2a67a802447c9a778d57b687dc6321f5fb14283 remotes/origin/benchmarks~449,"Fixes to coarse-grained Mesos scheduler in dealing with failed nodes
",1342142512
4e2fe0bdaf7c2626d8b8461fed36259c9830a25c remotes/origin/benchmarks~456,"Miscellaneous bug fixes
",1341617620
c53670b9bf709ab583cbc75e952026bc4abb6c5f remotes/origin/benchmarks~463,"Various code style fixes, mostly from IntelliJ IDEA
",1341020832
697b0bee2c6d7e7b6b02c627c84975066fc67b91 remotes/origin/benchmarks~464,"Scalacheck groupId has changed https://github.com/rickynils/scalacheck/issues/24. Necessary to build with scalaVersion 2.9.2. Works with 2.9.1 too.
",1341013325
3a58efa5a5da9a9a83bdaf0d4e5d4df6223e6a22 remotes/origin/benchmarks~467,"Allow binding to a free port and change Akka logging to use SLF4J. Also
fixes various bugs in the previous code when running on Mesos.
",1341010941
25972b52cdd6def2ad5f67cc20ea5a11066c2259 remotes/origin/benchmarks~222^2~21^2~7^2,Scalacheck groupId has changed https://github.com/rickynils/scalacheck/issues/24. Necessary to build with scalaVersion 2.9.2. Works with 2.9.1 too.,1340996423
3920189932e95f78f84ab400e3e779c1601f90f1 remotes/origin/benchmarks~469,"Upgraded to Akka 2 and fixed test execution (which was still parallel
across projects).
",1340952688
ede615d71965e9aaa0c7019ecb05777f7ca905f2 remotes/origin/benchmarks~470,"Fixed issues duplicate class issues in sbt assembly.
",1340402589
6ad3e1f1b4c35200426a44360dc2b3477c62459b remotes/origin/benchmarks~471,"Various fixes when running on Mesos
",1340174906
e896a505e273a5a275f4a4d58470beddea8146df remotes/origin/benchmarks~472,"Added testcase for ByteBufferInputStream bugs.
",1339974672
0e84d620e1109763d8f60243ecc75babf58aa424 remotes/origin/benchmarks~222^2~37,"Revert ""Various fixes to get unit tests running. In particular, shut down""

This reverts commit 2893b305501a0e04cabdaa2fbad06ef86076cdf8.
",1339968450
4749ec063cbd5975b9c03ba7c5e7263849447c51 remotes/origin/benchmarks~222^2~38,"Revert ""Fixed nasty corner case bug in ByteBufferInputStream. Could not add a test case for this as I could not figure out how to deterministically reproduce the bug in a short testcase.""

This reverts commit 40536e3668c3f8077c91170318f3bbd8f3060517.
",1339968438
40536e3668c3f8077c91170318f3bbd8f3060517 remotes/origin/benchmarks~474,"Fixed nasty corner case bug in ByteBufferInputStream. Could not add a test case for this as I could not figure out how to deterministically reproduce the bug in a short testcase.
",1339964921
2893b305501a0e04cabdaa2fbad06ef86076cdf8 remotes/origin/benchmarks~475,"Various fixes to get unit tests running. In particular, shut down
ConnectionManager and DAGScheduler properly, plus a fix to
LocalScheduler that was not merged in from 0.5 and was actually caught
by one of the tests.
",1339918125
5f54bdf98b4d986e0b03d02ea318f6224cd190d2 remotes/origin/benchmarks~479^2,"Added shutdown for akka to SparkContext.stop(). Helps a little, but many testsuites still fail.
",1339634940
c6156da9e27a8a247555c7b1b498d384377c0351 remotes/origin/benchmarks~479^2~1,"Multiple bug fixes to pass the testsuites ShuffleSuite and BlockManagerSuite.
",1339619209
4b05798c06fedfda3b38392cd1b3851624660b8e remotes/origin/benchmarks~478^2~4^2,"Further bug fix to HttpBroadcast
",1339284243
8ed662862e7f822f2f70ddc70f5492f784cd861f remotes/origin/benchmarks~478^2~5^2,"Bug fix to HttpBroadcast
",1339283815
69372b48f0edb27d2ae2d3da4a362f7d8be8d97a remotes/origin/benchmarks~486^2^2~1,"Merge pull request #133 from Benky/565245871f666c22aebb2c534f4fb7e947fca9f5

BoundedMemoryCache.put should fail when estimated size of 'value' is larger than cache capacity",1338062379
f162fc2bebeb88314ea2f7c072d75bdf03efc55e remotes/origin/benchmarks~486^2^2^2~4,"Formating fixed
",1337672738
565245871f666c22aebb2c534f4fb7e947fca9f5 remotes/origin/benchmarks~486^2^2~1^2,"BoundedMemoryCache.put fails when estimated size of 'value' is larger than cache capacity
",1337544815
822a4be37dd8b86228d1747ad3e2831ac258bafe remotes/origin/benchmarks~486^2^2^2~5,"Utils.memoryBytesToString fixed
",1337433200
16461e2edab5253e1cc3b9b8a74cc8adbb6b9be3 remotes/origin/benchmarks~486^2~3,"Updated Cache's put method to use a case class for response. Previously
it was pretty ugly that put() should return -1 for failures.
",1337067112
bd2ab635a784ea031c52421587ffcfd0e7711267 remotes/origin/benchmarks~487,"Fixed the way the JAR server is created after finding issue at Twitter
",1336273515
761ea65a98f448f158e17d7c03d1566fc06fb265 remotes/origin/benchmarks~486^2~6^2,"Added a test for the previous commit (failing to serialize task results
would throw an exception for local tasks).
",1335305675
3b745176e0ec5fda8c7afef04aec1040e1c649a9 remotes/origin/benchmarks~486^2~9,"Bug fix to pluggable closure serialization change
",1334253182
08cda89e8a05caf453f46fa1dcf00d67535805f1 remotes/origin/benchmarks~498,"Further fixes to how Mesos is found and used
",1332016754
c7af538ac160727147eea6a1008dc16a2efd802e remotes/origin/benchmarks~491^2~4,"Some fixes to sorting for when the RDD has fewer elements than the
number of partitions we ask to partition it into. Also, removed a test
that was taking way too long to run.
",1332014916
97eee50825e91258b44710c4af45be662935aa9d remotes/origin/benchmarks~502,"Fixes a nasty bug that could happen when tasks fail, because calling
wait() with a timeout of 0 on a Java object means ""wait forever"".
",1330638197
620798161be67fe0aefbd750211d20e9bbc9daf2 remotes/origin/benchmarks~503^2~2,"Added fixes to sorting
",1329120459
98f008b7211b746c76eff20645d5a26c3102ff45 remotes/origin/benchmarks~506,"Formatting fixes
",1328899923
b267175ab55bf591d94d720293c59a818b749a47 remotes/origin/benchmarks~515,"Synchronization fix in case SparkContext is used from multiple threads.
",1328567298
100e8007822040df34e4e47f872d183a48e9c7f4 remotes/origin/benchmarks~518,"Some fixes to the examples (mostly to use functional API)
",1327998798
fd5581a0d3a3be45e02adf67dff1da0d26833a4f remotes/origin/benchmarks~520,"Fixed a failure recovery bug and added some tests for fault recovery.
",1326511047
eb05154b7a08025fd95e321cd2ae0da08019f7eb remotes/origin/benchmarks~521,"Fixed a failure recovery bug and added some tests for fault recovery.
",1326510505
1ecc221f841d898d831499042f5bd27f667d2ae1 remotes/origin/benchmarks~519^2,"Fixed bugs

I've fixed the bugs detailed in the diff. One of the bugs was already
fixed on the local file (forgot to commit).
",1326139192
de01b6deaaee1b43321e0aac330f4a98c0ea61c6 remotes/origin/benchmarks~519^2~2,"Fixed bug in RDD

Math.min takes 2 args, not 1. This was not committed earlier for some
reason
",1322775277
35b6358a7c4e9558789577e07c1953c9008d3e9c remotes/origin/benchmarks~524^2~1,"Report errors in tasks to the driver via a Mesos status update

When a task throws an exception, the Spark executor previously just
logged it to a local file on the slave and exited. This commit causes
Spark to also report the exception back to the driver using a Mesos
status update, so the user doesn't have to look through a log file on
the slave.

Here's what the reporting currently looks like:

    # ./run spark.examples.ExceptionHandlingTest master@203.0.113.1:5050
    [...]
    11/10/26 21:04:13 INFO spark.SimpleJob: Lost TID 1 (task 0:1)
    11/10/26 21:04:13 INFO spark.SimpleJob: Loss was due to java.lang.Exception: Testing exception handling
    [...]
    11/10/26 21:04:16 INFO spark.SparkContext: Job finished in 5.988547328 s
",1321235693
07532021fee9e2d27ee954b21c30830687478d8b remotes/origin/benchmarks~530,"Bug fix: reject offers that we didn't find any tasks for
",1320822354
f346e64637fa4f9bd95fcc966caa496bea5feca0 remotes/origin/benchmarks~534,"Updates to the closure cleaner to work better with closures in classes.

Before, the cleaner attempted to clone $outer objects that were classes
(as opposed to nested closures) and preserve only their used fields,
which was bad because it would miss fields that are accessed indirectly
by methods, and in general it would confuse user code. Now we keep a
reference to those objects without cloning them. This is not perfect
because the user still needs to be careful of what they'll carry along
into closures, but it works better in some cases that seemed confusing
before. We need to improve the documentation on what variables get
passed along with a closure and possibly add some debugging tools for it
as well.

Fixes #71 -- that code now works in the REPL.
",1320741208
63da22c025e85e9652bdc22de8d63f9af9d38cae remotes/origin/benchmarks~536,"Update REPL code to use our own version of JLineReader, which fixes #89.
I'm not entirely sure why this broke in the jump from Scala 2.9.0.1 to
2.9.1 -- maybe something about name resolution changed?
",1320725785
3a0e6c436317457c8483c7106d5ba6551b6367c4 remotes/origin/benchmarks~541,"Miscellaneous fixes:
- Executor should initialize logging properly
- groupByKey should allow custom partitioner
",1318874855
06637cb69e736e84f5d1959020be961c0c99c5eb remotes/origin/benchmarks~543^2~5,"Fix PairRDDFunctions.groupWith partitioning

This commit fixes a bug in groupWith that was causing it to destroy
partitioning information. It replaces a call to map with a call to
mapValues, which preserves partitioning.
",1318200526
7e92ef9d198a90f21b322bc5d21fbb9d9ec0b2b8 remotes/origin/benchmarks~544^2~1,"Add workaround for bug in SBT (issue #206).
",1317078299
3562db6374d5f181a0446be322a0050fd7d48a07 remotes/origin/benchmarks~544^2~3,"Include ""spark-"" prefix in project name (used when artifact is published).
",1317073267
0c965a102eb553085a84256a877938f370b02757 remotes/origin/benchmarks~547,"Removed a debugging line
",1314684813
c22043f15062f0d0bd364ede4810ffd96babb088 remotes/origin/benchmarks~553,"Minor fix: can use >= when checking memory
",1312337477
f6c8c2a3948c40977702d48647f17664fc4daf2b remotes/origin/benchmarks~554,"Merge pull request #77 from ijuma/issue69

Enable -optimize in the build",1312325278
2b6fd3198d182cfbde18444865bbb8be0d06a83e remotes/origin/benchmarks~554^2,"Fix issue #69: Enable -optimize in the build
",1312277681
0fba22b3d216548e5e47a23a1b2e84e0e46835e9 remotes/origin/benchmarks~555^2~4,"Fix issue #65: Change @serializable to extends Serializable in 2.9 branch

Note that we use scala.Serializable introduced in Scala 2.9 instead of
java.io.Serializable. Also, case classes inherit from scala.Serializable by
default.
",1312276593
a0b7865037d6ba100abef7a1f69669d0d9b3febc remotes/origin/benchmarks~556^2,"Merge pull request #75 from ijuma/scala-2.9

Workaround for scalac bug and publishTo configuration",1312237591
7565cc2e339c26f1212f67c00ed68d38d56840b0 remotes/origin/benchmarks~556^2^2~1,"Add type annotation to result of depJarSettings to workaround scalac bug.
",1312139878
d12122502b866653735714adac491f3a162d2c0d remotes/origin/benchmarks~559,"Various improvements to Kryo serializer:

- Replaced modified Kryo version with the standard one augmented with
  the kryo-serializers package, which includes support for classes with
  no-arg constructors (that was why we had a modified Kryo before)
- The kryo-serializers version also fixes issue #72.
- Added a bunch of tests.
- Serialize maps and a few other common types properly by default.
",1311311373
baa72e274742145dec572f95ffd02721d64c568f remotes/origin/benchmarks~560,"Removed a debug statement that slipped in as a println
",1311289773
635f501492f04df531977c49208a20a166d8656a remotes/origin/benchmarks~557^2~5,"Fix copy & paste error in version.
",1310944417
c8eb8b2b90e35db7ddc2d8d539828c250123450d remotes/origin/benchmarks~565,"Set class loader for remote actors to fix a bug that happens in 2.9
",1310678951
797b4547c3ee0cad522b733eeb65cfacbef2f08c remotes/origin/benchmarks~568,"Fix tracking of updates in accumulators to solve an issue that would manifest in the 2.9 interpreter
",1310666914
969644df8eec13f4b89597a0682c8037949d855b remotes/origin/benchmarks~570,"Cleaned up a few issues to do with default parallelism levels. Also
renamed HadoopFileWriter to HadoopWriter (since it's not only for files)
and fixed a bug for lookup().
",1310661656
b7f1f62ff56212441bd346f6352dee99142555f4 remotes/origin/benchmarks~572^2~5,"bug fix
",1310251982
2f652f1656655e6966e3631de9ff2e783b51afef remotes/origin/benchmarks~575,"Fix a compile error
",1309223236
b5e6645505b0b88a3912f5ab10a5b6bc618c97e0 remotes/origin/benchmarks~576^2~4,"Cleaner reimplementation of HadoopFileWriter. Introduced TaskContext.
1> HadoopFileWriter works correctly with task failures
2> It can also take an user specified JobConf object for configuration settings
3> A Task can now get information like stage ID, split ID, and attempt ID using TaskContext class
4> Minor changes in SparkContext, DAGScheduler and subclasses to allow specification of TaskContext as a parameter
",1308283077
5166d76843d64ddf659660179be05ce09912e2de remotes/origin/benchmarks~587,"Ensure logging is initialized before spawning any threads to fix issue #45
",1306910852
90f924202b0eb7716a2c2eced23a357cb01490ea remotes/origin/benchmarks~557^2~29,"Another fix ported forward for the REPL
",1306908709
73975d7491bc413b5f19022ff1df92d6e0ce3c24 remotes/origin/benchmarks~557^2~34,"Further fixes to interpreter (adding in some code generation changes I
missed before and setting SparkEnv properly on the threads that execute
each line in the 2.9 interpreter).
",1306904724
4096c2287ec8e69b0c879ea0c512b9f7152e15ab remotes/origin/benchmarks~561^2,"Various fixes
",1306719961
50ac1d2a40c0165c9b2cd0f1c3aff19a484bcd1a remotes/origin/benchmarks~592,"Merge remote-tracking branch 'ijuma/issue51'
",1306708872
cec427e777fe2d6ef0dab285a1f4289d2ae4f89e remotes/origin/benchmarks~596,"Fixed a bug with preferred locations having changed meaning in new RDDs
",1306109549
9bde5a54cb8ebba615f6ed12dff04ae186ed61d3 remotes/origin/benchmarks~599,"class loader fix
",1306105241
91c07a33d90ab0357e8713507134ecef5c14e28a remotes/origin/benchmarks~600,"Various fixes to serialization
",1306043408
328e51b693db28be87140e88dec062543a03ee85 remotes/origin/benchmarks~604,"Various minor fixes
",1305829165
4db50e26c75263b2edae468b0e8a9283b5c2e6f1 remotes/origin/benchmarks~606,"Fixed unit tests by making them clean up the SparkContext after use and
thus clean up the various singletons (RDDCache, MapOutputTracker, etc).
This isn't perfect yet (ideally we shouldn't use singleton objects at
all) but we can fix that later.
",1305313438
1c8ca0ebe1537c8f424722294794a66ff123f132 remotes/origin/benchmarks~609^2~4,"Add Bagel test suite

Note: This test suite currently fails for the same reason that the
Spark Core test suite fails: Spark currently seems to have a bug where
any test after the first one fails.
",1304462431
db7a2c4897aabf0b00a8df347dae1ad579e767f9 remotes/origin/benchmarks~578^2,"Issue #42 fixed.
",1304026248
9d2d53349353267db80cd4bce6db059736237575 remotes/origin/benchmarks~601^2,"Temporary fix for issue #42.
",1303432826
5c9535228a00ccc7fe001dcd2aca8f1e55ccb0a2 remotes/origin/benchmarks~601^2~1,"fixed small bug when classpath has some strange formatting
",1303171949
0fb691dd28ffab6b7aa7dcb3520243f893ee76a8 remotes/origin/benchmarks~620,"Various fixes to get MesosScheduler working with new RDDs
",1299456998
021c50a8d4f00a008c7af24893fde702ea004523 remotes/origin/benchmarks~601^2~3,"Remove unnecessary lock which was there to work around a bug in
Configuration in Hadoop 0.20.0
",1299004118
447debb771ef7a42d9ac398dd1beeb5ca7f5f792 remotes/origin/benchmarks~601^2~5,"Updated Hadoop to 0.20.2 to include some bug fixes
",1299004068
2b946fb2d1cdee86246a9a5a78db94ad40014a2c remotes/origin/benchmarks~578^2~23,"pickBlockRarestFirst and gossips commented OUT for now.

Problem with the rarestFirst implemention is that we are picking peers randomly first and then picking blocks from the random peer using rarestFirst. NOT the right away to do it. It should be the other way around.
Problem with gossip is that peers might end up overwriting newer information by older ones. To fix that we either have to have timestamps or must match the bitVectors before overwriting.
",1297633995
9a6110fd990aae81fd5eea14596c6c52246f2fc8 remotes/origin/benchmarks~578^2~5^2~2,"Bug fix: a reducer never returns until its shuffleConsumer thread joins.
",1296014491
b9b461e78f96319b36dc52cfe154f14f1fcad5da remotes/origin/benchmarks~578^2~5^2~3,"Updating hasSplits variable inside a synchronized block now. This was causing a concurrency bug. However, this is a hacky solution and should be fixed.
",1295748218
888a5844345ab57a34ff57e470821abf1aa8c615 remotes/origin/benchmarks~578^2~5^2~4,"Bug fix in a log statement
",1295571713
3ab7ddd2e615489c9fb484f290b57a349df2749e remotes/origin/benchmarks~578^2~5^2~5,"Bug fixes + updated limit connecitons shuffle strategy to handle endgame situation.

After the last commit the bottleneck shifted from ""requesting to tracker for mappers"" (now done in batches) to ""notifying tracker when threads leave"" (done individually)
",1294998621
fceae31877fec9b9b0dbc077c52ac132e4f2f5e7 remotes/origin/benchmarks~578^2~32,"Bug fix in selectSuitableSources: was changing the index value before updating a bitset using that same index variable.
",1294721939
f5ba0b49c67fa54b3d22f6e3be295fabc753ed81 remotes/origin/benchmarks~578^2~35,"Bug fix + default config param values update.

Updated some default config parameters for broadcast parameters.
Fixed a concurrency bug in the variable that keeps track of how many receivers have finished.
",1294357464
69845d5648dfa890b0d06d5e171d2460aa2e370d remotes/origin/benchmarks~578^2~5^2~14,"trackerStrategy can now only be accessed by one thread at a time.

Previously, synchronized were around individual methods which were working on shared variables and two such methods could be accessed by different threads simultaneously. A source of possible concurrency issues.
",1294258682
a2c7caeac857170a1d90f9e067743dd08697387e remotes/origin/benchmarks~578^2~5^2~15,"Bug fixes, optimizations, changes to ShuffleClient/Tracker interface and updated BalanceRemainingShuffleTrackerStrategy.

There is still 1 or 10 bugs in tracker implementations. They are normally found in the last remaining clients.
 - Sometimes ShuffleClients just don't do anything for a while.
 - Sometimes the last ShuffleClient or 2 block on reading block size from ShuffleServer and the program does not proceed at all. This happens for larger shuffle (500000 keys)
",1294221517
7eb334d97ce3731d6397136e1df4c2c989a665f3 remotes/origin/benchmarks~578^2~5^2~18,"Bug fix: splitsInRequestBitVector(splitIndex) was wrongly set to false after receiving just one block in Blocked implementations that receive multiple blocks at a time.
",1294110352
b566be47d7b0ec93b2bee7d2eb76865b2f145c67 remotes/origin/benchmarks~578^2~5^2~22,"Bug fix/update: All the shuffle implementations are using consistent config parameters.
",1293760321
1e26fb39534cc5b79c6980e0974bedbf72e19c69 remotes/origin/benchmarks~578^2~5^2~24,"CustomBlockedLocalFileShuffle: reducers are reading multiple blocks per connections instead of just one.
Sometimes ShuffleServer fails to start for small shuffle data with small block size in local VM. No problem with large block size.
",1293744814
5074e8500a0ccd5dc63f11e07194d154e164bb15 remotes/origin/benchmarks~578^2~5^2~27," - Implemented TrackedCustomBlockedLocalFileShuffle.
 - Fixed several bugs. (Copy-paste is the bane of coding :|)
",1293578923
7ac3463ab88e81dfc7af4bac980ac03b65c499fd remotes/origin/benchmarks~578^2~5^2~31,"Bug fix: tracker (running in Spark master) wasn't initializing Shuffle object and was using inconsistent config values.
",1293447345
20eec59f04a2565598b96010bef439527e17edcf remotes/origin/benchmarks~578^2~38,"Bug fix + formatting.
",1293346486
90e467206dfa45e4f3a1f475352d16b7d254b2a8 remotes/origin/benchmarks~578^2~5^2~34,"Tracker framework is in place that supports pluggable tracker strategy. There are several bugs along with performance problems.

 - For larger data shuffle ShuffleServerThread gets ""Broken Pipe"" and ShuffleClient gets ""Connection Reset""
 - There is a bug in the accounting counters of BalanceConnectionsShuffleTrackerStrategy. Some of them go below zero while decrementing which is not supposed to happen.
",1293345950
23586d3bef257fe8bd4b9a3993e9c24c3cb5c50e remotes/origin/benchmarks~578^2~5^2~40,"Added an in-memory implementation of CustomParalleLFS. There is a serialization/deserialization bug in the implementation.
",1293065126
c4c8f72e98f10eddfaa88a2189fa1745ecf6e0dc remotes/origin/benchmarks~578^2~5^2~41,"Fixed an indexing bug in HttpBlockedLocalFileShuffle. It still doesn't work on EC2 with >5 nodes cluster.
",1293050891
5f0cdabd408cf8af3ebb15d78cd7072b396f8e47 remotes/origin/benchmarks~578^2~5^2~46,"Added a separate thread to deserialize (1 thread per reducer) in CustomParallelLocalFileShuffle
Upside: No synchronized blocking on ""combiners"" variable. 3x faster :)
Downside: Inefficient implementation. Requiring too much temporary data. Approx. 2x increase in memory requirement :( Should be fixed at some point.
",1292997157
f47fb44479da45a00cad57031163b42e845685cf remotes/origin/benchmarks~578^2~5^2~49," - Divided maxConnections to max[Rx|Tx]Connections.
 - Fixed config param loading bug in CustomParallelLFS
",1292981691
be0ce57de24a8e2a1941d7ce81089043624dc5c0 remotes/origin/benchmarks~578^2~43," - Fixed an compilation error due to wrong 'import' of legacy lzf libraries in DfsBroadcast.scala
 - Updated to use ning libraries.
 - Passes all unit tests
",1292466867
34395730dbae0913674212b0d8c981db122d1643 remotes/origin/benchmarks~578^2~44^2,"Someone forgot to pass the parameters: fixes SPARK_MEM set from main script but not passed to executor.
",1292189449
476a216d9df5c4d9eb6d6fae3d7bc6562421747d remotes/origin/benchmarks~578^2~5^2~59,"Parallel is working. Need to fix/finalize some config parameters.
",1291457141
0d7ca7751e004f2374c08ca707bc3c3d9e47fb44 remotes/origin/benchmarks~578^2~5^2~61,"Bug fixes. Not yet parallel.
",1291450007
9d0111659bc8f2e186cf28cf968013e121f96605 remotes/origin/benchmarks~578^2~49^2~7,"Solved compilation issue. Should've been more careful to do a clean compilation before committing.
",1290838238
af74217c9643a87cbd9df333aefbc7840b0cd188 remotes/origin/benchmarks~647,"Undo JLine fix that turns out to only be needed when buildr is running
on JRuby. This is quite ugly: JRuby has its own version of JLine which
is older than Scala's, and JLine changed API in such a way that code
written for the new version won't compile with the old one and vice
versa. Sadly, this might be a reason to drop buildr, unless we can
package a JRuby with it that uses the right version, or we ask people to
only use the C Ruby version of buildr (which doesn't work on OS X right
now!)
",1289702330
f8966ffc11773a4c090dcf359e3892027992ed98 remotes/origin/benchmarks~649,"Added a shuffle test with negative hash codes for some keys (this was a bug earlier)
",1289607525
88d08b7a895d80ee2b62f711b0cdc291688c6e9f remotes/origin/benchmarks~578^2~66,"Bug fix.
",1289437454
642098c21135d5944db224a255f229799e98ba6b remotes/origin/benchmarks~578^2~69,"Bug fixes and/or minor optimization.
",1289375946
1820634dbffab5bf2c9af43d4eba6db8fcb0ba06 remotes/origin/benchmarks~578^2~49^2~12,"Bug fix: java-opts had wrong (non-capitalized) variable names.
",1289269937
c37c74919f31ec69e359786bc70d40b7d7319e22 remotes/origin/benchmarks~578^2~74,"Bug fix + minor changes
",1289087695
4cc0984b43095aaebcb565bcb0ac0a71b6cef7ca remotes/origin/benchmarks~655^2,"Fixed a small bug in DFS shuffle -- the number of reduce tasks was not being set based on numOutputSplits
",1288906495
d947cb97789acd04b7f02410490fa3f91c2a7bca remotes/origin/benchmarks~659,"Fixed a bug with negative hashcodes
",1288849961
ae2c93a54b20ed2ababcfba095be84077fa78cfe remotes/origin/benchmarks~578^2~81,"Fixed some configuration bugs
",1288221506
3f96c9b7e8275d21d0f323c6218262e049fff504 remotes/origin/benchmarks~578^2~90,"Resolved some bugs. Apparently, objects deep inside other objects could be passed as references. Bad Scala!
",1288122987
dba92f7dbea2f0b13cbe98bbb5cdb599b5dbb86d remotes/origin/benchmarks~578^2~92,"- Fixed bugs with storing received blocks in appropriate indices.
- receiveBroadcast now returns, but still no good way to stop threads from running.
",1288053265
2099aadbbdb403edd6d860d8064bb418de310a60 remotes/origin/benchmarks~578^2~94,"Implemented rarest first policy. This isn't working as expected (i.e., not distributing load as expected), probably due to some issues with how we pick peers to talk to.
",1288043291
93a200bc7efd1a867d9cc3759e82327cfdf325b0 remotes/origin/benchmarks~667,"Renamed aggregateSplit() to splitRdd(), plus some style fixes
",1287873243
7a7123b5253c238ae72afcc31354615fd7ce58b9 remotes/origin/benchmarks~578^2~95,"Running in local Mesos. Multiple things have to be fixed though. Go through the TODOs...
",1287816191
787faf0d0e288a64ec92f1013526a35525b78084 remotes/origin/benchmarks~668,"Fixed a bug with scheduling of tasks that have no locality preferences.
These tasks were being subjected to delay scheduling but then counted as
having been launched on a preferred node. The solution is to have a
separate queue for them and treat them as preferred during scheduling.
",1287529678
dbdd7682eb4efc28062733f76a2bd5d04856a142 remotes/origin/benchmarks~670^2~12,"Bug fixes and improvements for MesosScheduler and SimpleJob
",1287250736
47b38fd207c703c4bb7e7b771f399c15292ab944 remotes/origin/benchmarks~670^2~14,"Bug fix in passing env vars to executors
",1287246103
aa8ccec315226f83081a774b351e0d8401cd252b remotes/origin/benchmarks~670^2~18,"Abort jobs if a task fails more than a limited number of times
",1287183446
34eccedbf5781b8723abdc1e6fd3d98e14056999 remotes/origin/benchmarks~674^2~3,"Fixed a rather bad bug in HDFS files that has been in for a while:
caching was not working because Split objects did not have a
consistent toString value
",1286082366
0d28bdcefd1f03db5ac027973b77961a59bfe1fa remotes/origin/benchmarks~674^2~4^2~2,"A couple of minor fixes:
- Don't include trailing $'s in class names of Scala objects
- Report errors using logError instead of printStackTrace
",1285744246
7090dea44bf9cc671bceb4b3dbee4ac3670c26b2 remotes/origin/benchmarks~674^2~4^2~4,"Changed printlns to log statements and fixed a bug in run that was causing it to fail on a Mesos cluster
",1285743269
b749f0e20961ae0517d3aa4f54c4f6391a5737a9 remotes/origin/benchmarks~674^2~7,"fixed typo in printing which task is already finished
",1285720134
3d8d7fd557776019670fe954fd2e89d526dc9a9c remotes/origin/benchmarks~687,"Bug fix from Justin
",1281724159
4488b3bc8a4483eeeb703d96842ea3e9695300c3 remotes/origin/benchmarks~689,"Fixed a bug where we would incorrectly decide we've finished a parallel operation if Mesos tells us a task is finished twice
",1281397574
ee6c524fdf2046a52152d5327789ca492169ecf4 remotes/origin/benchmarks~578^2~135,"Fixed some bugs in speed-based PQ-ing.
",1271392797
