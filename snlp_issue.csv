https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/259,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/259/comments,Vivekn Sentiment Performance improvements,"<!--- Provide a general summary of your changes in the Title above -->

## Description
<!--- Describe your changes in detail -->
This fixes a long intersection operation during prediction and training. Improves performance significantly.

## Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->

## How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->

## Screenshots (if appropriate):

## Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] Code improvements with no or little impact
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)

## Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [ ] My code follows the code style of this project.
- [ ] My change requires a change to the documentation.
- [ ] I have updated the documentation accordingly.
- [ ] I have read the [CONTRIBUTING](http://nlp.johnsnowlabs.com/contribute.html) page.
- [ ] I have added tests to cover my changes.
- [x] All new and existing tests passed.
"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/253,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/253/comments,NER label always 'O' with Pyspark ,"When training a ner label via Pyspark, the results of the NER tags always is 'O'.
This issue happens when following the provided example notebooks for NER tagging. One can simply reproduce this by following the example on Github: 
https://github.com/JohnSnowLabs/spark-nlp/blob/1.6.0/python/example/crf-ner/ner.ipynb

It seems to work fine when using the pretrained() POS and NER in a pipeline, but using a NerCrfApproach() in a pipeline results in 'O'. 


"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/250,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/250/comments,Issue while loading a saved model into a different cluster,"The below error occurred when I created and saved a custom model in CDH cluster and tried to load in AWS EMR.


Traceback (most recent call last):
  File ""/tmp/zeppelin_pyspark-5073389282467574021.py"", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File ""/tmp/zeppelin_pyspark-5073389282467574021.py"", line 360, in <module>
    exec(code, _zcUserQueryNameSpace)
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/spark/python/pyspark/ml/util.py"", line 257, in load
    return cls.read().load(path)
  File ""/usr/lib/spark/python/pyspark/ml/util.py"", line 197, in load
    java_obj = self._jread.load(path)
  File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""/usr/lib/spark/python/pyspark/sql/utils.py"", line 79, in deco
    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)
IllegalArgumentException: u'Wrong FS: s3://blah/model/ner_crf_custom_model/stages/4_NER_b3f45074bee5/embeddings, expected: hdfs://ip-.us-west-2.compute.internal:8020'"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/248,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/248/comments,Finisher throws Exception when flattening Regex results,"<!--- Provide a general summary of the issue in the Title above -->
When setting up a pipeline with RegexMatcher I got an exception executing the finisher step
## Description
<!--- Provide a more detailed introduction to the issue itself, and why you consider it to be a bug -->
1. Build test datafram
2. Load Regex pattern
3. Match
4. Run finisher
## Expected Behavior
<!--- Tell us what should happen -->
Expect new column in dataframe with results from RegexMatcher
## Current Behavior
<!--- Tell us what happens instead of the expected behavior -->
org.apache.spark.SparkException: Failed to execute user defined function(anonfun$flatten$1: (array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>>>) => string)
  at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1072)
  at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:144)
  at org.apache.spark.sql.catalyst.expressions.InterpretedProjection.apply(Projection.scala:48)
  at org.apache.spark.sql.catalyst.expressions.InterpretedProjection.apply(Projection.scala:30)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$21.applyOrElse(Optimizer.scala:1078)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$21.applyOrElse(Optimizer.scala:1073)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:287)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:277)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1073)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1072)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
  at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
  at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:73)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:73)
  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:79)
  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:75)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:84)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:84)
  at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2791)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2112)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2327)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:248)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:636)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:595)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:604)
  ... 82 elided
Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$dfAnnotate$1: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>>>)
  at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1072)
  at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:90)
  at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:88)
  at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1069)
  ... 133 more
Caused by: java.util.regex.PatternSyntaxException: Unclosed character class near index 0
[
^
  at java.util.regex.Pattern.error(Pattern.java:1955)
  at java.util.regex.Pattern.clazz(Pattern.java:2548)
  at java.util.regex.Pattern.sequence(Pattern.java:2063)
  at java.util.regex.Pattern.expr(Pattern.java:1996)
  at java.util.regex.Pattern.compile(Pattern.java:1696)
  at java.util.regex.Pattern.<init>(Pattern.java:1351)
  at java.util.regex.Pattern.compile(Pattern.java:1028)
  at scala.util.matching.Regex.<init>(Regex.scala:191)
  at scala.collection.immutable.StringLike$class.r(StringLike.scala:255)
  at scala.collection.immutable.StringOps.r(StringOps.scala:29)
  at scala.collection.immutable.StringLike$class.r(StringLike.scala:244)
  at scala.collection.immutable.StringOps.r(StringOps.scala:29)
  at com.johnsnowlabs.nlp.util.regex.RegexRule.<init>(RegexRule.scala:12)
  at com.johnsnowlabs.nlp.annotators.RegexMatcherModel$$anonfun$com$johnsnowlabs$nlp$annotators$RegexMatcherModel$$matchFactory$1.apply(RegexMatcherModel.scala:50)
  at com.johnsnowlabs.nlp.annotators.RegexMatcherModel$$anonfun$com$johnsnowlabs$nlp$annotators$RegexMatcherModel$$matchFactory$1.apply(RegexMatcherModel.scala:50)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
  at com.johnsnowlabs.nlp.annotators.RegexMatcherModel.com$johnsnowlabs$nlp$annotators$RegexMatcherModel$$matchFactory$lzycompute(RegexMatcherModel.scala:50)
  at com.johnsnowlabs.nlp.annotators.RegexMatcherModel.com$johnsnowlabs$nlp$annotators$RegexMatcherModel$$matchFactory(RegexMatcherModel.scala:48)
  at com.johnsnowlabs.nlp.annotators.RegexMatcherModel$$anonfun$annotate$1.apply(RegexMatcherModel.scala:55)
  at com.johnsnowlabs.nlp.annotators.RegexMatcherModel$$anonfun$annotate$1.apply(RegexMatcherModel.scala:54)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
  at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
  at com.johnsnowlabs.nlp.annotators.RegexMatcherModel.annotate(RegexMatcherModel.scala:54)
  at com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:43)
  at com.johnsnowlabs.nlp.AnnotatorModel$$anonfun$dfAnnotate$1.apply(AnnotatorModel.scala:42)
  at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:89)
  at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:88)
  at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1069)
  ... 136 more
ERROR
## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->
Looks like an issue when loading pattern from file. My Patternfile contains just a single line:

[regex.txt](https://github.com/JohnSnowLabs/spark-nlp/files/2208485/regex.txt)
[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}|email-address

## Steps to Reproduce
<!--- Provide a link to a live example, or an unambiguous set of steps to -->
<!--- reproduce this bug. Include code to reproduce, if relevant -->
val test = Seq(
(""mail to john.snow@mail.com"")
).toDF(""text"")
import com.johnsnowlabs.nlp.annotator._
import com.johnsnowlabs.nlp.base._
import com.johnsnowlabs.util.Benchmark
import org.apache.spark.ml.Pipeline

import com.johnsnowlabs.nlp.util.io.{ExternalResource, ReadAs, ResourceHelper}

val documentAssembler = new DocumentAssembler()
      .setInputCol(""text"")
      .setOutputCol(""document"")

val resource = ExternalResource(""../data/regex.txt"", ReadAs.LINE_BY_LINE,Map(""delimiter""->""|""))    
  
val mailExtractor = new RegexMatcher()
      .setInputCols(""document"")
      .setRules(resource)
      .setOutputCol(""contact"")
      
val finisher = new Finisher()
      .setInputCols(""contact"")
      .setOutputAsArray(false)
      .setAnnotationSplitSymbol(""@"")
      .setValueSplitSymbol(""#"")
      
val recursivePipeline = new RecursivePipeline()
      .setStages(Array(
        documentAssembler,
        mailExtractor,
        finisher
))

recursivePipeline.fit(test).transform(test).show
## Context
<!--- How has this bug affected you? What were you trying to accomplish? -->
Try to extract mail adresses from text 
## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Version used:Zeppelin notebook with /libs/spark-nlp-assembly-1.6.0.jar 
* Browser Name and version:
* Operating System and version (desktop or mobile):
* Link to your project:
"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/247,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/247/comments,Wrong FS when loading NerCrfModel,"<!--- Provide a general summary of the issue in the Title above -->
## Description
Saving a trained NerCrf model to s3 and loading it back throws a java.lang.IllegalArgumentException exception with the message ""Wrong FS, ... expected: file://....""

## Expected Behavior
<!--- Tell us what should happen -->
A saved model (either standalone or as part of a spark PipelineModel) should be loadable from s3

## Current Behavior
<!--- Tell us what happens instead of the expected behavior -->
While the model can be saved at the moment, reading it back throws an exception. A sample stacktrace looks like this:

`Py4JJavaError: An error occurred while calling o872.load.
: java.lang.IllegalArgumentException: Wrong FS: s3n://myntelligence-classifier-stage/stage/model/stages/4_NER_be413400d8f4/embeddings, expected: file:///
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:649)
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:82)
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:606)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)
	at org.apache.hadoop.fs.LocalFileSystem.copyToLocalFile(LocalFileSystem.java:88)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1979)
	at com.johnsnowlabs.nlp.embeddings.SparkWordEmbeddings$.indexEmbeddings(SparkWordEmbeddings.scala:69)
	at com.johnsnowlabs.nlp.embeddings.SparkWordEmbeddings$.apply(SparkWordEmbeddings.scala:108)
	at com.johnsnowlabs.nlp.HasWordEmbeddings$class.deserializeEmbeddings(HasWordEmbeddings.scala:61)
	at com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfModel.deserializeEmbeddings(NerCrfModel.scala:19)
	at com.johnsnowlabs.nlp.embeddings.EmbeddingsReadable$class.readEmbeddings(EmbeddingsReadable.scala:8)
	at com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfModel$.readEmbeddings(NerCrfModel.scala:84)
	at com.johnsnowlabs.nlp.embeddings.EmbeddingsReadable$$anonfun$1.apply(EmbeddingsReadable.scala:11)
	at com.johnsnowlabs.nlp.embeddings.EmbeddingsReadable$$anonfun$1.apply(EmbeddingsReadable.scala:11)
	at com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$com$johnsnowlabs$nlp$ParamsAndFeaturesReadable$$onRead$1.apply(ParamsAndFeaturesReadable.scala:31)
	at com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$com$johnsnowlabs$nlp$ParamsAndFeaturesReadable$$onRead$1.apply(ParamsAndFeaturesReadable.scala:30)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$class.com$johnsnowlabs$nlp$ParamsAndFeaturesReadable$$onRead(ParamsAndFeaturesReadable.scala:30)
	at com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$read$1.apply(ParamsAndFeaturesReadable.scala:41)
	at com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$read$1.apply(ParamsAndFeaturesReadable.scala:41)
	at com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:19)
	at com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:8)
	at org.apache.spark.ml.util.DefaultParamsReader$.loadParamsInstance(ReadWrite.scala:438)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$4.apply(Pipeline.scala:273)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$4.apply(Pipeline.scala:271)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:271)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:347)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
`

**pm = PipelineModel.read().load(""s3n://path"")**
This is the line that is throwing the above error

## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->

## Steps to Reproduce
<!--- Provide a link to a live example, or an unambiguous set of steps to -->
<!--- reproduce this bug. Include code to reproduce, if relevant -->
1.
2.
3.
4.

## Context
<!--- How has this bug affected you? What were you trying to accomplish? -->

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Version used: 1.5.4
* Browser Name and version:
* Operating System and version (desktop or mobile):
* Link to your project:

Please let me know if you need more information. Thanks
"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/246,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/246/comments,Scaladoc,Is this available somewhere? I'm working on the R API and it would be helpful to have documentation that's more details than http://nlp.johnsnowlabs.com/components.html.
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/245,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/245/comments,Error while loading model files,"<!--- Provide a general summary of the issue in the Title above -->

I am unable to load model files from local and from s3 in my spark application which i trained using NerCrfModel.  

## Description
<!--- Provide a more detailed introduction to the issue itself, and why you consider it to be a bug -->
While trying to load the model files from local directory and from s3 this is what I am getting.

`Exception in thread ""main"" org.apache.spark.SparkException: addFile does not support local directories when not running local mode.
	at org.apache.spark.SparkContext.addFile(SparkContext.scala:1537)
	at com.johnsnowlabs.nlp.embeddings.SparkWordEmbeddings$.copyIndexToCluster(SparkWordEmbeddings.scala:86)
	at com.johnsnowlabs.nlp.embeddings.SparkWordEmbeddings$.apply(SparkWordEmbeddings.scala:111)
	at com.johnsnowlabs.nlp.HasWordEmbeddings$class.deserializeEmbeddings(HasWordEmbeddings.scala:57)
	at com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfModel.deserializeEmbeddings(NerCrfModel.scala:19)
	at com.johnsnowlabs.nlp.embeddings.EmbeddingsReadable$class.readEmbeddings(EmbeddingsReadable.scala:8)
	at com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfModel$.readEmbeddings(NerCrfModel.scala:84)
	at com.johnsnowlabs.nlp.embeddings.EmbeddingsReadable$$anonfun$1.apply(EmbeddingsReadable.scala:11)
	at com.johnsnowlabs.nlp.embeddings.EmbeddingsReadable$$anonfun$1.apply(EmbeddingsReadable.scala:11)
	at com.johnsnowlabs.nlp.ParamsAndFeaturesReadable$$anonfun$com$johnsnowlabs$nlp$ParamsAndFeaturesReadable$$onRead$1.apply(ParamsAndFeaturesReadable.scala:31)
`

This is how i load the models  -

`val model = PipelineModel.read.load(""/opt/model/"")`

I have checked that the model files are present at this path on all the worker nodes. So it looks like the library is trying to call spark's addFile method which doesn't support local file directories in cluster or client mode.
Please let me know if you need more information.
Thanks.

## Expected Behavior
<!--- Tell us what should happen -->

## Current Behavior
<!--- Tell us what happens instead of the expected behavior -->

## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->

## Steps to Reproduce
<!--- Provide a link to a live example, or an unambiguous set of steps to -->
<!--- reproduce this bug. Include code to reproduce, if relevant -->
1.
2.
3.
4.

## Context
<!--- How has this bug affected you? What were you trying to accomplish? -->

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Version used:
* Browser Name and version:
* Operating System and version (desktop or mobile):
* Link to your project:
"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/243,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/243/comments,Feature request: setMaxLength for SentenceDetector,"The Stanford CoreNLP has a global flag to improve performance where it avoids very long sentence from being parsed. I am not sure how this library is parsing for sentence detection, but I am sure the length of sentences and characters are impacting performance.


## Description
`setMaxLength`: Add option to avoid parsing any sentence more than `maxLength` to boost performance/memory especially in training stage.

```
val sentenceDetector = new SentenceDetector()
  .setInputCols(""document"")
  .setOutputCol(""sentence"")
  .setMaxLength(""1000"") // avoid parsing any sentence more than 1000 characters 
```


"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/235,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/235/comments,'Finisher' object has no attribute 'setIncludeMetadata',"When working with PySpark Finisher does not seem to have this attribute implemented.

finisher = Finisher() \
  .setInputCols([""assembled""]) \
  .setCleanAnnotations(True) \
  .setOutputAsArray(False) \
  .setIncludeKeys(True) \
  .setIncludeMetadata(False)

This happens on machine with Apache Spark 2.3.1 and SparkNLP 1.5.4 
"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/225,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/225/comments,Getting typesafe config error while loading the model file,"<!--- Provide a general summary of the issue in the Title above -->
I have a project in scala and spark, that also uses typesafe config. I have already created a trained pipeline and model using the default training data provided in the examples. Now In this project, I am trying to load these models for a prediction on test data. While trying to load the model this is what I am getting -
```
Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'settings'
	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152)
	at com.typesafe.config.impl.SimpleConfig.findKey(SimpleConfig.java:145)
	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:172)
	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184)
	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189)
	at com.typesafe.config.impl.SimpleConfig.getString(SimpleConfig.java:246)
	at com.johnsnowlabs.util.ConfigLoader$.<init>(ConfigLoader.scala:10)
	at com.johnsnowlabs.util.ConfigLoader$.<clinit>(ConfigLoader.scala)

```

## Description
<!--- Provide a more detailed introduction to the issue itself, and why you consider it to be a bug -->
This is how i am loading the model and pipeline in the code -
 ```
val config: Config = ConfigFactory.load.getConfig(""application"")
val pipeline = Pipeline.read.load(config.getString(""nlp.pipeline.folder""))
val model = PipelineModel.read.load(config.getString(""nlp.model.folder""))
```
And the error comes in the last line _PipelineModel.read.load(config.getString(""nlp.model.folder""))_

**Some points to consider**
- If i print **config.getString(""nlp.pipeline.folder"")**, i get the correct path to the pipepline. So there is no issue in the path.
- I am able to load the pipeline so the error only comes while loading the model.
- And when i remove the typesafe library from my code completely, I am able to load both the pipeline and model. So the code is working fine when i don't use typesafe in my project.

It looks like the spark-nlp code is trying to find key 'settings' in the application.conf file that i created for my project instead of the library. 
I tried to exclude the typesafe from the library but it didn't work-

```
<dependency>
            <groupId>com.johnsnowlabs.nlp</groupId>
            <artifactId>spark-nlp_2.11</artifactId>
            <version>1.5.4</version>
            <exclusions>
                <exclusion>
                    <groupId>com.typesafe</groupId>
                    <artifactId>config</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
```

Is there anything that am i missing? 
Thanks"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/219,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/219/comments,Python setters are missing from python annotator models,"<!--- Provide a general summary of the issue in the Title above -->

## Description
<!--- Provide a more detailed introduction to the issue itself, and why you consider it to be a bug -->
We are not capable of overriding params once an approach has been trained into a model, in Python. Setters are missing."
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/215,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/215/comments,RecursivePipeline Error,"**I was trying to train using RecursivePipeline and I got this error while doing fit:**

Py4JJavaError                             Traceback (most recent call last)
~/Desktop/charles/workspace/spark2.30/python/pyspark/sql/utils.py in deco(*a, **kw)
     62         try:
---> 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:

~/Desktop/charles/workspace/spark2.30/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    319                     ""An error occurred while calling {0}{1}{2}.\n"".
--> 320                     format(target_id, ""."", name), value)
    321             else:

Py4JJavaError: An error occurred while calling o61.transform.
: org.apache.spark.sql.AnalysisException: Cannot resolve column name ""ner"" among (text, document, sentence, token, pos);
	at org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:222)
	at org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:222)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.Dataset.resolve(Dataset.scala:221)
	at org.apache.spark.sql.Dataset.col(Dataset.scala:1241)
	at com.johnsnowlabs.nlp.Finisher$$anonfun$transform$2.apply(Finisher.scala:99)
	at com.johnsnowlabs.nlp.Finisher$$anonfun$transform$2.apply(Finisher.scala:90)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at com.johnsnowlabs.nlp.Finisher.transform(Finisher.scala:90)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)


During handling of the above exception, another exception occurred:

AnalysisException                         Traceback (most recent call last)
<ipython-input-9-c7b12cdb6148> in <module>()
      2 print(""Start fitting"")
      3 # model = pipeline.fit(data)
----> 4 pipeline.fit(spark.createDataFrame([['']]).toDF(""text""))
      5 print(""Fitting is ended"")
      6 print (time.time() - start)

~/Desktop/charles/workspace/spark2.30/python/pyspark/ml/base.py in fit(self, dataset, params)
    130                 return self.copy(params)._fit(dataset)
    131             else:
--> 132                 return self._fit(dataset)
    133         else:
    134             raise ValueError(""Params must be either a param map or a list/tuple of param maps, ""

/tmp/spark-c0b09f09-dbb0-4ae5-ae7c-0d59514961c7/userFiles-58adcfbe-c750-48cb-b85a-17d600b21b6e/JohnSnowLabs_spark-nlp-1.5.3.jar/sparknlp/base.py in _fit(self, dataset)
    145             if isinstance(stage, Transformer):
    146                 transformers.append(stage)
--> 147                 dataset = stage.transform(dataset)
    148             elif isinstance(stage, JavaRecursiveEstimator):
    149                 model = stage.fit(dataset, pipeline=PipelineModel(transformers))

~/Desktop/charles/workspace/spark2.30/python/pyspark/ml/base.py in transform(self, dataset, params)
    171                 return self.copy(params)._transform(dataset)
    172             else:
--> 173                 return self._transform(dataset)
    174         else:
    175             raise ValueError(""Params must be a param map but got %s."" % type(params))

~/Desktop/charles/workspace/spark2.30/python/pyspark/ml/wrapper.py in _transform(self, dataset)
    303     def _transform(self, dataset):
    304         self._transfer_params_to_java()
--> 305         return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)
    306 
    307 

~/Desktop/charles/workspace/spark2.30/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1158         answer = self.gateway_client.send_command(command)
   1159         return_value = get_return_value(
-> 1160             answer, self.gateway_client, self.target_id, self.name)
   1161 
   1162         for temp_arg in temp_args:

~/Desktop/charles/workspace/spark2.30/python/pyspark/sql/utils.py in deco(*a, **kw)
     67                                              e.java_exception.getStackTrace()))
     68             if s.startswith('org.apache.spark.sql.AnalysisException: '):
---> 69                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)
     70             if s.startswith('org.apache.spark.sql.catalyst.analysis'):
     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)

AnalysisException: 'Cannot resolve column name ""ner"" among (text, document, sentence, token, pos);'


**The Pipeline code I am using is:**

documentAssembler = DocumentAssembler()\
  .setInputCol(""text"")\
  .setOutputCol(""document"")

sentenceDetector = SentenceDetector()\
  .setInputCols([""document""])\
  .setOutputCol(""sentence"")

tokenizer = Tokenizer()\
  .setInputCols([""document""])\
  .setOutputCol(""token"")

posTagger = PerceptronApproach()\
  .setIterations(1)\
  .setInputCols([""token"", ""document""])\
  .setOutputCol(""pos"")\
  .setCorpus(""file:///"" + os.getcwd() + ""/anc-pos-corpus-small/"", ""|"")

nerTagger = NerCrfApproach()\
  .setInputCols([""sentence"", ""token"", ""pos""])\
  .setLabelColumn(""label"")\
  .setOutputCol(""ner"")\
  .setMinEpochs(1)\
  .setMaxEpochs(1)\
  .setLossEps(1e-3)\
  .setEmbeddingsSource(""glove.6B.100d.txt"", 100, 2)\
  .setExternalFeatures(""file:///"" + os.getcwd() + ""/ner-corpus/dict.txt"", "","")\
  .setExternalDataset(""file:///"" + os.getcwd() + ""/ner2.train"")\
  .setL2(1)\
  .setC0(1250000)\
  .setRandomSeed(0)\
  .setVerbose(2)

finisher = Finisher() \
    .setInputCols([""ner""]) \
    .setIncludeKeys(True)

pipeline = RecursivePipeline(
    stages = [
    documentAssembler,
    sentenceDetector,
    tokenizer,
    posTagger,
    nerTagger,
    finisher
  ])"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/212,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/212/comments,Using pretrained in PerceptronModel results in NoClassDefFoundError,"Hi,

I am following up on an issue I reported before: https://github.com/JohnSnowLabs/spark-nlp/issues/162

I am still experiencing the same issue in version 1.5.4 on local Spark:
```
Exception in thread ""main"" java.lang.NoClassDefFoundError: com/amazonaws/auth/AnonymousAWSCredentials
	at com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.<init>(ResourceDownloader.scala:51)
	at com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.<clinit>(ResourceDownloader.scala)
	at com.johnsnowlabs.nlp.annotators.pos.perceptron.PretrainedPerceptronModel$class.pretrained$default$3(PerceptronModel.scala:72)
	at com.johnsnowlabs.nlp.annotator$PerceptronModel$.pretrained$default$3(annotator.scala:63)
	at Test_NLP_Libraries$.Test_English(Test_NLP_Libraries.scala:76)
	at Main$.main(Main.scala:18)
	at Main.main(Main.scala)
Caused by: java.lang.ClassNotFoundException: com.amazonaws.auth.AnonymousAWSCredentials
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 7 more
```
Here is my sbt build: https://github.com/multivacplatform/multivac-nlp/blob/master/build.sbt

Many thanks,
Maziyar"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/203,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/203/comments,Issue with spark-nlp_2.11-1.5.3.jar (Unable to find class: [Lcom.johnsnowlabs.nlp.annotators.pos.perceptron.AveragedPerceptron;),"#191 
Issue 191 is not resolved yet..

This issue still persists for me even after using the latest JAR (spark-nlp_2.11-1.5.3.jar
) file [Downloaded from maven]

Below is the scala code I have used:

**_%AddJar file:/home/cdsw/spark/spark-nlp_2.11-1.5.3.jar_**

**import com.johnsnowlabs.nlp.base._
import com.johnsnowlabs.nlp.annotator._
import org.apache.spark.ml.Pipeline
import com.johnsnowlabs.nlp.pretrained.ResourceDownloader
import com.johnsnowlabs.nlp.annotators.Lemmatizer
import com.johnsnowlabs.nlp.annotators.pos.perceptron.AveragedPerceptron**

**_val pos = PerceptronModel.pretrained()_**

Here is the error I get:

_val pos = PerceptronModel.pretrained()
18/05/11 07:22:54 ERROR scheduler.TaskResultGetter: Exception while getting task result
com.esotericsoftware.kryo.KryoException: Unable to find class: [Lcom.johnsnowlabs.nlp.annotators.pos.perceptron.AveragedPerceptron;
at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:156)
at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:133)
at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:670)
at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:781)
at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:330)
at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:88)
at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:72)
at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: com.johnsnowlabs.nlp.annotators.pos.perceptron.AveragedPerceptron
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:154)
... 13 more
Name: org.apache.spark.SparkException
Message: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: Unable to find class: [Lcom.johnsnowlabs.nlp.annotators.pos.perceptron.AveragedPerceptron;
StackTrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
at scala.Option.foreach(Option.scala:257)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)
at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
at org.apache.spark.rdd.RDD.take(RDD.scala:1327)
at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1368)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
at org.apache.spark.rdd.RDD.first(RDD.scala:1367)
at com.johnsnowlabs.nlp.serialization.StructFeature.deserializeObject(Feature.scala:117)
at com.johnsnowlabs.nlp.serialization.Feature.deserialize(Feature.scala:44)
at com.johnsnowlabs.nlp.FeaturesReader$$anonfun$load$1.apply(ParamsAndFeaturesReadable.scala:15)
at com.johnsnowlabs.nlp.FeaturesReader$$anonfun$load$1.apply(ParamsAndFeaturesReadable.scala:14)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:14)
at com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:8)
at com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:108)
at com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadModel(ResourceDownloader.scala:102)
at com.johnsnowlabs.nlp.annotators.pos.perceptron.PretrainedPerceptronModel$class.pretrained(PerceptronModel.scala:73)
at com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel$.pretrained(PerceptronModel.scala:76)_"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/166,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/166/comments,Does spark nlp work with Apache ignite?,I want to perform some ml task in application written in java and then some spark nlp task in python and / or scala applications. Is it possible to have the data shared between different applications written in different languages (one of which performs spark nlp processing) using Apache Ignite?
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/110,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/110/comments,Input format examples in DateMatcher(),"Could you please include some examples of accepted input date formats that would be automatically detected by the DateMatcher? I have a corpus using multiple formats that are not detected, here are some examples:
```
06JAN2018
June 30, 2021
MAY 2018
```
Is it be possible to add our own custom input formats?

Also, it is not clear what are the accepted InputCols as well. Having tried Tokens and Sentences crashes the pipeline execution."
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/108,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/108/comments,"SentenceDetector replaces ""ö"" with ""?""","The SentenceDetector is applied on German texts. If an ""ö"" occurs, it is replaced by ""?"".  However, if other German special characters (such as ä,ü,ß) occur, the SentenceDetector performs as it should, and does not replace them by a question mark.

## Expected Behavior
The SentenceDetector should not change the text within its annotation.

## Current Behavior
The SentenceDetector replaces ""ö"" by ""?"".

## Steps to Reproduce
1. Run the test-case:
[sentenceDetector.txt](https://github.com/JohnSnowLabs/spark-nlp/files/1741113/sentenceDetector.txt)

2. Show the transformed dataframe:
[output.txt](https://github.com/JohnSnowLabs/spark-nlp/files/1741105/output.txt)
The token of question is the second one within the second row.

## Context
Subsequent annotators perform worse, for example a lemma cannot be found if the token was changed.

## Your Environment
* Version used: 1.4.0"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/87,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/87/comments,All HTML pages should have unique title,"
## Description
Titles are same for most of the pages e.g. components.html, faqs.html. We should add unique and contextual titles for each page.
title tags are used in SEO and SERPs (Search engine result pages)

"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/52,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/52/comments,"Lemmatizer should use POS and token to generate Lemma, have a default dictionary, and allow addition + overriding","## Description

This is an enhancement request the Lemmatizer.

## Expected Behavior

1. The current lemmatizer appears to be dictionary based, and the caller needs to supply a mapping of word to lemma. From other dictionary based lemmatizers I have seen, the key is usually (token, pos). This is because certain words need to be lemmatized differently based on what POS it is - for example, stranger as NOUN is lemmatized to stranger, but stranger as ADJ is lemmatized to strange. The Transformer API allows this, I see others support setInputCols(Array[String]) signature, ie, setInputCols(Array(""token"", ""pos"")) instead of setInputCols(Array(""token"")) as it is currently. Only thing to be careful of is that the pos tag style output by the POSTagger must match the style in the Lemma dictionary key.

2. The implementation does not provide a lemma dictionary and requires the caller to supply one. There are good open source lemma dictionaries available thanks to the [FreeLing project](https://github.com/TALP-UPC/FreeLing) for various languages (look under data/{lang}/dictionary/entries), which could be leveraged and provided as part of the spark-nlp download (at least for language=en) so there would be a sensible default for most people. This would make it easier for people to start working with lemmatizers in spark-nlp.

3. Lemmatizer dictionaries are usually incomplete, so an addLemmaDict() call could be provided that allows user-supplied mappings to be added to the dictionary. The setLemmaDict() could still be used to override the default if needed.

## Possible Solution

I haven't provided a solution, since I figure that this is a fairly easy fix for people familiar with the codebase, but if you would prefer a pull request, please let me know and I can work on it.

## Your Environment

* Version used: spark-nlp-2.11-1.2.3
"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/40,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/40/comments,MLeap integration with spark-nlp,"MLeap provides a way to deploy Spark-trained pipelines to production-ready API servers

## Description
For NLP, the use cases for this abound. This library is great for training. Having an MLeap integration would mean we can deploy the models instantly to an API service.

Feel free to close this issue if you don't think it belongs here, I also made an issue in MLeap to track this integration: https://github.com/combust/mleap/issues/292
"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/36,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/36/comments,Implement Coreference Resolution,"Adding support in the pipeline for Coreference resolution will be really beneficial
https://nlp.stanford.edu/projects/coref.shtml
"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/34,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/34/comments,SparkR Support,"<!--- Provide a general summary of the issue in the Title above -->
Implement SparkR support for NLP annotators library.
"
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/33,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/33/comments,Include and perform performance measures,"<!--- Provide a general summary of the issue in the Title above -->
We want to measure our library in a stressful environment situation, not regarding much the training stage but most importantly the transform stages."
https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/32,https://api.github.com/repos/JohnSnowLabs/spark-nlp/issues/32/comments,Include and perform accuracy measures,"<!--- Provide a general summary of the issue in the Title above -->
We want to measure annotators accuracy both separately as well as combined against other similar processes within a same testing environment."
