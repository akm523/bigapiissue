https://api.github.com/repos/aseldawy/spatialhadoop2/issues/42,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/42/comments,My own test file,"I am able to run the test sample on shadoop virtual image, but how to create my own test file not the auto generated one.

I need to include my geospatial data.

is there any tutorial or steps to create custom test file ?
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/41,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/41/comments,Polygonal dataset error,"What is the format for a polygon, as similar to rectangle. I tried it in different ways,  as tiger sample, as given on the website. But all of them end with the same error when Indexing.  Input points must be greater than 0."
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/40,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/40/comments,No scaling with multiple computing nodes,"I am using SpatialHadoop to store and index a dataset with 87 million points. I then apply various range queries.

I tested on 3 different cluster configurations: 1 , 2 and 4 nodes. Unfortunately, I don't see a runtime decrease with growing node number.

Any ideas why there is no horizontal-scaling effect?"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/39,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/39/comments,how to enter visualizer?,"how to enter visualizer?  HTTP ERROR 404

Problem accessing /visualizer,jsp. Reason:

    NOT_FOUND

thank you~
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/38,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/38/comments,RTree knn ArrayIndexOutOfBoundsException," if (distances.elementAt(k - 1) > query_radius) {
          result_correct = false;

          //query_radius = distances.elementAt(k);//when shapes.size() == k  throw   ArrayIndexOutOfBoundsException
==>
          if (shapes.size() == k) {
        	  query_radius = distances.elementAt(k - 1) + 0.000001d;
          } else {
        	  query_radius = distances.elementAt(k);
          }

        } else {
          result_correct = true;
 }"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/37,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/37/comments,SpatialHadoop: installation issue on hadoop 2.x,"Hi,

I tried creating the tar file with the help of given commands in the post but was not able to create any tar or jar files.

Could you please provide detailed steps of installing spatialhadoop on hadoop clusters.

Thanks,
Nishant
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/36,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/36/comments,'java.lang.NumberFormatException: empty String' When indexing big files,"I suspect it is because the sampler generates an empty ""_SUCCESS"" file which is then tried to be read.
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/35,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/35/comments,Extensible data types only work if the name starts with a known Shape,"I created a new data type extended from Point. When trying to create an index I got the following error:

`~/tempdir$ shadoop index -libjars spatial-types.jar antenas_santiago_por_torre antennas.grid mbr:-100,-100,100,100 sindex:rtree shape:Antenna
16/04/07 15:26:52 INFO core.SpatialSite: Adding JAR 'spatial-types.jar' to job class path
16/04/07 15:26:53 INFO mapreduce.SpatialInputFormat3: No block filter specified
16/04/07 15:26:53 INFO input.FileInputFormat: Total input paths to process : 1
16/04/07 15:26:53 INFO spatialHadoop.OperationsParams: Input size is small enough to use local machine
16/04/07 15:26:53 INFO indexing.Indexer: Reading a sample of 1%
16/04/07 15:26:53 INFO client.RMProxy: Connecting to ResourceManager at HP-Z420-Workstation/192.168.6.11:8032
16/04/07 15:26:53 INFO client.RMProxy: Connecting to ResourceManager at HP-Z420-Workstation/192.168.6.11:8032
16/04/07 15:26:53 INFO client.RMProxy: Connecting to ResourceManager at HP-Z420-Workstation/192.168.6.11:8032
16/04/07 15:26:53 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
16/04/07 15:26:53 INFO mapred.FileInputFormat: No block filter specified
16/04/07 15:26:53 INFO mapred.FileInputFormat: Total input paths to process : 1
16/04/07 15:26:53 INFO mapreduce.JobSubmitter: number of splits:1
16/04/07 15:26:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1459964025423_0061
16/04/07 15:26:54 INFO impl.YarnClientImpl: Submitted application application_1459964025423_0061
16/04/07 15:26:54 INFO mapreduce.Job: The url to track the job: http://HP-Z420-Workstation:8088/proxy/application_1459964025423_0061/
16/04/07 15:26:54 INFO mapreduce.Job: Running job: job_1459964025423_0061
16/04/07 15:26:59 INFO mapreduce.Job: Job job_1459964025423_0061 running in uber mode : false
16/04/07 15:26:59 INFO mapreduce.Job:  map 0% reduce 0%
16/04/07 15:27:04 INFO mapreduce.Job: Task Id : attempt_1459964025423_0061_m_000000_0, Status : FAILED
Error: java.lang.RuntimeException: Error in configuring object
    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)
    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)
    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:449)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1707)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)
    ... 9 more
Caused by: java.lang.RuntimeException: Error in configuring object
    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)
    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)
    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
    at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)
    ... 14 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)
    ... 17 more
Caused by: java.lang.NullPointerException
    at edu.umn.cs.spatialHadoop.operations.Sampler$Map.configure(Sampler.java:100)
    ... 22 more
`

The problem was fixed when I renamed the custom class to 'PointAntenna' instead of 'Antenna'
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/34,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/34/comments,base on hadoop2.x hdfs webapps jsp is not run,"hadoop 2.x JspHelper 's package is changed. org.apache.hadoop.hdfs.server.common.JspHelper constructor method is privated.
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/32,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/32/comments,Problems indexing MOD09GQ.005,"Trying to import MOD09GQ.005 file in Spatial Hadoop 2.3 (installation shipped with apache hadoop).
File was taken from here:
http://e4ftl01.cr.usgs.gov/MOLT/MOD09GQ.005/2016.02.17/MOD09GQ.A2016048.h35v09.005.2016050064358.hdf

File was copied to HDFS as 1.hdf. Index command:
bin/shadoop index 1.hdf 1.grid mbr:0,0,100,100 sindex:rtree shape:nasapoint
returns the following error:

```
 bin/shadoop index 1.hdf 1.grid mbr:0,0,100,100 sindex:rtree shape:nasapoint
16/03/18 19:13:09 INFO util.NativeCodeLoader: Loaded the native-hadoop library
16/03/18 19:13:09 WARN snappy.LoadSnappy: Snappy native library not loaded
16/03/18 19:13:09 INFO mapred.FileInputFormat: No block filter specified
16/03/18 19:13:09 INFO mapred.FileInputFormat: Total input paths to process : 1
16/03/18 19:13:09 INFO operations.Repartition: Reading a sample of 10%
16/03/18 19:13:09 INFO mapred.FileInputFormat: No block filter specified
16/03/18 19:13:09 INFO mapred.FileInputFormat: Total input paths to process : 1
16/03/18 19:13:10 INFO mapred.JobClient: Running job: job_local161844959_0001
16/03/18 19:13:10 INFO mapred.LocalJobRunner: Waiting for map tasks
16/03/18 19:13:10 INFO mapred.LocalJobRunner: Starting task: attempt_local161844959_0001_m_000000_0
16/03/18 19:13:10 INFO util.ProcessTree: setsid exited with exit code 0
16/03/18 19:13:10 INFO mapred.Task:  Using ResourceCalculatorPlugin : org.apache.hadoop.util.LinuxResourceCalculatorPlugin@5a4ca8a8
16/03/18 19:13:10 INFO mapred.MapTask: Processing split: file:/root/spatialhadoop-2.3-hadoop-1.2.1/1.hdf:0+71498628
16/03/18 19:13:10 INFO mapred.SpatialRecordReader: Open a SpatialRecordReader to file: file:/root/spatialhadoop-2.3-hadoop-1.2.1/1.hdf
16/03/18 19:13:10 INFO mapred.MapTask: numReduceTasks: 1
16/03/18 19:13:10 INFO mapred.MapTask: io.sort.mb = 100
16/03/18 19:13:10 INFO mapred.MapTask: data buffer = 79691776/99614720
16/03/18 19:13:10 INFO mapred.MapTask: record buffer = 262144/327680
16/03/18 19:13:10 INFO mapred.LocalJobRunner: Map task executor complete.
16/03/18 19:13:10 WARN mapred.LocalJobRunner: job_local161844959_0001
java.lang.Exception: java.lang.NumberFormatException: empty String
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:354)
Caused by: java.lang.NumberFormatException: empty String
        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1011)
        at java.lang.Double.parseDouble(Double.java:540)
        at edu.umn.cs.spatialHadoop.io.TextSerializerHelper.consumeDouble(TextSerializerHelper.java:182)
        at edu.umn.cs.spatialHadoop.core.Point.fromText(Point.java:136)
        at edu.umn.cs.spatialHadoop.nasa.NASAPoint.fromText(NASAPoint.java:77)
        at edu.umn.cs.spatialHadoop.operations.Sampler$Map.map(Sampler.java:122)
        at edu.umn.cs.spatialHadoop.operations.Sampler$Map.map(Sampler.java:69)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:366)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
16/03/18 19:13:11 INFO mapred.JobClient:  map 0% reduce 0%
16/03/18 19:13:11 INFO mapred.JobClient: Job complete: job_local161844959_0001
16/03/18 19:13:11 INFO mapred.JobClient: Counters: 0
16/03/18 19:13:11 INFO mapred.JobClient: Job Failed: NA
java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)
        at edu.umn.cs.spatialHadoop.operations.Sampler.sampleMapReduceWithRatio(Sampler.java:211)
        at edu.umn.cs.spatialHadoop.operations.Sampler.sample(Sampler.java:543)
        at edu.umn.cs.spatialHadoop.operations.Repartition.packInRectangles(Repartition.java:494)
        at edu.umn.cs.spatialHadoop.operations.Repartition.packInRectangles(Repartition.java:463)
        at edu.umn.cs.spatialHadoop.operations.Repartition.repartitionLocal(Repartition.java:590)
        at edu.umn.cs.spatialHadoop.operations.Repartition.repartition(Repartition.java:676)
        at edu.umn.cs.spatialHadoop.operations.Repartition.main(Repartition.java:721)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
        at edu.umn.cs.spatialHadoop.operations.Main.main(Main.java:115)

```
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/29,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/29/comments, Utility classes should not have public constructors,"This pull request is focused on resolving occurrences of Sonar rule squid:S1118 - “Utility classes should not have public constructors ”. You can find more information about the issue here:
https://dev.eclipse.org/sonar/rules/show/squid:S1118
Please let me know if you have any questions.
Ayman Abdelghany.
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/24,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/24/comments,Polygon,"It seems that the Polygon Class just support the Integer type of X and Y, and could not support float or double type.
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/22,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/22/comments,shadoop is really slow,"I am not sure what is causing this but the shadoop command is much much slower than then hadoop one.
Observed this on two different machines, with and without the hadoop cluster running.

kaimast@kai-work:/mnt/hdd/tmp/hadoop$ time ./bin/shadoop 
An example program must be given as the first argument.
Valid program names are:
  closestpair: Computes the closest pair of point of an input set of points
  convexhull: Computes the convex hull of an input set of points
  delaunay: Computes the Delaunay triangulation for a set of points
  distcp: Copies a directory or file using a MapReduce job
  dj: Computes the spatial join between two input files using the distributed join algorithm
  farthestpair: Computes the farthest pair of point of an input set of points
  generate: Generates a random file containing spatial data
  gplot: Plots a file to an image
  hadoopviz: Run Hadoopviz Server
  hdfplot: Plots a heat map for a give NASA dataset
  hdfx: Extracts data from a set of HDF files to text files
  hplot: Plots a heat map to an image
  index: Spatially index a file using a specific indexer
  knn: Finds the k nearest neighbor in a file to a point
  lakesplot: Plots lakes to SVG image
  mbr: Finds the minimal bounding rectangle of an input file
  mplot: Plot using ImageMagick
  multihdfplot: Plots NASA datasets in the spatiotemporal range provided by user
  oldindex: Spatially index a file using a specific indexer
  rangequery: Finds all objects in the query range given by a rectangle
  readfile: Retrieve some information about the index of a file
  sample: Reads a random sample from the input file
  shahedindexer: Creates a multilevel spatio-temporal indexer for NASA data
  sjmr: Computes the spatial join between two input files using the SJMR algorithm
  skyline: Computes the skyline of an input set of points
  staggquery: Runs a spatio temporal aggregate query on HDF files
  union: Computes the union of input shapes
  uunion: Computes the union of input shapes using the UltimateUnion algorithm
  vizserver: Starts a server that handles visualization requests

real    0m0.267s
user    0m0.380s
sys 0m0.036s
kaimast@kai-work:/mnt/hdd/tmp/hadoop$ time ./bin/hadoop
Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]
  CLASSNAME            run the class named CLASSNAME
 or
  where COMMAND is one of:
  fs                   run a generic filesystem user client
  version              print the version
  jar <jar>            run a jar file
                       note: please use ""yarn jar"" to launch
                             YARN applications, not this command.
  checknative [-a|-h]  check native hadoop and compression libraries availability
  distcp <srcurl> <desturl> copy file or directories recursively
  archive -archiveName NAME -p <parent path> <src>\* <dest> create a hadoop archive
  classpath            prints the class path needed to get the
  credential           interact with credential providers
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
  trace                view and modify Hadoop tracing settings

Most commands print help when invoked w/o parameters.

real    0m0.011s
user    0m0.000s
sys 0m0.004s
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/20,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/20/comments,java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 0,"Compiled from the latest version after git cloning the repo. When I try to run the following

$ bin/shadoop generate test mbr:0,0,1000000,1000000
java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 0
        at java.util.Vector.get(Vector.java:744)
        at edu.umn.cs.spatialHadoop.OperationsParams.autoDetectShape(OperationsParams.java:705)
        at edu.umn.cs.spatialHadoop.OperationsParams.getShape(OperationsParams.java:336)
        at edu.umn.cs.spatialHadoop.OperationsParams.getShape(OperationsParams.java:341)
        at edu.umn.cs.spatialHadoop.OperationsParams.<init>(OperationsParams.java:88)
        at edu.umn.cs.spatialHadoop.OperationsParams.<init>(OperationsParams.java:81)
        at edu.umn.cs.spatialHadoop.RandomSpatialGenerator.main(RandomSpatialGenerator.java:220)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:152)
        at edu.umn.cs.spatialHadoop.operations.Main.main(Main.java:137)

What is the issue here?

$ java -version
java version ""1.7.0_80""
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)

$ hadoop version
Hadoop 2.7.0
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r d4c8d4d4d203c934e8074b31289a28724c0842cf
Compiled by jenkins on 2015-04-10T18:40Z
Compiled with protoc 2.5.0
From source with checksum a9e90912c37a35c3195d23951fd18f
This command was run using /home/hadoopuser/hadoop/share/hadoop/common/hadoop-common-2.7.0.jar
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/19,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/19/comments, HDP Issue with Large File," I am getting following error, this error only occurs when I run with larger file  e.g.
shadoop generate test.rects size:1.gb shape:rect mbr:0,0,1000,1000 

if I run with small file e.g
shadoop generate test.rects size:1.mb shape:rect mbr:0,0,1000,1000 
 This works fine.
I am using HDP-2.3.0.0-2557 (HortonWorks) sandbox runs on CentOS

I already copied files to 
/usr/hdp/2.3.0.0-2557/hadoop/lib
/usr/hdp/2.3.0.0-2557/hadoop/bin 
/usr/hdp/2.3.0.0-2557/hadoop/client 
/usr/hdp/2.3.0.0-2557/hadoop/conf 

Error: java.lang.ClassNotFoundException: com.vividsolutions.jts.io.ParseException  
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  
        at java.security.AccessController.doPrivileged(Native Method)  
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)  
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)  
        at edu.umn.cs.spatialHadoop.core.Rectangle.fromText(Rectangle.java:274)  
        at edu.umn.cs.spatialHadoop.OperationsParams.getTextSerializable(OperationsParams.java:418)  
        at edu.umn.cs.spatialHadoop.OperationsParams.getShape(OperationsParams.java:357)  
        at edu.umn.cs.spatialHadoop.OperationsParams.getShape(OperationsParams.java:352)  
        at edu.umn.cs.spatialHadoop.mapred.RandomShapeGenerator.<init>(RandomShapeGenerator.java:77)  
        at edu.umn.cs.spatialHadoop.mapred.RandomInputFormat.getRecordReader(RandomInputFormat.java:90)  
        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)  
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)  
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)  
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)  
        at java.security.AccessController.doPrivileged(Native Method)  
        at javax.security.auth.Subject.doAs(Subject.java:415)  
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)  
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)                         
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/17,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/17/comments,Intermediate files when running index,"I am running the index command and it seems to be saving intermediate files into `hdfs://user/filename`. However spatialhadoop2 is looking for it in `hdfs://user/my.username/filename`. This is occuring at `edu.umn.cs.spatialHadoop.operations.FileMBR.fileMBRMapReduce(FileMBR.java:252)`. This could well be something to do with the hadoop setup, but thought I would raise this here in case not.
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/14,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/14/comments,How to define my own data type with SpatialHadoop,"Hello, I want to define my own object with SpatialHadoop to run a few experiments, I've got this [tutorial](http://spatialhadoop.cs.umn.edu/datatypes.html) from your website, a nice tutorial for beginners. But there are still one thing unclear for me: 

When I want to use my own data type, the only way in my mind is that I can write a RectangleId class in package `edu.umn.cs.spatialhadoop.core` and recompile the whole project then run an experiment, is there any better solution?
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/10,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/10/comments,Using HDFToText problem,"I tried to use HDFToText over MODIS dataset available here:

http://aws.amazon.com/blogs/aws/process-earth-science-data-on-aws-with-nasa-nex/

But, it does not seem to work. Any clue?

Also, I could not find any documentation on HDFToText on the projects website (http://spatialhadoop.cs.umn.edu/)

Thanks in advanced
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/9,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/9/comments,who can tell me spets to install the spatialhadoop on the hadoop?,"Just like the title!
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/8,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/8/comments,Please analyse the function packInRectangles in Repartition.java,"I am reading the code of `src/edu/umn/cs/spatialHadoop/operations/Repartition.java`, which is to build index.

Anyone can help to analyse the following function:

```
public static CellInfo[] packInRectangles(Path[] files,
      Path outFile, OperationsParams params, Rectangle fileMBR)
      throws IOException 
```

I have no idea of what word it does. 
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/7,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/7/comments,Any suggestions about build index using Apache-spark?,"Suppose I have the the input file from `HDFS`, how to write code to build index using `Apache-spark`, just as `src/edu/umn/cs/spatialHadoop/operations/Repartition.java` do in spatial-hadoop?
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/4,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/4/comments,convexhull operation is giving nullpointerexception,"Hi,

I am trying to run the convex hull operation in spatialhadoop. Its giving nullpointerexception. Can you please help me?

The full stack trace is

bin/shadoop convexhull /user/hduser/points.grid /user/hduser/convexhull
14/07/28 05:14:41 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/07/28 05:14:41 WARN snappy.LoadSnappy: Snappy native library not loaded
14/07/28 05:14:41 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
14/07/28 05:14:41 INFO operations.ConvexHull: Processing 1 out of 1 partition
14/07/28 05:14:41 INFO mapred.FileInputFormat: Spatial filter function matched with 1 cells
14/07/28 05:14:42 INFO mapred.JobClient: Running job: job_201407230401_0013
14/07/28 05:14:43 INFO mapred.JobClient:  map 0% reduce 0%
14/07/28 05:14:54 INFO mapred.JobClient:  map 100% reduce 0%
14/07/28 05:15:03 INFO mapred.JobClient:  map 100% reduce 33%
14/07/28 05:15:06 INFO mapred.JobClient: Task Id : attempt_201407230401_0013_r_
000000_0, Status : FAILED
java.lang.NullPointerException
    at edu.umn.cs.spatialHadoop.core.GridRecordWriter.closeCell(GridRecordWriter.java:357)
    at edu.umn.cs.spatialHadoop.core.GridRecordWriter.close(GridRecordWriter.java:436)
    at edu.umn.cs.spatialHadoop.mapred.GridRecordWriter2.close(GridRecordWriter2.java:39)
    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:467)
    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:535)
    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)
    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
    at org.apache.hadoop.mapred.Child.main(Child.java:249)

attempt_201407230401_0013_r_000000_0: log4j:WARN No appenders could be found for logger (org.apache.hadoop.hdfs.DFSClient).
attempt_201407230401_0013_r_000000_0: log4j:WARN Please initialize the log4j system properly.
14/07/28 05:15:07 INFO mapred.JobClient:  map 100% reduce 0%
14/07/28 05:15:16 INFO mapred.JobClient:  map 100% reduce 33%
14/07/28 05:15:18 INFO mapred.JobClient: Task Id : attempt_201407230401_0013_r_000000_1, Status : FAILED
java.lang.NullPointerException
    at edu.umn.cs.spatialHadoop.core.GridRecordWriter.closeCell(GridRecordWriter.java:357)
    at edu.umn.cs.spatialHadoop.core.GridRecordWriter.close(GridRecordWriter.java:436)
    at edu.umn.cs.spatialHadoop.mapred.GridRecordWriter2.close(GridRecordWriter2.java:39)
    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:467)
    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:535)
    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)
    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
    at org.apache.hadoop.mapred.Child.main(Child.java:249)

attempt_201407230401_0013_r_000000_1: log4j:WARN No appenders could be found for logger (org.apache.hadoop.hdfs.DFSClient).
attempt_201407230401_0013_r_000000_1: log4j:WARN Please initialize the log4j system properly.
14/07/28 05:15:19 INFO mapred.JobClient:  map 100% reduce 0%
14/07/28 05:15:28 INFO mapred.JobClient:  map 100% reduce 33%
14/07/28 05:15:32 INFO mapred.JobClient: Task Id : attempt_201407230401_0013_r_000000_2, Status : FAILED
java.lang.NullPointerException
    at edu.umn.cs.spatialHadoop.core.GridRecordWriter.closeCell(GridRecordWriter.java:357)
    at edu.umn.cs.spatialHadoop.core.GridRecordWriter.close(GridRecordWriter.java:436)
    at edu.umn.cs.spatialHadoop.mapred.GridRecordWriter2.close(GridRecordWriter2.java:39)
    at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:467)
    at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:535)
    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)
    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
    at org.apache.hadoop.mapred.Child.main(Child.java:249)

attempt_201407230401_0013_r_000000_2: log4j:WARN No appenders could be found for logger (org.apache.hadoop.hdfs.DFSClient).
attempt_201407230401_0013_r_000000_2: log4j:WARN Please initialize the log4j system properly.
14/07/28 05:15:33 INFO mapred.JobClient:  map 100% reduce 0%
14/07/28 05:15:41 INFO mapred.JobClient:  map 100% reduce 33%
14/07/28 05:15:46 INFO mapred.JobClient:  map 100% reduce 0%
14/07/28 05:15:48 INFO mapred.JobClient: Job complete: job_201407230401_0013
14/07/28 05:15:48 INFO mapred.JobClient: Counters: 25
14/07/28 05:15:48 INFO mapred.JobClient:   Job Counters
14/07/28 05:15:48 INFO mapred.JobClient:     Launched reduce tasks=4
14/07/28 05:15:48 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=12354
14/07/28 05:15:48 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
14/07/28 05:15:48 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
14/07/28 05:15:48 INFO mapred.JobClient:     Launched map tasks=1
14/07/28 05:15:48 INFO mapred.JobClient:     Data-local map tasks=1
14/07/28 05:15:48 INFO mapred.JobClient:     Failed reduce tasks=1
14/07/28 05:15:48 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=51587
14/07/28 05:15:48 INFO mapred.JobClient:   File Input Format Counters
14/07/28 05:15:48 INFO mapred.JobClient:     Bytes Read=10502261
14/07/28 05:15:48 INFO mapred.JobClient:   FileSystemCounters
14/07/28 05:15:48 INFO mapred.JobClient:     FILE_BYTES_READ=930
14/07/28 05:15:48 INFO mapred.JobClient:     HDFS_BYTES_READ=10502381
14/07/28 05:15:48 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=59744
14/07/28 05:15:48 INFO mapred.JobClient:   Map-Reduce Framework
14/07/28 05:15:48 INFO mapred.JobClient:     Map output materialized bytes=924
14/07/28 05:15:48 INFO mapred.JobClient:     Map input records=288781
14/07/28 05:15:48 INFO mapred.JobClient:     Spilled Records=102
14/07/28 05:15:48 INFO mapred.JobClient:     Map output bytes=4620496
14/07/28 05:15:48 INFO mapred.JobClient:     Total committed heap usage (bytes)=157810688
14/07/28 05:15:48 INFO mapred.JobClient:     CPU time spent (ms)=2120
14/07/28 05:15:48 INFO mapred.JobClient:     Map input bytes=10965940
14/07/28 05:15:48 INFO mapred.JobClient:     SPLIT_RAW_BYTES=120
14/07/28 05:15:48 INFO mapred.JobClient:     Combine input records=288781
14/07/28 05:15:48 INFO mapred.JobClient:     Combine output records=51
14/07/28 05:15:48 INFO mapred.JobClient:     Physical memory (bytes) snapshot=196898816
14/07/28 05:15:48 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=759816192
14/07/28 05:15:48 INFO mapred.JobClient:     Map output records=288781
14/07/28 05:15:48 INFO mapred.JobClient: Job Failed: # of failed Reduce Tasks exceeded allowed limit. FailedCount: 1. LastFailedTask: task_201407230401_0013_r_000000
java.io.IOException: Job failed!
    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)
    at edu.umn.cs.spatialHadoop.operations.ConvexHull.convexHullMapReduce(ConvexHull.java:295)
    at edu.umn.cs.spatialHadoop.operations.ConvexHull.main(ConvexHull.java:333)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
    at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
    at edu.umn.cs.spatialHadoop.operations.Main.main(Main.java:97)
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/3,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/3/comments,Cannot generate random polygon,"In `RandomSpatialGenerator#printUsage`, it says that when shape is set to polygon, it will generate random polygon. But in `RandomShapeGenerator#generateShape`, it only handles point and rectangle. 
"
https://api.github.com/repos/aseldawy/spatialhadoop2/issues/1,https://api.github.com/repos/aseldawy/spatialhadoop2/issues/1/comments,Potential problem with points and r-tree indexes,"Creating an rtree index on point data fails.  Have not looked into what's causing this yet.  
Here's the command to create the sample data
hadoop jar /tmp/spatialhadoop-2-b2.jar generate testpoints mbr:0,0,10000000,10000000 size:100.mb shape:point -overwrite

And the command to index it that leads to the error
hadoop jar /tmp/spatialhadoop-2-b2.jar index testpoints testpoints.rtree sindex:rtree shape:point -overwrite

java.lang.IllegalArgumentException: Width (0) and height (600) cannot be <= 0
        at java.awt.image.DirectColorModel.createCompatibleWritableRaster(DirectColorModel.java:999)
at java.awt.image.BufferedImage.<init>(BufferedImage.java:321)
at edu.umn.cs.spatialHadoop.operations.Plot.plotLocal(Unknown Source)
at edu.umn.cs.spatialHadoop.operations.Repartition$RepartitionOutputCommitter.commitJob (Unknown Source)
"
