Summary,Issue key,Issue id,Project url,Priority,,Labels,Labels,Description,Environment
Incorrect handling of default values when deserializing python wrappers of scala transformers,SPARK-23244,13134140,http://spark.apache.org,Minor,io,,,"Default values are not handled properly when serializing/deserializing python trasnformers which are wrappers around scala objects. It looks like that after deserialization the default values which were based on uid do not get properly restored and values which were not set are set to their (original) default values.

Here's a simple code example using Bucketizer:

{code:python}
>>> from pyspark.ml.feature import Bucketizer
>>> a = Bucketizer() 
>>> a.save(""bucketizer0"")
>>> b = load(""bucketizer0"") 
>>> a._defaultParamMap[a.outputCol]
u'Bucketizer_440bb49206c148989db7__output'
>>> b._defaultParamMap[b.outputCol]
u'Bucketizer_41cf9afbc559ca2bfc9a__output'
>>> a.isSet(a.outputCol)
False 
>>> b.isSet(b.outputCol)
True
>>> a.getOutputCol()
u'Bucketizer_440bb49206c148989db7__output'
>>> b.getOutputCol()
u'Bucketizer_440bb49206c148989db7__output'
{code}",
ImageSchema.readImages incorrectly sets alpha channel to 255 for four-channel images,SPARK-23205,13133510,http://spark.apache.org,Critical,io,,,"When parsing raw image data in ImageSchema.decode(), we use a [java.awt.Color constructor|https://docs.oracle.com/javase/7/docs/api/java/awt/Color.html#Color(int)] that sets alpha = 255, even for four-channel images.

See the offending line here: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/image/ImageSchema.scala#L172

A fix is to simply update the line to: 

val color = new Color(img.getRGB(w, h), nChannels == 4)


instead of

val color = new Color(img.getRGB(w, h))",
Invalid guard condition in org.apache.spark.ml.classification.Classifier,SPARK-23152,13131983,http://spark.apache.org,Minor,,easyfix,,"When fitting a classifier that extends ""org.apache.spark.ml.classification.Classifier"" (NaiveBayes, DecisionTreeClassifier, RandomForestClassifier) a misleading NullPointerException is thrown.

Steps to reproduce: 
{code:java}
val data = spark.createDataset(Seq.empty[(Double, org.apache.spark.ml.linalg.Vector)])
new DecisionTreeClassifier().setLabelCol(""_1"").setFeaturesCol(""_2"").fit(data)
{code}
 The error: 
{code:java}
java.lang.NullPointerException: Value at index 0 is null

at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:165)
at org.apache.spark.ml.classification.Classifier.getNumClasses(Classifier.scala:115)
at org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:102)
at org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:45)
at org.apache.spark.ml.Predictor.fit(Predictor.scala:118){code}
  

The problem happens due to an incorrect guard condition in function getNumClasses at org.apache.spark.ml.classification.Classifier:106
{code:java}
val maxLabelRow: Array[Row] = dataset.select(max($(labelCol))).take(1)
if (maxLabelRow.isEmpty) {
  throw new SparkException(""ML algorithm was given empty dataset."")
}
{code}
When the input data is empty the result ""maxLabelRow"" array is not. Instead it contains a single Row(null) element.

 

Proposed solution: the condition can be modified to verify that.
{code:java}
if (maxLabelRow.isEmpty || maxLabelRow(0).get(0) == null) {
  throw new SparkException(""ML algorithm was given empty dataset."")
}
{code}
 

 ",
"Fix ChiSqSelectorModel, GaussianMixtureModel save implementation for Row order issues",SPARK-22905,13127308,http://spark.apache.org,Major,"data partition, parallelism",,,"Currently, in `ChiSqSelectorModel`, save:
{code}
spark.createDataFrame(dataArray).repartition(1).write...
{code}
The default partition number used by createDataFrame is ""defaultParallelism"",
Current RoundRobinPartitioning won't guarantee the ""repartition"" generating the same order result with local array. We need fix it.",
Chi Square selector not recognizing field in Data frame,SPARK-22295,13110073,http://spark.apache.org,Major,data structure,,,"ChiSquare selector is not recognizing the field 'class' which is present in the data frame while fitting the model. I am using PIMA Indians diabetes dataset of UCI. The complete code and output is below for reference. But, when some rows of the input file is created as a dataframe manually, it will work. Couldn't understand the pattern here.

Kindly help.

{code:python}
from pyspark.ml.feature import VectorAssembler, ChiSqSelector
import sys

file_name='data/pima-indians-diabetes.data'

df=spark.read.format(""csv"").option(""inferSchema"",""true"").option(""header"",""true"").load(file_name).cache()

df.show(1)
assembler = VectorAssembler(inputCols=['preg', ' plas', ' pres', ' skin', ' test', ' mass', ' pedi', ' age'],outputCol=""features"")
df=assembler.transform(df)
df.show(1)
try:
    css=ChiSqSelector(numTopFeatures=5, featuresCol=""features"",
                          outputCol=""selected"", labelCol='class').fit(df)
except:
    print(sys.exc_info())
{code}

Output:

+----+-----+-----+-----+-----+-----+-----+----+------+
|preg| plas| pres| skin| test| mass| pedi| age| class|
+----+-----+-----+-----+-----+-----+-----+----+------+
|   6|  148|   72|   35|    0| 33.6|0.627|  50|     1|
+----+-----+-----+-----+-----+-----+-----+----+------+
only showing top 1 row

+----+-----+-----+-----+-----+-----+-----+----+------+--------------------+
|preg| plas| pres| skin| test| mass| pedi| age| class|            features|
+----+-----+-----+-----+-----+-----+-----+----+------+--------------------+
|   6|  148|   72|   35|    0| 33.6|0.627|  50|     1|[6.0,148.0,72.0,3...|
+----+-----+-----+-----+-----+-----+-----+----+------+--------------------+
only showing top 1 row

(<class 'pyspark.sql.utils.IllegalArgumentException'>, IllegalArgumentException('Field ""class"" does not exist.', 'org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\t at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\t at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\t at scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\t at org.apache.spark.sql.types.StructType.apply(StructType.scala:263)\n\t at org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:71)\n\t at org.apache.spark.ml.feature.ChiSqSelector.transformSchema(ChiSqSelector.scala:183)\n\t at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\t at org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:159)\n\t at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t at java.lang.reflect.Method.invoke(Method.java:498)\n\t at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\t at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\t at py4j.Gateway.invoke(Gateway.java:280)\n\t at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\t at py4j.commands.CallCommand.execute(CallCommand.java:79)\n\t at py4j.GatewayConnection.run(GatewayConnection.java:214)\n\t at java.lang.Thread.run(Thread.java:745)'), <traceback object at 0x0B743BC0>)


*The below code works fine:
*
{code:python}
from pyspark.ml.feature import VectorAssembler, ChiSqSelector
import sys

file_name='data/pima-indians-diabetes.data'

#df=spark.read.format(""csv"").option(""inferSchema"",""true"").option(""header"",""true"").load(file_name).cache()

# Just pasted a few rows from the input file and created a data frome. This will work, but not the frame picked up from the file
df = spark.createDataFrame([
[6,148,72,35,0,33.6,0.627,50,1],
[1,85,66,29,0,26.6,0.351,31,0],
[8,183,64,0,0,23.3,0.672,32,1],
], ['preg', ' plas', ' pres', ' skin', ' test', ' mass', ' pedi', ' age', ""class""])


df.show(1)
assembler = VectorAssembler(inputCols=['preg', ' plas', ' pres', ' skin', ' test', ' mass', ' pedi', ' age'],outputCol=""features"")
df=assembler.transform(df)
df.show(1)
try:
    css=ChiSqSelector(numTopFeatures=5, featuresCol=""features"",
                          outputCol=""selected"", labelCol=""class"").fit(df)
except:
    print(sys.exc_info())

print(css.selectedFeatures)

{code}

Output:

+----+-----+-----+-----+-----+-----+-----+----+-----+
|preg| plas| pres| skin| test| mass| pedi| age|class|
+----+-----+-----+-----+-----+-----+-----+----+-----+
|   6|  148|   72|   35|    0| 33.6|0.627|  50|    1|
+----+-----+-----+-----+-----+-----+-----+----+-----+
only showing top 1 row

+----+-----+-----+-----+-----+-----+-----+----+-----+--------------------+
|preg| plas| pres| skin| test| mass| pedi| age|class|            features|
+----+-----+-----+-----+-----+-----+-----+----+-----+--------------------+
|   6|  148|   72|   35|    0| 33.6|0.627|  50|    1|[6.0,148.0,72.0,3...|
+----+-----+-----+-----+-----+-----+-----+----+-----+--------------------+
only showing top 1 row

[0, 1, 2, 3, 5]",
"CrossValidator's training and testing set with different set of labels, resulting in encoder transform error",SPARK-22034,13102730,http://spark.apache.org,Major,pipeline,,,"Let's say we have a VectorIndexer with maxCategories set to 13, and training set has a column containing month label.

In CrossValidator, dataframe is split into training and testing set automatically. If could happen that training set happens to lack month 2 (could happen by chance, or happen quite frequently if we have unbalanced label).

When training set is being trained within the cross validator, the pipeline is fitted with the training set only, resulting in a partial key map in VectorIndexer. When this pipeline is used to transform the predict set, VectorIndexer will throw  a ""key not found"" error.

Making CrossValidator also an estimator thus can be connected to a whole pipeline is a cool idea, but bug like this occurs, and is not expected.

The solution, I am guessing, would be to check each stage in the pipeline, and when we see encoder type stage, we fit the stage model with the complete dataset.","Ubuntu 16.04
Scala 2.11
Spark 2.2.0"
MultivariateOnlineSummarizer.variance generate negative result,SPARK-21818,13096988,http://spark.apache.org,Major,,,,"Because of numerical error, MultivariateOnlineSummarizer.variance is possible to generate negative variance.
This is a serious bug because many algos in MLLib use stddev computed from sqrt(variance),
it will generate NaN and crash the whole algorithm.

we can reproduce this bug use the following code:
{code}
    val summarizer1 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.7)
    val summarizer2 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.4)
    val summarizer3 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.5)
    val summarizer4 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.4)

    val summarizer = summarizer1
      .merge(summarizer2)
      .merge(summarizer3)
      .merge(summarizer4)

    println(summarizer.variance(0))
{code}",
KMeans performance regression (5-6x slowdown) in Spark 2.2,SPARK-21799,13096504,http://spark.apache.org,Major,job,,,"I've been running KMeans performance tests using [spark-sql-perf|https://github.com/databricks/spark-sql-perf/] and have noticed a regression (slowdowns of 5-6x) when running tests on large datasets in Spark 2.2 vs 2.1.

The test params are:
* Cluster: 510 GB RAM, 16 workers
* Data: 1000000 examples, 10000 features

After talking to [~josephkb], the issue seems related to the changes in [SPARK-18356|https://issues.apache.org/jira/browse/SPARK-18356] introduced in [this PR|https://github.com/apache/spark/pull/16295].

It seems `df.cache()` doesn't set the storageLevel of `df.rdd`, so `handlePersistence` is true even when KMeans is run on a cached DataFrame. This unnecessarily causes another copy of the input dataset to be persisted.

As of Spark 2.1 ([JIRA link|https://issues.apache.org/jira/browse/SPARK-16063]) `df.storageLevel` returns the correct result after calling `df.cache()`, so I'd suggest replacing instances of `df.rdd.getStorageLevel` with df.storageLevel` in MLlib algorithms (the same pattern shows up in LogisticRegression, LinearRegression, and others). I've verified this behavior in [this notebook|https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5211178207246023/950505630032626/7788830288800223/latest.html]

",
Correct validateAndTransformSchema in GaussianMixture and AFTSurvivalRegression,SPARK-21793,13096276,http://spark.apache.org,Minor,job,,,"From user sharp-pixel:

The line SchemaUtils.appendColumn(schema, $(predictionCol), IntegerType) did not modify the variable schema, hence only the last line had any effect. A temporary variable is used to correctly append the two columns predictionCol and probabilityCol.",
Fix broken redirect in collaborative filtering docs to databricks training repo,SPARK-21615,13091944,http://spark.apache.org,Trivial,,documentation,easyfix,"* Current [MLlib Collaborative Filtering tutorial|https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html] points to broken links to old databricks website.
* Databricks moved all their content to [git repo|https://github.com/databricks/spark-training]
* Two links needs to be fixed:
** [training exercises|https://databricks-training.s3.amazonaws.com/index.html]
** [personalized movie recommendation with spark.mllib|https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html]",
Multinomial logistic regression model fitting fails with ERROR StrongWolfeLineSearch,SPARK-21614,13091914,http://spark.apache.org,Major,model,,,"Fitting a simple multinomial logistic regression model fails with:

17/08/02 14:53:23 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:23 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed

Example repro case:


from pyspark.sql import Row
from pyspark.ml.linalg import Vectors
from pyspark.ml.classification import LogisticRegression

df = spark.createDataFrame([
    Row(label=0, features=Vectors.dense([0.0, 0.0, 0.0, 0.0, 2.9, 0.0, 2.9, 2.9, 0.0, 0.0, 0.0, 0.0, 2.9, 0.0, 0.0, 2.9, 2.9, 0.0, 0.0, 0.0, 2.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9, 2.9, 2.9, 2.9, 0.0, 2.9, 0.0, 0.0, 2.9, 0.0, 2.9, 2.9, 0.0, 2.9, 2.9, 0.0, 0.0, 2.9, 2.9, 2.9, 0.0, 2.9, 2.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9, 2.9, 0.0, 2.9, 2.9, 2.9, 2.9, 0.0, 0.0, 2.9, 2.9, 0.0, 0.0, 0.0, 2.9, 2.9, 0.0, 2.9, 2.9, 2.9, 0.0, 0.0, 2.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),
    Row(label=1, features=Vectors.dense([1.8, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 1.9, 1.9, 1.9, 1.9, 1.9, 1.8, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 0.0, 1.9, 0.0, 1.9, 1.9, 0.0, 1.9, 1.9, 0.0, 1.8, 1.9, 0.0, 0.0, 1.9, 0.0, 1.9, 0.0, 1.9, 1.9, 1.9, 1.9, 0.0, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9, 1.9, 1.9, 0.0, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 0.0, 0.0, 0.0, 1.9, 0.0, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 0.0, 0.0, 1.9, 1.9, 0.0, 0.0, 0.0])),
    Row(label=2, features=Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 1.6, 1.6, 1.6, 1.6, 0.0, 1.6, 1.6, 1.6, 1.6, 1.6, 0.0, 1.6, 1.6, 0.0, 1.6, 1.6, 1.6, 0.0, 1.6, 1.6, 0.0, 1.6, 1.6, 1.6, 1.6, 0.0, 1.6, 1.6, 1.6, 1.6, 0.0, 0.0, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 0.0, 1.6, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 0.0, 0.0, 0.0, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 0.0, 1.6, 1.6, 0.0, 1.6, 1.6, 0.0, 0.0, 1.6])),
    Row(label=3, features=Vectors.dense([0.0, 0.0, 0.0, 1.4, 0.7, 1.1, 0.0, 0.0, 0.7, 0.0, 1.4, 1.1, 1.4, 0.0, 1.1, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1, 0.0, 0.0, 0.0, 0.7, 0.0, 0.7, 0.0, 0.0, 1.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 1.4, 0.0, 0.0, 0.0, 0.0, 1.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 1.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4, 0.7, 0.0, 0.0, 0.0, 0.0, 1.4, 0.7, 1.9, 0.0, 0.0, 0.0, 1.1, 1.4, 0.0, 0.0, 0.0, 2.1, 2.1, 2.1, 1.6, 1.9, 1.8, 2.1, 2.1, 1.9, 2.1, 1.6, 1.8, 1.6, 2.1, 1.8, 1.9, 2.1, 2.1, 2.1, 2.1, 2.1, 1.8, 2.1, 0.0, 1.9, 2.1, 0.0, 2.1, 2.1, 0.0, 1.8, 2.1, 2.1, 0.0, 1.9, 0.0, 1.9, 0.0, 2.1, 1.8, 2.1, 2.1, 0.0, 2.1, 0.0, 0.0, 1.9, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1, 2.1, 2.1, 1.9, 2.1, 2.1, 2.1, 1.8, 2.1, 2.1, 2.1, 0.0, 0.0, 0.0, 1.6, 1.9, 2.1, 2.1, 2.1, 2.1, 1.6, 1.9, 0.7, 2.1, 0.0, 0.0, 1.8, 1.6, 0.0, 0.0, 2.1])),
    Row(label=4, features=Vectors.dense([0.0, 2.8, 2.8, 0.0, 0.0, 2.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8, 0.0, 2.8, 0.0, 2.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8, 0.0, 0.0, 2.8, 2.8, 0.0, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 0.0, 2.8, 0.0, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 0.0, 0.0, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 0.0, 2.8, 0.0, 0.0, 0.0, 2.8, 0.0, 0.0, 2.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 0.0, 0.0, 0.0, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 0.0, 2.8, 2.8, 0.0, 0.0, 2.8])),
    Row(label=5, features=Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0, 2.6, 0.0, 0.0, 0.0, 1.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4, 1.1, 2.6, 0.0, 0.0, 0.0, 0.0, 2.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1, 0.0, 2.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4, 0.0, 0.0, 0.0, 1.1, 2.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6, 2.6, 2.6, 2.6, 2.6, 0.0, 2.6, 2.6, 2.6, 2.4, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 1.1, 2.6, 2.6, 0.0, 2.6, 2.6, 1.1, 2.4, 0.0, 2.6, 0.0, 2.6, 0.0, 1.1, 0.0, 0.0, 0.0, 2.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.4, 2.6, 0.0, 2.6, 0.0, 0.0, 0.0, 2.6, 2.6, 2.6, 1.1, 2.6, 2.6, 2.6, 2.4, 0.0, 2.6, 0.0, 0.0, 2.6, 2.6, 0.0, 0.0, 2.6])),
])

lr = LogisticRegression()
model = lr.fit(df)

'''
17/08/02 14:53:21 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:23 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:23 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:23 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:23 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed
17/08/02 14:53:23 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:25 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:25 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:25 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:25 ERROR LBFGS: Failure again! Giving up and returning. Maybe the objective is just poorly behaved?
'''


I'm on Amazon EMR release emr-5.3.1 running Spark 2.1.0
",
Debug issues for SparkML(scala.Predef$.any2ArrowAssoc),SPARK-21557,13090715,http://spark.apache.org,Critical,usability,,,"Hi Team,

Can you please see the below error ,when I am running the below program using below mvn config.Kindly tell me which version I have to use.I am running this program from eclipse neon.

Error at Runtime:- 

Exception in thread ""main"" java.lang.NoSuchMethodError: scala.Predef$.any2ArrowAssoc(Ljava/lang/Object;)Ljava/lang/Object;
	at org.apache.spark.sql.SparkSession$Builder.config(SparkSession.scala:750)
	at org.apache.spark.sql.SparkSession$Builder.appName(SparkSession.scala:741)
	at com.MLTest.JavaPCAExample.main(JavaPCAExample.java:20)

Java Class:-

package com.MLTest;

import org.apache.spark.sql.SparkSession;

import java.util.Arrays;
import java.util.List;
import org.apache.spark.ml.feature.PCA;
import org.apache.spark.ml.feature.PCAModel;
import org.apache.spark.ml.linalg.VectorUDT;
import org.apache.spark.ml.linalg.Vectors;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.types.Metadata;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

public class JavaPCAExample {
	public static void main(String[] args) {
		SparkSession spark = SparkSession.builder().appName(""JavaPCAExample3"")
				.config(""spark.some.config.option"", ""some-value"").getOrCreate();

		List<Row> data = Arrays.asList(
				RowFactory.create(Vectors.sparse(5, new int[] { 1, 3 }, new double[] { 1.0, 7.0 })),
				RowFactory.create(Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0)),
				RowFactory.create(Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)));

		StructType schema = new StructType(
				new StructField[] { new StructField(""features"", new VectorUDT(), false, Metadata.empty()), });

		Dataset<Row> df = spark.createDataFrame(data, schema);

		PCAModel pca = new PCA().setInputCol(""features"").setOutputCol(""pcaFeatures"").setK(3).fit(df);

		Dataset<Row> result = pca.transform(df).select(""pcaFeatures"");
		result.show(false);

		spark.stop();
	}
}

pom.xml:-

<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
	xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
	<modelVersion>4.0.0</modelVersion>
	<groupId>SparkMLTest</groupId>
	<artifactId>SparkMLTest</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<build>
		<sourceDirectory>src</sourceDirectory>
		<plugins>
			<plugin>
				<artifactId>maven-compiler-plugin</artifactId>
				<version>3.5.1</version>
				<configuration>
					<source>1.8</source>
					<target>1.8</target>
				</configuration>
			</plugin>
		</plugins>
	</build>
	<dependencies>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_2.10</artifactId>
			<version>2.2.0</version>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-streaming_2.10</artifactId>
			<version>2.1.1</version>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-mllib_2.10</artifactId>
			<version>2.1.1</version>
			<scope>provided</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-sql_2.10</artifactId>
			<version>2.1.1</version>
		</dependency>
		<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-library</artifactId>
			<version>2.13.0-M1</version>
		</dependency>
		<dependency>
			<groupId>org.apache.parquet</groupId>
			<artifactId>parquet-hadoop-bundle</artifactId>
			<version>1.8.1</version>
		</dependency>
	</dependencies>
</project>



",
Fix bug of strong wolfe linesearch `init` parameter lose effectiveness,SPARK-21523,13089729,http://spark.apache.org,Critical,param,,,"We need merge this breeze bugfix into spark because it influence a series of algos in MLlib which use LBFGS.
https://github.com/scalanlp/breeze/pull/651",
java.lang.NullPointerException for certain methods in classes of MLlib,SPARK-21331,13085334,http://spark.apache.org,Major,usability,,,"I am trying to run the following code using sbt package and sbt run. I am getting a runtime error that seems to be a bug since the same code works great on spark-shell with Scala. The error occurs when executing the computeSVD line. If this line is commented out, the program works fine. I am having similar issues for other methods for classes in MLlib as well. This looks like a bug to me.",Spark running locally on OSX at spark://127.0.0.1:7077.
OOM with 2 handred million vertex when mitrx multply,SPARK-21118,13080331,http://spark.apache.org,Major,memory,,,"i have 2 matrix each one is 200milions*200milions.
i want to multiply them ,but run out with oom .
finally i find the oom appear at blockmatrix.simulateMultiply . there is a collect action at this method. 
 the collect will return all dataset that is too large to driver so the driver will go to oom.

class BlockMatrix @Since(""1.3.0"") (
private[distributed] def simulateMultiply(
      other: BlockMatrix,
      partitioner: GridPartitioner): (BlockDestinations, BlockDestinations) = {
    val leftMatrix = {color:red}blockInfo.keys.collect() {color}// blockInfo should already be cached
    val rightMatrix = other.blocks.keys.collect()
......

","on yarn cluster,19 node.30GB per node"
Should we create a constructor for LabelsPoint which is using ml.linalg.Vectors?,SPARK-20893,13075071,http://spark.apache.org,Major,,,,"From Spark2.0 org.apache.spark.mllib.linalg.Vectors;
was deprecated by org.apache.spark.ml.linalg.Vectors;

But for LabelsPoint, 
https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/mllib/regression/LabeledPoint.html

the only constructor as LabeledPoint(double label, Vector features) 
is using mllib.linalg.Vectors, should we create another version which is using 
ml.linalg.Vectors?",
LogisticRegressionModel throws TypeError,SPARK-20862,13074377,http://spark.apache.org,Minor,data formats,,,"LogisticRegressionModel throws a TypeError using python3 and numpy 1.12.1:

**********************************************************************
File ""/Users/bago/repos/spark/python/pyspark/mllib/classification.py"", line 155, in __main__.LogisticRegressionModel
Failed example:
    mcm = LogisticRegressionWithLBFGS.train(data, iterations=10, numClasses=3)
Exception raised:
    Traceback (most recent call last):
      File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/doctest.py"", line 1330, in __run
        compileflags, 1), test.globs)
      File ""<doctest __main__.LogisticRegressionModel[23]>"", line 1, in <module>
        mcm = LogisticRegressionWithLBFGS.train(data, iterations=10, numClasses=3)
      File ""/Users/bago/repos/spark/python/pyspark/mllib/classification.py"", line 398, in train
        return _regression_train_wrapper(train, LogisticRegressionModel, data, initialWeights)
      File ""/Users/bago/repos/spark/python/pyspark/mllib/regression.py"", line 216, in _regression_train_wrapper
        return modelClass(weights, intercept, numFeatures, numClasses)
      File ""/Users/bago/repos/spark/python/pyspark/mllib/classification.py"", line 176, in __init__
        self._dataWithBiasSize)
    TypeError: 'float' object cannot be interpreted as an integer
",
ALS with implicit feedback ignores negative values,SPARK-20790,13072912,http://spark.apache.org,Major,,,,"The refactorization that was done in https://github.com/apache/spark/pull/5314/files introduced a bug, whereby for implicit feedback negative ratings just get ignored. Prior to that commit they were not ignored, but the absolute value was used as the confidence and the  preference was set to 0. The preservation of comments and absolute value indicate that this was unintentional.",
mllib.Matrices.fromBreeze may crash when converting from Breeze sparse matrix,SPARK-20687,13070680,http://spark.apache.org,Minor,,,,"Conversion of Breeze sparse matrices to Matrix is broken when matrices are product of certain operations. This problem I think is caused by the update method in Breeze CSCMatrix when they add provisional zeros to the data for efficiency.

This bug is serious and may affect at least BlockMatrix addition and substraction

http://stackoverflow.com/questions/33528555/error-thrown-when-using-blockmatrix-add/43883458#43883458

The following code, reproduces the bug (Check test(""breeze conversion bug""))

https://github.com/ghoto/spark/blob/test-bug/CSCMatrixBreeze/mllib/src/test/scala/org/apache/spark/mllib/linalg/MatricesSuite.scala

{code:title=MatricesSuite.scala|borderStyle=solid}

  test(""breeze conversion bug"") {
    // (2, 0, 0)
    // (2, 0, 0)
    val mat1Brz = Matrices.sparse(2, 3, Array(0, 2, 2, 2), Array(0, 1), Array(2, 2)).asBreeze
    // (2, 1E-15, 1E-15)
    // (2, 1E-15, 1E-15
    val mat2Brz = Matrices.sparse(2, 3, Array(0, 2, 4, 6), Array(0, 0, 0, 1, 1, 1), Array(2, 1E-15, 1E-15, 2, 1E-15, 1E-15)).asBreeze
    // The following shouldn't break
    val t01 = mat1Brz - mat1Brz
    val t02 = mat2Brz - mat2Brz
    val t02Brz = Matrices.fromBreeze(t02)
    val t01Brz = Matrices.fromBreeze(t01)

    val t1Brz = mat1Brz - mat2Brz
    val t2Brz = mat2Brz - mat1Brz
    // The following ones should break
    val t1 = Matrices.fromBreeze(t1Brz)
    val t2 = Matrices.fromBreeze(t2Brz)

  }

{code}",
result of MLlib KMeans cluster is not stabilize,SPARK-20634,13069863,http://spark.apache.org,Critical,,,,"1.Get a DataFrame through python with Cx_Oracle lib.
2.Start a local Spark Session.
3.Convert the dataset for Kmeansmodel train.
4.Train the KMeans model and predict the same data.just set K =3
5.Get the ClassifierFeature of the KMeans model'predict.
6.Get the count of every ClassifierFeature.
7.Loop 4-6 for 20 times.
8.Compare the result of every time.
9.Find the KMeans result dose not stabilize.
10.The same dataset and param for ML package'KMeans, its result is the same.
","Windows 10
spark 2.0.2 standalone
spyder 3.1.4
Anaconda 4.3.0
python 3.5.2"
SparseVector.argmax throws IndexOutOfBoundsException when the sparse vector has a size greater than zero but no elements defined.,SPARK-20615,13069489,http://spark.apache.org,Minor,,,,"org.apache.spark.ml.linalg.SparseVector.argmax throws an IndexOutOfRangeException when the vector size is greater than zero and no values are defined.  The toString() representation of such a vector is "" (100000,[],[])"".  This is because the argmax function tries to get the value at indexes(0) without checking the size of the array.

Code inspection reveals that the mllib version of SparseVector should have the same issue.",
Load doesn't work in PCAModel ,SPARK-20526,13067770,http://spark.apache.org,Major,"io, model",,,"Error occurs during loading PCAModel. Saved model doesn't load.

",Windows
"pyspark.sql.utils.IllegalArgumentException: u'DecisionTreeClassifier was given input with invalid label column label, without the number of classes specified. See StringIndexer",SPARK-20445,13066282,http://spark.apache.org,Major,job,,," #Load the CSV file into a RDD
    irisData = sc.textFile(""/home/infademo/surya/iris.csv"")
    irisData.cache()
    irisData.count()

    #Remove the first line (contains headers)
    dataLines = irisData.filter(lambda x: ""Sepal"" not in x)
    dataLines.count()

    from pyspark.sql import Row
    #Create a Data Frame from the data
    parts = dataLines.map(lambda l: l.split("",""))
    irisMap = parts.map(lambda p: Row(SEPAL_LENGTH=float(p[0]),\
                                    SEPAL_WIDTH=float(p[1]), \
                                    PETAL_LENGTH=float(p[2]), \
                                    PETAL_WIDTH=float(p[3]), \
                                    SPECIES=p[4] ))

    # Infer the schema, and register the DataFrame as a table.
    irisDf = sqlContext.createDataFrame(irisMap)
    irisDf.cache()

    #Add a numeric indexer for the label/target column
    from pyspark.ml.feature import StringIndexer
    stringIndexer = StringIndexer(inputCol=""SPECIES"", outputCol=""IND_SPECIES"")
    si_model = stringIndexer.fit(irisDf)
    irisNormDf = si_model.transform(irisDf)

    irisNormDf.select(""SPECIES"",""IND_SPECIES"").distinct().collect()
    irisNormDf.cache()

    """"""--------------------------------------------------------------------------
    Perform Data Analytics
    -------------------------------------------------------------------------""""""

    #See standard parameters
    irisNormDf.describe().show()

    #Find correlation between predictors and target
    for i in irisNormDf.columns:
        if not( isinstance(irisNormDf.select(i).take(1)[0][0], basestring)) :
            print( ""Correlation to Species for "", i, \
                        irisNormDf.stat.corr('IND_SPECIES',i))



    #Transform to a Data Frame for input to Machine Learing
    #Drop columns that are not required (low correlation)

    from pyspark.mllib.linalg import Vectors
    from pyspark.mllib.linalg import SparseVector
    from pyspark.mllib.regression import LabeledPoint
    from pyspark.mllib.util import MLUtils
    import org.apache.spark.mllib.linalg.{Matrix, Matrices}
    from pyspark.mllib.linalg.distributed import RowMatrix

    from pyspark.ml.linalg import Vectors
    pyspark.mllib.linalg.Vector
    def transformToLabeledPoint(row) :
        lp = ( row[""SPECIES""], row[""IND_SPECIES""], \
                    Vectors.dense([row[""SEPAL_LENGTH""],\
                            row[""SEPAL_WIDTH""], \
                            row[""PETAL_LENGTH""], \
                            row[""PETAL_WIDTH""]]))
        return lp




    irisLp = irisNormDf.rdd.map(transformToLabeledPoint)
    irisLpDf = sqlContext.createDataFrame(irisLp,[""species"",""label"", ""features""])
    irisLpDf.select(""species"",""label"",""features"").show(10)
    irisLpDf.cache()

    """"""--------------------------------------------------------------------------
    Perform Machine Learning
    -------------------------------------------------------------------------""""""
    #Split into training and testing data
    (trainingData, testData) = irisLpDf.randomSplit([0.9, 0.1])
    trainingData.count()
    testData.count()
    testData.collect()

    from pyspark.ml.classification import DecisionTreeClassifier
    from pyspark.ml.evaluation import MulticlassClassificationEvaluator

    #Create the model
    dtClassifer = DecisionTreeClassifier(maxDepth=2, labelCol=""label"",\
                    featuresCol=""features"")

   dtModel = dtClassifer.fit(trainingData)
   
   issue part:-
   
   dtModel = dtClassifer.fit(trainingData) Traceback (most recent call last): File """", line 1, in File ""/opt/mapr/spark/spark-1.6.1-bin-hadoop2.6/python/pyspark/ml/pipeline.py"", line 69, in fit return self._fit(dataset) File ""/opt/mapr/spark/spark-1.6.1-bin-hadoop2.6/python/pyspark/ml/wrapper.py"", line 133, in _fit java_model = self._fit_java(dataset) File ""/opt/mapr/spark/spark-1.6.1-bin-hadoop2.6/python/pyspark/ml/wrapper.py"", line 130, in _fit_java return self._java_obj.fit(dataset._jdf) File ""/opt/mapr/spark/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py"", line 813, in call File ""/opt/mapr/spark/spark-1.6.1-bin-hadoop2.6/python/pyspark/sql/utils.py"", line 53, in deco raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace) pyspark.sql.utils.IllegalArgumentException: u'DecisionTreeClassifier was given input with invalid label column label, without the number of classes specified. See StringIndexer.'",
"pyspark.sql.utils.IllegalArgumentException: u'DecisionTreeClassifier was given input with invalid label column label, without the number of classes specified. See StringIndexer",SPARK-20444,13066281,http://spark.apache.org,Major,job,,,,
pyspark linalg _convert_to_vector should check for sorted indices,SPARK-20214,13061551,http://spark.apache.org,Major,data structure,,,"I've seen a few failures of this line: https://github.com/apache/spark/blame/402bf2a50ddd4039ff9f376b641bd18fffa54171/python/pyspark/mllib/tests.py#L847

It converts a scipy.sparse.lil_matrix to a dok_matrix and then to a pyspark.mllib.linalg.Vector.  The failure happens in the conversion to a vector and indicates that the dok_matrix is not returning its values in sorted order. (Actually, the failure is in _convert_to_vector, which converts the dok_matrix to a csc_matrix and then passes the CSC data to the MLlib Vector constructor.) Here's the stack trace:
{code}
Traceback (most recent call last):
  File ""/home/jenkins/workspace/python/pyspark/mllib/tests.py"", line 847, in test_serialize
    self.assertEqual(sv, _convert_to_vector(lil.todok()))
  File ""/home/jenkins/workspace/python/pyspark/mllib/linalg/__init__.py"", line 78, in _convert_to_vector
    return SparseVector(l.shape[0], csc.indices, csc.data)
  File ""/home/jenkins/workspace/python/pyspark/mllib/linalg/__init__.py"", line 556, in __init__
    % (self.indices[i], self.indices[i + 1]))
TypeError: Indices 3 and 1 are not strictly increasing
{code}

This seems like a bug in _convert_to_vector, where we really should check {{csc_matrix.has_sorted_indices}} first.

I haven't seen this bug in pyspark.ml.linalg, but it probably exists there too.",
ArrayIndexOutOfBoundsException in ALS,SPARK-19600,13043044,http://spark.apache.org,Blocker,parallelism,,,"Understand issue SPARK-3080 closed, but I don't understand yet what cause the issue: memory, parallelism, negative userID or product ID?

I consistently ran into this issue with different set of training set, can you suggest any area to look at?

java.lang.ArrayIndexOutOfBoundsException: 221529807
        at org.apache.spark.ml.recommendation.ALS$$anonfun$partitionRatings$1$$anonfun$apply$6.apply(ALS.scala:944)
        at org.apache.spark.ml.recommendation.ALS$$anonfun$partitionRatings$1$$anonfun$apply$6.apply(ALS.scala:940)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:211)
        at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:200)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
        at org.apache.spark.scheduler.Task.run(Task.scala:86)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
",
Warning MLlib netlib,SPARK-19423,13039475,http://spark.apache.org,Minor,usability,,,"Hello,

I have used the glm to create a linear regressio model but there is these warning messages: 

WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK

to solve this warnings i have added the dependencies in the spark-defaults.conf as described in mllib guide (https://spark.apache.org/docs/1.2.1/mllib-guide.html):

spark.jars.packages=com.github.fommil.netlib:core:1.1.2

but after i obtain a null pointer exception:

The jars for the packages stored in: /root/.ivy2/jars
:: loading settings :: url = jar:file:/home/user/spark-2.0.1-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
com.github.fommil.netlib#all added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found com.github.fommil.netlib#all;1.1.2 in central
	found net.sourceforge.f2j#arpack_combined_all;0.1 in central
	found com.github.fommil.netlib#core;1.1.2 in central
	found com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1 in central
	found com.github.fommil.netlib#native_ref-java;1.1 in central
	found com.github.fommil#jniloader;1.1 in central
	found com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1 in central
	found com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1 in central
	found com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1 in central
	found com.github.fommil.netlib#netlib-native_ref-win-i686;1.1 in central
	found com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1 in central
	found com.github.fommil.netlib#native_system-java;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-linux-i686;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-win-i686;1.1 in central
:: resolution report :: resolve 1151ms :: artifacts dl 46ms
	:: modules in use:
	com.github.fommil#jniloader;1.1 from central in [default]
	com.github.fommil.netlib#all;1.1.2 from central in [default]
	com.github.fommil.netlib#core;1.1.2 from central in [default]
	com.github.fommil.netlib#native_ref-java;1.1 from central in [default]
	com.github.fommil.netlib#native_system-java;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-win-i686;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-linux-i686;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-win-i686;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1 from central in [default]
	net.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]
	:: evicted modules:
	com.github.fommil.netlib#core;1.1 by [com.github.fommil.netlib#core;1.1.2] in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   19  |   0   |   0   |   1   ||   17  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
	confs: [default]
	0 artifacts copied, 17 already retrieved (0kB/46ms)
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.spark.deploy.RPackageUtils$.checkManifestForR(RPackageUtils.scala:95)
	at org.apache.spark.deploy.RPackageUtils$$anonfun$checkAndBuildRPackage$1.apply(RPackageUtils.scala:179)
	at org.apache.spark.deploy.RPackageUtils$$anonfun$checkAndBuildRPackage$1.apply(RPackageUtils.scala:175)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.deploy.RPackageUtils$.checkAndBuildRPackage(RPackageUtils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:306)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:158)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Error in sparkR.sparkContext(master, appName, sparkHome, sparkConfigMap,  : 
  JVM is not ready after 10 seconds
Calls: sparkR.session -> sparkR.sparkContext

Seems there are problems with JVM,Anyoune knows how is possible solve this problem?

Thanks
Marco

Fai clic qui per Rispondere
",
StatFunctions.multipleApproxQuantiles can give NoSuchElementException: next on empty iterator,SPARK-19339,13037201,http://spark.apache.org,Minor,job,,,"This problem is easy to reproduce by running StatFunctions.multipleApproxQuantiles on an empty dataset, but I think it can occur in other cases, like if the column is all null or all one value.
I have unit tests that can hit it in several different cases.

The fix that I have introduced locally is to return
{code}
 if (sampled.length == 0) 0 else sampled.last.value
{code}
instead of 
{code}
sampled.last.value
{code}
at the end of QuantileSummaries.query.
Below is the exception:
{code}
next on empty iterator
java.util.NoSuchElementException: next on empty iterator
	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)
	at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)
	at scala.collection.IterableLike$class.head(IterableLike.scala:107)
	at scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:186)
	at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126)
	at scala.collection.mutable.ArrayOps$ofRef.head(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.last(TraversableLike.scala:459)
	at scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$last(ArrayOps.scala:186)
	at scala.collection.IndexedSeqOptimized$class.last(IndexedSeqOptimized.scala:132)
	at scala.collection.mutable.ArrayOps$ofRef.last(ArrayOps.scala:186)
	at org.apache.spark.sql.catalyst.util.QuantileSummaries.query(QuantileSummaries.scala:207)
	at org.apache.spark.sql.SparkPercentileCalculator$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply$mcDD$sp(SparkPercentileCalculator.scala:91)
	at org.apache.spark.sql.SparkPercentileCalculator$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply(SparkPercentileCalculator.scala:91)
	at org.apache.spark.sql.SparkPercentileCalculator$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply(SparkPercentileCalculator.scala:91)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.SparkPercentileCalculator$$anonfun$multipleApproxQuantiles$1.apply(SparkPercentileCalculator.scala:91)
	at org.apache.spark.sql.SparkPercentileCalculator$$anonfun$multipleApproxQuantiles$1.apply(SparkPercentileCalculator.scala:91)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.SparkPercentileCalculator.multipleApproxQuantiles(SparkPercentileCalculator.scala:91)
	at com.mineset.spark.statistics.model.ContinuousMinesetStats.quartiles$lzycompute(ContinuousMinesetStats.scala:274)
	at com.mineset.spark.statistics.model.ContinuousMinesetStats.quartiles(ContinuousMinesetStats.scala:272)
	at com.mineset.spark.statistics.model.MinesetStats.com$mineset$spark$statistics$model$MinesetStats$$serializeContinuousFeature$1(MinesetStats.scala:66)
	at com.mineset.spark.statistics.model.MinesetStats$$anonfun$calculateWithColumns$1.apply(MinesetStats.scala:118)
	at com.mineset.spark.statistics.model.MinesetStats$$anonfun$calculateWithColumns$1.apply(MinesetStats.scala:114)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at com.mineset.spark.statistics.model.MinesetStats.calculateWithColumns(MinesetStats.scala:114)
	at com.mineset.spark.statistics.model.MinesetStats.toJson(MinesetStats.scala:46)
	at com.mineset.spark.statistics.model.MinesetStatsSuite$$anonfun$8.apply$mcV$sp(MinesetStatsSuite.scala:93)
	at com.mineset.spark.statistics.model.MinesetStatsSuite$$anonfun$8.apply(MinesetStatsSuite.scala:90)
	at com.mineset.spark.statistics.model.MinesetStatsSuite$$anonfun$8.apply(MinesetStatsSuite.scala:90)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at com.mineset.spark.statistics.model.MinesetStatsSuite.org$scalatest$BeforeAndAfterAll$$super$run(MinesetStatsSuite.scala:30)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at com.mineset.spark.statistics.model.MinesetStatsSuite.run(MinesetStatsSuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.run(Runner.scala:883)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:138)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
{code}",
GaussianMixture throws cryptic error when number of features is too high,SPARK-19313,13036649,http://spark.apache.org,Minor,memory,,,"The following fails

{code}
    val df = Seq(
      Vectors.sparse(46400, Array(0, 4), Array(3.0, 8.0)),
      Vectors.sparse(46400, Array(1, 5), Array(4.0, 9.0)))
      .map(Tuple1.apply).toDF(""features"")
    val gm = new GaussianMixture()
    gm.fit(df)
{code}

It fails because GMMs allocate an array of size {{numFeatures * numFeatures}} and in this case we'll get integer overflow. We should limit the number of features appropriately.",
"Fix several sql, mllib and status api examples not working",SPARK-19134,13033045,http://spark.apache.org,Minor,job,,,"*binary_classification_metrics_example.py*

{code}
./bin/spark-submit examples/src/main/python/mllib/binary_classification_metrics_example.py
{code}

{code}
  File "".../spark/examples/src/main/python/mllib/binary_classification_metrics_example.py"", line 39, in <lambda>
    .rdd.map(lambda row: LabeledPoint(row[0], row[1]))
  File "".../spark/python/pyspark/mllib/regression.py"", line 54, in __init__
    self.features = _convert_to_vector(features)
  File "".../spark/python/pyspark/mllib/linalg/__init__.py"", line 80, in _convert_to_vector
    raise TypeError(""Cannot convert type %s into Vector"" % type(l))
TypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector
{code}

*status_api_demo.py*

{code}
PYSPARK_PYTHON=python3 ./bin/spark-submit examples/src/main/python/status_api_demo.py
{code}

{code}
Traceback (most recent call last):
  File "".../spark/examples/src/main/python/status_api_demo.py"", line 22, in <module>
    import Queue
ImportError: No module named 'Queue'
{code}

*bisecting_k_means_example.py*

{code}
./bin/spark-submit examples/src/main/python/mllib/bisecting_k_means_example.py
{code}

{code}
Traceback (most recent call last):
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/examples/src/main/python/mllib/bisecting_k_means_example.py"", line 46, in <module>
    model.save(sc, path)
AttributeError: 'BisectingKMeansModel' object has no attribute 'save'
{code}

*elementwise_product_example.py*

{code}
./bin/spark-submit examples/src/main/python/mllib/elementwise_product_example.py
{code}

{code}
Traceback (most recent call last):
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/examples/src/main/python/mllib/elementwise_product_example.py"", line 48, in <module>
    for each in transformedData2.collect():
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/pyspark/mllib/linalg/__init__.py"", line 478, in __getattr__
    return getattr(self.array, item)
AttributeError: 'numpy.ndarray' object has no attribute 'collect'
{code}

*hive.py*
{code}
./bin/spark-submit examples/src/main/python/sql/hive.py
{code}

{code}
Traceback (most recent call last):
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/examples/src/main/python/sql/hive.py"", line 47, in <module>
    spark.sql(""CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"")
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/lib/pyspark.zip/pyspark/sql/session.py"", line 541, in sql
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 69, in deco
pyspark.sql.utils.AnalysisException: 'org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./spark-warehouse);'
{code}


*SparkHiveExample*

{code}
./bin/run-example sql.hive.SparkHiveExample
{code}

{code}
Exception in thread ""main"" org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./spark-warehouse
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:498)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:484)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1668)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.sql.hive.client.Shim_v0_14.loadTable(HiveShim.scala:722)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply$mcV$sp(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:230)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:229)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:272)
	at org.apache.spark.sql.hive.client.HiveClientImpl.loadTable(HiveClientImpl.scala:685)
{code}


*JavaSparkHiveExample*

{code}
./bin/run-example sql.hive.JavaSparkHiveExample
{code}

{code}
Exception in thread ""main"" org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./spark-warehouse
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:498)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:484)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1668)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.sql.hive.client.Shim_v0_14.loadTable(HiveShim.scala:722)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply$mcV$sp(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
{code}",
DistributedLDAModel returns different logPrior for original and loaded model,SPARK-19110,13032667,http://spark.apache.org,Major,"parallelism, model",,,"While adding DistributedLDAModel training summary for SparkR, I found that the logPrior for original and loaded model is different.
For example, in the test(""read/write DistributedLDAModel""), I add the test:
val logPrior = model.asInstanceOf[DistributedLDAModel].logPrior
      val logPrior2 = model2.asInstanceOf[DistributedLDAModel].logPrior
      assert(logPrior === logPrior2)
The test fails:
-4.394180878889078 did not equal -4.294290536919573


",
Correlation causes Error “Cannot determine the number of cols”,SPARK-18562,13022851,http://spark.apache.org,Major,job,,,"I followed the MLlib docs on how to calculate a correlation. I'm using Spark 1.6.1.

First my application filters out elements that do not have all the values I'm looking for. Afterwards, I'm mapping each of the remaining elements to a dense Vector, as seen in the docs. Then I'm passing the RDD[Vector] to the MLlib function.

My code is the following:
{code}
val filteredRdd = rdd.filter(document => document.containsKey(""SomeValue1"")
  && document.containsKey(""SomeValue2"") && document.containsKey(""SomeValue3""))

val vectorRdd: RDD[Vector] = filteredRdd.map(document => {
  Vectors.dense(document.getDouble(""SomeValue1""), document.getDouble(""SomeValue2""), document.getDouble(""SomeValue3""))
})

val correlation_matrix = Statistics.corr(vectorRdd, method = ""spearman"")
println(""Spearman: "" + correlation_matrix.toString())

val correlation_matrix_pearson = Statistics.corr(vectorRdd, method = ""pearson"")
println(""Pearson: "" + correlation_matrix_pearson.toString())
{code}

This is the error that gets thrown:
{code}
16/11/23 13:19:51 ERROR ApplicationMaster: User class threw exception: 

java.lang.RuntimeException: Cannot determine the number of cols because it is not specified in the constructor and the rows RDD is empty.
java.lang.RuntimeException: Cannot determine the number of cols because it is not specified in the constructor and the rows RDD is empty.
    at scala.sys.package$.error(package.scala:27)
    at org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:64)
    at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:328)
    at org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)
    at org.apache.spark.mllib.stat.correlation.SpearmanCorrelation$.computeCorrelationMatrix(SpearmanCorrelation.scala:91)
    at org.apache.spark.mllib.stat.correlation.Correlations$.corrMatrix(Correlation.scala:66)
    at org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:74)
{code}

Because I filter out the elements which would cause an empty vector, I don't see how this is related to my code? Thus I created this issue. ",Ubuntu 14.04LTS
"Fix default Locale used in DateFormat, NumberFormat to Locale.US",SPARK-18076,13014750,http://spark.apache.org,Major,,releasenotes,,"Many parts of the code use {{DateFormat}} and {{NumberFormat}} instances. Although the behavior of these format is mostly determined by things like format strings, the exact behavior can vary according to the platform's default locale. Although the locale defaults to ""en"", it can be set to something else by env variables. And if it does, it can cause the same code to succeed or fail based just on locale:

{code}
import java.text._
import java.util._

def parse(s: String, l: Locale) = new SimpleDateFormat(""yyyyMMMdd"", l).parse(s)

parse(""1989Dec31"", Locale.US)
Sun Dec 31 00:00:00 GMT 1989

parse(""1989Dec31"", Locale.UK)
Sun Dec 31 00:00:00 GMT 1989

parse(""1989Dec31"", Locale.CHINA)
java.text.ParseException: Unparseable date: ""1989Dec31""
  at java.text.DateFormat.parse(DateFormat.java:366)
  at .parse(<console>:18)
  ... 32 elided

parse(""1989Dec31"", Locale.GERMANY)
java.text.ParseException: Unparseable date: ""1989Dec31""
  at java.text.DateFormat.parse(DateFormat.java:366)
  at .parse(<console>:18)
  ... 32 elided
{code}

Where not otherwise specified, I believe all instances in the code should default to some fixed value, and that should probably be {{Locale.US}}. This matches the JVM's default, and specifies both language (""en"") and region (""US"") to remove ambiguity. This most closely matches what the current code behavior would be (unless default locale was changed), because it will currently default to ""en"".

This affects SQL date/time functions. At the moment, the only SQL function that lets the user specify language/country is ""sentences"", which is consistent with Hive.

It affects dates passed in the JSON API. 

It affects some strings rendered in the UI, potentially. Although this isn't a correctness issue, there may be an argument for not letting that vary (?)

It affects a bunch of instances where dates are formatted into strings for things like IDs or file names, which is far less likely to cause a problem, but worth making consistent.

The other occurrences are in tests.


The downside to this change is also its upside: the behavior doesn't depend on default JVM locale, but, also can't be affected by the default JVM locale. For example, if you wanted to parse some dates in a way that depended on an non-US locale (not just the format string) then it would no longer be possible. There's no means of specifying this, for example, in SQL functions for parsing dates. However, controlling this by globally changing the locale isn't exactly great either.

The purpose of this change is to make the current default behavior deterministic and fixed. PR coming.

CC [~hyukjin.kwon]",
Decision Trees do not handle edge cases,SPARK-18036,13014079,http://spark.apache.org,Minor,,,,"Decision trees/GBT/RF do not handle edge cases such as constant features or empty features. For example:

{code}
val dt = new DecisionTreeRegressor()
val data = Seq(LabeledPoint(1.0, Vectors.dense(Array.empty[Double]))).toDF()
dt.fit(data)

java.lang.UnsupportedOperationException: empty.max
  at scala.collection.TraversableOnce$class.max(TraversableOnce.scala:229)
  at scala.collection.mutable.ArrayOps$ofInt.max(ArrayOps.scala:234)
  at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:207)
  at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:105)
  at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:93)
  at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:46)
  at org.apache.spark.ml.Predictor.fit(Predictor.scala:90)
  ... 52 elided

{code}

as well as 

{code}
val dt = new DecisionTreeRegressor()
val data = Seq(LabeledPoint(1.0, Vectors.dense(0.0, 0.0, 0.0))).toDF()
dt.fit(data)

java.lang.UnsupportedOperationException: empty.maxBy
at scala.collection.TraversableOnce$class.maxBy(TraversableOnce.scala:236)
at scala.collection.SeqViewLike$AbstractTransformed.maxBy(SeqViewLike.scala:37)
at org.apache.spark.ml.tree.impl.RandomForest$.binsToBestSplit(RandomForest.scala:846)
{code}",
EMLDAOptimizer fails with ClassCastException on YARN,SPARK-17975,13012970,http://spark.apache.org,Major,job,,,"I'm able to reproduce the error consistently with a 2000 record text file with each record having 1-5 terms and checkpointing enabled. It looks like the problem was introduced with the resolution for SPARK-13355.

The EdgeRDD class seems to be lying about it's type in a way that causes RDD.mapPartitionsWithIndex method to be unusable when it's referenced as an RDD of Edge elements.

{code}
val spark = SparkSession.builder.appName(""lda"").getOrCreate()
spark.sparkContext.setCheckpointDir(""hdfs:///tmp/checkpoints"")
val data: RDD[(Long, Vector)] = // snip
data.setName(""data"").cache()
val lda = new LDA
val optimizer = new EMLDAOptimizer
lda.setOptimizer(optimizer)
  .setK(10)
  .setMaxIterations(400)
  .setAlpha(-1)
  .setBeta(-1)
  .setCheckpointInterval(7)
val ldaModel = lda.run(data)
{code}

{noformat}
16/10/16 23:53:54 WARN TaskSetManager: Lost task 3.0 in stage 348.0 (TID 1225, server2.domain): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.graphx.Edge
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1$$anonfun$apply$1.apply(EdgeRDD.scala:107)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:107)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:105)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
{noformat}","Centos 6, CDH 5.7, Java 1.7u80"
ML/MLLIB: ChiSquareSelector based on Statistics.chiSqTest(RDD) is wrong ,SPARK-17870,13011295,http://spark.apache.org,Critical,job,,,"The method to count ChiSqureTestResult in mllib/feature/ChiSqSelector.scala  (line 233) is wrong.

For feature selection method ChiSquareSelector, it is based on the ChiSquareTestResult.statistic (ChiSqure value) to select the features. It select the features with the largest ChiSqure value. But the Degree of Freedom (df) of ChiSqure value is different in Statistics.chiSqTest(RDD), and for different df, you cannot base on ChiSqure value to select features.

Because of the wrong method to count ChiSquare value, the feature selection results are strange.
Take the test suite in ml/feature/ChiSqSelectorSuite.scala as an example:
If use selectKBest to select: the feature 3 will be selected.
If use selectFpr to select: feature 1 and 2 will be selected. 
This is strange. 

I use scikit learn to test the same data with the same parameters. 
When use selectKBest to select: feature 1 will be selected. 
When use selectFpr to select: feature 1 and 2 will be selected. 
This result is make sense. because the df of each feature in scikit learn is the same.

I plan to submit a PR for this problem.
 

 
",
org.apache.spark.mllib.linalg.VectorUDT cannot be cast to org.apache.spark.sql.types.StructType,SPARK-17765,13009210,http://spark.apache.org,Major,data structure,,,"The issue in subject happens on attempt to transform DataFrame in Parquet format into ORC while DF contains SparseVector/DenseVector data.

In [sources|https://github.com/apache/spark/blob/v1.6.1/mllib/src/main/scala/org/apache/spark/mllib/linalg/Vectors.scala#L192] it looks like that there shouldn't be any serialization issues, but they happens.

{code}
In[4] pqtdf = hqlctx.read.parquet(pqt_feature)

In[5] pqtdf.take(1)
Out[5]: [Row(foo=u'abc, bar=SparseVector(100, {74: 1.0}))]

In[6]: pqtdf.write.format('orc').save('/tmp/orc')
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-5-57e68fd0c5cb> in <module>()
----> pqtdf.write.format('orc').save('/tmp/orc')

/usr/local/share/spark/python/pyspark/sql/readwriter.pyc in save(self, path, format, mode, partitionBy, **options)
    395             self._jwrite.save()
    396         else:
--> 397             self._jwrite.save(path)
    398 
    399     @since(1.4)

/usr/local/lib/python2.7/site-packages/py4j/java_gateway.pyc in __call__(self, *args)
    811         answer = self.gateway_client.send_command(command)
    812         return_value = get_return_value(
--> 813             answer, self.gateway_client, self.target_id, self.name)
    814 
    815         for temp_arg in temp_args:

/usr/local/share/spark/python/pyspark/sql/utils.pyc in deco(*a, **kw)
     43     def deco(*a, **kw):
     44         try:
---> 45             return f(*a, **kw)
     46         except py4j.protocol.Py4JJavaError as e:
     47             s = e.java_exception.toString()

/usr/local/lib/python2.7/site-packages/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name)
    306                 raise Py4JJavaError(
    307                     ""An error occurred while calling {0}{1}{2}.\n"".
--> 308                     format(target_id, ""."", name), value)
    309             else:
    310                 raise Py4JError(

Py4JJavaError: An error occurred while calling o62.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:156)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:256)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:139)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:209)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 185, node123.example.com): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:272)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: org.apache.spark.mllib.linalg.VectorUDT cannot be cast to org.apache.spark.sql.types.StructType
	at org.apache.spark.sql.hive.HiveInspectors$class.wrap(HiveInspectors.scala:554)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrap(OrcRelation.scala:66)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrapOrcStruct(OrcRelation.scala:128)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.writeInternal(OrcRelation.scala:139)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:264)
	... 8 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:150)
	... 27 more
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:272)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	... 1 more
Caused by: java.lang.ClassCastException: org.apache.spark.mllib.linalg.VectorUDT cannot be cast to org.apache.spark.sql.types.StructType
	at org.apache.spark.sql.hive.HiveInspectors$class.wrap(HiveInspectors.scala:554)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrap(OrcRelation.scala:66)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrapOrcStruct(OrcRelation.scala:128)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.writeInternal(OrcRelation.scala:139)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:264)
	... 8 more
{code}",
Erroneous computation in multiplication of transposed SparseMatrix with SparseVector,SPARK-17721,13008479,http://spark.apache.org,Critical,data formats,correctness,,"There is a bug in how a transposed SparseMatrix (isTransposed=true) does multiplication with a SparseVector. The bug is present (for v. > 2.0.0) in both org.apache.spark.mllib.linalg.BLAS (mllib) and org.apache.spark.ml.linalg.BLAS (mllib-local) in the private gemv method with signature:
bq. gemv(alpha: Double, A: SparseMatrix, x: SparseVector, beta: Double, y: DenseVector).

This bug can be verified by running the following snippet in a Spark shell (here using v1.6.1):
{code:java}
import com.holdenkarau.spark.testing.SharedSparkContext
import org.apache.spark.mllib.linalg._

val A = Matrices.dense(3, 2, Array[Double](0, 2, 1, 1, 2, 0)).asInstanceOf[DenseMatrix].toSparse.transpose
val b = Vectors.sparse(3, Seq[(Int, Double)]((1, 2), (2, 1))).asInstanceOf[SparseVector]

A.multiply(b)
A.multiply(b.toDense)
{code}
The first multiply with the SparseMatrix returns the incorrect result:
{code:java}
org.apache.spark.mllib.linalg.DenseVector = [5.0,0.0]
{code}
whereas the correct result is returned by the second multiply:
{code:java}
org.apache.spark.mllib.linalg.DenseVector = [5.0,4.0]
{code}",Verified on OS X with Spark 1.6.1 and on Databricks running Spark 1.6.1
SparseVector __getitem__ should follow __getitem__ contract,SPARK-17587,13005785,http://spark.apache.org,Minor,prarllelism,,,"According to {{\_\_getitem\_\_}} [contract|https://docs.python.org/3/reference/datamodel.html#object.__getitem__]:

{quote}
if of a value outside the set of indexes for the sequence (after any special interpretation of negative values), {{IndexError}} should be raised.
{quote}

This required for example for correct iteration over the structure.

Right now it throws {{ValueError}} what results in a quite confusing behavior when attempt to iterate over a vector results in a {{ValueError}} due to unterminated iteration:

{code}

In [1]: from pyspark.mllib.linalg import SparseVector

In [2]: list(SparseVector(4, [0], [0]))
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-147f3bb0a47d> in <module>()
----> 1 list(SparseVector(4, [0], [0]))

/opt/spark-2.0/python/pyspark/mllib/linalg/__init__.py in __getitem__(self, index)
    803 
    804         if index >= self.size or index < -self.size:
--> 805             raise ValueError(""Index %d out of bounds."" % index)
    806         if index < 0:
    807             index += self.size

ValueError: Index 4 out of bounds.
{code}

",
PeriodicGraphCheckpointer did not persist edges as expected in some cases,SPARK-17559,13005336,http://spark.apache.org,Minor,job,,,"When use PeriodicGraphCheckpointer to persist graph, sometimes the edge isn't persisted. As currently only when vertices's storage level is none, graph is persisted. However there is a chance vertices's storage level is not none while edges's is none. Eg. graph created by a outerJoinVertices operation, vertices is automatically cached while edges is not. In this way, edges will not be persisted if we use PeriodicGraphCheckpointer do persist.

See below minimum example:
   val graphCheckpointer = new PeriodicGraphCheckpointer[Array[String], Int](2, sc)
    val users = sc.textFile(""data/graphx/users.txt"")
      .map(line => line.split("","")).map(parts => (parts.head.toLong, parts.tail))
    val followerGraph = GraphLoader.edgeListFile(sc, ""data/graphx/followers.txt"")

    val graph = followerGraph.outerJoinVertices(users) {
      case (uid, deg, Some(attrList)) => attrList
      case (uid, deg, None) => Array.empty[String]
    }
    graphCheckpointer.update(graph)    ",
Word2VecModel.findSynonyms can spuriously reject the best match when invoked with a vector,SPARK-17548,13005020,http://spark.apache.org,Minor,,,,"The `findSynonyms` method in `Word2VecModel` currently rejects the best match a priori. When `findSynonyms` is invoked with a word, the best match is almost certain to be that word, but `findSynonyms` can also be invoked with a vector, which might not correspond to any of the words in the model's vocabulary.  In the latter case, rejecting the best match is spurious.",any
IsotonicRegression takes non-polynomial time for some inputs,SPARK-17455,13003666,http://spark.apache.org,Major,data partition,,,"The Pool Adjacent Violators Algorithm (PAVA) implementation that's currently in MLlib can take O(N!) time for certain inputs, when it should have worst-case complexity of O(N^2).

To reproduce this, I pulled the private method poolAdjacentViolators out of mllib.regression.IsotonicRegression and into a benchmarking harness.

Given this input
{code}
val x = (1 to length).toArray.map(_.toDouble)
val y = x.reverse.zipWithIndex.map{ case (yi, i) => if (i % 2 == 1) yi - 1.5 else yi}
val w = Array.fill(length)(1d)

val input: Array[(Double, Double, Double)] = (y zip x zip w) map{ case ((y, x), w) => (y, x, w)}
{code}

I vary the length of the input to get these timings:

|| Input Length || Time (us) ||
| 100 | 1.35 |
| 200 | 3.14 | 
| 400 | 116.10 |
| 800 | 2134225.90 |

(tests were performed using https://github.com/sirthias/scala-benchmarking-template)

I can also confirm that I run into this issue on a real dataset I'm working on when trying to calibrate random forest probability output. Some partitions take > 12 hours to run. This isn't a skew issue, since the largest partitions finish in minutes. I can only assume that some partitions cause something approaching this worst-case complexity.

I'm working on a patch that borrows the implementation that is used in scikit-learn and the R ""iso"" package, both of which handle this particular input in linear time and are quadratic in the worst case.",
fix MultivariateOnlineSummerizer.numNonZeros,SPARK-17363,13002126,http://spark.apache.org,Minor,,,,"The MultivariantOnlineSummerizer.numNonZeros method is wrong.

it should return the nnz array, not weightSum array.",
fix MultivariantOnlineSummerizer.numNonZeros,SPARK-17362,13002125,http://spark.apache.org,Major,,,,"The MultivariantOnlineSummerizer.numNonZeros method is wrong.

it should return the nnz array, not weightSum array.",
Comparing Vector in relative tolerance or absolute tolerance in UnitTests error ,SPARK-17207,12999507,http://spark.apache.org,Major,,,,"The result of compare two vectors using UnitTests (org.apache.spark.mllib.util.TestingUtils) is not right sometime.
For example:
val a = Vectors.dense(Arrary(1.0, 2.0))
val b = Vectors.zeros(0)
a ~== b absTol 1e-1 // the result is true. 

",
"""Pipeline guide"" link is broken in MLlib Guide main page",SPARK-17202,12999240,http://spark.apache.org,Trivial,,,,"Steps to reproduce:
1) Check http://spark.apache.org/docs/latest/ml-guide.html 
2) Link in sentence ""See the Pipelines guide for details"" is broken, it points to https://spark.apache.org/docs/latest/ml-pipeline.md

Expected result: ""Pipeline guide"" link should point to https://spark.apache.org/docs/latest/ml-pipeline.html


",
SparseVectors.apply and SparseVectors.toArray have different returns when creating with a illegal indices,SPARK-17130,12998199,http://spark.apache.org,Minor,,,,"One of my colleagues ran into a bug of SparseVectors. He called the Vectors.sparse(size: Int, indices: Array[Int], values: Array[Double]) without noticing that the indices are assumed to be ordered.

The vector he created has all value of 0.0 (without any warning), if we try to get value via apply method. However, SparseVector.toArray will generates a array using a method that is order insensitive. Hence, you will get a 0.0 when you call apply method, while you can get correct result using toArray or toDense method. The result of SparseVector.toArray is actually misleading.



It could be safer if there is a validation of indices in the constructor or at least make the returns of apply method and toArray method the same.",spark 1.6.1 + scala
Fix bound checking for SparseVector,SPARK-16965,12995844,http://spark.apache.org,Minor,,,,"There's several issues in the bound checking of SparseVector

1. In scala, miss negative index checking and different bound checking is scattered in several places. Should put them in one place
2. In python, miss low/upper bound checking of indices. ",
"Improve ANN training, add training data persist if needed",SPARK-16880,12994627,http://spark.apache.org,Minor,job,,,"The ANN layer training does not persist input data RDD,
so that it may cause overhead cost if the RDD need to compute from lineage.",
fix latex formula syntax error in mllib,SPARK-16600,12990320,http://spark.apache.org,Trivial,,,,"I found several place the latex formula use
\partial\xx
it should be 
\partial xx
",
Bug in SparseMatrix multiplication with SparseVector,SPARK-16566,12989712,http://spark.apache.org,Major,job,,,"In the org.apache.spark.mllib.linalg.BLAS.scala, the multiplication between SparseMatrix (sm) and SparseVector (sv) when sm is not transposed assume that the indices is sorted, but there is no validation to make sure that is the case, making the result returned wrongly.

This can be replicated simply by using spark-shell and entering these commands:

import org.apache.spark.mllib.linalg.SparseMatrix
import org.apache.spark.mllib.linalg.SparseVector
import org.apache.spark.mllib.linalg.DenseVector
import scala.collection.mutable.ArrayBuffer

val vectorIndices = Array(3,2)
val vectorValues = Array(0.1,0.2)
val size = 4

val sm = new SparseMatrix(size, size, Array(0, 0, 0, 1, 1), Array(0), Array(1.0))
val dm = sm.toDense
val sv = new SparseVector(size, vectorIndices, vectorValues)
val dv = new DenseVector(s.toArray)

sm.multiply(dv) == sm.multiply(sv)

sm.multiply(dv)
sm.multiply(sv)",
Potential numerical problem in MultivariateOnlineSummarizer min/max,SPARK-16561,12989660,http://spark.apache.org,Major,,,,"In `MultivariateOnlineSummarizer` min/max method, 
use judgement ""nnz(i) < weightSum"", it will cause some numerial problem
and make result unstable.

for example,
add two vector:
[10, -10] with weight 1e10
[0, 0] with weight 1e-10

using MultivariateOnlineSummarizer.min/max we will get
minVector = [10, -10]
maxVector = [10, -10]

but the right result should be
minVector = [0, -10]
maxVector = [10, 0]

The bug reason is that
(1e10 + 1e-10) == 1e10 (Double type)
because of the floating rounding.
and different accumulating or merging order may cause different result,
such as:

[10, -10] with weight 1e10
[0, 0] with weight 1e-7
....
(100 lines data [0, 0] with weight 1e-7)

using the input data order listed above, we will get the result:
minVector = [10, -10]
maxVector = [10, -10]

but if the input data order is as following:

[0, 0] with weight 1e-7
....
(100 lines data [0, 0] with weight 1e-7)
[10, -10] with weight 1e10

than it the result will be:
minVector = [0, -10]
maxVector = [10, 0]

that's because:
1e10 + 1e-7 + ... + 1e-7(add 100 times) == 1e10  (Double type)
but
1e-7 + ... + 1e-7(add 100 times) + 1e10 = 1.000000000000001E10 != 1e10  (Double type)



",
examples/mllib/LDAExample should use MLVector instead of MLlib Vector,SPARK-16558,12989622,http://spark.apache.org,Minor,,,,"mllib.LDAExample uses ML pipeline and MLlib LDA algorithm. The former transforms original data into MLVector format, while the latter uses MLlibVector format.",
BisectingKMeans Algorithm failing with java.util.NoSuchElementException: key not found,SPARK-16473,12988121,http://spark.apache.org,Major,job,,,"Hello , 

I am using apache spark 1.6.1. 
I am executing bisecting k means algorithm on a specific dataset .
Dataset details :- 
K=100,
input vector =100K*100k
Memory assigned 16GB per node ,
number of nodes =2.

 Till K=75 it os working fine , but when I set k=100 , it fails with java.util.NoSuchElementException: key not found. 

*I suspect it is failing because of lack of some resources , but somehow exception does not convey anything as why this spark job failed.* 

Please can someone point me to root cause of this exception , why it is failing. 

This is the exception stack-trace:- 
{code}
java.util.NoSuchElementException: key not found: 166 
        at scala.collection.MapLike$class.default(MapLike.scala:228) 
        at scala.collection.AbstractMap.default(Map.scala:58) 
        at scala.collection.MapLike$class.apply(MapLike.scala:141) 
        at scala.collection.AbstractMap.apply(Map.scala:58) 
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1$$anonfun$2.apply$mcDJ$sp(BisectingKMeans.scala:338)
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1$$anonfun$2.apply(BisectingKMeans.scala:337)
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1$$anonfun$2.apply(BisectingKMeans.scala:337)
        at scala.collection.TraversableOnce$$anonfun$minBy$1.apply(TraversableOnce.scala:231) 
        at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111) 
        at scala.collection.immutable.List.foldLeft(List.scala:84) 
        at scala.collection.LinearSeqOptimized$class.reduceLeft(LinearSeqOptimized.scala:125) 
        at scala.collection.immutable.List.reduceLeft(List.scala:84) 
        at scala.collection.TraversableOnce$class.minBy(TraversableOnce.scala:231) 
        at scala.collection.AbstractTraversable.minBy(Traversable.scala:105) 
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1.apply(BisectingKMeans.scala:337) 
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1.apply(BisectingKMeans.scala:334) 
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) 
        at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:389) 
{code}

Issue is that , it is failing but not giving any explicit message as to why it failed.",AWS EC2 linux instance. 
Undeleted broadcast variables in Word2Vec causing OoM for long runs ,SPARK-16440,12987775,http://spark.apache.org,Major,"job,memory",,,"Three broadcast variables created at the beginning of {{Word2Vec.fit()}} are never deleted nor unpersisted. This seems to cause excessive memory consumption on the driver for a job running hundreds of successive training.

They are 
{code}
    val expTable = sc.broadcast(createExpTable())
    val bcVocab = sc.broadcast(vocab)
    val bcVocabHash = sc.broadcast(vocabHash)
{code}

",
IsotonicRegression produces NaNs with certain data,SPARK-16426,12987590,http://spark.apache.org,Minor,data partition,,,"{code}
val r = sc.parallelize(Seq[(Double, Double, Double)]((2, 1, 1), (1, 1, 1), (0, 2, 1), (1, 2, 1), (0.5, 3, 1), (0, 3, 1)), 2)
val i = new IsotonicRegression().run(r)

scala> i.predict(3.0)
res12: Double = NaN

scala> i.predictions
res13: Array[Double] = Array(0.75, 0.75, NaN, NaN)
{code}

I believe I understand the problem so I'll submit a PR shortly.

The problem happens when rows with the same feature value but different labels end up on different partitions. The merge function in poolAdjacentViolators introduces 0-weight points to be used for linear interpolation. This works fine, as long as they are always next to a non-0-weight point, but in the above case, you can end up with two 0-weight points  with the same feature value, which end up next to each other in the final PAV step. If these points are pooled, it creates a NaN.

One solution to this is to ensure that the all points with identical feature values end up on the same partition. This is the solution I intend to submit a PR for. Another option would be to try to get rid of the 0-weight points, but that seems trickier to me.",
Spark MLlib: MultilayerPerceptronClassifier - error while training,SPARK-16377,12986773,http://spark.apache.org,Major,usability,,,"Hi, 

I am trying to train model by MultilayerPerceptronClassifier. 

It works on sample data from data/mllib/sample_multiclass_classification_data.txt with 4 features, 3 classes and layers [4, 4, 3]. 
But when I try to use other input files with other features and classes (from here for example: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html) 
then I get errors. 

Example: 
Input file aloi (128 features, 1000 classes, layers [128, 128, 1000]): 


with block size = 1: 
ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to Infinity 
ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed 
ERROR LBFGS: Failure again! Giving up and returning. Maybe the objective is just poorly behaved? 


with default block size = 128: 
 java.lang.ArrayIndexOutOfBoundsException 
  at java.lang.System.arraycopy(Native Method) 
  at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:629) 
  at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:628) 
   at scala.collection.immutable.List.foreach(List.scala:381) 
   at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3.apply(Layer.scala:628) 
   at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3.apply(Layer.scala:624) 



Even if I modify sample_multiclass_classification_data.txt file (rename all 4-th features to 5-th) and run with layers [5, 5, 3] then I also get the same errors as for file above. 


So to resume: 
I can't run training with default block size and with more than 4 features. 
If I set  block size to 1 then some actions are happened but I get errors from LBFGS. 
It is reproducible with Spark 1.5.2 and from master branch on github (from 4-th July). 

Did somebody already met with such behavior? 
Is there bug in MultilayerPerceptronClassifier or I use it incorrectly? 

Thanks.",
Retag RDD to tallSkinnyQR of RowMatrix,SPARK-16372,12986684,http://spark.apache.org,Major,,,,"The following Java code because of type erasing:

{code}
JavaRDD<Vector> rows = jsc.parallelize(...);
RowMatrix mat = new RowMatrix(rows.rdd());
QRDecomposition<RowMatrix, Matrix> result = mat.tallSkinnyQR(true);
{code}

We should use retag to restore the type to prevent the following exception:

{code}
java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lorg.apache.spark.mllib.linalg.Vector;
{code}",
tallSkinnyQR of RowMatrix should aware of empty partition,SPARK-16369,12986673,http://spark.apache.org,Minor,job,,,"tallSkinnyQR of RowMatrix should aware of empty partition, which could cause exception from Breeze qr decomposition.

See the [archived dev mail|https://mail-archives.apache.org/mod_mbox/spark-dev/201510.mbox/%3CCAF7ADNrycvPL3qX-VZJhq4OYmiUUhoscut_tkOm63Cm18iK1tQ@mail.gmail.com%3E] for more details.",
RowMatrıx Covariance,SPARK-16156,12982104,http://spark.apache.org,Major,performance,,,Spark doesn't provide a good solution of covariance for RowMatrix with large columns. This can be fixed with using efficient stable computations and approximating to the true mean.    ,Spark MLLIB
The SparseVector parser fails checking for valid end parenthesis,SPARK-16035,12980362,http://spark.apache.org,Minor,,,,"Running
                      SparseVector.parse(' (4, [0,1 ],[ 4.0,5.0] ')
will not raise an exception as expected, although it parses it as if there was an end parenthesis.

This can be fixed by replacing

                      if start == -1:
                               raise ValueError(""Tuple should end with ')'"")

with
                     if end == -1:
                               raise ValueError(""Tuple should end with ')'"")

Please see posted PR",
SVMWithSGD requires LabeledPoint of Regression,SPARK-15986,12979736,http://spark.apache.org,Major,data structure,,,"SVMWithSGD uses the LabelledPoint in mllib-regression however a classification routine that extends ml's Predictor class uses LabelledPoint of ml.feature. So basically any rotutine, for ex:  a classifier in ml pipeline cannot uses SVMWithSGD. This consistency problem can be solved by removing LabelledPoint in regression. Two different labelled points (one uses linalg.Vector, and other ml.linalg.Vector) is also confusing.         ",Spark 2.0 Snapshot
BlockMatrix to IndexedRowMatrix throws an error,SPARK-15922,12978163,http://spark.apache.org,Major,data strcuture,,,"{code}
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.mllib.linalg._

val rows = IndexedRow(0L, new DenseVector(Array(1,2,3))) :: IndexedRow(1L, new DenseVector(Array(1,2,3))):: IndexedRow(2L, new DenseVector(Array(1,2,3))):: Nil
val rdd = sc.parallelize(rows)
val matrix = new IndexedRowMatrix(rdd, 3, 3)
val bmat = matrix.toBlockMatrix

val imat = bmat.toIndexedRowMatrix
imat.rows.collect // this throws an error - Caused by: java.lang.IllegalArgumentException: requirement failed: Vectors must be the same length!
{code}",
Constructing FPGrowth fails when no numPartitions specified in pyspark,SPARK-15750,12975391,http://spark.apache.org,Major,"data partition, parallelism",,,"{code}
>>> model1 = FPGrowth.train(rdd, 0.6)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/jzhang/github/spark-2/python/pyspark/mllib/fpm.py"", line 96, in train
    model = callMLlibFunc(""trainFPGrowthModel"", data, float(minSupport), int(numPartitions))
  File ""/Users/jzhang/github/spark-2/python/pyspark/mllib/common.py"", line 130, in callMLlibFunc
    return callJavaFunc(sc, api, *args)
  File ""/Users/jzhang/github/spark-2/python/pyspark/mllib/common.py"", line 123, in callJavaFunc
    return _java2py(sc, func(*args))
  File ""/Users/jzhang/github/spark-2/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py"", line 933, in __call__
  File ""/Users/jzhang/github/spark-2/python/pyspark/sql/utils.py"", line 79, in deco
    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.IllegalArgumentException: u'requirement failed: Number of partitions must be positive but got -1'
{code}",
"Word2VecSuite ""big model load / save"" caused OOM in maven jenkins builds",SPARK-15740,12975323,http://spark.apache.org,Critical,"mem issue, model",,,"[~andrewor14] noticed some OOM errors caused by ""test big model load / save"" in Word2VecSuite, e.g., https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-master-test-maven-hadoop-2.2/1168/consoleFull. It doesn't show up in the test result because it was OOMed.

I'm going to disable the test first and leave this open for a proper fix.

cc [~tmnd91]",
Replace FileSystem.get(conf) with path.getFileSystem(conf) when removing CheckpointFile in MLlib,SPARK-15664,12974155,http://spark.apache.org,Major,,,,"if sparkContext.set CheckpointDir to another Dir that is not default FileSystem, it will throw exception when removing CheckpointFile in MLlib.",
DecisionTreeClassificationModel can't be saved within in  Pipeline caused by not implement Writable ,SPARK-15497,12972271,http://spark.apache.org,Major,IO,,,"Here is my code
{code}
SQLContext sqlContext = getSQLContext();
		DataFrame data = sqlContext.read().format(""libsvm"").load(""file:///E:/workspace-mars/bigdata/sparkjob/data/mllib/sample_libsvm_data.txt"");
		// Index labels, adding metadata to the label column.
		// Fit on whole dataset to include all labels in index.
		StringIndexerModel labelIndexer = new StringIndexer()
		  .setInputCol(""label"")
		  .setOutputCol(""indexedLabel"")
		  .fit(data);
		// Automatically identify categorical features, and index them.
		VectorIndexerModel featureIndexer = new VectorIndexer()
		  .setInputCol(""features"")
		  .setOutputCol(""indexedFeatures"")
		  .setMaxCategories(4) // features with > 4 distinct values are treated as continuous
		  .fit(data);

		// Split the data into training and test sets (30% held out for testing)
		DataFrame[] splits = data.randomSplit(new double[]{0.7, 0.3});
		DataFrame trainingData = splits[0];
		DataFrame testData = splits[1];

		// Train a DecisionTree model.
		DecisionTreeClassifier dt = new DecisionTreeClassifier()
		  .setLabelCol(""indexedLabel"")
		  .setFeaturesCol(""indexedFeatures"");

		// Convert indexed labels back to original labels.
		IndexToString labelConverter = new IndexToString()
		  .setInputCol(""prediction"")
		  .setOutputCol(""predictedLabel"")
		  .setLabels(labelIndexer.labels());

		// Chain indexers and tree in a Pipeline
		Pipeline pipeline = new Pipeline()
		  .setStages(new PipelineStage[]{labelIndexer, featureIndexer, dt, labelConverter});

		// Train model.  This also runs the indexers.
		PipelineModel model = pipeline.fit(trainingData);
		model.save(""file:///e:/tmpmodel"");
{code}

and here is the exception
{code}
Exception in thread ""main"" java.lang.UnsupportedOperationException: Pipeline write will fail on this Pipeline because it contains a stage which does not implement Writable. Non-Writable stage: dtc_7bdeae1c4fb8 of type class org.apache.spark.ml.classification.DecisionTreeClassificationModel
	at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$validateStages$1.apply(Pipeline.scala:218)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$validateStages$1.apply(Pipeline.scala:215)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.validateStages(Pipeline.scala:215)
	at org.apache.spark.ml.PipelineModel$PipelineModelWriter.<init>(Pipeline.scala:325)
	at org.apache.spark.ml.PipelineModel.write(Pipeline.scala:309)
	at org.apache.spark.ml.util.MLWritable$class.save(ReadWrite.scala:131)
	at org.apache.spark.ml.PipelineModel.save(Pipeline.scala:280)
	at com.bjdv.spark.job.Testjob.main(Testjob.java:142)
{code}

sample_libsvm_data.txt is included in the 1.6.1 release tar",
LinearRegressionWithSGD fails on files more than 12Mb data ,SPARK-15403,12971102,http://spark.apache.org,Blocker,memory,,,"I parse my json-like data, passing by DataFrame and SparkSql facilities and then scale one numerical feature and create dummy variables for categorical features. So far from initial 14 keys of my json-like file I get about 200-240 features in the final LabeledPoint. The final data is sparse and every file contains as minimum 50000 of observations. I try to run two types of algorithms on data :  LinearRegressionWithSGD or LassoWithSGD, since the data is sparse and regularization might be required.  For data larger than 11MB  LinearRegressionWithSGD fails with the following error:
{quote} org.apache.spark.SparkException: Job aborted due to stage failure: Task 58 in stage 346.0 failed 1 times, most recent failure: Lost task 58.0 in stage 346.0 (TID 18140, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 179307 ms {quote}

I tried to reproduce this bug with bug in smaller example, and I suppose that something wrong could be with LinearRegressionWithSGD on large sets of data. I notices that while using StandardScaler on preprocessing step and counts on Linear Regression step, collect() method is perform, that can cause the bug. So the possibility to scale Linear regression is questioned, cause, as I far as I understand it, collect() performs on driver and so the sens of scaled calculations is lost. 

{code:scala}
import java.io.{File}

import org.apache.spark.mllib.linalg.{Vectors}
import org.apache.spark.mllib.regression.{LabeledPoint, LassoWithSGD}
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{SQLContext}

import scala.language.postfixOps


object Main2 {

  def main(args: Array[String]): Unit = {


    // Spark configuration is defined for execution on local computer, 4 cores 8Mb Ram
    val conf = new SparkConf()
      .setMaster(s""local[*]"")
      .setAppName(""spark_linear_regression_bug_report"")
      //multiple configurations were tried for driver/executor memories, including default configurations
      .set(""spark.driver.memory"", ""3g"")
      .set(""spark.executor.memory"", ""3g"")
      .set(""spark.executor.heartbeatInterval"", ""30s"")

    // Spark context and SQL context definitions
    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)

    val countFeatures = 500
    val countList = 500000

    val features = sc.broadcast(1 to countFeatures)

    val rdd: RDD[LabeledPoint] = sc.range(1, countList).map { i =>
      LabeledPoint(
        label = i.toDouble,
        features = Vectors.dense(features.value.map(_ => scala.util.Random.nextInt(2).toDouble).toArray)
      )
    }.persist()

    val numIterations = 1000
    val stepSize = 0.3
    val algorithm = new LassoWithSGD() //LassoWithSGD() 
    algorithm.setIntercept(true)
    algorithm.optimizer
      .setNumIterations(numIterations)
      .setStepSize(stepSize)
    val model = algorithm.run(rdd)

  }
}
{code}

the complete Error of the bug :
{quote}  [info] Running Main 
WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
WARN  org.apache.spark.util.Utils - Your hostname, julien-ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.0.49 instead (on interface wlan0)
WARN  org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
INFO  Remoting - Starting remoting
INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.0.49:59897]
INFO  org.spark-project.jetty.server.Server - jetty-8.y.z-SNAPSHOT
INFO  org.spark-project.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
WARN  com.github.fommil.netlib.BLAS - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
WARN  com.github.fommil.netlib.BLAS - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
[Stage 51:===========================================>              (3 + 1) / 4]ERROR org.apache.spark.util.Utils - Uncaught exception in thread driver-heartbeater
java.io.IOException: java.lang.ClassNotFoundException: org.apache.spark.storage.BroadcastBlockId
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1207) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.TaskMetrics.readObject(TaskMetrics.scala:219) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source) ~[na:na]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_91]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_91]
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373) ~[na:1.8.0_91]
	at org.apache.spark.util.Utils$.deserialize(Utils.scala:92) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1$$anonfun$apply$6.apply(Executor.scala:437) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1$$anonfun$apply$6.apply(Executor.scala:427) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at scala.Option.foreach(Option.scala:257) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1.apply(Executor.scala:427) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1.apply(Executor.scala:425) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at scala.collection.Iterator$class.foreach(Iterator.scala:742) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:425) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:470) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:470) [spark-core_2.11-1.6.1.jar:1.6.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_91]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_91]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_91]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_91]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_91]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_91]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
Caused by: java.lang.ClassNotFoundException: org.apache.spark.storage.BroadcastBlockId
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_91]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_91]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_91]
	at java.lang.Class.forName0(Native Method) ~[na:1.8.0_91]
	at java.lang.Class.forName(Class.java:348) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:628) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373) ~[na:1.8.0_91]
	at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[na:na]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_91]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_91]
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:503) ~[na:1.8.0_91]
	at org.apache.spark.executor.TaskMetrics$$anonfun$readObject$1.apply$mcV$sp(TaskMetrics.scala:220) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1204) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	... 32 common frames omitted
WARN  org.apache.spark.HeartbeatReceiver - Removing executor driver with no recent heartbeats: 175339 ms exceeds timeout 120000 ms
ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor driver on localhost: Executor heartbeat timed out after 175339 ms
WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 105.0 (TID 420, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 175339 ms
ERROR org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 105.0 failed 1 times; aborting job
WARN  org.apache.spark.SparkContext - Killing executors is only supported in coarse-grained mode
[error] (run-main-0) org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 105.0 failed 1 times, most recent failure: Lost task 1.0 in stage 105.0 (TID 420, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 175339 ms
[error] Driver stacktrace:
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 105.0 failed 1 times, most recent failure: Lost task 1.0 in stage 105.0 (TID 420, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 175339 ms
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1127)
	at org.apache.spark.mllib.optimization.GradientDescent$.runMiniBatchSGD(GradientDescent.scala:227)
	at org.apache.spark.mllib.optimization.GradientDescent.optimize(GradientDescent.scala:128)
	at org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:308)
	at org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:229)
	at Main$.main(Main.scala:85)
	at Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
[trace] Stack trace suppressed: run last compile:run for the full output.
ERROR org.apache.spark.ContextCleaner - Error in cleaning thread
java.lang.InterruptedException: null
	at java.lang.Object.wait(Native Method) ~[na:1.8.0_91]
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143) ~[na:1.8.0_91]
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:176) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1180) [spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:173) [spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:68) [spark-core_2.11-1.6.1.jar:1.6.1]
ERROR org.apache.spark.util.Utils - uncaught error in thread SparkListenerBus, stopping SparkContext
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998) ~[na:1.8.0_91]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304) ~[na:1.8.0_91]
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312) ~[na:1.8.0_91]
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:66) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1180) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63) [spark-core_2.11-1.6.1.jar:1.6.1]
java.lang.RuntimeException: Nonzero exit code: 1
	at scala.sys.package$.error(package.scala:27)
[trace] Stack trace suppressed: run last compile:run for the full output.
[error] (compile:run) Nonzero exit code: 1 {quote}","Ubuntu 14.04 with 8 Gb Ram, scala 2.11.7 with following memory settings for my project:  JAVA_OPTS=""-Xmx8G -Xms2G"" ."
Wrong equation is used in the method of org.apache.spark.mllib.clustering.KMeans,SPARK-15399,12971043,http://spark.apache.org,Major,,,,"the method is in org.apache.spark.mllib.clustering.KMeans
the equation |a-b|=||a|-|b|| is wrong when a and b are vector. but it is used in the spark-1.6.1.
private[mllib] def findClosest(
      centers: TraversableOnce[VectorWithNorm],
      point: VectorWithNorm): (Int, Double) = {
    var bestDistance = Double.PositiveInfinity
    var bestIndex = 0
    var i = 0
    centers.foreach { center =>
      // Since `\|a - b\| \geq |\|a\| - \|b\||`, we can use this lower bound to avoid unnecessary
      // distance computation.
      var lowerBoundOfSqDist = center.norm - point.norm
      lowerBoundOfSqDist = lowerBoundOfSqDist * lowerBoundOfSqDist
      if (lowerBoundOfSqDist < bestDistance) {
        val distance: Double = fastSquaredDistance(center, point)
        if (distance < bestDistance) {
          bestDistance = distance
          bestIndex = i
        }
      }
      i += 1
    }
    (bestIndex, bestDistance)
  }
the center and the point in the source code are vector. and I suggest the code is that
private[mllib] def findClosest(
      centers: TraversableOnce[VectorWithNorm],
      point: VectorWithNorm): (Int, Double) = {
    var bestDistance = Double.PositiveInfinity
    var bestIndex = 0
    var i = 0
    centers.foreach { center =>
      // distance computation.
      val distance: Double = fastSquaredDistance(center, point)
      if (distance < bestDistance) {
        bestDistance = distance
        bestIndex = i
      }
      i += 1
    }
    (bestIndex, bestDistance)
  }",windows 64bit
Fix and re-enable flaky test: mllib.stat.JavaStatisticsSuite.testCorr,SPARK-15043,12964108,http://spark.apache.org,Critical,,,,"It looks like the {{mllib.stat.JavaStatisticsSuite.testCorr}} test has become flaky:

https://spark-tests.appspot.com/tests/org.apache.spark.mllib.stat.JavaStatisticsSuite/testCorr

The first observed failure was in https://spark-tests.appspot.com/builds/spark-master-test-maven-hadoop-2.6/816

{code}
java.lang.AssertionError: expected:<0.9986422261219262> but was:<0.9986422261219272>
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr(JavaStatisticsSuite.java:75)
{code}

I'm going to ignore this test now, but we need to come back and fix it.",
RankingMetrics.ndcgAt  throw  java.lang.ArrayIndexOutOfBoundsException,SPARK-14886,12962129,http://spark.apache.org,Minor,,,,"{code} 
@Since(""1.2.0"")
  def ndcgAt(k: Int): Double = {
    require(k > 0, ""ranking position k should be positive"")
    predictionAndLabels.map { case (pred, lab) =>
      val labSet = lab.toSet

      if (labSet.nonEmpty) {
        val labSetSize = labSet.size
        val n = math.min(math.max(pred.length, labSetSize), k)
        var maxDcg = 0.0
        var dcg = 0.0
        var i = 0
        while (i < n) {
          val gain = 1.0 / math.log(i + 2)
          if (labSet.contains(pred(i))) {
            dcg += gain
          }
          if (i < labSetSize) {
            maxDcg += gain
          }
          i += 1
        }
        dcg / maxDcg
      } else {
        logWarning(""Empty ground truth set, check input data"")
        0.0
      }
    }.mean()
  }
{code}

""if (labSet.contains(pred(i)))"" will throw ArrayIndexOutOfBoundsException when pred's size less then k.
That meas the true relevant documents has less size then the param k.
just try this with sample_movielens_data.txt
for example set pred.size to 5,labSetSize to 10,k to 20,then the n is 10. pred[10] not exists;
precisionAt is ok just because it has         
val n = math.min(pred.length, k)


",
Error while encoding: java.lang.ClassCastException with LibSVMRelation,SPARK-14843,12961583,http://spark.apache.org,Major,usability,,,"While trying to run some example ML linear regression code, I came across the following. In fact this error occurs when doing {{./bin/run-example ml.LinearRegressionWithElasticNetExample}}.

{code}
scala> import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.regression.LinearRegression

scala> import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.mllib.linalg.Vector

scala> import org.apache.spark.sql.Row
import org.apache.spark.sql.Row

scala> val data = sqlContext.read.format(""libsvm"").load(""data/mllib/sample_linear_regression_data.txt"")
data: org.apache.spark.sql.DataFrame = [label: double, features: vector]

scala> val model = lr.fit(data)
{code}
Stack trace:
{code}
Driver stacktrace:
...
  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1276)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:357)
  at org.apache.spark.rdd.RDD.take(RDD.scala:1250)
  at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1290)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:357)
  at org.apache.spark.rdd.RDD.first(RDD.scala:1289)
  at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:165)
  at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:69)
  at org.apache.spark.ml.Predictor.fit(Predictor.scala:90)
  ... 48 elided
Caused by: java.lang.RuntimeException: Error while encoding: java.lang.ClassCastException: java.lang.Double cannot be cast to org.apache.spark.mllib.linalg.Vector
if (input[0, org.apache.spark.sql.Row].isNullAt) null else newInstance(class org.apache.spark.mllib.linalg.VectorUDT).serialize
:- input[0, org.apache.spark.sql.Row].isNullAt
:  :- input[0, org.apache.spark.sql.Row]
:  +- 0
:- null
+- newInstance(class org.apache.spark.mllib.linalg.VectorUDT).serialize
   :- newInstance(class org.apache.spark.mllib.linalg.VectorUDT)
   +- input[0, org.apache.spark.sql.Row].get
      :- input[0, org.apache.spark.sql.Row]
      +- 0

  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:230)
  at org.apache.spark.ml.source.libsvm.DefaultSource$$anonfun$buildReader$1$$anonfun$8.apply(LibSVMRelation.scala:209)
  at org.apache.spark.ml.source.libsvm.DefaultSource$$anonfun$buildReader$1$$anonfun$8.apply(LibSVMRelation.scala:207)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:90)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegen$$anonfun$7$$anon$1.hasNext(WholeStageCodegen.scala:362)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
  at org.apache.spark.scheduler.Task.run(Task.scala:85)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: java.lang.Double cannot be cast to org.apache.spark.mllib.linalg.Vector
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:227)
  ... 17 more
{code}

The error is triggered by L163 of {{LinearRegression}}:
{code}
    val numFeatures = dataset.select(col($(featuresCol))).limit(1).rdd.map {
      case Row(features: Vector) => features.size
    }.first()
{code}

Using the above example, the following works:
{code}
scala> data.select(""label"").rdd.map { case Row(d: Double) => d }.first
res49: Double = -9.490009878824548
{code}
But this triggers the exception:
{code}
scala> data.select(""features"").rdd.map { case Row(d: Vector) => d }.first
16/04/22 11:25:20 ERROR Executor: Exception in task 0.0 in stage 87.0 (TID 98)
java.lang.RuntimeException: Error while encoding: java.lang.ClassCastException: java.lang.Double cannot be cast to org.apache.spark.mllib.linalg.Vector
...
{code}",
CrossValidatorModel.bestModel does not include hyper-parameters,SPARK-14740,12960186,http://spark.apache.org,Major,parameter,,,"If you tune hyperparameters using a CrossValidator object in PySpark, you may not be able to extract the parameter values of the best model.

{noformat}
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.mllib.linalg import Vectors
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

dataset = sqlContext.createDataFrame(
    [(Vectors.dense([0.0]), 0.0),
     (Vectors.dense([0.4]), 1.0),
     (Vectors.dense([0.5]), 0.0),
     (Vectors.dense([0.6]), 1.0),
     (Vectors.dense([1.0]), 1.0)] * 10,
    [""features"", ""label""])
lr = LogisticRegression()
grid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01, 0.001, 0.0001]).build()
evaluator = BinaryClassificationEvaluator()
cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)
cvModel = cv.fit(dataset)
{noformat}

I can get the regression coefficient out, but I can't get the regularization parameter

{noformat}
In [3]: cvModel.bestModel.coefficients
Out[3]: DenseVector([3.1573])

In [4]: cvModel.bestModel.explainParams()
Out[4]: ''

In [5]: cvModel.bestModel.extractParamMap()
Out[5]: {}

In [15]: cvModel.params
Out[15]: []

In [36]: cvModel.bestModel.params
Out[36]: []
{noformat}

For the original issue raised on StackOverflow please see http://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark

",
Vectors.parse doesn't handle dense vectors of size 0 and sparse vectors with no indices,SPARK-14739,12960167,http://spark.apache.org,Major,data strcuture,,,"DenseVector:

{code}
Vectors.parse(str(Vectors.dense([])))
## ValueError                                Traceback (most recent call last)
## .. 
## ValueError: Unable to parse values from
{code}

SparseVector:

{code}
Vectors.parse(str(Vectors.sparse(5, [], [])))
## ValueError                                Traceback (most recent call last)
##  ... 
## ValueError: Unable to parse indices from .
{code}",
mllib DenseMatrix toArray could use the internal values,SPARK-14697,12959604,http://spark.apache.org,Minor,data formats,,,"When toArray is called from an mllib Densematrix, it calls the function from the superclass Matrix, where it creates a new array (and iterates through all the values).

This is not necessary as values are stored in DenseMatrix, in exactly the format that this function would require. Why not just call that?

",
Update RDD.treeAggregate not to use reduce,SPARK-14408,12956227,http://spark.apache.org,Minor,job,,,"**Issue**
In MLlib, we have assumed that {{RDD.treeAggregate}} allows the {{seqOp}} and {{combOp}} functions to modify and return their first argument, just like {{RDD.aggregate}}.  However, it is not documented that way.

I started to add docs to this effect, but then noticed that {{treeAggregate}} uses {{reduceByKey}} and {{reduce}} in its implementation, neither of which technically allows the seq/combOps to modify and return their first arguments.

**Question**: Is the implementation safe, or does it need to be updated?

**Decision**: Avoid using reduce.  Use fold instead.",
Use treeAggregate instead of reduce in OnlineLDAOptimizer,SPARK-14322,12955140,http://spark.apache.org,Major,"job, performace",,,"OnlineLDAOptimizer uses {{RDD.reduce}} in two places where it could use treeAggregate.  This can cause scalability issues.  This should be an easy fix.

This is also a bug since it modifies the first argument to reduce, so we should use aggregate or treeAggregate.

See this line: [https://github.com/apache/spark/blob/f12f11e578169b47e3f8b18b299948c0670ba585/mllib/src/main/scala/org/apache/spark/mllib/clustering/LDAOptimizer.scala#L452]
and a few lines below it.",
LDA should support disable checkpoint,SPARK-14298,12954987,http://spark.apache.org,Minor,,,,LDA should support disable checkpoint by setting checkpointInterval = -1,
Incorrect use of binarysearch in SparseMatrix,SPARK-14187,12953896,http://spark.apache.org,Minor,,,,"{{SparseMatrix}} use binarySearch to find {{index}}. 

{quote}
binarySearch returns index of the search key, if it is contained in the array within the specified range; otherwise, (-(insertion point) - 1).
{quote}

On the other hand, {{update}} method only compare the returned index with -1 to decide whether it is found or not.",
Add a constructor parameter `regParam` to (Streaming)LinearRegressionWithSGD,SPARK-13686,12947095,http://spark.apache.org,Minor,,,,"`LinearRegressionWithSGD` and `StreamingLinearRegressionWithSGD` does not have `regParam` as their constructor arguments. They just depends on GradientDescent's default reqParam values. 

To be consistent with other algorithms, we had better add them.",
Fix mismatched default values for regParam in LogisticRegression,SPARK-13676,12946958,http://spark.apache.org,Major,model parameter,,,"The default value of regularization parameter for `LogisticRegression` algorithm is different in Scala and Python. We should provide the same value.

{code:title=Scala|borderStyle=solid}
scala> new org.apache.spark.ml.classification.LogisticRegression().getRegParam
res0: Double = 0.0
{code}

{code:title=Python|borderStyle=solid}
>>> from pyspark.ml.classification import LogisticRegression
>>> LogisticRegression().getRegParam()
0.1
{code}",
Use approxQuantile from DataFrame stats in QuantileDiscretizer,SPARK-13600,12945902,http://spark.apache.org,Major,data partition,,,"For consistency and code reuse, QuantileDiscretizer should use approxQuantile to find splits in the data rather than implement it's own method.

Additionally, making this change should remedy a bug where QuantileDiscretizer fails to calculate the correct splits in certain circumstances, resulting in an incorrect number of buckets/bins.

E.g.

val df = sc.parallelize(1.0 to 10.0 by 1.0).map(Tuple1.apply).toDF(""x"")
val discretizer = new QuantileDiscretizer().setInputCol(""x"").setOutputCol(""y"").setNumBuckets(5)
discretizer.fit(df).getSplits

gives:
Array(-Infinity, 2.0, 4.0, 6.0, 8.0, 10.0, Infinity)
which corresponds to 6 buckets (not 5).
",
Fix the wrong parameter in R code comment in AssociationRulesSuite ,SPARK-13506,12944813,http://spark.apache.org,Trivial,model parameter,,,"The following R Snippet in AssociationRulesSuite is wrong:

    /* Verify results using the `R` code:
       transactions = as(sapply(
         list(""r z h k p"",
              ""z y x w v u t s"",
              ""s x o n r"",
              ""x z y m t s q e"",
              ""z"",
              ""x z y r q t p""),
         FUN=function(x) strsplit(x,"" "",fixed=TRUE)),
         ""transactions"")
       ars = apriori(transactions,
                     parameter = list(support = 0.0, confidence = 0.5, target=""rules"", minlen=2))
       arsDF = as(ars, ""data.frame"")
       arsDF$support = arsDF$support * length(transactions)
       names(arsDF)[names(arsDF) == ""support""] = ""freq""
       > nrow(arsDF)
       [1] 23
       > sum(arsDF$confidence == 1)
       [1] 23
     */

The real outputs are:
> nrow(arsDF)
[1] 441838
> sum(arsDF$confidence == 1)
[1] 441592

It is found that the parameters in apriori function were wrong.",
QuantileDiscretizer chooses bad splits on large DataFrames,SPARK-13444,12941270,http://spark.apache.org,Major,job,,,"In certain circumstances, QuantileDiscretizer fails to calculate the correct splits and will instead split data into two bins regardless of the value specified in numBuckets.

For example, supposed dataset.count is 200 million.  And we do

val discretizer = new QuantileDiscretizer().setNumBuckets(10)
  ... set output and input columns ...
val dataWithBins = discretizer.fit(dataset).transform(dataset)

In this case, dataWithBins will have only two distinct bins versus the expected 10.

Problem is in line 113 and 114 of QuantileDiscretizer.scala and can be fixed by changing line 113 like so:
before: val requiredSamples = math.max(numBins * numBins, 10000)
after: val requiredSamples = math.max(numBins * numBins, 10000.0)

",
MLlib LogisticRegressionWithLBFGS swaps L1 and L2 incorrectly ,SPARK-13379,12940370,http://spark.apache.org,Major,,,,"We should correct MLlib LogisticRegressionWithLBFGS regularization map as:
SquaredL2Updater -> ""elasticNetParam = 0.0""
L1Updater -> ""elasticNetParam = 1.0""",
Replace GraphImpl.fromExistingRDDs by Graph,SPARK-13355,12939678,http://spark.apache.org,Major,job,,,`GraphImpl.fromExistingRDDs` expects preprocessed vertex RDD as input. We call it in LDA without validating this requirement. So it might introduce errors. Replacing it by `Gpaph.apply` would be safer and more proper because it is a public API. ,
[ML] Allow setting 'degree' parameter to 1 for PolynomialExpansion,SPARK-13338,12939481,http://spark.apache.org,Major,,,,"PolynomialExpansion has bug in validation of 'degree' parameter.
It does not allow setting degree to 1",
Word2Vec generate infinite distances when numIterations>5,SPARK-13289,12938623,http://spark.apache.org,Major,parallelism,features,,"I recently ran some word2vec experiments on a cluster with 50 executors on some large text dataset but find out that when number of iterations is larger than 5 the distance between words will be all infinite. My code looks like this:

val text = sc.textFile(""/project/NLP/1_biliion_words/train"").map(_.split("" "").toSeq)
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
val word2vec = new Word2Vec().setMinCount(25).setVectorSize(96).setNumPartitions(99).setNumIterations(10).setWindowSize(5)
val model = word2vec.fit(text)
val synonyms = model.findSynonyms(""who"", 40)
for((synonym, cosineSimilarity) <- synonyms) {
  println(s""$synonym $cosineSimilarity"")
}

The results are: 
to Infinity
and Infinity
that Infinity
with Infinity
said Infinity
it Infinity
by Infinity
be Infinity
have Infinity
he Infinity
has Infinity
his Infinity
an Infinity
) Infinity
not Infinity
who Infinity
I Infinity
had Infinity
their Infinity
were Infinity
they Infinity
but Infinity
been Infinity

I tried many different datasets and different words for finding synonyms.","Linux, Scala"
RandomForest is stuck at computing same stage over and over,SPARK-13115,12935706,http://spark.apache.org,Major,parallelism,,,"While running the RandomForest regression, the algorithm keeps computing the same stage and does not proceed any further. I have observed the same stage being computed for more than 11 hours. Attached are some of the captures from Spark WebUI.

Also, the spark event logs for this model run could be fetched from Spark Event Logs (https://s3.amazonaws.com/com.tookitaki.public.logs/spark-event-logs). I am running spark-1.5.2 in the standalone local mode. Also, I wanted to know why any stage is marked skipped? 

Let me know if you would need more information.",
HashTF dosn't count TF correctly,SPARK-13103,12935354,http://spark.apache.org,Major,,,,"I wrote a Python program to calculate frequencies of n-gram sequences with HashTF.
But it generate a strange output. It found more ""一一下嗎"" than ""一一下"".

HashTF gets words' index with hash()
But hashes of some Chinese words are negative.
Ex:
>>> hash('一一下嗎')
-6433835193350070115
>>> hash('一一下')
-5938108283593463272","Ubuntu 14.04
Python 3.4.3"
EMLDAOptimizer deletes dependent checkpoint of DistributedLDAModel,SPARK-13048,12934567,http://spark.apache.org,Major,parallelism,,,"In EMLDAOptimizer, all checkpoints are deleted before returning the DistributedLDAModel.

The most recent checkpoint is still necessary for operations on the DistributedLDAModel under a couple scenarios:
- The graph doesn't fit in memory on the worker nodes (e.g. very large data set).
- Late worker failures that require reading the now-dependent checkpoint.

I ran into this problem running a 10M record LDA model in a memory starved environment. The model consistently failed in either the {{collect at LDAModel.scala:528}} stage (when converting to a LocalLDAModel) or in the {{reduce at LDAModel.scala:563}} stage (when calling ""describeTopics"" on the model). In both cases, a FileNotFoundException is thrown attempting to access a checkpoint file.

I'm not sure what the correct fix is here; it might involve a class signature change. An alternative simple fix is to leave the last checkpoint around and expect the user to clean the checkpoint directory themselves.

{noformat}
java.io.FileNotFoundException: File does not exist: /hdfs/path/to/checkpoints/c8bd2b4e-27dd-47b3-84ec-3ff0bac04587/rdd-635/part-00071
{noformat}

Relevant code is included below.

LDAOptimizer.scala:
{noformat}
  override private[clustering] def getLDAModel(iterationTimes: Array[Double]): LDAModel = {
    require(graph != null, ""graph is null, EMLDAOptimizer not initialized."")
    this.graphCheckpointer.deleteAllCheckpoints()
    // The constructor's default arguments assume gammaShape = 100 to ensure equivalence in
    // LDAModel.toLocal conversion
    new DistributedLDAModel(this.graph, this.globalTopicTotals, this.k, this.vocabSize,
      Vectors.dense(Array.fill(this.k)(this.docConcentration)), this.topicConcentration,
      iterationTimes)
  }
{noformat}

PeriodicCheckpointer.scala

{noformat}
  /**
   * Call this at the end to delete any remaining checkpoint files.
   */
  def deleteAllCheckpoints(): Unit = {
    while (checkpointQueue.nonEmpty) {
      removeCheckpointFile()
    }
  }

  /**
   * Dequeue the oldest checkpointed Dataset, and remove its checkpoint files.
   * This prints a warning but does not fail if the files cannot be removed.
   */
  private def removeCheckpointFile(): Unit = {
    val old = checkpointQueue.dequeue()
    // Since the old checkpoint is not deleted by Spark, we manually delete it.
    val fs = FileSystem.get(sc.hadoopConfiguration)
    getCheckpointFiles(old).foreach { checkpointFile =>
      try {
        fs.delete(new Path(checkpointFile), true)
      } catch {
        case e: Exception =>
          logWarning(""PeriodicCheckpointer could not remove old checkpoint file: "" +
            checkpointFile)
      }
    }
  }
{noformat}",Standalone Spark cluster
Fix pydoc warnings in mllib/regression.py,SPARK-12986,12933926,http://spark.apache.org,Minor,,,,"Got those warnings by running ""make html"" under ""python/docs/"":

{code}
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.LinearRegressionWithSGD:3: ERROR: Unexpected indentation.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.LinearRegressionWithSGD:4: WARNING: Block quote ends without a blank line; unexpected unindent.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.RidgeRegressionWithSGD:3: ERROR: Unexpected indentation.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.RidgeRegressionWithSGD:4: WARNING: Block quote ends without a blank line; unexpected unindent.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.LassoWithSGD:3: ERROR: Unexpected indentation.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.LassoWithSGD:4: WARNING: Block quote ends without a blank line; unexpected unindent.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.IsotonicRegression:7: ERROR: Unexpected indentation.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.IsotonicRegression:12: ERROR: Unexpected indentation.
{code}",
EMLDAOptimizer initialize should return EMLDAOptimizer other than its parent class,SPARK-12952,12933000,http://spark.apache.org,Trivial,,,,"EMLDAOptimizer initialize should return EMLDAOptimizer other than its parent class, like OnlineLDAOptimizer.",
Fix LinearRegression.train for the case when label is constant and fitIntercept=false,SPARK-12732,12928820,http://spark.apache.org,Minor,,,,"If the target variable is constant, then the linear regression must check if the fitIntercept is true or false, and handle these two cases separately.

If the fitIntercept is true, then there is no training needed and we set the intercept equal to the mean of y.

But if the fit intercept is false, then the model should still train.

Currently, LinearRegression handles both cases in the same way. It doesn't train the model and sets the intercept equal to the mean of y. Which, means that it returns a non-zero intercept even when the user forces the regression through the origin.",
ML StopWordsRemover does not protect itself from column name duplication,SPARK-12711,12928629,http://spark.apache.org,Trivial,pipeline building,ml,mllib,"At work we were 'taking a closer look' at ML transformers&estimators and I spotted that anomally.
On first look, resolution looks simple:
Add to StopWordsRemover.transformSchema line (as is done in e.g. PCA.transformSchema, StandardScaler.transformSchema, OneHotEncoder.transformSchema):
{code}
require(!schema.fieldNames.contains($(outputCol)), s""Output column ${$(outputCol)} already exists."")
{code}

Am I correct? Is that a bug?    If yes - I am willing to prepare an appropriate pull request.
Maybe a better idea is to make use of super.transformSchema in StopWordsRemover (and possibly in all other places)?


Links to files at github, mentioned above:
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/StopWordsRemover.scala#L147
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/Transformer.scala#L109-L111
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/StandardScaler.scala#L101-L102
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/PCA.scala#L138-L139
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/OneHotEncoder.scala#L75-L76
",
word2vec trainWordsCount gets overflow,SPARK-12685,12928229,http://spark.apache.org,Minor,memory,,,"the log of word2vec  reports 
trainWordsCount = -785727483
during computation over a large dataset.

I'll also add vocabsize to the log.

Update the priority as it will affects the computation process.
alpha =
learningRate * (1 - numPartitions * wordCount.toDouble / (trainWordsCount + 1))",
"Loading Word2Vec model in pyspark gives ""ValueError: too many values to unpack"" in  findSynonyms",SPARK-12680,12928119,http://spark.apache.org,Major,"model, io",,,"I can train a model with Word2Vec and then persist it with Word2VecModel#save.  If I load the saved model in pyspark (using python 2.7.10), I get the following error (model.transform included to show that other methods work).
{code}
In [3]: from pyspark.mllib.feature import Word2VecModel

In [4]: model = Word2VecModel.load(sc,""word_vec_from_cleaned_query.model"")
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
16/01/06 12:36:11 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
16/01/06 12:36:11 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS

In [5]: model.findSynonyms('white',10)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-6105c004edd9> in <module>()
----> 1 model.findSynonyms('white',10)

/usr/local/Cellar/apache-spark/1.5.2/libexec/python/pyspark/mllib/feature.pyc in findSynonyms(self, word, num)
    448         if not isinstance(word, basestring):
    449             word = _convert_to_vector(word)
--> 450         words, similarity = self.call(""findSynonyms"", word, num)
    451         return zip(words, similarity)
    452 

ValueError: too many values to unpack

In [6]: model.transform('white')
Out[6]: DenseVector([-0.0213, 0.2292, -0.2012, 0.107, -0.1475, 0.0578, 0.0731, -0.098, -0.1528, 0.1077, 0.0158, -0.0155, -0.1487, 0.0343, 0.2244, 0.0447, 0.2362, -0.1767, 0.064, -0.0148, -0.1291, -0.0171, -0.0642, -0.0754, 0.0417, 0.1547, 0.2745, -0.1178, -0.2895, -0.1314, 0.1023, -0.11, 0.0142, 0.0156, 0.1102, 0.0785, -0.0981, 0.0504, -0.0627, -0.0773, 0.0023, 0.1826, 0.1759, -0.1581, 0.3913, 0.0829, 0.0728, 0.1478, -0.0123, -0.1745, 0.2762, 0.0312, 0.138, 0.0786, -0.0546, 0.5123, 0.237, -0.0241, 0.1594, -0.0645, -0.0425, 0.1265, 0.0305, -0.3164, 0.0601, 0.0565, 0.0066, -0.0818, -0.384, -0.1513, 0.0775, -0.2278, -0.1478, -0.0659, -0.0778, 0.3194, -0.1931, -0.2991, 0.1629, 0.1018, -0.0603, 0.1091, -0.0334, -0.0513, 0.1067, 0.1273, 0.1187, 0.0461, 0.0407, 0.0515, 0.0958, 0.0498, -0.1561, 0.1726, -0.006, -0.0262, -0.0106, 0.1623, 0.1477, -0.0509])

In [7]: 
{code}

I think that this is a pyspark-specific error, since I can load the trained model in the scala spark-shell and use findSynonyms:
{code}
scala> import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}

scala> val model = Word2VecModel.load(sc,""word_vec_from_cleaned_query.model"")
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
16/01/06 14:17:14 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
16/01/06 14:17:14 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
model: org.apache.spark.mllib.feature.Word2VecModel = org.apache.spark.mllib.feature.Word2VecModel@2e7da886

scala> model.findSynonyms(""white"",10)
res0: Array[(String, Double)] = Array((stylish,0.8347662648041679), (shirt,0.7721922530954246), (stripe,0.7311193884955149), (striped,0.7033047124091971), (buttons,0.6891310548525095), (womens,0.671437501511924), (zaful,0.6659281321485323), (dorateymur,0.6654344754707424), (womenns,0.6637001786899768), (long,0.6573707323598634))

scala> 
{code}",
mllib deprecation messages mention non-existent version 1.7.0,SPARK-12651,12927322,http://spark.apache.org,Trivial,,,,"Might be a problem in 1.6 also?

{code}
  @Since(""1.4.0"")
  @deprecated(""Support for runs is deprecated. This param will have no effect in 1.7.0."", ""1.6.0"")
  def getRuns: Int = runs
{code}",
Fix minor issues found by Findbugs,SPARK-12489,12923848,http://spark.apache.org,Minor,,,,"Just used FindBugs to scan the codes and fixed some real issues:

1. Close `java.sql.Statement`
2. Fix incorrect `asInstanceOf`.
3. Remove unnecessary `synchronized` and `ReentrantLock`.",
