Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Cloners),Outward issue link (Container),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Required),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Date of First Response),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Shepherd),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Sprint,Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Testcase included),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Incorrect handling of default values when deserializing python wrappers of scala transformers,SPARK-23244,13134140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Duplicate,,tomas.nykodym,tomas.nykodym,26/Jan/18 23:18,21/Mar/18 06:14,15/Aug/18 23:02,21/Mar/18 06:11,2.2.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,1,,,,,"Default values are not handled properly when serializing/deserializing python trasnformers which are wrappers around scala objects. It looks like that after deserialization the default values which were based on uid do not get properly restored and values which were not set are set to their (original) default values.

Here's a simple code example using Bucketizer:

{code:python}
>>> from pyspark.ml.feature import Bucketizer
>>> a = Bucketizer() 
>>> a.save(""bucketizer0"")
>>> b = load(""bucketizer0"") 
>>> a._defaultParamMap[a.outputCol]
u'Bucketizer_440bb49206c148989db7__output'
>>> b._defaultParamMap[b.outputCol]
u'Bucketizer_41cf9afbc559ca2bfc9a__output'
>>> a.isSet(a.outputCol)
False 
>>> b.isSet(b.outputCol)
True
>>> a.getOutputCol()
u'Bucketizer_440bb49206c148989db7__output'
>>> b.getOutputCol()
u'Bucketizer_440bb49206c148989db7__output'
{code}",,bryanc,hyukjin.kwon,mgaido,tomas.nykodym,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23455,SPARK-21685,SPARK-23234,SPARK-23234,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2018-02-08 10:46:58.469,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 21 06:14:54 UTC 2018,,,,,0|i3pfnb:,9223372036854775807,,,,,,,,,,,,,"08/Feb/18 10:46;mgaido;maybe we can close this as a duplicate of SPARK-23234. Anyway, IMHO this shows that we do need SPARK-23234.

cc [~mlnick] [~podongfeng] [~bryanc] [~srowen]","08/Feb/18 17:34;tomas.nykodym;I might be wrong but I don't think this is a duplicate. There is an overlap between the two in that the default values are set as real values, but:
1) I am not sure if it is in the same context. I looked at the PR addressing SPARK-23234 and the changes were in a an unrelated method to my problem. 
2) I don't see how SPARK-23234 addresses how default values based on uid are set after deserialization of JavaTransformer.

","08/Feb/18 18:07;tomas.nykodym;I might be wrong but I don't think this is a duplicate. There is an overlap between the two in that the default values are set as real values, but:
1) I am not sure if it is in the same context. I looked at the PR addressing SPARK-23234 and the changes were in a an unrelated method to my problem. 
2) I don't see how SPARK-23234 addresses how default values based on uid are set after deserialization of JavaTransformer.

","08/Feb/18 19:43;mgaido;The change is related because your problem is caused by the python api setting (wrongly) all the values (default and not default) as real values. So the model is persisted with all the default values set as they were actually set by the user. That PR is avoiding the default values being actually set, so the persisted model will treat them all as defaults and the newly loaded model will be right.

If you have more questions feel free to ask. And feel free to try my patch on your own to check whether your problem is solved or not and to provide feedbacks on it if you want.",08/Feb/18 20:07;bryanc;This is the same issue as SPARK-21685 caused by pyspark not differentiating from defaults and set params when transferring values to Java. I think we can close this and SPARK-23234 also.,19/Mar/18 07:32;hyukjin.kwon;Shall we resolve all duplicates for now? Looks a bit confusing.,"21/Mar/18 06:10;bryanc;I looked into this and it is a little bit different because with save/load, params are only transferred from Java to Python.  So the actual problem is in Scala:
{code:java}
scala> import org.apache.spark.ml.feature.Bucketizer
import org.apache.spark.ml.feature.Bucketizer

scala> val a = new Bucketizer()
a: org.apache.spark.ml.feature.Bucketizer = bucketizer_30c66d09db18

scala> a.isSet(a.outputCol)
res2: Boolean = false

scala> a.save(""bucketizer0"")

scala> val b = Bucketizer.load(""bucketizer0"")
b: org.apache.spark.ml.feature.Bucketizer = bucketizer_30c66d09db18

scala> b.isSet(b.outputCol)
res4: Boolean = true{code}
It seems this is being worked on in SPARK-23455, so I'll still close this as a duplicate","21/Mar/18 06:14;bryanc;Just to clarify, the PySpark save/load is just a wrapper making the same calls in Java, so that will fix the root cause of the issue",,,,,,,,,,,,,,,,
ImageSchema.readImages incorrectly sets alpha channel to 255 for four-channel images,SPARK-23205,13133510,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Siddharth Murching,Siddharth Murching,Siddharth Murching,24/Jan/18 22:21,26/Jan/18 00:16,15/Aug/18 23:03,26/Jan/18 00:16,2.3.0,,,,,,,,,,,,,,,2.3.0,,,,,,ML,MLlib,,,,0,,,,,"When parsing raw image data in ImageSchema.decode(), we use a [java.awt.Color constructor|https://docs.oracle.com/javase/7/docs/api/java/awt/Color.html#Color(int)] that sets alpha = 255, even for four-channel images.

See the offending line here: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/image/ImageSchema.scala#L172

A fix is to simply update the line to: 

val color = new Color(img.getRGB(w, h), nChannels == 4)


instead of

val color = new Color(img.getRGB(w, h))",,apachespark,Siddharth Murching,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2018-01-24 22:41:04.329,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 26 00:16:14 UTC 2018,,,,,0|i3pbrr:,9223372036854775807,,,,,,,,,,,,,24/Jan/18 22:40;Siddharth Murching;I'm working on a PR to address this issue if that's alright :),"24/Jan/18 22:41;apachespark;User 'smurching' has created a pull request for this issue:
https://github.com/apache/spark/pull/20389","26/Jan/18 00:16;srowen;Issue resolved by pull request 20389
[https://github.com/apache/spark/pull/20389]",,,,,,,,,,,,,,,,,,,,,
Invalid guard condition in org.apache.spark.ml.classification.Classifier,SPARK-23152,13131983,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tovbinm,tovbinm,tovbinm,18/Jan/18 19:29,24/Jan/18 18:15,15/Aug/18 23:03,24/Jan/18 18:15,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,2.1.2,2.1.3,2.3.0,2.3.1,,,,,,,2.4.0,,,,,,ML,MLlib,,,,0,easyfix,,,,"When fitting a classifier that extends ""org.apache.spark.ml.classification.Classifier"" (NaiveBayes, DecisionTreeClassifier, RandomForestClassifier) a misleading NullPointerException is thrown.

Steps to reproduce: 
{code:java}
val data = spark.createDataset(Seq.empty[(Double, org.apache.spark.ml.linalg.Vector)])
new DecisionTreeClassifier().setLabelCol(""_1"").setFeaturesCol(""_2"").fit(data)
{code}
 The error: 
{code:java}
java.lang.NullPointerException: Value at index 0 is null

at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:472)
at org.apache.spark.sql.Row$class.getDouble(Row.scala:248)
at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:165)
at org.apache.spark.ml.classification.Classifier.getNumClasses(Classifier.scala:115)
at org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:102)
at org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:45)
at org.apache.spark.ml.Predictor.fit(Predictor.scala:118){code}
  

The problem happens due to an incorrect guard condition in function getNumClasses at org.apache.spark.ml.classification.Classifier:106
{code:java}
val maxLabelRow: Array[Row] = dataset.select(max($(labelCol))).take(1)
if (maxLabelRow.isEmpty) {
  throw new SparkException(""ML algorithm was given empty dataset."")
}
{code}
When the input data is empty the result ""maxLabelRow"" array is not. Instead it contains a single Row(null) element.

 

Proposed solution: the condition can be modified to verify that.
{code:java}
if (maxLabelRow.isEmpty || maxLabelRow(0).get(0) == null) {
  throw new SparkException(""ML algorithm was given empty dataset."")
}
{code}
 

 ",,apachespark,tovbinm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2018-01-18 20:46:04.296,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 24 18:15:11 UTC 2018,,,,,0|i3p2xr:,9223372036854775807,,,,,,,,,,,,,"18/Jan/18 20:46;apachespark;User 'tovbinm' has created a pull request for this issue:
https://github.com/apache/spark/pull/20321","24/Jan/18 18:15;srowen;Issue resolved by pull request 20321
[https://github.com/apache/spark/pull/20321]",,,,,,,,,,,,,,,,,,,,,,
"Fix ChiSqSelectorModel, GaussianMixtureModel save implementation for Row order issues",SPARK-22905,13127308,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WeichenXu123,WeichenXu123,WeichenXu123,27/Dec/17 04:38,13/Aug/18 20:05,15/Aug/18 23:03,29/Dec/17 01:33,2.2.1,,,,,,,,,,,,,,,2.3.0,,,,,,MLlib,,,,,0,,,,,"Currently, in `ChiSqSelectorModel`, save:
{code}
spark.createDataFrame(dataArray).repartition(1).write...
{code}
The default partition number used by createDataFrame is ""defaultParallelism"",
Current RoundRobinPartitioning won't guarantee the ""repartition"" generating the same order result with local array. We need fix it.",,apachespark,josephkb,podongfeng,WeichenXu123,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-12-27 04:42:04.182,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 13 20:05:06 UTC 2018,,,,,0|i3ob1b:,9223372036854775807,josephkb,,,,,,,,,,,,"27/Dec/17 04:42;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/20088",29/Dec/17 01:33;josephkb;Resolved by https://github.com/apache/spark/pull/20088,"29/Dec/17 02:08;podongfeng;Many other models are saved in the same way {{sparkSession.createDataFrame(...).repartition(1).write.parquet}}, are they needed to be fixed?","29/Dec/17 03:08;WeichenXu123;[~podongfeng] Some of them only including one row to save so there's no bug, some case including row-number column and when reading it will sort to get stable order. But I am not sure I miss some cases, it will great if you help check.","29/Dec/17 06:06;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/20113","29/Dec/17 06:07;podongfeng;[~WeichenXu123] I made a check and found that same issue exists in {{GaussianMixtureModel}}, otherwise looks fine.",29/Dec/17 18:08;josephkb;Merged https://github.com/apache/spark/pull/20113 to fix GaussianMixtureModel issue too.,"13/Aug/18 20:05;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/22079",,,,,,,,,,,,,,,,
Chi Square selector not recognizing field in Data frame,SPARK-22295,13110073,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,cheburakshu,cheburakshu,17/Oct/17 18:18,19/Oct/17 15:24,15/Aug/18 23:03,17/Oct/17 18:40,2.1.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"ChiSquare selector is not recognizing the field 'class' which is present in the data frame while fitting the model. I am using PIMA Indians diabetes dataset of UCI. The complete code and output is below for reference. But, when some rows of the input file is created as a dataframe manually, it will work. Couldn't understand the pattern here.

Kindly help.

{code:python}
from pyspark.ml.feature import VectorAssembler, ChiSqSelector
import sys

file_name='data/pima-indians-diabetes.data'

df=spark.read.format(""csv"").option(""inferSchema"",""true"").option(""header"",""true"").load(file_name).cache()

df.show(1)
assembler = VectorAssembler(inputCols=['preg', ' plas', ' pres', ' skin', ' test', ' mass', ' pedi', ' age'],outputCol=""features"")
df=assembler.transform(df)
df.show(1)
try:
    css=ChiSqSelector(numTopFeatures=5, featuresCol=""features"",
                          outputCol=""selected"", labelCol='class').fit(df)
except:
    print(sys.exc_info())
{code}

Output:

+----+-----+-----+-----+-----+-----+-----+----+------+
|preg| plas| pres| skin| test| mass| pedi| age| class|
+----+-----+-----+-----+-----+-----+-----+----+------+
|   6|  148|   72|   35|    0| 33.6|0.627|  50|     1|
+----+-----+-----+-----+-----+-----+-----+----+------+
only showing top 1 row

+----+-----+-----+-----+-----+-----+-----+----+------+--------------------+
|preg| plas| pres| skin| test| mass| pedi| age| class|            features|
+----+-----+-----+-----+-----+-----+-----+----+------+--------------------+
|   6|  148|   72|   35|    0| 33.6|0.627|  50|     1|[6.0,148.0,72.0,3...|
+----+-----+-----+-----+-----+-----+-----+----+------+--------------------+
only showing top 1 row

(<class 'pyspark.sql.utils.IllegalArgumentException'>, IllegalArgumentException('Field ""class"" does not exist.', 'org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\t at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\t at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\t at scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\t at org.apache.spark.sql.types.StructType.apply(StructType.scala:263)\n\t at org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:71)\n\t at org.apache.spark.ml.feature.ChiSqSelector.transformSchema(ChiSqSelector.scala:183)\n\t at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\t at org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:159)\n\t at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t at java.lang.reflect.Method.invoke(Method.java:498)\n\t at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\t at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\t at py4j.Gateway.invoke(Gateway.java:280)\n\t at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\t at py4j.commands.CallCommand.execute(CallCommand.java:79)\n\t at py4j.GatewayConnection.run(GatewayConnection.java:214)\n\t at java.lang.Thread.run(Thread.java:745)'), <traceback object at 0x0B743BC0>)


*The below code works fine:
*
{code:python}
from pyspark.ml.feature import VectorAssembler, ChiSqSelector
import sys

file_name='data/pima-indians-diabetes.data'

#df=spark.read.format(""csv"").option(""inferSchema"",""true"").option(""header"",""true"").load(file_name).cache()

# Just pasted a few rows from the input file and created a data frome. This will work, but not the frame picked up from the file
df = spark.createDataFrame([
[6,148,72,35,0,33.6,0.627,50,1],
[1,85,66,29,0,26.6,0.351,31,0],
[8,183,64,0,0,23.3,0.672,32,1],
], ['preg', ' plas', ' pres', ' skin', ' test', ' mass', ' pedi', ' age', ""class""])


df.show(1)
assembler = VectorAssembler(inputCols=['preg', ' plas', ' pres', ' skin', ' test', ' mass', ' pedi', ' age'],outputCol=""features"")
df=assembler.transform(df)
df.show(1)
try:
    css=ChiSqSelector(numTopFeatures=5, featuresCol=""features"",
                          outputCol=""selected"", labelCol=""class"").fit(df)
except:
    print(sys.exc_info())

print(css.selectedFeatures)

{code}

Output:

+----+-----+-----+-----+-----+-----+-----+----+-----+
|preg| plas| pres| skin| test| mass| pedi| age|class|
+----+-----+-----+-----+-----+-----+-----+----+-----+
|   6|  148|   72|   35|    0| 33.6|0.627|  50|    1|
+----+-----+-----+-----+-----+-----+-----+----+-----+
only showing top 1 row

+----+-----+-----+-----+-----+-----+-----+----+-----+--------------------+
|preg| plas| pres| skin| test| mass| pedi| age|class|            features|
+----+-----+-----+-----+-----+-----+-----+----+-----+--------------------+
|   6|  148|   72|   35|    0| 33.6|0.627|  50|    1|[6.0,148.0,72.0,3...|
+----+-----+-----+-----+-----+-----+-----+----+-----+--------------------+
only showing top 1 row

[0, 1, 2, 3, 5]",,cheburakshu,peng.meng@intel.com,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22277,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-10-18 01:58:36.059,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 19 15:24:00 UTC 2017,,,,,0|i3ldg7:,9223372036854775807,,,,,,,,,,,,,"18/Oct/17 01:58;peng.meng@intel.com;Hi [~cheburakshu] ， thanks for reporting this bug and helpful code. 
This is caused by similar problem but not the same thing as SPARK-22277. 
The reason is when transform a dataframe, the field/attribute is not correctly set.

Maybe there are some other similar bugs in the code, we can solve them separately, or solve them together.   

[~yanboliang] [~mlnick] [~srowen]","18/Oct/17 11:24;peng.meng@intel.com;Please use  labelCol="" class"", not  labelCol=""class"".","18/Oct/17 11:26;peng.meng@intel.com;Why there is a space for the column names?  ' plas', ' pres', ' skin', ' test', ' mass', ' pedi', ' age', ","19/Oct/17 15:23;cheburakshu;Yes. I realized that and marked my ticket as ""Invalid""


","19/Oct/17 15:24;cheburakshu;But, I had one problem with Chi Square on this data set. I will not choose
BMI (mass in dataset) as a prominent feature. I don't know why.


",,,,,,,,,,,,,,,,,,,
"CrossValidator's training and testing set with different set of labels, resulting in encoder transform error",SPARK-22034,13102730,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Later,,schwannden,schwannden,16/Sep/17 02:19,26/Sep/17 00:00,15/Aug/18 23:01,26/Sep/17 00:00,2.2.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"Let's say we have a VectorIndexer with maxCategories set to 13, and training set has a column containing month label.

In CrossValidator, dataframe is split into training and testing set automatically. If could happen that training set happens to lack month 2 (could happen by chance, or happen quite frequently if we have unbalanced label).

When training set is being trained within the cross validator, the pipeline is fitted with the training set only, resulting in a partial key map in VectorIndexer. When this pipeline is used to transform the predict set, VectorIndexer will throw  a ""key not found"" error.

Making CrossValidator also an estimator thus can be connected to a whole pipeline is a cool idea, but bug like this occurs, and is not expected.

The solution, I am guessing, would be to check each stage in the pipeline, and when we see encoder type stage, we fit the stage model with the complete dataset.","Ubuntu 16.04
Scala 2.11
Spark 2.2.0",bryanc,schwannden,WeichenXu123,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-09-21 13:15:41.348,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 25 23:59:27 UTC 2017,,,,,0|i3k5e7:,9223372036854775807,,,,,,,,,,,,,"21/Sep/17 13:15;WeichenXu123;Do you mean a pipeline including stage VectorIndexer + stage CrossValidator ?
Can you post a minimal program which can reproduce the bug ?","25/Sep/17 23:16;bryanc;You would normally fit the VectorIndexer on the entire dataset and then put the resulting transformer in the pipeline for cross validation.  This is not a bug unless I'm mistaken.

For example: https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala#L52","25/Sep/17 23:59;schwannden;Okay I see. I guess intuitively I thought I could put everything into a pipeline. This is not a bug, still it would be cool if we could have an intelligent pipeline with cross validator that uses the entire dataset to fit the stage when we see encoder or vector indexer.",,,,,,,,,,,,,,,,,,,,,
MultivariateOnlineSummarizer.variance generate negative result,SPARK-21818,13096988,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WeichenXu123,WeichenXu123,WeichenXu123,23/Aug/17 10:51,28/Aug/17 07:00,15/Aug/18 23:03,28/Aug/17 06:41,2.2.0,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,,ML,MLlib,,,,0,,,,,"Because of numerical error, MultivariateOnlineSummarizer.variance is possible to generate negative variance.
This is a serious bug because many algos in MLLib use stddev computed from sqrt(variance),
it will generate NaN and crash the whole algorithm.

we can reproduce this bug use the following code:
{code}
    val summarizer1 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.7)
    val summarizer2 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.4)
    val summarizer3 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.5)
    val summarizer4 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.4)

    val summarizer = summarizer1
      .merge(summarizer2)
      .merge(summarizer3)
      .merge(summarizer4)

    println(summarizer.variance(0))
{code}",,facai,WeichenXu123,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-08-28 06:41:58.851,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 06:41:58 UTC 2017,,,,,0|i3j6o7:,9223372036854775807,,,,,,,,,,,,,"28/Aug/17 06:41;srowen;Issue resolved by pull request 19029
[https://github.com/apache/spark/pull/19029]",,,,,,,,,,,,,,,,,,,,,,,
KMeans performance regression (5-6x slowdown) in Spark 2.2,SPARK-21799,13096504,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,Siddharth Murching,Siddharth Murching,21/Aug/17 17:57,11/Sep/17 21:05,15/Aug/18 23:03,11/Sep/17 21:05,2.2.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"I've been running KMeans performance tests using [spark-sql-perf|https://github.com/databricks/spark-sql-perf/] and have noticed a regression (slowdowns of 5-6x) when running tests on large datasets in Spark 2.2 vs 2.1.

The test params are:
* Cluster: 510 GB RAM, 16 workers
* Data: 1000000 examples, 10000 features

After talking to [~josephkb], the issue seems related to the changes in [SPARK-18356|https://issues.apache.org/jira/browse/SPARK-18356] introduced in [this PR|https://github.com/apache/spark/pull/16295].

It seems `df.cache()` doesn't set the storageLevel of `df.rdd`, so `handlePersistence` is true even when KMeans is run on a cached DataFrame. This unnecessarily causes another copy of the input dataset to be persisted.

As of Spark 2.1 ([JIRA link|https://issues.apache.org/jira/browse/SPARK-16063]) `df.storageLevel` returns the correct result after calling `df.cache()`, so I'd suggest replacing instances of `df.rdd.getStorageLevel` with df.storageLevel` in MLlib algorithms (the same pattern shows up in LogisticRegression, LinearRegression, and others). I've verified this behavior in [this notebook|https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5211178207246023/950505630032626/7788830288800223/latest.html]

",,apachespark,josephkb,kiszk,mlnick,Siddharth Murching,viirya,WeichenXu123,zahili,,,,,,,,,,,,,,,,,,,,,,SPARK-18608,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-08-22 02:19:52.706,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 11 21:05:12 UTC 2017,,,,,0|i3j3q7:,9223372036854775807,,,,,,,,,,,,,"22/Aug/17 02:19;viirya;Hmm, I go to check ML KMeans codes where I don't find {{cache}} on input dataset. So you mean you call {{df.cache}} before fitting the dataset?

In the KMeans, actually it selects the {{featuresCol}} column as {{RDD[OldVector]}} and calls {{persis}} on the rdd if necessary. So I think the behavior is correct. It doesn't actually work with the dataset.",22/Aug/17 02:24;viirya;So I think the problem is you shouldn't do {{df.cache}} before fitting on it?,22/Aug/17 10:51;mlnick;Refer to SPARK-18608 and SPARK-19422. There is some work on it. Haven't looked at it for a while but I recall that it was a little more complex than initially expected. ,"23/Aug/17 14:19;zahili;[~Siddharth Murching], sorry about that,
I think that the best solution is to verify ""cache"" on dataframe instead of rdd. and i think that there are some contributors working on it, i can give a hand to resolve this issue if you want.
","24/Aug/17 09:16;WeichenXu123;[~Siddharth Murching]
+1 This will cause double cache.","24/Aug/17 09:17;WeichenXu123;[~Siddharth Murching]
Already have another jira & PR, take a look!","24/Aug/17 15:34;zahili;[~WeichenXu123], df.rdd.getStorageLevel return none even if df is cached.
The solution is to check cache parameter on df.
When i create PR SPARK-18356 , we didn't have df.storageLevel.
But now, there are some people working on this solution.",25/Aug/17 07:30;WeichenXu123;[~zahili] hmm..You're right. We are hard to get the precise cache info otherwise checking the df parents.,"02/Sep/17 04:33;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/19107","11/Sep/17 21:05;josephkb;Now that I've caught up on these, this is just a special case of the bug in [SPARK-18608].  I'm going to close this issue and ask for a PR like [~podongfeng]'s original PR be sent for [SPARK-18608], fixing the use of {{dataset.rdd.getStorageLevel}}.  I think we should fix it for all algorithms, not just K-Means.",,,,,,,,,,,,,,
Correct validateAndTransformSchema in GaussianMixture and AFTSurvivalRegression,SPARK-21793,13096276,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sharp-pixel,sharp-pixel,srowen,20/Aug/17 10:04,20/Aug/17 10:07,15/Aug/18 23:03,20/Aug/17 10:06,2.1.1,2.2.0,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,,MLlib,,,,,0,,,,,"From user sharp-pixel:

The line SchemaUtils.appendColumn(schema, $(predictionCol), IntegerType) did not modify the variable schema, hence only the last line had any effect. A temporary variable is used to correctly append the two columns predictionCol and probabilityCol.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-08-20 10:06:35.012,,false,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 20 10:06:35 UTC 2017,,,,,0|i1u552:i,9223372036854775807,,,,,,,,,,,,,"20/Aug/17 10:06;srowen;Issue resolved by pull request 18980
[https://github.com/apache/spark/pull/18980]",,,,,,,,,,,,,,,,,,,,,,,
Fix broken redirect in collaborative filtering docs to databricks training repo,SPARK-21615,13091944,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,iblong2iyush,iblong2iyush,iblong2iyush,02/Aug/17 17:06,03/Aug/17 08:59,15/Aug/18 23:03,03/Aug/17 08:59,2.2.0,,,,,,,,,,,,,,,2.3.0,,,,,,Documentation,MLlib,,,,0,documentation,easyfix,,,"* Current [MLlib Collaborative Filtering tutorial|https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html] points to broken links to old databricks website.
* Databricks moved all their content to [git repo|https://github.com/databricks/spark-training]
* Two links needs to be fixed:
** [training exercises|https://databricks-training.s3.amazonaws.com/index.html]
** [personalized movie recommendation with spark.mllib|https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html]",,iblong2iyush,,,,,,,,,,,,,,,,,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-08-02 17:27:38.511,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 03 08:59:02 UTC 2017,,,,,0|i3ic07:,9223372036854775807,,,,,,,,,,,,,"02/Aug/17 17:27;srowen;Sure, just open a PR. If this is old content and third-party content, we might rethink linking to it in the project docs. But I don't feel strongly about it.","02/Aug/17 17:43;iblong2iyush;[~srowen] Hi, I opened a [PR|https://github.com/apache/spark/pull/18821] but not able to assign the issue to myself. ","02/Aug/17 19:23;hyukjin.kwon;User 'singhay' has created a pull request for this issue:
https://github.com/apache/spark/pull/18821","03/Aug/17 01:11;iblong2iyush;[~hyukjin.kwon] That's me, is there any way to link my git account with this ?","03/Aug/17 01:16;hyukjin.kwon;It should have been done automatically but there looks a problem for now. Please refer http://apache-spark-developers-list.1001551.n3.nabble.com/Some-PRs-not-automatically-linked-to-JIRAs-td22067.html. Usually, I guess there is nothing to do with git and JIRA.","03/Aug/17 08:59;srowen;Issue resolved by pull request 18821
[https://github.com/apache/spark/pull/18821]",,,,,,,,,,,,,,,,,,
Multinomial logistic regression model fitting fails with ERROR StrongWolfeLineSearch,SPARK-21614,13091914,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,jseppanen,jseppanen,02/Aug/17 15:03,02/Aug/17 15:07,15/Aug/18 23:03,02/Aug/17 15:07,2.1.0,,,,,,,,,,,,,,,,,,,,,ML,MLlib,,,,0,,,,,"Fitting a simple multinomial logistic regression model fails with:

17/08/02 14:53:23 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:23 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed

Example repro case:


from pyspark.sql import Row
from pyspark.ml.linalg import Vectors
from pyspark.ml.classification import LogisticRegression

df = spark.createDataFrame([
    Row(label=0, features=Vectors.dense([0.0, 0.0, 0.0, 0.0, 2.9, 0.0, 2.9, 2.9, 0.0, 0.0, 0.0, 0.0, 2.9, 0.0, 0.0, 2.9, 2.9, 0.0, 0.0, 0.0, 2.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9, 2.9, 2.9, 2.9, 0.0, 2.9, 0.0, 0.0, 2.9, 0.0, 2.9, 2.9, 0.0, 2.9, 2.9, 0.0, 0.0, 2.9, 2.9, 2.9, 0.0, 2.9, 2.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9, 2.9, 0.0, 2.9, 2.9, 2.9, 2.9, 0.0, 0.0, 2.9, 2.9, 0.0, 0.0, 0.0, 2.9, 2.9, 0.0, 2.9, 2.9, 2.9, 0.0, 0.0, 2.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),
    Row(label=1, features=Vectors.dense([1.8, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 1.9, 1.9, 1.9, 1.9, 1.9, 1.8, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 0.0, 1.9, 0.0, 1.9, 1.9, 0.0, 1.9, 1.9, 0.0, 1.8, 1.9, 0.0, 0.0, 1.9, 0.0, 1.9, 0.0, 1.9, 1.9, 1.9, 1.9, 0.0, 1.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9, 1.9, 1.9, 0.0, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 0.0, 0.0, 0.0, 1.9, 0.0, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 1.9, 0.0, 0.0, 1.9, 1.9, 0.0, 0.0, 0.0])),
    Row(label=2, features=Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 1.6, 1.6, 1.6, 1.6, 0.0, 1.6, 1.6, 1.6, 1.6, 1.6, 0.0, 1.6, 1.6, 0.0, 1.6, 1.6, 1.6, 0.0, 1.6, 1.6, 0.0, 1.6, 1.6, 1.6, 1.6, 0.0, 1.6, 1.6, 1.6, 1.6, 0.0, 0.0, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 0.0, 1.6, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 0.0, 0.0, 0.0, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 0.0, 1.6, 1.6, 0.0, 1.6, 1.6, 0.0, 0.0, 1.6])),
    Row(label=3, features=Vectors.dense([0.0, 0.0, 0.0, 1.4, 0.7, 1.1, 0.0, 0.0, 0.7, 0.0, 1.4, 1.1, 1.4, 0.0, 1.1, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1, 0.0, 0.0, 0.0, 0.7, 0.0, 0.7, 0.0, 0.0, 1.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 1.4, 0.0, 0.0, 0.0, 0.0, 1.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 1.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4, 0.7, 0.0, 0.0, 0.0, 0.0, 1.4, 0.7, 1.9, 0.0, 0.0, 0.0, 1.1, 1.4, 0.0, 0.0, 0.0, 2.1, 2.1, 2.1, 1.6, 1.9, 1.8, 2.1, 2.1, 1.9, 2.1, 1.6, 1.8, 1.6, 2.1, 1.8, 1.9, 2.1, 2.1, 2.1, 2.1, 2.1, 1.8, 2.1, 0.0, 1.9, 2.1, 0.0, 2.1, 2.1, 0.0, 1.8, 2.1, 2.1, 0.0, 1.9, 0.0, 1.9, 0.0, 2.1, 1.8, 2.1, 2.1, 0.0, 2.1, 0.0, 0.0, 1.9, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1, 2.1, 2.1, 1.9, 2.1, 2.1, 2.1, 1.8, 2.1, 2.1, 2.1, 0.0, 0.0, 0.0, 1.6, 1.9, 2.1, 2.1, 2.1, 2.1, 1.6, 1.9, 0.7, 2.1, 0.0, 0.0, 1.8, 1.6, 0.0, 0.0, 2.1])),
    Row(label=4, features=Vectors.dense([0.0, 2.8, 2.8, 0.0, 0.0, 2.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8, 0.0, 2.8, 0.0, 2.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8, 0.0, 0.0, 2.8, 2.8, 0.0, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 0.0, 2.8, 0.0, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 0.0, 0.0, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 0.0, 2.8, 0.0, 0.0, 0.0, 2.8, 0.0, 0.0, 2.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 0.0, 0.0, 0.0, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 0.0, 2.8, 2.8, 0.0, 0.0, 2.8])),
    Row(label=5, features=Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0, 2.6, 0.0, 0.0, 0.0, 1.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4, 1.1, 2.6, 0.0, 0.0, 0.0, 0.0, 2.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1, 0.0, 2.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4, 0.0, 0.0, 0.0, 1.1, 2.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6, 2.6, 2.6, 2.6, 2.6, 0.0, 2.6, 2.6, 2.6, 2.4, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 1.1, 2.6, 2.6, 0.0, 2.6, 2.6, 1.1, 2.4, 0.0, 2.6, 0.0, 2.6, 0.0, 1.1, 0.0, 0.0, 0.0, 2.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.4, 2.6, 0.0, 2.6, 0.0, 0.0, 0.0, 2.6, 2.6, 2.6, 1.1, 2.6, 2.6, 2.6, 2.4, 0.0, 2.6, 0.0, 0.0, 2.6, 2.6, 0.0, 0.0, 2.6])),
])

lr = LogisticRegression()
model = lr.fit(df)

'''
17/08/02 14:53:21 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:23 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:23 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:23 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:23 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed
17/08/02 14:53:23 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:24 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:25 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:25 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:25 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to NaN
17/08/02 14:53:25 ERROR LBFGS: Failure again! Giving up and returning. Maybe the objective is just poorly behaved?
'''


I'm on Amazon EMR release emr-5.3.1 running Spark 2.1.0
",,jseppanen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21523,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,9223372036854775807,,,2017-08-02 15:03:13.0,,,,,0|i3ibtj:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debug issues for SparkML(scala.Predef$.any2ArrowAssoc),SPARK-21557,13090715,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Invalid,,Prabirbh,Prabirbh,28/Jul/17 08:16,28/Jul/17 08:24,15/Aug/18 23:03,28/Jul/17 08:24,2.1.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"Hi Team,

Can you please see the below error ,when I am running the below program using below mvn config.Kindly tell me which version I have to use.I am running this program from eclipse neon.

Error at Runtime:- 

Exception in thread ""main"" java.lang.NoSuchMethodError: scala.Predef$.any2ArrowAssoc(Ljava/lang/Object;)Ljava/lang/Object;
	at org.apache.spark.sql.SparkSession$Builder.config(SparkSession.scala:750)
	at org.apache.spark.sql.SparkSession$Builder.appName(SparkSession.scala:741)
	at com.MLTest.JavaPCAExample.main(JavaPCAExample.java:20)

Java Class:-

package com.MLTest;

import org.apache.spark.sql.SparkSession;

import java.util.Arrays;
import java.util.List;
import org.apache.spark.ml.feature.PCA;
import org.apache.spark.ml.feature.PCAModel;
import org.apache.spark.ml.linalg.VectorUDT;
import org.apache.spark.ml.linalg.Vectors;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.types.Metadata;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

public class JavaPCAExample {
	public static void main(String[] args) {
		SparkSession spark = SparkSession.builder().appName(""JavaPCAExample3"")
				.config(""spark.some.config.option"", ""some-value"").getOrCreate();

		List<Row> data = Arrays.asList(
				RowFactory.create(Vectors.sparse(5, new int[] { 1, 3 }, new double[] { 1.0, 7.0 })),
				RowFactory.create(Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0)),
				RowFactory.create(Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)));

		StructType schema = new StructType(
				new StructField[] { new StructField(""features"", new VectorUDT(), false, Metadata.empty()), });

		Dataset<Row> df = spark.createDataFrame(data, schema);

		PCAModel pca = new PCA().setInputCol(""features"").setOutputCol(""pcaFeatures"").setK(3).fit(df);

		Dataset<Row> result = pca.transform(df).select(""pcaFeatures"");
		result.show(false);

		spark.stop();
	}
}

pom.xml:-

<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
	xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
	<modelVersion>4.0.0</modelVersion>
	<groupId>SparkMLTest</groupId>
	<artifactId>SparkMLTest</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<build>
		<sourceDirectory>src</sourceDirectory>
		<plugins>
			<plugin>
				<artifactId>maven-compiler-plugin</artifactId>
				<version>3.5.1</version>
				<configuration>
					<source>1.8</source>
					<target>1.8</target>
				</configuration>
			</plugin>
		</plugins>
	</build>
	<dependencies>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_2.10</artifactId>
			<version>2.2.0</version>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-streaming_2.10</artifactId>
			<version>2.1.1</version>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-mllib_2.10</artifactId>
			<version>2.1.1</version>
			<scope>provided</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-sql_2.10</artifactId>
			<version>2.1.1</version>
		</dependency>
		<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-library</artifactId>
			<version>2.13.0-M1</version>
		</dependency>
		<dependency>
			<groupId>org.apache.parquet</groupId>
			<artifactId>parquet-hadoop-bundle</artifactId>
			<version>1.8.1</version>
		</dependency>
	</dependencies>
</project>



",,Prabirbh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-07-28 08:24:53.558,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 28 08:24:53 UTC 2017,,,,,0|i3i4gn:,9223372036854775807,,,,,,,,,,,,,28/Jul/17 08:24;srowen;JIRA isn't for questions - stackoverflow maybe. ,,,,,,,,,,,,,,,,,,,,,,,
Fix bug of strong wolfe linesearch `init` parameter lose effectiveness,SPARK-21523,13089729,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,WeichenXu123,WeichenXu123,WeichenXu123,24/Jul/17 22:56,14/Sep/17 22:10,15/Aug/18 23:03,09/Aug/17 06:49,2.2.0,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,,MLlib,,,,,0,,,,,"We need merge this breeze bugfix into spark because it influence a series of algos in MLlib which use LBFGS.
https://github.com/scalanlp/breeze/pull/651",,ibobak,josephkb,mgrover,mlnick,WeichenXu123,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-07-24 23:00:17.28,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 14 22:10:27 UTC 2017,,,,,0|i3hye7:,9223372036854775807,,,,,,2.2.1,,,,,,,24/Jul/17 22:57;WeichenXu123;I will work on this once the breeze cut a new version for this bugfix.,24/Jul/17 23:00;josephkb;CC [~yanboliang] [~yuhaoyan] [~dbtsai] making a few people aware of this,28/Jul/17 18:15;srowen;I think this is fairly critical actually -- would like to get this into a 2.2.1 release.,14/Sep/17 22:10;ibobak;For us it is also critical issue:  we faced with the same problem.,,,,,,,,,,,,,,,,,,,,
java.lang.NullPointerException for certain methods in classes of MLlib,SPARK-21331,13085334,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,anirband,anirband,06/Jul/17 21:51,07/Jul/17 18:00,15/Aug/18 23:03,07/Jul/17 18:00,2.1.0,,,,,,,,,,,,,,,,,,,,,Build,MLlib,,,,1,,,,,"I am trying to run the following code using sbt package and sbt run. I am getting a runtime error that seems to be a bug since the same code works great on spark-shell with Scala. The error occurs when executing the computeSVD line. If this line is commented out, the program works fine. I am having similar issues for other methods for classes in MLlib as well. This looks like a bug to me.",Spark running locally on OSX at spark://127.0.0.1:7077.,anirband,avradip,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-07-07 05:20:39.488,"Code:

package com.sracr.test

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.SingularValueDecomposition
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.distributed.RowMatrix

object Test { 


       def main(args: Array[ String ]) {

         val conf = new SparkConf().setAppName(""MySparkApp"").setMaster(""spark://127.0.0.1:7077"")

         var ctx : SparkContext = new SparkContext(conf)

         ctx.addJar(""target/scala-2.11/spark-test_2.11-1.0.jar"")

         println(""Hello, This is a start!"")

         val data = List(
           Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),
           Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),
           Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0))

         val dataRDD = ctx.parallelize(data)

         val mat: RowMatrix = new RowMatrix(dataRDD)

         println(mat.numCols())

         val svd: SingularValueDecomposition[RowMatrix, Matrix] = mat.computeSVD(2, computeU = true)

         println(mat.numRows())

         println(""It Works!!!!!"")

         ctx.stop()


       }

}

Error:

$sbt run
[warn] Executing in batch mode.
[warn]   For better performance, hit [ENTER] to switch to interactive mode, or
[warn]   consider launching sbt without any commands, or explicitly passing 'shell'
[info] Loading project definition from /Users/ani.das/Projects/spark/MySpark/spark-test/project
[info] Set current project to spark test (in build file:/Users/ani.das/Projects/spark/MySpark/spark-test/)
[info] Running com.sracr.test.Test 
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/07/06 10:51:18 INFO SparkContext: Running Spark version 2.1.0
17/07/06 10:51:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/06 10:51:18 WARN Utils: Your hostname, 127.0.0.1 resolves to a loopback address: 127.0.0.1; using 105.145.28.172 instead (on interface en0)
17/07/06 10:51:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/06 10:51:18 INFO SecurityManager: Changing view acls to: ani.das
17/07/06 10:51:18 INFO SecurityManager: Changing modify acls to: ani.das
17/07/06 10:51:18 INFO SecurityManager: Changing view acls groups to: 
17/07/06 10:51:18 INFO SecurityManager: Changing modify acls groups to: 
17/07/06 10:51:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ani.das); groups with view permissions: Set(); users  with modify permissions: Set(ani.das); groups with modify permissions: Set()
17/07/06 10:51:19 INFO Utils: Successfully started service 'sparkDriver' on port 65196.
17/07/06 10:51:19 INFO SparkEnv: Registering MapOutputTracker
17/07/06 10:51:19 INFO SparkEnv: Registering BlockManagerMaster
17/07/06 10:51:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/07/06 10:51:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/07/06 10:51:19 INFO DiskBlockManager: Created local directory at /private/var/folders/4c/s3nt_0s96z57zfxc3dlyq3swjl4fq9/T/blockmgr-5317c7b1-ff8d-463a-8405-dd7c3f12074a
17/07/06 10:51:19 INFO MemoryStore: MemoryStore started with capacity 408.9 MB
17/07/06 10:51:19 INFO SparkEnv: Registering OutputCommitCoordinator
17/07/06 10:51:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/07/06 10:51:19 INFO Utils: Successfully started service 'SparkUI' on port 4041.
17/07/06 10:51:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://105.145.28.172:4041
17/07/06 10:51:19 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://127.0.0.1:7077...
17/07/06 10:51:19 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:7077 after 31 ms (0 ms spent in bootstraps)
17/07/06 10:51:19 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170706105119-0007
17/07/06 10:51:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170706105119-0007/0 on worker-20170706103441-105.145.28.172-64630 (105.145.28.172:64630) with 8 cores
17/07/06 10:51:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20170706105119-0007/0 on hostPort 105.145.28.172:64630 with 8 cores, 1024.0 MB RAM
17/07/06 10:51:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170706105119-0007/0 is now RUNNING
17/07/06 10:51:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65198.
17/07/06 10:51:19 INFO NettyBlockTransferService: Server created on 105.145.28.172:65198
17/07/06 10:51:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/07/06 10:51:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 105.145.28.172, 65198, None)
17/07/06 10:51:19 INFO BlockManagerMasterEndpoint: Registering block manager 105.145.28.172:65198 with 408.9 MB RAM, BlockManagerId(driver, 105.145.28.172, 65198, None)
17/07/06 10:51:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 105.145.28.172, 65198, None)
17/07/06 10:51:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 105.145.28.172, 65198, None)
17/07/06 10:51:19 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/07/06 10:51:19 INFO SparkContext: Added JAR target/scala-2.11/spark-test_2.11-1.0.jar at spark://105.145.28.172:65196/jars/spark-test_2.11-1.0.jar with timestamp 1499363479936
Hello, This is a start!
17/07/06 10:51:20 INFO SparkContext: Starting job: first at RowMatrix.scala:61
17/07/06 10:51:20 INFO DAGScheduler: Got job 0 (first at RowMatrix.scala:61) with 1 output partitions
17/07/06 10:51:20 INFO DAGScheduler: Final stage: ResultStage 0 (first at RowMatrix.scala:61)
17/07/06 10:51:20 INFO DAGScheduler: Parents of final stage: List()
17/07/06 10:51:20 INFO DAGScheduler: Missing parents: List()
17/07/06 10:51:20 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at test.scala:27), which has no missing parents
17/07/06 10:51:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1408.0 B, free 408.9 MB)
17/07/06 10:51:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 958.0 B, free 408.9 MB)
17/07/06 10:51:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 105.145.28.172:65198 (size: 958.0 B, free: 408.9 MB)
17/07/06 10:51:20 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
17/07/06 10:51:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at test.scala:27)
17/07/06 10:51:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/07/06 10:51:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(null) (105.145.28.172:65200) with ID 0
17/07/06 10:51:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 105.145.28.172, executor 0, partition 0, PROCESS_LOCAL, 6215 bytes)
17/07/06 10:51:21 INFO BlockManagerMasterEndpoint: Registering block manager 105.145.28.172:65202 with 366.3 MB RAM, BlockManagerId(0, 105.145.28.172, 65202, None)
17/07/06 10:51:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 105.145.28.172:65202 (size: 958.0 B, free: 366.3 MB)
17/07/06 10:51:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 503 ms on 105.145.28.172 (executor 0) (1/1)
17/07/06 10:51:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/07/06 10:51:22 INFO DAGScheduler: ResultStage 0 (first at RowMatrix.scala:61) finished in 1.712 s
17/07/06 10:51:22 INFO DAGScheduler: Job 0 finished: first at RowMatrix.scala:61, took 1.885817 s
5
17/07/06 10:51:22 INFO SparkContext: Starting job: treeAggregate at RowMatrix.scala:122
17/07/06 10:51:22 INFO DAGScheduler: Got job 1 (treeAggregate at RowMatrix.scala:122) with 2 output partitions
17/07/06 10:51:22 INFO DAGScheduler: Final stage: ResultStage 1 (treeAggregate at RowMatrix.scala:122)
17/07/06 10:51:22 INFO DAGScheduler: Parents of final stage: List()
17/07/06 10:51:22 INFO DAGScheduler: Missing parents: List()
17/07/06 10:51:22 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[1] at treeAggregate at RowMatrix.scala:122), which has no missing parents
17/07/06 10:51:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.6 KB, free 408.9 MB)
17/07/06 10:51:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1560.0 B, free 408.9 MB)
17/07/06 10:51:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 105.145.28.172:65198 (size: 1560.0 B, free: 408.9 MB)
17/07/06 10:51:22 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996
17/07/06 10:51:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[1] at treeAggregate at RowMatrix.scala:122)
17/07/06 10:51:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
17/07/06 10:51:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 105.145.28.172, executor 0, partition 0, PROCESS_LOCAL, 6223 bytes)
17/07/06 10:51:22 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, 105.145.28.172, executor 0, partition 1, PROCESS_LOCAL, 6258 bytes)
17/07/06 10:51:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 105.145.28.172:65202 (size: 1560.0 B, free: 366.3 MB)
17/07/06 10:51:22 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, 105.145.28.172, executor 0): java.lang.NullPointerException
    at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2028)
    at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2028)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:99)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

17/07/06 10:51:22 INFO TaskSetManager: Lost task 1.0 in stage 1.0 (TID 2) on 105.145.28.172, executor 0: java.lang.NullPointerException (null) [duplicate 1]
17/07/06 10:51:22 INFO TaskSetManager: Starting task 1.1 in stage 1.0 (TID 3, 105.145.28.172, executor 0, partition 1, PROCESS_LOCAL, 6258 bytes)
17/07/06 10:51:22 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 4, 105.145.28.172, executor 0, partition 0, PROCESS_LOCAL, 6223 bytes)
17/07/06 10:51:22 INFO TaskSetManager: Lost task 1.1 in stage 1.0 (TID 3) on 105.145.28.172, executor 0: java.lang.NullPointerException (null) [duplicate 2]
17/07/06 10:51:22 INFO TaskSetManager: Starting task 1.2 in stage 1.0 (TID 5, 105.145.28.172, executor 0, partition 1, PROCESS_LOCAL, 6258 bytes)
17/07/06 10:51:22 INFO TaskSetManager: Lost task 0.1 in stage 1.0 (TID 4) on 105.145.28.172, executor 0: java.lang.NullPointerException (null) [duplicate 3]
17/07/06 10:51:22 INFO TaskSetManager: Starting task 0.2 in stage 1.0 (TID 6, 105.145.28.172, executor 0, partition 0, PROCESS_LOCAL, 6223 bytes)
17/07/06 10:51:22 INFO TaskSetManager: Lost task 0.2 in stage 1.0 (TID 6) on 105.145.28.172, executor 0: java.lang.NullPointerException (null) [duplicate 4]
17/07/06 10:51:22 INFO TaskSetManager: Starting task 0.3 in stage 1.0 (TID 7, 105.145.28.172, executor 0, partition 0, PROCESS_LOCAL, 6223 bytes)
17/07/06 10:51:22 INFO TaskSetManager: Lost task 1.2 in stage 1.0 (TID 5) on 105.145.28.172, executor 0: java.lang.NullPointerException (null) [duplicate 5]
17/07/06 10:51:22 INFO TaskSetManager: Starting task 1.3 in stage 1.0 (TID 8, 105.145.28.172, executor 0, partition 1, PROCESS_LOCAL, 6258 bytes)
17/07/06 10:51:22 INFO TaskSetManager: Lost task 0.3 in stage 1.0 (TID 7) on 105.145.28.172, executor 0: java.lang.NullPointerException (null) [duplicate 6]
17/07/06 10:51:22 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job
17/07/06 10:51:22 INFO TaskSetManager: Lost task 1.3 in stage 1.0 (TID 8) on 105.145.28.172, executor 0: java.lang.NullPointerException (null) [duplicate 7]
17/07/06 10:51:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/07/06 10:51:22 INFO TaskSchedulerImpl: Cancelling stage 1
17/07/06 10:51:22 INFO DAGScheduler: ResultStage 1 (treeAggregate at RowMatrix.scala:122) failed in 0.391 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, 105.145.28.172, executor 0): java.lang.NullPointerException
    at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2028)
    at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2028)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:99)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
17/07/06 10:51:22 INFO DAGScheduler: Job 1 failed: treeAggregate at RowMatrix.scala:122, took 0.403468 s
[error] (run-main-0) org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, 105.145.28.172, executor 0): java.lang.NullPointerException
[error]     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2028)
[error]     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2028)
[error]     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
[error]     at org.apache.spark.scheduler.Task.run(Task.scala:99)
[error]     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
[error]     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[error]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[error]     at java.lang.Thread.run(Thread.java:745)
[error] 
[error] Driver stacktrace:
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7, 105.145.28.172, executor 0): java.lang.NullPointerException
    at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2028)
    at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2028)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:99)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)
    at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
    at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
    at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1150)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
    at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1127)
    at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeGramianMatrix(RowMatrix.scala:122)
    at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:259)
    at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:194)
    at com.sracr.test.Test$.main(test.scala:33)
    at com.sracr.test.Test.main(test.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
Caused by: java.lang.NullPointerException
    at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2028)
    at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2028)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:99)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[trace] Stack trace suppressed: run last compile:run for the full output.
17/07/06 10:51:22 ERROR ContextCleaner: Error in cleaning thread
java.lang.InterruptedException
    at java.lang.Object.wait(Native Method)
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
    at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:175)
    at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1245)
    at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:172)
    at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:67)
17/07/06 10:51:22 ERROR Utils: uncaught error in thread SparkListenerBus, stopping SparkContext
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
    at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:80)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:78)
    at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1245)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:77)
17/07/06 10:51:22 ERROR Utils: throw uncaught fatal error in thread SparkListenerBus
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
    at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:80)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:78)
    at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1245)
    at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:77)
17/07/06 10:51:22 INFO SparkUI: Stopped Spark web UI at http://105.145.28.172:4041
17/07/06 10:51:22 INFO StandaloneSchedulerBackend: Shutting down all executors
java.lang.RuntimeException: Nonzero exit code: 1
    at scala.sys.package$.error(package.scala:27)
[trace] Stack trace suppressed: run last compile:run for the full output.
[error] (compile:run) Nonzero exit code: 1
[error] Total time: 5 s, completed Jul 6, 2017 10:51:22 AM
17/07/06 10:51:22 INFO DiskBlockManager: Shutdown hook called
17/07/06 10:51:22 INFO ShutdownHookManager: Shutdown hook called
17/07/06 10:51:22 INFO ShutdownHookManager: Deleting directory /private/var/folders/4c/s3nt_0s96z57zfxc3dlyq3swjl4fq9/T/spark-4bc93888-b930-4621-9cc8-b44fc3b6bd9e
17/07/06 10:51:22 INFO ShutdownHookManager: Deleting directory /private/var/folders/4c/s3nt_0s96z57zfxc3dlyq3swjl4fq9/T/spark-4bc93888-b930-4621-9cc8-b44fc3b6bd9e/userFiles-c5140f8e-6edb-4c89-b113-02d406019feb",false,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 07 18:00:11 UTC 2017,,,,,0|i3h7r3:,9223372036854775807,,,,,,,,,,,,,"07/Jul/17 05:20;facai;Hi, I run the code in description on mac, spark-2.1.1. It works out successfully without any exception. 

{code}
scala> val svd: SingularValueDecomposition[RowMatrix, Matrix] = mat.computeSVD(2, computeU = true)
17/07/07 13:17:12 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
17/07/07 13:17:12 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
17/07/07 13:17:12 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
17/07/07 13:17:12 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
svd: org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix] =
SingularValueDecomposition(org.apache.spark.mllib.linalg.distributed.RowMatrix@5a4f7911,[13.029275535600473,5.368578733451684],-0.31278534337232633   0.3116713569157832
-0.029801450130953977  -0.17133211263608739
-0.12207248163673157   0.15256470925290191
-0.7184789931874109    -0.6809628499946365
-0.6084105917199364    0.6217072292290715    )
{code}","07/Jul/17 05:29;facai;[~anirband] How about using this code?

{code}
val conf = new SparkConf().setAppName(""MySparkApp"").setMaster(""local"") 
{code}",07/Jul/17 05:32;anirband;The code works fine on spark-shell as you saw. But the problem happens when you use sbt run to run it (after using sbt package). I think the issue is with sbt and MLlib. ,"07/Jul/17 06:48;srowen;I don't see evidence that the NPE is related to computeSVD. The line the error occurs on in your stack trace is blank in Spark 2.1.0, so are you sure that's the version you're using? whatever the problem is we could probably find a better error, but it's not clear what the message. Are you sure this isn't a problem with how you built it locally, like did you do a clean build? Ideally, reproduce this vs a released version to rule that out.","07/Jul/17 17:59;anirband;Thanks for pointing it out. I feel stupid now, it was a version issue. I was using mllib 2.1.0 from maven but spark was 2.1.1. ",07/Jul/17 18:00;anirband;This was a version issue. I was using mllib 2.1.0 with spark 2.1.1.,,,,,,,,,,,,,,,,,,
OOM with 2 handred million vertex when mitrx multply,SPARK-21118,13080331,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,icesxrun,icesxrun,16/Jun/17 06:54,16/Jun/17 11:04,15/Aug/18 23:03,16/Jun/17 09:46,2.1.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"i have 2 matrix each one is 200milions*200milions.
i want to multiply them ,but run out with oom .
finally i find the oom appear at blockmatrix.simulateMultiply . there is a collect action at this method. 
 the collect will return all dataset that is too large to driver so the driver will go to oom.

class BlockMatrix @Since(""1.3.0"") (
private[distributed] def simulateMultiply(
      other: BlockMatrix,
      partitioner: GridPartitioner): (BlockDestinations, BlockDestinations) = {
    val leftMatrix = {color:red}blockInfo.keys.collect() {color}// blockInfo should already be cached
    val rightMatrix = other.blocks.keys.collect()
......

","on yarn cluster,19 node.30GB per node",icesxrun,LorenzB,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-06-16 08:02:46.102,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 16 11:04:20 UTC 2017,,,,,0|i3gcyn:,9223372036854775807,,,,,,,,,,,,,"16/Jun/17 08:02;srowen;It's not clear what you're describing, but it sounds like something to try again on the mailing list, not JIRA",16/Jun/17 08:41;icesxrun;is there other way to do big matirx multiply,"16/Jun/17 08:46;icesxrun;i add some  information to desciption,please have a look ","16/Jun/17 09:11;LorenzB;The first point would be to use a subject title without typos. I mean ""handred"" and ""mitrx multply""? Come on - how can others search for similar problems?!

Secondly, you're using `collect()` for both matrices. That's more or less breaking the idea of Spark, since you're collecting everything to the driver in memory. Of course, this will mean for large data to an OOM. You should read more about the principles of Spark I guess.","16/Jun/17 09:46;srowen;[~icesxrun] do not reopen this. This doesn't describe a problem with Spark, but a question about usage.","16/Jun/17 11:04;icesxrun;ok,thanks
",,,,,,,,,,,,,,,,,,
Should we create a constructor for LabelsPoint which is using ml.linalg.Vectors?,SPARK-20893,13075071,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,freemail165,freemail165,26/May/17 04:51,26/May/17 05:18,15/Aug/18 23:03,26/May/17 05:13,2.1.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"From Spark2.0 org.apache.spark.mllib.linalg.Vectors;
was deprecated by org.apache.spark.ml.linalg.Vectors;

But for LabelsPoint, 
https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/mllib/regression/LabeledPoint.html

the only constructor as LabeledPoint(double label, Vector features) 
is using mllib.linalg.Vectors, should we create another version which is using 
ml.linalg.Vectors?",,freemail165,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-05-26 05:13:49.973,,false,,,,,,,,,,,,,9223372036854775807,,,Fri May 26 05:18:33 UTC 2017,,,,,0|i3fi5b:,9223372036854775807,,,,,,,,,,,,,"26/May/17 05:13;srowen;Please read http://spark.apache.org/contributing.html first. This isn't a bug, it's a question, and that should go to the mailing list.

LabeledPoint is used with .mllib code, so I think the answer is 'no'.","26/May/17 05:18;freemail165;I agree that this is not a bug, just didn't find the right type :)",,,,,,,,,,,,,,,,,,,,,,
LogisticRegressionModel throws TypeError,SPARK-20862,13074377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bago.amirbekian,bago.amirbekian,bago.amirbekian,24/May/17 01:12,24/May/17 15:02,15/Aug/18 23:03,24/May/17 15:02,2.1.1,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,,MLlib,PySpark,,,,0,,,,,"LogisticRegressionModel throws a TypeError using python3 and numpy 1.12.1:

**********************************************************************
File ""/Users/bago/repos/spark/python/pyspark/mllib/classification.py"", line 155, in __main__.LogisticRegressionModel
Failed example:
    mcm = LogisticRegressionWithLBFGS.train(data, iterations=10, numClasses=3)
Exception raised:
    Traceback (most recent call last):
      File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/doctest.py"", line 1330, in __run
        compileflags, 1), test.globs)
      File ""<doctest __main__.LogisticRegressionModel[23]>"", line 1, in <module>
        mcm = LogisticRegressionWithLBFGS.train(data, iterations=10, numClasses=3)
      File ""/Users/bago/repos/spark/python/pyspark/mllib/classification.py"", line 398, in train
        return _regression_train_wrapper(train, LogisticRegressionModel, data, initialWeights)
      File ""/Users/bago/repos/spark/python/pyspark/mllib/regression.py"", line 216, in _regression_train_wrapper
        return modelClass(weights, intercept, numFeatures, numClasses)
      File ""/Users/bago/repos/spark/python/pyspark/mllib/classification.py"", line 176, in __init__
        self._dataWithBiasSize)
    TypeError: 'float' object cannot be interpreted as an integer
",,apachespark,bago.amirbekian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-05-24 01:21:03.306,,false,,,,,,,,,,,,,9223372036854775807,,,Wed May 24 01:21:03 UTC 2017,,,,,0|i3fdv3:,9223372036854775807,,,,,,,,,,,,,"24/May/17 01:21;apachespark;User 'MrBago' has created a pull request for this issue:
https://github.com/apache/spark/pull/18081",,,,,,,,,,,,,,,,,,,,,,,
ALS with implicit feedback ignores negative values,SPARK-20790,13072912,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davidjeis,davidjeis,davidjeis,17/May/17 19:38,02/Jun/17 14:01,15/Aug/18 23:03,31/May/17 12:53,1.3.1,1.4.0,1.4.1,1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,1.6.2,1.6.3,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,2.2.0,,,,,,ML,MLlib,,,,0,,,,,"The refactorization that was done in https://github.com/apache/spark/pull/5314/files introduced a bug, whereby for implicit feedback negative ratings just get ignored. Prior to that commit they were not ignored, but the absolute value was used as the confidence and the  preference was set to 0. The preservation of comments and absolute value indicate that this was unintentional.",,apachespark,davidjeis,shubhamc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-05-17 20:24:06.208,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 02 14:01:03 UTC 2017,,,,,0|i3f4tj:,9223372036854775807,,,,,,,,,,,,,"17/May/17 20:24;srowen;This needs more detail. Where is the logic problem, what's a reproduction, etc?","17/May/17 20:40;davidjeis;See https://github.com/apache/spark/pull/5314/files#diff-be65dd1d6adc53138156641b610fcadaR1118
ls.add is only called if rating > 0, which is not what was done prior to the commit, nor does it represent what is conveyed in the comment on the previous line.
To reproduce, you can run ALS with implicit feedback with a ratings matrix with any negative values and it will be identical to running with the same ratings matrix with the negative values zeroed out.","17/May/17 21:07;apachespark;User 'davideis' has created a pull request for this issue:
https://github.com/apache/spark/pull/18022","31/May/17 12:53;srowen;Issue resolved by pull request 18022
[https://github.com/apache/spark/pull/18022]","02/Jun/17 14:01;apachespark;User 'davideis' has created a pull request for this issue:
https://github.com/apache/spark/pull/18188",,,,,,,,,,,,,,,,,,,
mllib.Matrices.fromBreeze may crash when converting from Breeze sparse matrix,SPARK-20687,13070680,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,elghoto,elghoto,elghoto,10/May/17 04:24,22/May/17 09:29,15/Aug/18 23:03,22/May/17 09:28,2.1.1,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,,MLlib,,,,,0,,,,,"Conversion of Breeze sparse matrices to Matrix is broken when matrices are product of certain operations. This problem I think is caused by the update method in Breeze CSCMatrix when they add provisional zeros to the data for efficiency.

This bug is serious and may affect at least BlockMatrix addition and substraction

http://stackoverflow.com/questions/33528555/error-thrown-when-using-blockmatrix-add/43883458#43883458

The following code, reproduces the bug (Check test(""breeze conversion bug""))

https://github.com/ghoto/spark/blob/test-bug/CSCMatrixBreeze/mllib/src/test/scala/org/apache/spark/mllib/linalg/MatricesSuite.scala

{code:title=MatricesSuite.scala|borderStyle=solid}

  test(""breeze conversion bug"") {
    // (2, 0, 0)
    // (2, 0, 0)
    val mat1Brz = Matrices.sparse(2, 3, Array(0, 2, 2, 2), Array(0, 1), Array(2, 2)).asBreeze
    // (2, 1E-15, 1E-15)
    // (2, 1E-15, 1E-15
    val mat2Brz = Matrices.sparse(2, 3, Array(0, 2, 4, 6), Array(0, 0, 0, 1, 1, 1), Array(2, 1E-15, 1E-15, 2, 1E-15, 1E-15)).asBreeze
    // The following shouldn't break
    val t01 = mat1Brz - mat1Brz
    val t02 = mat2Brz - mat2Brz
    val t02Brz = Matrices.fromBreeze(t02)
    val t01Brz = Matrices.fromBreeze(t01)

    val t1Brz = mat1Brz - mat2Brz
    val t2Brz = mat2Brz - mat1Brz
    // The following ones should break
    val t1 = Matrices.fromBreeze(t1Brz)
    val t2 = Matrices.fromBreeze(t2Brz)

  }

{code}",,apachespark,elghoto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-05-10 09:24:55.919,,false,,,,,Patch,,,,,,,,9223372036854775807,,,Mon May 22 09:28:03 UTC 2017,,,,,0|i3er33:,9223372036854775807,,,,,,,,,,,,,"10/May/17 04:28;elghoto;The bug on SPARK-11507 is caused by this conversion, and therefore the problem wasn't patched correctly.","10/May/17 09:24;srowen;This doesn't say what the problem is. What goes wrong?
","10/May/17 16:34;elghoto;When you try to do operations like addition or subtraction between 2 mllib.distributed.BlockMatrices that store in blocks sparse matrices, these are operated using breeze and then converted back to Matrices again. Sometimes this conversion back produces crashes, even though the resulting matrix is valid, because this method in Matrices.fromBreeze doesn't extract correctly the data hold in CSC breeze matrix.

Unfortunately, I'm not able to show some code with block matrices, but I can show you some backtrace. I manually debugged the crashes, and found the culprit, so that's why I posted in the description a quite more simplified snippet that reproduces the error.

The snippet that causes the crash in BlockMatrix lines 374-379

{code:title:BlockMatrix.scala:blockMap}
          } else if (b.isEmpty) {
            new MatrixBlock((blockRowIndex, blockColIndex), a.head)
          } else {
            val result = binMap(a.head.asBreeze, b.head.asBreeze)
            new MatrixBlock((blockRowIndex, blockColIndex), Matrices.fromBreeze(result)) // <--not able to get results
          }
{code}


The trace after the operation between 2 spark block matrices:

{code:text}
Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 34, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: The last value of colPtrs must equal the number of elements. values.length: 28, colPtrs.last: 15
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.mllib.linalg.SparseMatrix.<init>(Matrices.scala:590)
	at org.apache.spark.mllib.linalg.SparseMatrix.<init>(Matrices.scala:618)
	at org.apache.spark.mllib.linalg.Matrices$.fromBreeze(Matrices.scala:995)
	at org.apache.spark.mllib.linalg.distributed.BlockMatrix$$anonfun$10.apply(BlockMatrix.scala:378)
	at org.apache.spark.mllib.linalg.distributed.BlockMatrix$$anonfun$10.apply(BlockMatrix.scala:365)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212)
	at scala.collection.AbstractIterator.fold(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1087)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1087)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2119)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2119)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}
",10/May/17 18:30;elghoto;Proposing a patch in PR https://github.com/apache/spark/pull/17940,"11/May/17 17:21;apachespark;User 'ghoto' has created a pull request for this issue:
https://github.com/apache/spark/pull/17940","22/May/17 09:28;srowen;Issue resolved by pull request 17940
[https://github.com/apache/spark/pull/17940]",,,,,,,,,,,,,,,,,,
result of MLlib KMeans cluster is not stabilize,SPARK-20634,13069863,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Invalid,,ximen120,ximen120,08/May/17 03:17,08/May/17 09:36,15/Aug/18 23:03,08/May/17 08:58,2.0.2,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"1.Get a DataFrame through python with Cx_Oracle lib.
2.Start a local Spark Session.
3.Convert the dataset for Kmeansmodel train.
4.Train the KMeans model and predict the same data.just set K =3
5.Get the ClassifierFeature of the KMeans model'predict.
6.Get the count of every ClassifierFeature.
7.Loop 4-6 for 20 times.
8.Compare the result of every time.
9.Find the KMeans result dose not stabilize.
10.The same dataset and param for ML package'KMeans, its result is the same.
","Windows 10
spark 2.0.2 standalone
spyder 3.1.4
Anaconda 4.3.0
python 3.5.2",ximen120,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-05-08 08:58:13.236,"
The Scipt  for python like follow:
Get data:
import pandas as pd
from V003.DataPrepare.getData4oracle import getOracleData


ip = '10.10.50.60'
sid = 'XE'
port = '1521'
username = 'sjp'
passwd = 'sjp'
qx_sql = ""select substr(dcdxxtm,1,6) as xzqh,count(distinct substr(dcdxxtm,1,14)) as pcxqs,count(*) as gths,sum(c_7) as cyry,sum(to_number(c_5)) as gszz,round(sum(to_number(c_5))/count(*)*100,4) as gszzlv,sum(to_number(c_6)) as swdjz,round(sum(to_number(c_6))/count(*)*100,4) as swdjjzlv,count(distinct c_9) as hygs from YOUXIAODATA group by substr(dcdxxtm,1,6)""
#以dataframe的形式获取数据集
qx_dataset  = pd.DataFrame(getOracleData(ip,port,sid,username,passwd,qx_sql)).sort_index(axis=1, ascending = True)
qx_dataset.columns = [""XZQH"",""PCXQS"",""GTHS"",""CYRY"",""GSZZ"",""GSZZLV"",""SWDJZ"",""SWDJZLV"",""HYGS""]
qx_data = qx_dataset.loc[:,[""CYRY"",""GSZZ"",""GSZZLV"",""SWDJZ"",""SWDJZLV"",""HYGS""]]
#KMeans part
#spark mllib edition
from pyspark import SparkContext
from pyspark.ml.clustering import KMeans as KMeansml
from pyspark.sql import SQLContext
from pyspark.ml.linalg import Vectors
from pyspark.mllib.clustering import KMeans as KMeansmllib

master  = ""local""
sc = SparkContext(master, ""QX_gth App"")


qx_dataset_cluster = []
for i in range(len(qx_data)):
    qx_dataset_cluster.append(tuple(list(qx_data.loc[i])))

for i in range(20):
    result_tem = KMeansmllib.train(sc.parallelize(qx_dataset_cluster), 3, initializationMode='k-means||')
    classifier = pd.DataFrame(result_tem.predict(sc.parallelize(qx_dataset_cluster)).collect())
    print(""第 %d 次聚类："" % i)
    print(""聚类第一类个数"",classifier[classifier[0]==0].count())
    print(""聚类第二类个数"",classifier[classifier[0]==1].count())
    print(""聚类第三类个数"",classifier[classifier[0]==2].count())
    print(""===================================================="")",false,,,,,,,,,,,,,9223372036854775807,,,Mon May 08 09:36:40 UTC 2017,,,,,0|i3em2f:,9223372036854775807,,,,,,,,,,,,,08/May/17 08:58;srowen;I can't understand what this is describing; please read http://spark.apache.org/contributing.html . This doesn't specify any particular problem. You would not expect k-means results to be the same each time. It's stochastic.,"08/May/17 09:36;ximen120;hi：
     it is really stochastic, but the same dataset, use the KMeans in sparkml lib ,the result is stabilize. Is that okay?

2017-05-08 

ffdd-120 



发件人：""Sean Owen (JIRA)"" <jira@apache.org>
发送时间：2017-05-08 16:59
主题：[jira] [Resolved] (SPARK-20634) result of MLlib KMeans cluster is not stabilize
收件人：""ffdd-120""<ffdd-120@163.com>
抄送：


     [ https://issues.apache.org/jira/browse/SPARK-20634?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ] 

Sean Owen resolved SPARK-20634. 
------------------------------- 
    Resolution: Invalid 

I can't understand what this is describing; please read http://spark.apache.org/contributing.html . This doesn't specify any particular problem. You would not expect k-means results to be the same each time. It's stochastic. 




-- 
This message was sent by Atlassian JIRA 
(v6.3.15#6346) 
",,,,,,,,,,,,,,,,,,,,,,
SparseVector.argmax throws IndexOutOfBoundsException when the sparse vector has a size greater than zero but no elements defined.,SPARK-20615,13069489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mclean,mclean,mclean,05/May/17 15:17,11/May/17 20:00,15/Aug/18 23:03,09/May/17 08:48,2.1.0,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,,ML,MLlib,,,,0,,,,,"org.apache.spark.ml.linalg.SparseVector.argmax throws an IndexOutOfRangeException when the vector size is greater than zero and no values are defined.  The toString() representation of such a vector is "" (100000,[],[])"".  This is because the argmax function tries to get the value at indexes(0) without checking the size of the array.

Code inspection reveals that the mllib version of SparseVector should have the same issue.",,apachespark,mclean,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-05-05 15:26:49.422,,false,,,,,,,,,,,,,9223372036854775807,,,Tue May 09 08:48:33 UTC 2017,,,,,0|i3ejrb:,9223372036854775807,,,,,,,,,,,,,"05/May/17 15:26;srowen;Agree, I think you just want to return 0 if numActives == 0 early in the method.",05/May/17 15:29;mclean;Thank you.  I will submit a patch with tests.,"05/May/17 21:22;apachespark;User 'jonmclean' has created a pull request for this issue:
https://github.com/apache/spark/pull/17877","09/May/17 08:48;srowen;Issue resolved by pull request 17877
[https://github.com/apache/spark/pull/17877]",,,,,,,,,,,,,,,,,,,,
Load doesn't work in PCAModel ,SPARK-20526,13067770,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,volkanagun@gmail.com,volkanagun@gmail.com,28/Apr/17 18:27,06/May/17 09:31,15/Aug/18 23:03,06/May/17 09:31,2.1.0,,,,,,,,,,,,,,,,,,,,,ML,MLlib,,,,0,,,,,"Error occurs during loading PCAModel. Saved model doesn't load.

",Windows,facai,volkanagun@gmail.com,yuhaoyan,,,,,,,,,,,,,,,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-04-28 23:46:27.373,"Exception in thread ""main"" scala.MatchError: [0.013294340532645242    0.0016042024204431548   ... (5 total)
6.959479744808873E-4    0.0021153567698887744   ...
-0.005102706589702275   -0.0024711505381889974  ...
0.004074732733668404    2.7418753398099228E-5   ...
1.2945571430384731E-5   -6.199644880804413E-4   ...
-0.002360651710795367   -0.001654674676494886   ...
0.011204035953495471    -0.006378848577626251   ...
0.005389744287901279    0.002231283133537626    ...
0.001869465361516778    0.00718830947511206     ...
0.014358425264812121    -3.3012287407489587E-4  ...
5.69120821472734E-4     7.12296896795406E-4     ...
7.004861275124608E-4    -3.005783472514015E-4   ...
-0.0023551893808336144  -6.550226159071732E-4   ...
0.003040562443159995    -2.104740430126494E-4   ...
-0.0012236712374116975  9.447103224574776E-5    ...
0.0027361041139605316   6.149814920605533E-4    ...
3.6796837622507023E-4   6.196104767073818E-5    ...
-0.003274200696197547   0.001101451285991231    ...
5.476218005798688E-4    -8.984321215193931E-4   ...
0.0017501700257706659   0.0018409166858105298   ...
0.0010868712651468198   -0.0012077469591495575  ...
4.883239856147703E-4    -0.004061966914046383   ...
8.696693477406195E-4    7.895297250061177E-4    ...
-5.16716587049303E-4    0.003568495609414725    ...
-0.001725522384991993   6.875536983903931E-4    ...
-3.1468316306898974E-4  0.0023660019562011434   ...
0.0016973165489721752   2.32553221836286E-5     ...
-0.0032321799020498536  -1.49314155878987E-4    ...
0.0020875267486072653   8.627111034699803E-5    ...
-0.0020307189366613374  -2.0411663157615174E-4  ...
0.0035042690894366937   0.0020074521457729456   ...
1.975099042249683E-4    7.598235662157596E-5    ...
1.1038458079010497E-4   7.315760439103337E-4    ...
5.813814776254902E-4    2.4961248992173254E-4   ...
1.942492538757402E-4    -1.997055002482181E-4   ...
-0.0048300667924791015  8.904724070762993E-4    ...
0.007657836248733474    5.599957843364689E-4    ...
0.0035438317095165006   -7.233154410053809E-4   ...
0.001878212773273337    7.581745687932052E-4    ...
0.010131750657836754    2.1920093393849584E-4   ...
0.001753212312785938    0.0021439741519462254   ...
-3.909070794555408E-5   2.590156820794557E-4    ...
-6.882939737245035E-4   6.322893839752083E-4    ...
-3.789646830698587E-4   2.3693169226883575E-4   ...
-1.9255047330779288E-5  -6.39070485583997E-4    ...
8.102445125106021E-4    -0.0022050737338066943  ...
-4.2366142922951286E-5  5.955362472555986E-4    ...
-0.0015144226968579067  -8.242779881549397E-5   ...
-0.0012702345966236904  -0.0011649024357358907  ...
0.0019522031734763496   -0.0014982265326082904  ...
0.0013131531024736557   -0.004567948857565027   ...
6.350826570800948E-5    0.005774327745980014    ...
-4.4850536696514554E-5  0.0028443123379698803   ...
-2.5064580774477354E-4  8.95739432897984E-4     ...
1.834977614079032E-4    0.008221133644026017    ...
0.0018062022121061486   7.246471146423551E-4    ...
5.9897017042246675E-5   -1.2325795466039025E-4  ...
-1.7413537817280457E-5  -2.0411784824145887E-4  ...
3.066745552430336E-4    -3.808399510189892E-5   ...
9.544536541794614E-5    2.093047319375699E-4    ...
0.0011688209826714033   0.001109210469508907    ...
2.6188159891769984E-5   3.718695256861684E-4    ...
-3.293180913549028E-5   -7.829311303267258E-4   ...
-1.997465784229415E-4   4.138523645641945E-4    ...
4.1050123440446314E-4   -3.179659591332949E-4   ...
-2.3737137533953541E-4  0.0018753605300340299   ...
4.855542635108417E-4    1.8158699544433212E-5   ...
7.592864584485916E-4    4.501817004860302E-5    ...
3.716315137264757E-4    2.296858273875808E-4    ...
0.0016068602017658778   2.2475511453569504E-4   ...
2.4258883780829324E-4   1.0184326323849888E-4   ...
-4.7932713918963295E-4  5.901835122112097E-4    ...
-1.8856979123338564E-4  -9.558656625437273E-4   ...
5.416204201570484E-4    1.6524898278998823E-4   ...
-5.021696888207173E-6   -4.484910926573574E-4   ...
0.014613602238701812    -0.001184805160991072   ...
0.0020454693567747008   7.434028750721513E-4    ...
6.692585039526249E-4    0.007590369501765805    ...
0.0027298594632468257   -0.002701863486350074   ...
... (33850 total),null] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)",false,,,,,Important,,,,,,,,9223372036854775807,,,Tue May 02 05:50:00 UTC 2017,,,,,0|i3e95j:,9223372036854775807,,,,,,,,,,,,,28/Apr/17 23:46;yuhaoyan;Can you please provide more context? like which version of Spark did you use for saving and loading respectively. And perhaps share the save/load code. You can also check the explainedVariance in PCAModel to see if it's null.,29/Apr/17 00:16;volkanagun@gmail.com;I think the problem is about explainedVariance. It is simply null.  ,02/May/17 05:50;facai;Can you give a sample code?,,,,,,,,,,,,,,,,,,,,,
"pyspark.sql.utils.IllegalArgumentException: u'DecisionTreeClassifier was given input with invalid label column label, without the number of classes specified. See StringIndexer",SPARK-20445,13066282,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Cannot Reproduce,,surya78,surya78,24/Apr/17 09:38,26/Apr/17 00:34,15/Aug/18 23:03,25/Apr/17 07:12,1.6.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,," #Load the CSV file into a RDD
    irisData = sc.textFile(""/home/infademo/surya/iris.csv"")
    irisData.cache()
    irisData.count()

    #Remove the first line (contains headers)
    dataLines = irisData.filter(lambda x: ""Sepal"" not in x)
    dataLines.count()

    from pyspark.sql import Row
    #Create a Data Frame from the data
    parts = dataLines.map(lambda l: l.split("",""))
    irisMap = parts.map(lambda p: Row(SEPAL_LENGTH=float(p[0]),\
                                    SEPAL_WIDTH=float(p[1]), \
                                    PETAL_LENGTH=float(p[2]), \
                                    PETAL_WIDTH=float(p[3]), \
                                    SPECIES=p[4] ))

    # Infer the schema, and register the DataFrame as a table.
    irisDf = sqlContext.createDataFrame(irisMap)
    irisDf.cache()

    #Add a numeric indexer for the label/target column
    from pyspark.ml.feature import StringIndexer
    stringIndexer = StringIndexer(inputCol=""SPECIES"", outputCol=""IND_SPECIES"")
    si_model = stringIndexer.fit(irisDf)
    irisNormDf = si_model.transform(irisDf)

    irisNormDf.select(""SPECIES"",""IND_SPECIES"").distinct().collect()
    irisNormDf.cache()

    """"""--------------------------------------------------------------------------
    Perform Data Analytics
    -------------------------------------------------------------------------""""""

    #See standard parameters
    irisNormDf.describe().show()

    #Find correlation between predictors and target
    for i in irisNormDf.columns:
        if not( isinstance(irisNormDf.select(i).take(1)[0][0], basestring)) :
            print( ""Correlation to Species for "", i, \
                        irisNormDf.stat.corr('IND_SPECIES',i))



    #Transform to a Data Frame for input to Machine Learing
    #Drop columns that are not required (low correlation)

    from pyspark.mllib.linalg import Vectors
    from pyspark.mllib.linalg import SparseVector
    from pyspark.mllib.regression import LabeledPoint
    from pyspark.mllib.util import MLUtils
    import org.apache.spark.mllib.linalg.{Matrix, Matrices}
    from pyspark.mllib.linalg.distributed import RowMatrix

    from pyspark.ml.linalg import Vectors
    pyspark.mllib.linalg.Vector
    def transformToLabeledPoint(row) :
        lp = ( row[""SPECIES""], row[""IND_SPECIES""], \
                    Vectors.dense([row[""SEPAL_LENGTH""],\
                            row[""SEPAL_WIDTH""], \
                            row[""PETAL_LENGTH""], \
                            row[""PETAL_WIDTH""]]))
        return lp




    irisLp = irisNormDf.rdd.map(transformToLabeledPoint)
    irisLpDf = sqlContext.createDataFrame(irisLp,[""species"",""label"", ""features""])
    irisLpDf.select(""species"",""label"",""features"").show(10)
    irisLpDf.cache()

    """"""--------------------------------------------------------------------------
    Perform Machine Learning
    -------------------------------------------------------------------------""""""
    #Split into training and testing data
    (trainingData, testData) = irisLpDf.randomSplit([0.9, 0.1])
    trainingData.count()
    testData.count()
    testData.collect()

    from pyspark.ml.classification import DecisionTreeClassifier
    from pyspark.ml.evaluation import MulticlassClassificationEvaluator

    #Create the model
    dtClassifer = DecisionTreeClassifier(maxDepth=2, labelCol=""label"",\
                    featuresCol=""features"")

   dtModel = dtClassifer.fit(trainingData)
   
   issue part:-
   
   dtModel = dtClassifer.fit(trainingData) Traceback (most recent call last): File """", line 1, in File ""/opt/mapr/spark/spark-1.6.1-bin-hadoop2.6/python/pyspark/ml/pipeline.py"", line 69, in fit return self._fit(dataset) File ""/opt/mapr/spark/spark-1.6.1-bin-hadoop2.6/python/pyspark/ml/wrapper.py"", line 133, in _fit java_model = self._fit_java(dataset) File ""/opt/mapr/spark/spark-1.6.1-bin-hadoop2.6/python/pyspark/ml/wrapper.py"", line 130, in _fit_java return self._java_obj.fit(dataset._jdf) File ""/opt/mapr/spark/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py"", line 813, in call File ""/opt/mapr/spark/spark-1.6.1-bin-hadoop2.6/python/pyspark/sql/utils.py"", line 53, in deco raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace) pyspark.sql.utils.IllegalArgumentException: u'DecisionTreeClassifier was given input with invalid label column label, without the number of classes specified. See StringIndexer.'",,hyukjin.kwon,surya78,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-04-24 09:58:17.509,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 26 00:34:08 UTC 2017,,,,,0|i3dzz3:,9223372036854775807,,,,,,,,,,,,,"24/Apr/17 09:58;hyukjin.kwon;I can't reproduce this with the codes below against the current master:

{code}
irisData = sc.textFile(""./iris.csv"")
irisData.cache()
irisData.count()

dataLines = irisData.filter(lambda x: ""Sepal"" not in x)
dataLines.count()

from pyspark.sql import Row
#Create a Data Frame from the data
parts = dataLines.map(lambda l: l.split("",""))
irisMap = parts.map(lambda p: Row(SEPAL_LENGTH=float(p[0]),\
                                SEPAL_WIDTH=float(p[1]), \
                                PETAL_LENGTH=float(p[2]), \
                                PETAL_WIDTH=float(p[3]), \
                                SPECIES=p[4]))

# Infer the schema, and register the DataFrame as a table.
irisDf = sqlContext.createDataFrame(irisMap)
irisDf.cache()

#Add a numeric indexer for the label/target column
from pyspark.ml.feature import StringIndexer
stringIndexer = StringIndexer(inputCol=""SPECIES"", outputCol=""IND_SPECIES"")
si_model = stringIndexer.fit(irisDf)
irisNormDf = si_model.transform(irisDf)

irisNormDf.select(""SPECIES"",""IND_SPECIES"").distinct().collect()
irisNormDf.cache()

""""""--------------------------------------------------------------------------
Perform Data Analytics
-------------------------------------------------------------------------""""""

#See standard parameters
irisNormDf.describe().show()

#Find correlation between predictors and target
for i in irisNormDf.columns:
    if not( isinstance(irisNormDf.select(i).take(1)[0][0], basestring)) :
        print( ""Correlation to Species for "", i, \
                    irisNormDf.stat.corr('IND_SPECIES',i))




#Transform to a Data Frame for input to Machine Learing
#Drop columns that are not required (low correlation)

from pyspark.mllib.linalg import Vectors
from pyspark.mllib.linalg import SparseVector
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.util import MLUtils
from pyspark.mllib.linalg.distributed import RowMatrix

from pyspark.ml.linalg import Vectors
pyspark.mllib.linalg.Vector
def transformToLabeledPoint(row) :
    lp = ( row[""SPECIES""], row[""IND_SPECIES""], \
                Vectors.dense([row[""SEPAL_LENGTH""],\
                        row[""SEPAL_WIDTH""], \
                        row[""PETAL_LENGTH""], \
                        row[""PETAL_WIDTH""]]))
    return lp




irisLp = irisNormDf.rdd.map(transformToLabeledPoint)
irisLpDf = sqlContext.createDataFrame(irisLp,[""species"",""label"", ""features""])
irisLpDf.select(""species"",""label"",""features"").show(10)
irisLpDf.cache()

""""""--------------------------------------------------------------------------
Perform Machine Learning
-------------------------------------------------------------------------""""""
#Split into training and testing data
(trainingData, testData) = irisLpDf.randomSplit([0.9, 0.1])
trainingData.count()
testData.count()
testData.collect()

from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

#Create the model
dtClassifer = DecisionTreeClassifier(maxDepth=2, labelCol=""label"",\
                featuresCol=""features"")

dtModel = dtClassifer.fit(trainingData)
{code}

with the data https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv

I manually removed the header. It looks fixed somewhere.
",24/Apr/17 09:58;hyukjin.kwon;[~surya78] are you able to test this against higher version or current master?,25/Apr/17 07:12;hyukjin.kwon;I am resolving this as I can't reproduce this in the current master. Please reopen this if this still exists in the current master.,"25/Apr/17 12:50;surya78;Hello Hyukjin Kwon 
Thxz for reply.
I tried many times but getting same issue.
I am using Spark 1.6.1 version.
Please help me to sort out this issue.",25/Apr/17 13:02;hyukjin.kwon;Are you maybe able to try this against the current master or higher versions?,"25/Apr/17 15:32;surya78;Hello Hyukjin Kwon ,
Thanks for fast reply
I am not getting your answer   ""current master""
What is current master and what should we check?
Please explain

Note- I am running this code on Mapr having Spark version 1.6.1 ","26/Apr/17 00:34;hyukjin.kwon;I meant the current codebase, latest build. Probably, I guess testing against 2.1.0 might be enough.

In guide lines - http://spark.apache.org/contributing.html

> For issues that can’t be reproduced against master as reported, resolve as Cannot Reproduce

I could not reproduce this in the current master. So, I resolved this as {{Cannot Reproduce}}. I assume there is a JIRA fixing this issue.
You (or anyone) can identify the JIRA and then make a backport if applicable. ",,,,,,,,,,,,,,,,,
"pyspark.sql.utils.IllegalArgumentException: u'DecisionTreeClassifier was given input with invalid label column label, without the number of classes specified. See StringIndexer",SPARK-20444,13066281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,surya78,surya78,24/Apr/17 09:36,24/Apr/17 09:45,15/Aug/18 23:03,24/Apr/17 09:45,1.6.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,,,hyukjin.kwon,surya78,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20445,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-04-24 09:43:44.986,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 24 09:44:37 UTC 2017,,,,,0|i3dzyv:,9223372036854775807,,,,,,,,,,,,,24/Apr/17 09:43;hyukjin.kwon;Can you fill up the description with expected input/output and reproducible steps? It is not clear what the issue describes. ,"24/Apr/17 09:44;hyukjin.kwon;Probably, it would be more helpful if you are able to test this against the current master or higher version to check if it has already been fixed.",,,,,,,,,,,,,,,,,,,,,,
pyspark linalg _convert_to_vector should check for sorted indices,SPARK-20214,13061551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,josephkb,josephkb,04/Apr/17 22:03,06/Apr/17 00:59,15/Aug/18 23:03,06/Apr/17 00:51,1.5.2,1.6.3,2.0.2,2.1.1,2.2.0,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,,ML,MLlib,PySpark,Tests,,0,,,,,"I've seen a few failures of this line: https://github.com/apache/spark/blame/402bf2a50ddd4039ff9f376b641bd18fffa54171/python/pyspark/mllib/tests.py#L847

It converts a scipy.sparse.lil_matrix to a dok_matrix and then to a pyspark.mllib.linalg.Vector.  The failure happens in the conversion to a vector and indicates that the dok_matrix is not returning its values in sorted order. (Actually, the failure is in _convert_to_vector, which converts the dok_matrix to a csc_matrix and then passes the CSC data to the MLlib Vector constructor.) Here's the stack trace:
{code}
Traceback (most recent call last):
  File ""/home/jenkins/workspace/python/pyspark/mllib/tests.py"", line 847, in test_serialize
    self.assertEqual(sv, _convert_to_vector(lil.todok()))
  File ""/home/jenkins/workspace/python/pyspark/mllib/linalg/__init__.py"", line 78, in _convert_to_vector
    return SparseVector(l.shape[0], csc.indices, csc.data)
  File ""/home/jenkins/workspace/python/pyspark/mllib/linalg/__init__.py"", line 556, in __init__
    % (self.indices[i], self.indices[i + 1]))
TypeError: Indices 3 and 1 are not strictly increasing
{code}

This seems like a bug in _convert_to_vector, where we really should check {{csc_matrix.has_sorted_indices}} first.

I haven't seen this bug in pyspark.ml.linalg, but it probably exists there too.",,apachespark,josephkb,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-04-05 03:23:55.06,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 06 00:51:08 UTC 2017,,,,,0|i3d7en:,9223372036854775807,josephkb,,,,,2.0.3,2.1.2,2.2.0,,,,,"05/Apr/17 03:23;viirya;Confirmed that dok_matrix.tocsc() won't guarantee sorted indices:

{code}
>>> from scipy.sparse import lil_matrix
>>> lil = lil_matrix((4, 1))
>>> lil[1, 0] = 1
>>> lil[3, 0] = 2
>>> dok = lil.todok()
>>> csc = dok.tocsc()
>>> csc.has_sorted_indices
0
>>> csc.indices
array([3, 1], dtype=int32)
{code}

I checked the source codes of scipy. The only way to guarantee it is {{csc_matrix.tocsr()}} and {{csr_matrix.tocsc()}}.
","05/Apr/17 03:31;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/17532","06/Apr/17 00:51;josephkb;Issue resolved by pull request 17532
[https://github.com/apache/spark/pull/17532]",,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException in ALS,SPARK-19600,13043044,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Duplicate,,zxpan,zxpan,14/Feb/17 19:29,14/Feb/17 20:06,15/Aug/18 23:03,14/Feb/17 19:49,2.0.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"Understand issue SPARK-3080 closed, but I don't understand yet what cause the issue: memory, parallelism, negative userID or product ID?

I consistently ran into this issue with different set of training set, can you suggest any area to look at?

java.lang.ArrayIndexOutOfBoundsException: 221529807
        at org.apache.spark.ml.recommendation.ALS$$anonfun$partitionRatings$1$$anonfun$apply$6.apply(ALS.scala:944)
        at org.apache.spark.ml.recommendation.ALS$$anonfun$partitionRatings$1$$anonfun$apply$6.apply(ALS.scala:940)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:211)
        at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:200)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
        at org.apache.spark.scheduler.Task.run(Task.scala:86)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
",,zxpan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3080,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-02-14 19:49:02.602,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 20:06:51 UTC 2017,,,,,0|i3a2rr:,9223372036854775807,,,,,,,,,,,,,14/Feb/17 19:49;srowen;Questions go to the mailing list. Don't open duplicate JIRAs to re-ask.,14/Feb/17 19:57;zxpan; are you sure it is duplicated issue as SPARK-3080 even without looking at?,"14/Feb/17 20:06;srowen;You indicated it was the same issue, but, it's still not the right place to ask the question.",,,,,,,,,,,,,,,,,,,,,
Warning MLlib netlib,SPARK-19423,13039475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Invalid,,marco90,marco90,01/Feb/17 14:18,01/Feb/17 14:34,15/Aug/18 23:03,01/Feb/17 14:34,2.0.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"Hello,

I have used the glm to create a linear regressio model but there is these warning messages: 

WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK

to solve this warnings i have added the dependencies in the spark-defaults.conf as described in mllib guide (https://spark.apache.org/docs/1.2.1/mllib-guide.html):

spark.jars.packages=com.github.fommil.netlib:core:1.1.2

but after i obtain a null pointer exception:

The jars for the packages stored in: /root/.ivy2/jars
:: loading settings :: url = jar:file:/home/user/spark-2.0.1-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
com.github.fommil.netlib#all added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found com.github.fommil.netlib#all;1.1.2 in central
	found net.sourceforge.f2j#arpack_combined_all;0.1 in central
	found com.github.fommil.netlib#core;1.1.2 in central
	found com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1 in central
	found com.github.fommil.netlib#native_ref-java;1.1 in central
	found com.github.fommil#jniloader;1.1 in central
	found com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1 in central
	found com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1 in central
	found com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1 in central
	found com.github.fommil.netlib#netlib-native_ref-win-i686;1.1 in central
	found com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1 in central
	found com.github.fommil.netlib#native_system-java;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-linux-i686;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1 in central
	found com.github.fommil.netlib#netlib-native_system-win-i686;1.1 in central
:: resolution report :: resolve 1151ms :: artifacts dl 46ms
	:: modules in use:
	com.github.fommil#jniloader;1.1 from central in [default]
	com.github.fommil.netlib#all;1.1.2 from central in [default]
	com.github.fommil.netlib#core;1.1.2 from central in [default]
	com.github.fommil.netlib#native_ref-java;1.1 from central in [default]
	com.github.fommil.netlib#native_system-java;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-win-i686;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-linux-i686;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-win-i686;1.1 from central in [default]
	com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1 from central in [default]
	net.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]
	:: evicted modules:
	com.github.fommil.netlib#core;1.1 by [com.github.fommil.netlib#core;1.1.2] in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   19  |   0   |   0   |   1   ||   17  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
	confs: [default]
	0 artifacts copied, 17 already retrieved (0kB/46ms)
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.spark.deploy.RPackageUtils$.checkManifestForR(RPackageUtils.scala:95)
	at org.apache.spark.deploy.RPackageUtils$$anonfun$checkAndBuildRPackage$1.apply(RPackageUtils.scala:179)
	at org.apache.spark.deploy.RPackageUtils$$anonfun$checkAndBuildRPackage$1.apply(RPackageUtils.scala:175)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.deploy.RPackageUtils$.checkAndBuildRPackage(RPackageUtils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:306)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:158)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Error in sparkR.sparkContext(master, appName, sparkHome, sparkConfigMap,  : 
  JVM is not ready after 10 seconds
Calls: sparkR.session -> sparkR.sparkContext

Seems there are problems with JVM,Anyoune knows how is possible solve this problem?

Thanks
Marco

Fai clic qui per Rispondere
",,marco90,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-02-01 14:34:11.881,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 01 14:34:11 UTC 2017,,,,,0|i39gtr:,9223372036854775807,,,,,,,,,,,,,"01/Feb/17 14:34;srowen;This isn't really a Spark question, but a netlib-java question. My guess is that you did not install libgfortran3.
Questions belong on the mailing list rather than JIRA.",,,,,,,,,,,,,,,,,,,,,,,
StatFunctions.multipleApproxQuantiles can give NoSuchElementException: next on empty iterator,SPARK-19339,13037201,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Duplicate,,barrybecker4,barrybecker4,23/Jan/17 22:44,03/Mar/17 10:50,15/Aug/18 23:03,03/Mar/17 10:50,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"This problem is easy to reproduce by running StatFunctions.multipleApproxQuantiles on an empty dataset, but I think it can occur in other cases, like if the column is all null or all one value.
I have unit tests that can hit it in several different cases.

The fix that I have introduced locally is to return
{code}
 if (sampled.length == 0) 0 else sampled.last.value
{code}
instead of 
{code}
sampled.last.value
{code}
at the end of QuantileSummaries.query.
Below is the exception:
{code}
next on empty iterator
java.util.NoSuchElementException: next on empty iterator
	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)
	at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)
	at scala.collection.IterableLike$class.head(IterableLike.scala:107)
	at scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:186)
	at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126)
	at scala.collection.mutable.ArrayOps$ofRef.head(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.last(TraversableLike.scala:459)
	at scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$last(ArrayOps.scala:186)
	at scala.collection.IndexedSeqOptimized$class.last(IndexedSeqOptimized.scala:132)
	at scala.collection.mutable.ArrayOps$ofRef.last(ArrayOps.scala:186)
	at org.apache.spark.sql.catalyst.util.QuantileSummaries.query(QuantileSummaries.scala:207)
	at org.apache.spark.sql.SparkPercentileCalculator$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply$mcDD$sp(SparkPercentileCalculator.scala:91)
	at org.apache.spark.sql.SparkPercentileCalculator$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply(SparkPercentileCalculator.scala:91)
	at org.apache.spark.sql.SparkPercentileCalculator$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply(SparkPercentileCalculator.scala:91)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.SparkPercentileCalculator$$anonfun$multipleApproxQuantiles$1.apply(SparkPercentileCalculator.scala:91)
	at org.apache.spark.sql.SparkPercentileCalculator$$anonfun$multipleApproxQuantiles$1.apply(SparkPercentileCalculator.scala:91)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.SparkPercentileCalculator.multipleApproxQuantiles(SparkPercentileCalculator.scala:91)
	at com.mineset.spark.statistics.model.ContinuousMinesetStats.quartiles$lzycompute(ContinuousMinesetStats.scala:274)
	at com.mineset.spark.statistics.model.ContinuousMinesetStats.quartiles(ContinuousMinesetStats.scala:272)
	at com.mineset.spark.statistics.model.MinesetStats.com$mineset$spark$statistics$model$MinesetStats$$serializeContinuousFeature$1(MinesetStats.scala:66)
	at com.mineset.spark.statistics.model.MinesetStats$$anonfun$calculateWithColumns$1.apply(MinesetStats.scala:118)
	at com.mineset.spark.statistics.model.MinesetStats$$anonfun$calculateWithColumns$1.apply(MinesetStats.scala:114)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at com.mineset.spark.statistics.model.MinesetStats.calculateWithColumns(MinesetStats.scala:114)
	at com.mineset.spark.statistics.model.MinesetStats.toJson(MinesetStats.scala:46)
	at com.mineset.spark.statistics.model.MinesetStatsSuite$$anonfun$8.apply$mcV$sp(MinesetStatsSuite.scala:93)
	at com.mineset.spark.statistics.model.MinesetStatsSuite$$anonfun$8.apply(MinesetStatsSuite.scala:90)
	at com.mineset.spark.statistics.model.MinesetStatsSuite$$anonfun$8.apply(MinesetStatsSuite.scala:90)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at com.mineset.spark.statistics.model.MinesetStatsSuite.org$scalatest$BeforeAndAfterAll$$super$run(MinesetStatsSuite.scala:30)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at com.mineset.spark.statistics.model.MinesetStatsSuite.run(MinesetStatsSuite.scala:30)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.run(Runner.scala:883)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:138)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
{code}",,barrybecker4,mlnick,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19573,,,,,,,SPARK-19573,,,,,,0.0,,,,,,,,,,,,,,,,2017-03-03 07:32:23.609,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 03 07:32:23 UTC 2017,,,,,0|i393ov:,9223372036854775807,,,,,,,,,,,,,03/Mar/17 07:32;mlnick;This should be addressed by SPARK-19573 - empty (or all null) columns will return empty Array rather than throw exception.,,,,,,,,,,,,,,,,,,,,,,,
GaussianMixture throws cryptic error when number of features is too high,SPARK-19313,13036649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sethah,sethah,sethah,20/Jan/17 19:13,25/Jan/17 15:14,15/Aug/18 23:03,25/Jan/17 15:14,,,,,,,,,,,,,,,,2.2.0,,,,,,ML,MLlib,,,,0,,,,,"The following fails

{code}
    val df = Seq(
      Vectors.sparse(46400, Array(0, 4), Array(3.0, 8.0)),
      Vectors.sparse(46400, Array(1, 5), Array(4.0, 9.0)))
      .map(Tuple1.apply).toDF(""features"")
    val gm = new GaussianMixture()
    gm.fit(df)
{code}

It fails because GMMs allocate an array of size {{numFeatures * numFeatures}} and in this case we'll get integer overflow. We should limit the number of features appropriately.",,apachespark,sethah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-01-20 19:24:05.746,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 20 19:24:05 UTC 2017,,,,,0|i390a7:,9223372036854775807,yanboliang,,,,,,,,,,,,"20/Jan/17 19:24;apachespark;User 'sethah' has created a pull request for this issue:
https://github.com/apache/spark/pull/16661",,,,,,,,,,,,,,,,,,,,,,,
"Fix several sql, mllib and status api examples not working",SPARK-19134,13033045,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hyukjin.kwon,hyukjin.kwon,hyukjin.kwon,09/Jan/17 11:29,20/Jan/17 13:20,15/Aug/18 23:03,20/Jan/17 13:19,,,,,,,,,,,,,,,,2.2.0,,,,,,MLlib,PySpark,SQL,,,0,,,,,"*binary_classification_metrics_example.py*

{code}
./bin/spark-submit examples/src/main/python/mllib/binary_classification_metrics_example.py
{code}

{code}
  File "".../spark/examples/src/main/python/mllib/binary_classification_metrics_example.py"", line 39, in <lambda>
    .rdd.map(lambda row: LabeledPoint(row[0], row[1]))
  File "".../spark/python/pyspark/mllib/regression.py"", line 54, in __init__
    self.features = _convert_to_vector(features)
  File "".../spark/python/pyspark/mllib/linalg/__init__.py"", line 80, in _convert_to_vector
    raise TypeError(""Cannot convert type %s into Vector"" % type(l))
TypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector
{code}

*status_api_demo.py*

{code}
PYSPARK_PYTHON=python3 ./bin/spark-submit examples/src/main/python/status_api_demo.py
{code}

{code}
Traceback (most recent call last):
  File "".../spark/examples/src/main/python/status_api_demo.py"", line 22, in <module>
    import Queue
ImportError: No module named 'Queue'
{code}

*bisecting_k_means_example.py*

{code}
./bin/spark-submit examples/src/main/python/mllib/bisecting_k_means_example.py
{code}

{code}
Traceback (most recent call last):
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/examples/src/main/python/mllib/bisecting_k_means_example.py"", line 46, in <module>
    model.save(sc, path)
AttributeError: 'BisectingKMeansModel' object has no attribute 'save'
{code}

*elementwise_product_example.py*

{code}
./bin/spark-submit examples/src/main/python/mllib/elementwise_product_example.py
{code}

{code}
Traceback (most recent call last):
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/examples/src/main/python/mllib/elementwise_product_example.py"", line 48, in <module>
    for each in transformedData2.collect():
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/pyspark/mllib/linalg/__init__.py"", line 478, in __getattr__
    return getattr(self.array, item)
AttributeError: 'numpy.ndarray' object has no attribute 'collect'
{code}

*hive.py*
{code}
./bin/spark-submit examples/src/main/python/sql/hive.py
{code}

{code}
Traceback (most recent call last):
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/examples/src/main/python/sql/hive.py"", line 47, in <module>
    spark.sql(""CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"")
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/lib/pyspark.zip/pyspark/sql/session.py"", line 541, in sql
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 69, in deco
pyspark.sql.utils.AnalysisException: 'org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./spark-warehouse);'
{code}


*SparkHiveExample*

{code}
./bin/run-example sql.hive.SparkHiveExample
{code}

{code}
Exception in thread ""main"" org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./spark-warehouse
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:498)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:484)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1668)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.sql.hive.client.Shim_v0_14.loadTable(HiveShim.scala:722)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply$mcV$sp(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:230)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:229)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:272)
	at org.apache.spark.sql.hive.client.HiveClientImpl.loadTable(HiveClientImpl.scala:685)
{code}


*JavaSparkHiveExample*

{code}
./bin/run-example sql.hive.JavaSparkHiveExample
{code}

{code}
Exception in thread ""main"" org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./spark-warehouse
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:498)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:484)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1668)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.sql.hive.client.Shim_v0_14.loadTable(HiveShim.scala:722)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply$mcV$sp(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
{code}",,apachespark,hyukjin.kwon,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-01-09 11:33:06.615,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 20 13:20:50 UTC 2017,,,,,0|i38fuf:,9223372036854775807,,,,,,,,,,,,,"09/Jan/17 11:33;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/16515","20/Jan/17 12:45;hyukjin.kwon;Oh [~yanboliang], it seems mistakenly not resolved.. :)","20/Jan/17 13:20;yanboliang;[~hyukjin.kwon] Thanks for kindly reminding, done.",,,,,,,,,,,,,,,,,,,,,
DistributedLDAModel returns different logPrior for original and loaded model,SPARK-19110,13032667,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wm624,wm624,wm624,06/Jan/17 20:38,10/Jan/17 00:14,15/Aug/18 23:03,07/Jan/17 19:42,1.3.1,1.4.1,1.5.2,1.6.3,2.0.2,2.1.0,2.2.0,,,,,,,,,2.0.3,2.1.1,2.2.0,,,,ML,MLlib,,,,0,,,,,"While adding DistributedLDAModel training summary for SparkR, I found that the logPrior for original and loaded model is different.
For example, in the test(""read/write DistributedLDAModel""), I add the test:
val logPrior = model.asInstanceOf[DistributedLDAModel].logPrior
      val logPrior2 = model2.asInstanceOf[DistributedLDAModel].logPrior
      assert(logPrior === logPrior2)
The test fails:
-4.394180878889078 did not equal -4.294290536919573


",,apachespark,josephkb,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2017-01-06 20:45:05.302,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 10 00:14:04 UTC 2017,,,,,0|i38dif:,9223372036854775807,josephkb,,,,,2.0.3,2.1.1,2.2.0,,,,,"06/Jan/17 20:45;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/16491","07/Jan/17 19:42;josephkb;Issue resolved by pull request 16491
[https://github.com/apache/spark/pull/16491]","10/Jan/17 00:14;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/16524",,,,,,,,,,,,,,,,,,,,,
Correlation causes Error “Cannot determine the number of cols”,SPARK-18562,13022851,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Not A Bug,,Kuku1,Kuku1,23/Nov/16 13:12,23/Nov/16 13:57,15/Aug/18 23:03,23/Nov/16 13:57,1.6.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"I followed the MLlib docs on how to calculate a correlation. I'm using Spark 1.6.1.

First my application filters out elements that do not have all the values I'm looking for. Afterwards, I'm mapping each of the remaining elements to a dense Vector, as seen in the docs. Then I'm passing the RDD[Vector] to the MLlib function.

My code is the following:
{code}
val filteredRdd = rdd.filter(document => document.containsKey(""SomeValue1"")
  && document.containsKey(""SomeValue2"") && document.containsKey(""SomeValue3""))

val vectorRdd: RDD[Vector] = filteredRdd.map(document => {
  Vectors.dense(document.getDouble(""SomeValue1""), document.getDouble(""SomeValue2""), document.getDouble(""SomeValue3""))
})

val correlation_matrix = Statistics.corr(vectorRdd, method = ""spearman"")
println(""Spearman: "" + correlation_matrix.toString())

val correlation_matrix_pearson = Statistics.corr(vectorRdd, method = ""pearson"")
println(""Pearson: "" + correlation_matrix_pearson.toString())
{code}

This is the error that gets thrown:
{code}
16/11/23 13:19:51 ERROR ApplicationMaster: User class threw exception: 

java.lang.RuntimeException: Cannot determine the number of cols because it is not specified in the constructor and the rows RDD is empty.
java.lang.RuntimeException: Cannot determine the number of cols because it is not specified in the constructor and the rows RDD is empty.
    at scala.sys.package$.error(package.scala:27)
    at org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:64)
    at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:328)
    at org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)
    at org.apache.spark.mllib.stat.correlation.SpearmanCorrelation$.computeCorrelationMatrix(SpearmanCorrelation.scala:91)
    at org.apache.spark.mllib.stat.correlation.Correlations$.corrMatrix(Correlation.scala:66)
    at org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:74)
{code}

Because I filter out the elements which would cause an empty vector, I don't see how this is related to my code? Thus I created this issue. ",Ubuntu 14.04LTS,Kuku1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 23 13:57:02 UTC 2016,,,,,0|i36oyf:,9223372036854775807,,,,,,,,,,,,,"23/Nov/16 13:57;Kuku1;I did a mistake, my RDD did get corrupt at one point and thus causing empty RDDs. ",,,,,,,,,,,,,,,,,,,,,,,
"Fix default Locale used in DateFormat, NumberFormat to Locale.US",SPARK-18076,13014750,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,24/Oct/16 14:48,02/Nov/16 09:39,15/Aug/18 23:03,02/Nov/16 09:39,2.0.1,,,,,,,,,,,,,,,2.1.0,,,,,,MLlib,Spark Core,SQL,,,1,releasenotes,,,,"Many parts of the code use {{DateFormat}} and {{NumberFormat}} instances. Although the behavior of these format is mostly determined by things like format strings, the exact behavior can vary according to the platform's default locale. Although the locale defaults to ""en"", it can be set to something else by env variables. And if it does, it can cause the same code to succeed or fail based just on locale:

{code}
import java.text._
import java.util._

def parse(s: String, l: Locale) = new SimpleDateFormat(""yyyyMMMdd"", l).parse(s)

parse(""1989Dec31"", Locale.US)
Sun Dec 31 00:00:00 GMT 1989

parse(""1989Dec31"", Locale.UK)
Sun Dec 31 00:00:00 GMT 1989

parse(""1989Dec31"", Locale.CHINA)
java.text.ParseException: Unparseable date: ""1989Dec31""
  at java.text.DateFormat.parse(DateFormat.java:366)
  at .parse(<console>:18)
  ... 32 elided

parse(""1989Dec31"", Locale.GERMANY)
java.text.ParseException: Unparseable date: ""1989Dec31""
  at java.text.DateFormat.parse(DateFormat.java:366)
  at .parse(<console>:18)
  ... 32 elided
{code}

Where not otherwise specified, I believe all instances in the code should default to some fixed value, and that should probably be {{Locale.US}}. This matches the JVM's default, and specifies both language (""en"") and region (""US"") to remove ambiguity. This most closely matches what the current code behavior would be (unless default locale was changed), because it will currently default to ""en"".

This affects SQL date/time functions. At the moment, the only SQL function that lets the user specify language/country is ""sentences"", which is consistent with Hive.

It affects dates passed in the JSON API. 

It affects some strings rendered in the UI, potentially. Although this isn't a correctness issue, there may be an argument for not letting that vary (?)

It affects a bunch of instances where dates are formatted into strings for things like IDs or file names, which is far less likely to cause a problem, but worth making consistent.

The other occurrences are in tests.


The downside to this change is also its upside: the behavior doesn't depend on default JVM locale, but, also can't be affected by the default JVM locale. For example, if you wanted to parse some dates in a way that depended on an non-US locale (not just the format string) then it would no longer be possible. There's no means of specifying this, for example, in SQL functions for parsing dates. However, controlling this by globally changing the locale isn't exactly great either.

The purpose of this change is to make the current default behavior deterministic and fixed. PR coming.

CC [~hyukjin.kwon]",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-10-24 14:58:05.72,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 02 09:39:33 UTC 2016,,,,,0|i35azb:,9223372036854775807,,,,,,,,,,,,,"24/Oct/16 14:58;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15610","02/Nov/16 09:39;srowen;Issue resolved by pull request 15610
[https://github.com/apache/spark/pull/15610]",,,,,,,,,,,,,,,,,,,,,,
Decision Trees do not handle edge cases,SPARK-18036,13014079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,imatiach,sethah,sethah,20/Oct/16 22:40,24/Jan/17 18:25,15/Aug/18 23:03,24/Jan/17 18:25,,,,,,,,,,,,,,,,2.2.0,,,,,,ML,MLlib,,,,0,,,,,"Decision trees/GBT/RF do not handle edge cases such as constant features or empty features. For example:

{code}
val dt = new DecisionTreeRegressor()
val data = Seq(LabeledPoint(1.0, Vectors.dense(Array.empty[Double]))).toDF()
dt.fit(data)

java.lang.UnsupportedOperationException: empty.max
  at scala.collection.TraversableOnce$class.max(TraversableOnce.scala:229)
  at scala.collection.mutable.ArrayOps$ofInt.max(ArrayOps.scala:234)
  at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:207)
  at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:105)
  at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:93)
  at org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:46)
  at org.apache.spark.ml.Predictor.fit(Predictor.scala:90)
  ... 52 elided

{code}

as well as 

{code}
val dt = new DecisionTreeRegressor()
val data = Seq(LabeledPoint(1.0, Vectors.dense(0.0, 0.0, 0.0))).toDF()
dt.fit(data)

java.lang.UnsupportedOperationException: empty.maxBy
at scala.collection.TraversableOnce$class.maxBy(TraversableOnce.scala:236)
at scala.collection.SeqViewLike$AbstractTransformed.maxBy(SeqViewLike.scala:37)
at org.apache.spark.ml.tree.impl.RandomForest$.binsToBestSplit(RandomForest.scala:846)
{code}",,apachespark,dmcwhorter,imatiach,josephkb,sethah,WeichenXu123,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-12-20 19:29:02.938,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 24 18:25:31 UTC 2017,,,,,0|i356u7:,9223372036854775807,,,,,,,,,,,,,"20/Dec/16 19:29;imatiach;Weichen Xu, are you working on this issue or have you resolved it?  I am interested in investigating this bug.","21/Dec/16 06:10;WeichenXu123;Oh, I'm too busy recently to work on it, it would be great if you can resolve it, thanks! ","21/Dec/16 22:30;apachespark;User 'imatiach-msft' has created a pull request for this issue:
https://github.com/apache/spark/pull/16377","21/Dec/16 22:35;imatiach;Thanks, I've sent a pull request to fix this.","24/Jan/17 18:25;josephkb;Issue resolved by pull request 16377
[https://github.com/apache/spark/pull/16377]",,,,,,,,,,,,,,,,,,,
EMLDAOptimizer fails with ClassCastException on YARN,SPARK-17975,13012970,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,jvstein,jvstein,17/Oct/16 19:49,09/Feb/17 18:53,15/Aug/18 23:03,09/Feb/17 18:53,2.0.1,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,,MLlib,,,,,1,,,,,"I'm able to reproduce the error consistently with a 2000 record text file with each record having 1-5 terms and checkpointing enabled. It looks like the problem was introduced with the resolution for SPARK-13355.

The EdgeRDD class seems to be lying about it's type in a way that causes RDD.mapPartitionsWithIndex method to be unusable when it's referenced as an RDD of Edge elements.

{code}
val spark = SparkSession.builder.appName(""lda"").getOrCreate()
spark.sparkContext.setCheckpointDir(""hdfs:///tmp/checkpoints"")
val data: RDD[(Long, Vector)] = // snip
data.setName(""data"").cache()
val lda = new LDA
val optimizer = new EMLDAOptimizer
lda.setOptimizer(optimizer)
  .setK(10)
  .setMaxIterations(400)
  .setAlpha(-1)
  .setBeta(-1)
  .setCheckpointInterval(7)
val ldaModel = lda.run(data)
{code}

{noformat}
16/10/16 23:53:54 WARN TaskSetManager: Lost task 3.0 in stage 348.0 (TID 1225, server2.domain): java.lang.ClassCastException: scala.Tuple2 cannot be cast to org.apache.spark.graphx.Edge
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1$$anonfun$apply$1.apply(EdgeRDD.scala:107)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:107)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:105)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.graphx.EdgeRDD.compute(EdgeRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
{noformat}","Centos 6, CDH 5.7, Java 1.7u80",apachespark,gstvolvr,imatiach,josephkb,jvstein,michaelmalak,peng.meng@intel.com,timothylaurent,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14804,,,,,,,04/Jan/17 19:45;jvstein;docs.txt;https://issues.apache.org/jira/secure/attachment/12845603/docs.txt,,,1.0,,,,,,,,,,,,,,,,2016-12-22 16:32:18.709,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 09 18:49:07 UTC 2017,,,,,0|i3500f:,9223372036854775807,,,,,,,,,,,,,17/Oct/16 19:58;jvstein;This change resolves the issue for me: https://github.com/jvstein/spark/tree/lda-edgerdd,17/Oct/16 20:04;jvstein;Adding a link to another issue that seems to be related to EdgeRDD partition problems.,22/Dec/16 16:32;imatiach;Could you send a link to the repro dataset?  I could work on this issue but it looks like you have a fix already.  For any fixes we need tests to validate them.,"04/Jan/17 19:45;jvstein;Attaching vertical bar delimited documents (one per line).

With my quick fix, I'm seeing a lot more persisted RDDs on the ""Storage"" tab. I'm either not cleaning something up or there's another issue related to that.","06/Jan/17 06:00;imatiach;Thank you for sending the dataset, I'm working on this issue.","07/Jan/17 01:06;apachespark;User 'imatiach-msft' has created a pull request for this issue:
https://github.com/apache/spark/pull/16494",07/Jan/17 01:07;imatiach;I was able to reproduce the issue based on your dataset and I've made the suggested fix in the pull request.  I added a test case that had a similar issue to your dataset and could reproduce the error.  Thank you!,"26/Jan/17 01:42;josephkb;[SPARK-14804] was just fixed.  [~jvstein], do you have time to test master with your code to see if the bug you hit is fixed?  Thanks!",26/Jan/17 18:09;imatiach;[~josephkb] I was able to verify that this issue is now fixed after rebasing to master - can you please close the bug?  Thank you!,"09/Feb/17 18:49;josephkb;Will do, thanks!",,,,,,,,,,,,,,
ML/MLLIB: ChiSquareSelector based on Statistics.chiSqTest(RDD) is wrong ,SPARK-17870,13011295,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,peng.meng@intel.com,peng.meng@intel.com,peng.meng@intel.com,11/Oct/16 08:50,25/Oct/16 19:04,15/Aug/18 23:03,14/Oct/16 11:49,,,,,,,,,,,,,,,,2.1.0,,,,,,ML,MLlib,,,,0,,,,,"The method to count ChiSqureTestResult in mllib/feature/ChiSqSelector.scala  (line 233) is wrong.

For feature selection method ChiSquareSelector, it is based on the ChiSquareTestResult.statistic (ChiSqure value) to select the features. It select the features with the largest ChiSqure value. But the Degree of Freedom (df) of ChiSqure value is different in Statistics.chiSqTest(RDD), and for different df, you cannot base on ChiSqure value to select features.

Because of the wrong method to count ChiSquare value, the feature selection results are strange.
Take the test suite in ml/feature/ChiSqSelectorSuite.scala as an example:
If use selectKBest to select: the feature 3 will be selected.
If use selectFpr to select: feature 1 and 2 will be selected. 
This is strange. 

I use scikit learn to test the same data with the same parameters. 
When use selectKBest to select: feature 1 will be selected. 
When use selectFpr to select: feature 1 and 2 will be selected. 
This result is make sense. because the df of each feature in scikit learn is the same.

I plan to submit a PR for this problem.
 

 
",,apachespark,avulanov,peng.meng@intel.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17017,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-10-11 09:04:22.999,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 14 11:49:11 UTC 2016,,,,,0|i34psf:,9223372036854775807,,,,,,,,,,,,,"11/Oct/16 09:04;srowen;Oof, I'm pretty certain you're correct. You can rank on the p-value (which is a function of DoF) but not the raw statistic. It's an easy change at least because this is already computed. Can't believe I missed that.","11/Oct/16 09:48;peng.meng@intel.com;hi [~srowen], thanks very much for you quickly reply. 
yes,the p-value is better than raw statistic in this case, because p-value is count  based on DoF and raw statistic.
raw statistic is also popular for feature selection. The SelectKBest and SelectPercentile in scikit learn is based on raw statistic. 
The question here is we should use the same DoF like scikit learn to count ChiSquare value. 
For this JIRA, I propose to change the method to count ChiSquare value like what is done in scikit learn (change Statistics.chiSqTest(RDD)). 

Thanks very much.  ","11/Oct/16 11:13;srowen;I don't think the raw statistic can be directly compared here because the features do not have even nearly the same number of 'buckets', not necessarily. A given test statistic value is ""less remarkable"" when there are more DoF; what's high for a binary-valued feature may not be high at all for one taking on 100 values.

Does scikit really use the statistic? because you're also saying it does something that gives different results from ranking on the statistic.","11/Oct/16 11:44;peng.meng@intel.com;yes, the selectKBest and selectPercentile in scikit learn only use statistic.
Because the method to count ChiSquare value is different, the DoF of all features in scikit learn are the same. so it can do that.

The ChiSquare Value compute process is like this:
 suppose we have data:
X = [ 8 7 0
         0 9 6
         0 9 8
         8 9 5]
y = [0 1 1 2]T, this is the test suite data of ml/feature/ChiSquareSelectorSuite.scala
sci-kit learn to compute chiSquare value is like this:
first:
Y = [1 0 0
        0 1 0
        0  1 0
        0  0 1]
observed = Y'*X=
[8  7    0
 0  18 14
 8   9   5]
expected = 
[4 8.5 4.75
 8 17  9.5
 4  8.5  4.75]
_chisquare(ovserved, expected): to compute all features ChiSquare value, we can see all the DF of each feature is the same.

Bug for spark Statistics.chiSqTest(RDD), is use another method, for each feature, construct a contingency table. So the DF is different for each feature.  

For ""gives different results from ranking on the statistic"", this is because the parameters different.
For previous example, if use SelectKBest(2), the selected feature is the same as SelectFpr(0.2) in scikit learn


         
","11/Oct/16 11:54;srowen;I don't quite understand this example, can you point me to the source? the chi-squared statistic is indeed a function of observed and expected counts, but I'd expect those to be a vector of counts, one for each class. If you're saying that each row contains observed counts for one feature's classes, then yes in this particular construction each of them has the same number of classes (columns). But that isn't generally true; that can't be an assumption scikit makes? I bet I'm missing something.","11/Oct/16 12:02;peng.meng@intel.com;The scikit learn code is here: https://github.com/scikit-learn/scikit-learn/blob/412996f09b6756752dfd3736c306d46fca8f1aa1/sklearn/feature_selection/univariate_selection.py, line 422 for selectKBest, chiSquare compute is also on the same page.

For the last example, each row of X is a sample, it contain three features, totally 4 samples. Y is the label.
Thanks very much.  
","11/Oct/16 12:42;peng.meng@intel.com;https://github.com/apache/spark/pull/1484#issuecomment-51024568
Hi [~mengxr] and [~avulanov] , what do you think about this JIRA. ","11/Oct/16 19:01;srowen;OK I get it, they're doing different things really. The scikit version is computing the statistic for count-valued features vs categorical label, and the Spark version is computing this for categorical features vs categorical labels. Although the number of label classes is constant in both cases, the Spark computation would depend on the number of feature classes too. Yes, it does need to be changed in Spark.","11/Oct/16 20:20;avulanov;[`SelectKBest`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) works with ""a Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores"". According to what you observe, it uses pvalues for sorting of `chi2` outputs. Indeed, it is the case for all functions that return two arrays: https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_selection/univariate_selection.py#L331. Alternative, one case use raw `chi2` scores for sorting. She need to pass only the first array from `chi2` to `SelectKBest`. As far as I remember, using raw chi2 scores is default in Weka's [ChiSquaredAttributeEval](http://weka.sourceforge.net/doc.stable/weka/attributeSelection/ChiSquaredAttributeEval.html). So, I would not claim that either of approaches is incorrect. According to [Introduction to IR](http://nlp.stanford.edu/IR-book/html/htmledition/assessing-as-a-feature-selection-methodassessing-chi-square-as-a-feature-selection-method-1.html), there might be an issue with computing p-values because then chi2-test is used multiple times. Using plain chi2 values does not involve statistical test, so it might be treated as just some ranking with no statistical implications.","11/Oct/16 20:58;srowen;If the degrees of freedom are the same across the tests, then ranking on p-value or statistic should give the same ranking because the p-value is a monotonically decreasing function of the statistic. That's the case in what the scikit code is effectively doing because there are always (# label classes - 1) degrees of freedom. Really the p-value is the comparable quantity, but there's no point computing it in this case because it's just for ranking.

The Spark code performs a chi-squared test but applies it to answer a different question, where DOF is no longer the same; it's (# label classes - 1) * (# feature classes - 1) in the contingency table here. p-value is no longer always smaller when the statistic is larger. So it's necessary to actually use the p-values for what Spark is doing.","12/Oct/16 01:10;peng.meng@intel.com;hi [~avulanov], the question here is not use raw chi2 scores or pvalues, the question is if use raw chi2 scores, the DoF should be the same.   
""chi2-test is used multiple times"" is another problem.  According to (http://nlp.stanford.edu/IR-book/html/htmledition/assessing-as-a-feature-selection-methodassessing-chi-square-as-a-feature-selection-method-1.html),""whenever a statistical test is used multiple times, then the probability of getting at least one error increases."", this problem is partially solved by Select the p-values corresponding to Family-wise error rate (SelectFwe, SPARK-17645). Thanks very much.

Hi [~srowen], I totally agree with your comments. Based on the DoF is different in Spark ChiSquare value, we can use the p-values for Spark SelectKBest, and SelectPercentile. Thanks very much.

I will submit a pr for this.","12/Oct/16 01:48;apachespark;User 'mpjlu' has created a pull request for this issue:
https://github.com/apache/spark/pull/15444","14/Oct/16 11:49;srowen;Issue resolved by pull request 15444
[https://github.com/apache/spark/pull/15444]",,,,,,,,,,,
org.apache.spark.mllib.linalg.VectorUDT cannot be cast to org.apache.spark.sql.types.StructType,SPARK-17765,13009210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hyukjin.kwon,kxepal,kxepal,03/Oct/16 11:13,20/Oct/17 12:38,15/Aug/18 23:03,21/Nov/16 19:12,1.6.1,2.0.0,,,,,,,,,,,,,,2.1.0,,,,,,MLlib,PySpark,SQL,,,0,,,,,"The issue in subject happens on attempt to transform DataFrame in Parquet format into ORC while DF contains SparseVector/DenseVector data.

In [sources|https://github.com/apache/spark/blob/v1.6.1/mllib/src/main/scala/org/apache/spark/mllib/linalg/Vectors.scala#L192] it looks like that there shouldn't be any serialization issues, but they happens.

{code}
In[4] pqtdf = hqlctx.read.parquet(pqt_feature)

In[5] pqtdf.take(1)
Out[5]: [Row(foo=u'abc, bar=SparseVector(100, {74: 1.0}))]

In[6]: pqtdf.write.format('orc').save('/tmp/orc')
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-5-57e68fd0c5cb> in <module>()
----> pqtdf.write.format('orc').save('/tmp/orc')

/usr/local/share/spark/python/pyspark/sql/readwriter.pyc in save(self, path, format, mode, partitionBy, **options)
    395             self._jwrite.save()
    396         else:
--> 397             self._jwrite.save(path)
    398 
    399     @since(1.4)

/usr/local/lib/python2.7/site-packages/py4j/java_gateway.pyc in __call__(self, *args)
    811         answer = self.gateway_client.send_command(command)
    812         return_value = get_return_value(
--> 813             answer, self.gateway_client, self.target_id, self.name)
    814 
    815         for temp_arg in temp_args:

/usr/local/share/spark/python/pyspark/sql/utils.pyc in deco(*a, **kw)
     43     def deco(*a, **kw):
     44         try:
---> 45             return f(*a, **kw)
     46         except py4j.protocol.Py4JJavaError as e:
     47             s = e.java_exception.toString()

/usr/local/lib/python2.7/site-packages/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name)
    306                 raise Py4JJavaError(
    307                     ""An error occurred while calling {0}{1}{2}.\n"".
--> 308                     format(target_id, ""."", name), value)
    309             else:
    310                 raise Py4JError(

Py4JJavaError: An error occurred while calling o62.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:156)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:70)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:130)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:256)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:139)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:209)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 185, node123.example.com): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:272)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: org.apache.spark.mllib.linalg.VectorUDT cannot be cast to org.apache.spark.sql.types.StructType
	at org.apache.spark.sql.hive.HiveInspectors$class.wrap(HiveInspectors.scala:554)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrap(OrcRelation.scala:66)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrapOrcStruct(OrcRelation.scala:128)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.writeInternal(OrcRelation.scala:139)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:264)
	... 8 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:150)
	... 27 more
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:272)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	... 1 more
Caused by: java.lang.ClassCastException: org.apache.spark.mllib.linalg.VectorUDT cannot be cast to org.apache.spark.sql.types.StructType
	at org.apache.spark.sql.hive.HiveInspectors$class.wrap(HiveInspectors.scala:554)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrap(OrcRelation.scala:66)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.wrapOrcStruct(OrcRelation.scala:128)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.writeInternal(OrcRelation.scala:139)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:264)
	... 8 more
{code}",,apachespark,hyukjin.kwon,kxepal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-10-05 10:23:05.687,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 05 10:45:05 UTC 2016,,,,,0|i34cxr:,9223372036854775807,,,,,,,,,,,,,05/Oct/16 10:23;hyukjin.kwon;It seems this one can be quickly fixed. Let me please submit a PR.,05/Oct/16 10:29;kxepal;[~hyukjin.kwon] Please do! Thank you (:,"05/Oct/16 10:45;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/15361",,,,,,,,,,,,,,,,,,,,,
Erroneous computation in multiplication of transposed SparseMatrix with SparseVector,SPARK-17721,13008479,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,bfruergaard,bfruergaard,bfruergaard,29/Sep/16 09:22,02/Oct/16 02:30,15/Aug/18 23:03,02/Oct/16 02:29,1.4.1,1.5.2,1.6.2,2.0.0,,,,,,,,,,,,1.5.3,1.6.3,2.0.2,2.1.0,,,ML,MLlib,,,,0,correctness,,,,"There is a bug in how a transposed SparseMatrix (isTransposed=true) does multiplication with a SparseVector. The bug is present (for v. > 2.0.0) in both org.apache.spark.mllib.linalg.BLAS (mllib) and org.apache.spark.ml.linalg.BLAS (mllib-local) in the private gemv method with signature:
bq. gemv(alpha: Double, A: SparseMatrix, x: SparseVector, beta: Double, y: DenseVector).

This bug can be verified by running the following snippet in a Spark shell (here using v1.6.1):
{code:java}
import com.holdenkarau.spark.testing.SharedSparkContext
import org.apache.spark.mllib.linalg._

val A = Matrices.dense(3, 2, Array[Double](0, 2, 1, 1, 2, 0)).asInstanceOf[DenseMatrix].toSparse.transpose
val b = Vectors.sparse(3, Seq[(Int, Double)]((1, 2), (2, 1))).asInstanceOf[SparseVector]

A.multiply(b)
A.multiply(b.toDense)
{code}
The first multiply with the SparseMatrix returns the incorrect result:
{code:java}
org.apache.spark.mllib.linalg.DenseVector = [5.0,0.0]
{code}
whereas the correct result is returned by the second multiply:
{code:java}
org.apache.spark.mllib.linalg.DenseVector = [5.0,4.0]
{code}",Verified on OS X with Spark 1.6.1 and on Databricks running Spark 1.6.1,apachespark,bfruergaard,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-09-29 10:26:56.663,,false,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 02 02:29:43 UTC 2016,,,,,0|i348fz:,9223372036854775807,,,,,,1.5.3,1.6.3,2.0.2,2.1.0,,,,29/Sep/16 09:34;bfruergaard;I already have a suggested fix in the pipeline; expect a PR soon,29/Sep/16 10:26;srowen;Am I missing something or should this result in an error? A is 3x2 and b is 3x1 and so A x b is invalid. ,"29/Sep/16 10:59;bfruergaard;The 3x2 matrix gets transposed, so it becomes a 2x3 matrix which is multiplied with the 3x1, so it is valid. Also note, that if I construct an equivalent SparseMatrix that is 2x3 directly (i.e., not the transpose of the 3x2 matrix), then it works. There only exists an error when the matrix I'm calling multiply on is transposed.","29/Sep/16 11:27;apachespark;User 'bwahlgreen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15296",29/Sep/16 11:36;srowen;Oh right I somehow didn't read the .transpose.,29/Sep/16 21:02;josephkb;Noting here: We should audit MLlib for uses of this multiply to see what algorithms it might have affected.  I'm hoping the effects will have been minimal.,"29/Sep/16 23:13;josephkb;OK I did an audit, and this will not have affected any algorithms in 2.0 or before.  But it will affect sparse logistic regression in 2.1!  Thanks for finding this bug.

If users have called Matrix.multiply directly, then they could be affected.","30/Sep/16 09:26;apachespark;User 'bwahlgreen' has created a pull request for this issue:
https://github.com/apache/spark/pull/15311","02/Oct/16 02:29;josephkb;Issue resolved by pull request 15311
[https://github.com/apache/spark/pull/15311]",,,,,,,,,,,,,,,
SparseVector __getitem__ should follow __getitem__ contract,SPARK-17587,13005785,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,18/Sep/16 20:18,04/Oct/16 12:43,15/Aug/18 23:03,04/Oct/16 00:58,1.6.2,2.0.0,,,,,,,,,,,,,,2.0.2,2.1.0,,,,,ML,MLlib,PySpark,,,0,,,,,"According to {{\_\_getitem\_\_}} [contract|https://docs.python.org/3/reference/datamodel.html#object.__getitem__]:

{quote}
if of a value outside the set of indexes for the sequence (after any special interpretation of negative values), {{IndexError}} should be raised.
{quote}

This required for example for correct iteration over the structure.

Right now it throws {{ValueError}} what results in a quite confusing behavior when attempt to iterate over a vector results in a {{ValueError}} due to unterminated iteration:

{code}

In [1]: from pyspark.mllib.linalg import SparseVector

In [2]: list(SparseVector(4, [0], [0]))
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-147f3bb0a47d> in <module>()
----> 1 list(SparseVector(4, [0], [0]))

/opt/spark-2.0/python/pyspark/mllib/linalg/__init__.py in __getitem__(self, index)
    803 
    804         if index >= self.size or index < -self.size:
--> 805             raise ValueError(""Index %d out of bounds."" % index)
    806         if index < 0:
    807             index += self.size

ValueError: Index 4 out of bounds.
{code}

",,apachespark,josephkb,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-09-18 20:24:10.52,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 04 12:43:37 UTC 2016,,,,,0|i33ruv:,9223372036854775807,,,,,,2.0.2,2.1.0,,,,,,"18/Sep/16 20:24;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/15144","04/Oct/16 00:55;josephkb;I set the target versions to 2.1.0 and 2.0.2 (since it will miss 2.0.1).  I figure it's probably OK not to change this in 1.6, but let me know if you disagree.  I'm hesitant about 1.6 since this is sort of a breaking API change, as you pointed out.","04/Oct/16 00:58;josephkb;Issue resolved by pull request 15144
[https://github.com/apache/spark/pull/15144]",04/Oct/16 12:43;zero323;I would probably go with 2.1.0 alone and definitely wouldn't bother with 1.6.,,,,,,,,,,,,,,,,,,,,
PeriodicGraphCheckpointer did not persist edges as expected in some cases,SPARK-17559,13005336,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dingding,ding,ding,16/Sep/16 04:49,04/Oct/16 17:07,15/Aug/18 23:03,04/Oct/16 07:00,,,,,,,,,,,,,,,,2.0.2,2.1.0,,,,,MLlib,,,,,0,,,,,"When use PeriodicGraphCheckpointer to persist graph, sometimes the edge isn't persisted. As currently only when vertices's storage level is none, graph is persisted. However there is a chance vertices's storage level is not none while edges's is none. Eg. graph created by a outerJoinVertices operation, vertices is automatically cached while edges is not. In this way, edges will not be persisted if we use PeriodicGraphCheckpointer do persist.

See below minimum example:
   val graphCheckpointer = new PeriodicGraphCheckpointer[Array[String], Int](2, sc)
    val users = sc.textFile(""data/graphx/users.txt"")
      .map(line => line.split("","")).map(parts => (parts.head.toLong, parts.tail))
    val followerGraph = GraphLoader.edgeListFile(sc, ""data/graphx/followers.txt"")

    val graph = followerGraph.outerJoinVertices(users) {
      case (uid, deg, Some(attrList)) => attrList
      case (uid, deg, None) => Array.empty[String]
    }
    graphCheckpointer.update(graph)    ",,apachespark,ding,josephkb,michaelmalak,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-09-16 04:50:06.178,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 04 17:07:04 UTC 2016,,,,,0|i33p3b:,9223372036854775807,,,,,,2.0.2,2.1.0,,,,,,"16/Sep/16 04:50;apachespark;User 'dding3' has created a pull request for this issue:
https://github.com/apache/spark/pull/15116","17/Sep/16 01:22;apachespark;User 'dding3' has created a pull request for this issue:
https://github.com/apache/spark/pull/15124","04/Oct/16 07:00;josephkb;Issue resolved by pull request 15124
[https://github.com/apache/spark/pull/15124]","04/Oct/16 07:02;josephkb;Did I use the correct JIRA username for the ""Assignee?""  I tried ""ding"" but it wasn't working with JIRA admin, but ""dingding"" worked and has the same email.","04/Oct/16 17:07;ding;Yes, it's the correct username. Thank you for your reviewing.",,,,,,,,,,,,,,,,,,,
Word2VecModel.findSynonyms can spuriously reject the best match when invoked with a vector,SPARK-17548,13005020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,willbenton,willbenton,willbenton,14/Sep/16 22:32,17/Sep/16 11:51,15/Aug/18 23:03,17/Sep/16 11:50,1.4.1,1.5.2,1.6.2,2.0.0,,,,,,,,,,,,2.0.1,2.1.0,,,,,MLlib,,,,,0,,,,,"The `findSynonyms` method in `Word2VecModel` currently rejects the best match a priori. When `findSynonyms` is invoked with a word, the best match is almost certain to be that word, but `findSynonyms` can also be invoked with a vector, which might not correspond to any of the words in the model's vocabulary.  In the latter case, rejecting the best match is spurious.",any,apachespark,willbenton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-09-14 22:34:06.332,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 17 11:50:26 UTC 2016,,,,,0|i33n53:,9223372036854775807,,,,,,,,,,,,,"14/Sep/16 22:34;apachespark;User 'willb' has created a pull request for this issue:
https://github.com/apache/spark/pull/15105","17/Sep/16 11:50;srowen;Issue resolved by pull request 15105
[https://github.com/apache/spark/pull/15105]",,,,,,,,,,,,,,,,,,,,,,
IsotonicRegression takes non-polynomial time for some inputs,SPARK-17455,13003666,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nseggert,nseggert,nseggert,08/Sep/16 19:07,23/Jan/17 21:22,15/Aug/18 23:03,23/Jan/17 21:21,1.3.1,1.4.1,1.5.2,1.6.2,2.0.0,,,,,,,,,,,2.2.0,,,,,,MLlib,,,,,0,,,,,"The Pool Adjacent Violators Algorithm (PAVA) implementation that's currently in MLlib can take O(N!) time for certain inputs, when it should have worst-case complexity of O(N^2).

To reproduce this, I pulled the private method poolAdjacentViolators out of mllib.regression.IsotonicRegression and into a benchmarking harness.

Given this input
{code}
val x = (1 to length).toArray.map(_.toDouble)
val y = x.reverse.zipWithIndex.map{ case (yi, i) => if (i % 2 == 1) yi - 1.5 else yi}
val w = Array.fill(length)(1d)

val input: Array[(Double, Double, Double)] = (y zip x zip w) map{ case ((y, x), w) => (y, x, w)}
{code}

I vary the length of the input to get these timings:

|| Input Length || Time (us) ||
| 100 | 1.35 |
| 200 | 3.14 | 
| 400 | 116.10 |
| 800 | 2134225.90 |

(tests were performed using https://github.com/sirthias/scala-benchmarking-template)

I can also confirm that I run into this issue on a real dataset I'm working on when trying to calibrate random forest probability output. Some partitions take > 12 hours to run. This isn't a skew issue, since the largest partitions finish in minutes. I can only assume that some partitions cause something approaching this worst-case complexity.

I'm working on a patch that borrows the implementation that is used in scikit-learn and the R ""iso"" package, both of which handle this particular input in linear time and are quadratic in the worst case.",,apachespark,dougb,josephkb,nseggert,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-09-08 22:02:07.57,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 23 21:21:30 UTC 2017,,,,,0|i33es7:,9223372036854775807,josephkb,,,,,2.2.0,,,,,,,"08/Sep/16 22:02;apachespark;User 'neggert' has created a pull request for this issue:
https://github.com/apache/spark/pull/15018",05/Jan/17 23:05;josephkb;I'm changing the target version since the final patch is pretty involved.  Let's not backport it.,05/Jan/17 23:07;nseggert;Fine by me,"23/Jan/17 21:21;josephkb;Issue resolved by pull request 15018
[https://github.com/apache/spark/pull/15018]",,,,,,,,,,,,,,,,,,,,
fix MultivariateOnlineSummerizer.numNonZeros,SPARK-17363,13002126,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,WeichenXu123,WeichenXu123,WeichenXu123,01/Sep/16 17:15,18/Nov/16 21:30,15/Aug/18 23:03,03/Sep/16 17:06,2.1.0,,,,,,,,,,,,,,,2.1.0,,,,,,ML,MLlib,,,,0,,,,,"The MultivariantOnlineSummerizer.numNonZeros method is wrong.

it should return the nnz array, not weightSum array.",,apachespark,WeichenXu123,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-09-01 17:20:06.364,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 03 17:06:39 UTC 2016,,,,,0|i335an:,9223372036854775807,,,,,,,,,,,,,"01/Sep/16 17:20;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/14923",03/Sep/16 17:06;srowen;Resolved by https://github.com/apache/spark/pull/14923,,,,,,,,,,,,,,,,,,,,,,
fix MultivariantOnlineSummerizer.numNonZeros,SPARK-17362,13002125,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,WeichenXu123,WeichenXu123,01/Sep/16 17:14,03/Sep/16 08:57,15/Aug/18 23:03,01/Sep/16 18:01,2.0.1,2.1.0,,,,,,,,,,,,,,,,,,,,ML,MLlib,,,,0,,,,,"The MultivariantOnlineSummerizer.numNonZeros method is wrong.

it should return the nnz array, not weightSum array.",,WeichenXu123,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,SPARK-17363,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,9223372036854775807,,,2016-09-01 17:14:43.0,,,,,0|i335af:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Comparing Vector in relative tolerance or absolute tolerance in UnitTests error ,SPARK-17207,12999507,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,peng.meng@intel.com,peng.meng@intel.com,peng.meng@intel.com,24/Aug/16 05:59,08/Oct/16 09:18,15/Aug/18 23:03,26/Aug/16 18:54,,,,,,,,,,,,,,,,2.1.0,,,,,,ML,MLlib,,,,0,,,,,"The result of compare two vectors using UnitTests (org.apache.spark.mllib.util.TestingUtils) is not right sometime.
For example:
val a = Vectors.dense(Arrary(1.0, 2.0))
val b = Vectors.zeros(0)
a ~== b absTol 1e-1 // the result is true. 

",,apachespark,dbtsai,peng.meng@intel.com,qhuang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-08-24 07:22:54.507,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 26 18:54:31 UTC 2016,,,,,0|i32p5j:,9223372036854775807,,,,,,,,,,,,,"24/Aug/16 06:14;peng.meng@intel.com;This is caused by two Vector zip problem:
    def absTol(eps: Double): CompareVectorRightSide = CompareVectorRightSide(
      (x: Vector, y: Vector, eps: Double) => {
        x.toArray.zip(y.toArray).forall(x => x._1 ~= x._2 absTol eps)  
      }, x, eps, ABS_TOL_MSG)

// forall () always return true if x or y is zero element Vector

val a = Vectors.dense(Array(1.0, 2.0))
val b = Vectors.dense(Array(1.0))
a ~== b absTol 1e-1 // this also return true. Because,  a.toArray.zip(b.toArray) = Array((1.0, 1.0))

","24/Aug/16 07:22;qhuang;So we should check if their length is equal first?
cc [~dbtsai]","24/Aug/16 08:54;srowen;Oof, yeah the array lengths have to be checked for vectors and matrices, and for absolute and relative tolerance. Good catch.","24/Aug/16 08:59;peng.meng@intel.com;Thanks Owen, I am testing the code with array length check. will submit PR soon. ","24/Aug/16 10:44;apachespark;User 'mpjlu' has created a pull request for this issue:
https://github.com/apache/spark/pull/14785","25/Aug/16 13:41;peng.meng@intel.com;Hi, org.apache.spark.ml.feature.CountVectorizerSuite.CountVectorizer can not pass this PR.
Because org.scalatest.exceptions.TestFailedException: Expected (2,[0,1],[1.0,1.0]) and (3,[0,1],[1.0,1.0]) to be within 1.0E-14 using absolute tolerance for all elements.
The size of this two vectors is not the same, so it throws exception.
Should we fix this PR or fix CountVectorizerSuite?  ","25/Aug/16 13:46;srowen;Sounds like the test needs to be fixed. Yes, you would fix it in this PR, assuming it's clearly just a latent bug in the test uncovered by this fix. I'd look into it more but not sure which test you refer to here.

(Also I see that the error message is a little misleading, because the cause of the failure isn't that the elements mismatch but the sizes. Maybe we can throw a more specific error here.)","25/Aug/16 14:31;peng.meng@intel.com;This is the bug information: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/64418/testReport/org.apache.spark.ml.feature/CountVectorizerSuite/CountVectorizer_vocabSize_and_minDF/
For the following case: 
a = Array(1.0, 1.0)
b = Array(1.0, 1.0, 0)
a ~== b absTol 1e-1 the result should be true or false. maybe this can discuss again.  I tend to this should be false.
Another case: 
a = Array (1.0, 1.0)
b = Array (1.0, 1.0, 2.0)
a ~== b absTol 1e-1 should be true or false? 
I tend to both this two cases should be false. ","25/Aug/16 14:37;srowen;I think vectors of different dimensions can never be nearly-equal. In the first case, there is no third dimension to compare on, because one lacks a third dimension at all; it's not implicitly 0, not even in the sparse case if the dimension is < 3. Yes, both should be false.

Yes, in the second part of that test case, it expects counts over just two words, but the expected output is still of dimension 3. I think you just make the dimension 2 in each of the sparse vectors.","25/Aug/16 14:42;peng.meng@intel.com;Ok, thanks. I will fix CountVectorizerSuite test error in this PR. ","26/Aug/16 18:54;dbtsai;Issue resolved by pull request 14785
[https://github.com/apache/spark/pull/14785]",,,,,,,,,,,,,
"""Pipeline guide"" link is broken in MLlib Guide main page",SPARK-17202,12999240,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Duplicate,,kotobot,kotobot,23/Aug/16 14:45,23/Aug/16 15:21,15/Aug/18 23:03,23/Aug/16 15:21,2.0.0,,,,,,,,,,,,,,,,,,,,,Documentation,MLlib,,,,0,,,,,"Steps to reproduce:
1) Check http://spark.apache.org/docs/latest/ml-guide.html 
2) Link in sentence ""See the Pipelines guide for details"" is broken, it points to https://spark.apache.org/docs/latest/ml-pipeline.md

Expected result: ""Pipeline guide"" link should point to https://spark.apache.org/docs/latest/ml-pipeline.html


",,kotobot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16761,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-08-23 15:21:20.092,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 23 15:21:20 UTC 2016,,,,,0|i32nif:,9223372036854775807,,,,,,,,,,,,,"23/Aug/16 15:21;srowen;Yep, though this was already fixed in master. The next release docs would contain the fix.",,,,,,,,,,,,,,,,,,,,,,,
SparseVectors.apply and SparseVectors.toArray have different returns when creating with a illegal indices,SPARK-17130,12998199,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Duplicate,,adamzjw,adamzjw,18/Aug/16 13:38,18/Aug/16 17:06,15/Aug/18 23:03,18/Aug/16 16:52,1.6.2,2.0.0,,,,,,,,,,,,,,,,,,,,ML,MLlib,,,,0,,,,,"One of my colleagues ran into a bug of SparseVectors. He called the Vectors.sparse(size: Int, indices: Array[Int], values: Array[Double]) without noticing that the indices are assumed to be ordered.

The vector he created has all value of 0.0 (without any warning), if we try to get value via apply method. However, SparseVector.toArray will generates a array using a method that is order insensitive. Hence, you will get a 0.0 when you call apply method, while you can get correct result using toArray or toDense method. The result of SparseVector.toArray is actually misleading.



It could be safer if there is a validation of indices in the constructor or at least make the returns of apply method and toArray method the same.",spark 1.6.1 + scala,adamzjw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16965,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-08-18 13:49:06.955,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 18 17:06:33 UTC 2016,,,,,0|i32h33:,9223372036854775807,,,,,,,,,,,,,"18/Aug/16 13:49;srowen;Yeah, didn't you just comment on https://github.com/apache/spark/pull/14555 ? that's already being fixed there. This is a duplicate of SPARK-16965","18/Aug/16 16:43;adamzjw;Yep, I wrote a comment there but I deleted since I'm not sure whether they are fixing this problem together.

The problem mentioned at SPARK-16965 is more about negative indices. Are they also concerning about unordered indices?","18/Aug/16 16:52;srowen;Oh yeah but along the way the validation is also all moved into the constructor. That was actually the last comment on the PR -- sorry thought that's what you saw and were even responding to. See https://github.com/apache/spark/pull/14555/files#diff-84f492e3a9c1febe833709960069b1b2R553  I think the issue was that Vectors.sparse does validate but new SparseVector() does not? well, both will be validated now. I'll say this is a duplicate because we should definitely resolve both at once.",18/Aug/16 17:06;adamzjw;Thanks for posting the code. The problem is solved clearly.,,,,,,,,,,,,,,,,,,,,
Fix bound checking for SparseVector,SPARK-16965,12995844,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zjffdu,zjffdu,zjffdu,09/Aug/16 05:29,19/Aug/16 11:38,15/Aug/18 23:03,19/Aug/16 11:38,2.0.0,,,,,,,,,,,,,,,2.1.0,,,,,,MLlib,PySpark,,,,0,,,,,"There's several issues in the bound checking of SparseVector

1. In scala, miss negative index checking and different bound checking is scattered in several places. Should put them in one place
2. In python, miss low/upper bound checking of indices. ",,apachespark,kiszk,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-08-09 05:33:04.4,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 19 11:38:29 UTC 2016,,,,,0|i322k7:,9223372036854775807,,,,,,2.1.0,,,,,,,"09/Aug/16 05:33;apachespark;User 'zjffdu' has created a pull request for this issue:
https://github.com/apache/spark/pull/14555","19/Aug/16 11:38;srowen;Issue resolved by pull request 14555
[https://github.com/apache/spark/pull/14555]",,,,,,,,,,,,,,,,,,,,,,
"Improve ANN training, add training data persist if needed",SPARK-16880,12994627,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,WeichenXu123,WeichenXu123,WeichenXu123,03/Aug/16 16:18,04/Aug/16 20:42,15/Aug/18 23:03,04/Aug/16 20:41,,,,,,,,,,,,,,,,2.0.1,2.1.0,,,,,ML,MLlib,,,,0,,,,,"The ANN layer training does not persist input data RDD,
so that it may cause overhead cost if the RDD need to compute from lineage.",,apachespark,michaelmalak,WeichenXu123,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-08-03 16:28:07.997,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 04 20:41:54 UTC 2016,,,,,0|i31v1r:,9223372036854775807,,,,,,,,,,,,,"03/Aug/16 16:28;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/14483","04/Aug/16 20:41;srowen;Issue resolved by pull request 14483
[https://github.com/apache/spark/pull/14483]",,,,,,,,,,,,,,,,,,,,,,
fix latex formula syntax error in mllib,SPARK-16600,12990320,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,WeichenXu123,WeichenXu123,WeichenXu123,18/Jul/16 09:29,20/Jul/16 02:31,15/Aug/18 23:03,19/Jul/16 11:08,,,,,,,,,,,,,,,,2.0.0,,,,,,Documentation,MLlib,,,,0,,,,,"I found several place the latex formula use
\partial\xx
it should be 
\partial xx
",,apachespark,WeichenXu123,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-07-18 09:33:06.061,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 19 11:08:10 UTC 2016,,,,,0|i314hb:,9223372036854775807,,,,,,,,,,,,,"18/Jul/16 09:33;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/14246","19/Jul/16 11:08;srowen;Issue resolved by pull request 14246
[https://github.com/apache/spark/pull/14246]",,,,,,,,,,,,,,,,,,,,,,
Bug in SparseMatrix multiplication with SparseVector,SPARK-16566,12989712,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,wilson.lauw,wilson.lauw,15/Jul/16 08:31,22/Aug/16 23:22,15/Aug/18 23:03,22/Aug/16 23:22,1.6.2,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"In the org.apache.spark.mllib.linalg.BLAS.scala, the multiplication between SparseMatrix (sm) and SparseVector (sv) when sm is not transposed assume that the indices is sorted, but there is no validation to make sure that is the case, making the result returned wrongly.

This can be replicated simply by using spark-shell and entering these commands:

import org.apache.spark.mllib.linalg.SparseMatrix
import org.apache.spark.mllib.linalg.SparseVector
import org.apache.spark.mllib.linalg.DenseVector
import scala.collection.mutable.ArrayBuffer

val vectorIndices = Array(3,2)
val vectorValues = Array(0.1,0.2)
val size = 4

val sm = new SparseMatrix(size, size, Array(0, 0, 0, 1, 1), Array(0), Array(1.0))
val dm = sm.toDense
val sv = new SparseVector(size, vectorIndices, vectorValues)
val dv = new DenseVector(s.toArray)

sm.multiply(dv) == sm.multiply(sv)

sm.multiply(dv)
sm.multiply(sv)",,apachespark,josephkb,kiszk,wilson.lauw,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14707,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-07-15 08:37:05.324,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 22 23:22:05 UTC 2016,,,,,0|i310q7:,9223372036854775807,,,,,,,,,,,,,"15/Jul/16 08:37;apachespark;User 'wilson-lauw' has created a pull request for this issue:
https://github.com/apache/spark/pull/14219",15/Jul/16 08:45;srowen;This has been discussed before. I think the issue is that validating it everywhere is significant overhead.,"15/Jul/16 08:50;wilson.lauw;But for this case, the result is returned wrongly without any exception/warning, which can cause users to assume everything is alright, while actually the result is not correct.",15/Jul/16 09:18;srowen;Adding it where you did could be OK; I think previously the question had been whether every vector creation should validate it.,22/Aug/16 23:22;josephkb;Linking existing JIRA which this one is duplicating.  Could you please work under the other JIRA instead of this one?  Thanks!,,,,,,,,,,,,,,,,,,,
Potential numerical problem in MultivariateOnlineSummarizer min/max,SPARK-16561,12989660,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WeichenXu123,WeichenXu123,WeichenXu123,15/Jul/16 04:27,18/Nov/16 22:23,15/Aug/18 23:03,23/Jul/16 11:32,2.0.0,,,,,,,,,,,,,,,2.1.0,,,,,,MLlib,,,,,0,,,,,"In `MultivariateOnlineSummarizer` min/max method, 
use judgement ""nnz(i) < weightSum"", it will cause some numerial problem
and make result unstable.

for example,
add two vector:
[10, -10] with weight 1e10
[0, 0] with weight 1e-10

using MultivariateOnlineSummarizer.min/max we will get
minVector = [10, -10]
maxVector = [10, -10]

but the right result should be
minVector = [0, -10]
maxVector = [10, 0]

The bug reason is that
(1e10 + 1e-10) == 1e10 (Double type)
because of the floating rounding.
and different accumulating or merging order may cause different result,
such as:

[10, -10] with weight 1e10
[0, 0] with weight 1e-7
....
(100 lines data [0, 0] with weight 1e-7)

using the input data order listed above, we will get the result:
minVector = [10, -10]
maxVector = [10, -10]

but if the input data order is as following:

[0, 0] with weight 1e-7
....
(100 lines data [0, 0] with weight 1e-7)
[10, -10] with weight 1e10

than it the result will be:
minVector = [0, -10]
maxVector = [10, 0]

that's because:
1e10 + 1e-7 + ... + 1e-7(add 100 times) == 1e10  (Double type)
but
1e-7 + ... + 1e-7(add 100 times) + 1e10 = 1.000000000000001E10 != 1e10  (Double type)



",,apachespark,WeichenXu123,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-07-15 04:32:03.975,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 23 11:32:49 UTC 2016,,,,,0|i310en:,9223372036854775807,,,,,,,,,,,,,"15/Jul/16 04:32;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/14216","23/Jul/16 11:32;srowen;Issue resolved by pull request 14216
[https://github.com/apache/spark/pull/14216]",,,,,,,,,,,,,,,,,,,,,,
examples/mllib/LDAExample should use MLVector instead of MLlib Vector,SPARK-16558,12989622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yinxusen,yinxusen,yinxusen,14/Jul/16 23:40,02/Aug/16 14:33,15/Aug/18 23:03,02/Aug/16 14:33,,,,,,,,,,,,,,,,2.0.1,2.1.0,,,,,Examples,MLlib,,,,0,,,,,"mllib.LDAExample uses ML pipeline and MLlib LDA algorithm. The former transforms original data into MLVector format, while the latter uses MLlibVector format.",,apachespark,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-07-14 23:43:04.716,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 23:43:04 UTC 2016,,,,,0|i31067:,9223372036854775807,,,,,,,,,,,,,"14/Jul/16 23:43;apachespark;User 'yinxusen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14212",,,,,,,,,,,,,,,,,,,,,,,
BisectingKMeans Algorithm failing with java.util.NoSuchElementException: key not found,SPARK-16473,12988121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imatiach,alokob.be@gmail.com,alokob.be@gmail.com,10/Jul/16 11:09,05/Oct/17 21:04,15/Aug/18 23:03,24/Jan/17 11:30,1.6.1,2.0.0,,,,,,,,,,,,,,2.1.1,2.2.0,,,,,ML,MLlib,,,,0,,,,,"Hello , 

I am using apache spark 1.6.1. 
I am executing bisecting k means algorithm on a specific dataset .
Dataset details :- 
K=100,
input vector =100K*100k
Memory assigned 16GB per node ,
number of nodes =2.

 Till K=75 it os working fine , but when I set k=100 , it fails with java.util.NoSuchElementException: key not found. 

*I suspect it is failing because of lack of some resources , but somehow exception does not convey anything as why this spark job failed.* 

Please can someone point me to root cause of this exception , why it is failing. 

This is the exception stack-trace:- 
{code}
java.util.NoSuchElementException: key not found: 166 
        at scala.collection.MapLike$class.default(MapLike.scala:228) 
        at scala.collection.AbstractMap.default(Map.scala:58) 
        at scala.collection.MapLike$class.apply(MapLike.scala:141) 
        at scala.collection.AbstractMap.apply(Map.scala:58) 
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1$$anonfun$2.apply$mcDJ$sp(BisectingKMeans.scala:338)
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1$$anonfun$2.apply(BisectingKMeans.scala:337)
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1$$anonfun$2.apply(BisectingKMeans.scala:337)
        at scala.collection.TraversableOnce$$anonfun$minBy$1.apply(TraversableOnce.scala:231) 
        at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111) 
        at scala.collection.immutable.List.foldLeft(List.scala:84) 
        at scala.collection.LinearSeqOptimized$class.reduceLeft(LinearSeqOptimized.scala:125) 
        at scala.collection.immutable.List.reduceLeft(List.scala:84) 
        at scala.collection.TraversableOnce$class.minBy(TraversableOnce.scala:231) 
        at scala.collection.AbstractTraversable.minBy(Traversable.scala:105) 
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1.apply(BisectingKMeans.scala:337) 
        at org.apache.spark.mllib.clustering.BisectingKMeans$$anonfun$org$apache$spark$mllib$clustering$BisectingKMeans$$updateAssignments$1.apply(BisectingKMeans.scala:334) 
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) 
        at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:389) 
{code}

Issue is that , it is failing but not giving any explicit message as to why it failed.",AWS EC2 linux instance. ,alokob.be@gmail.com,apachespark,imatiach,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-12-19 22:42:03.377,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 05 21:04:39 UTC 2017,,,,,0|i30ref:,9223372036854775807,,,,,,2.1.1,2.2.0,,,,,,"25/Jul/16 11:09;alokob.be@gmail.com;After reducing maxIterations for BisectingKMeans it finished successfully , does that mean maxIterations are data-set specific ?","27/Oct/16 09:35;alokob.be@gmail.com;This issue continue to exist for spark 2.0 ""ml"" library. Is this feature going to get any support?","08/Nov/16 10:28;alokob.be@gmail.com;[~josephkb] , I have just found that you have worked on mllib , please can you help me out getting input about this issue.",19/Dec/16 22:42;imatiach;I'm interested in looking into this issue.  Would it be possible to get a dataset (either the original one or some mock dataset) which can be used to reproduce this error?,"20/Dec/16 05:36;alokob.be@gmail.com;[~imatiach] , thanks for showing interest in this issue. I will try to share the dataset with you , please can you suggest where should I share it ? should I share it through github? is it fine?

Also , I have tried to diagnose this issue on my own , from my analysis it looks like , it is failing if it tries to bisect a node which does not have any children. I also have added a code fix , but not sure if this is the correct solution :- 

*Suggested solution*
{code:title=BisectingkMeans.scala}
 private def updateAssignments(
      assignments: RDD[(Long, VectorWithNorm)],
      divisibleIndices: Set[Long],
      newClusterCenters: Map[Long, VectorWithNorm]): RDD[(Long, VectorWithNorm)] = {
    assignments.map { case (index, v) =>
      if (divisibleIndices.contains(index)) {
        val children = Seq(leftChildIndex(index), rightChildIndex(index))
        if ( children.length>0 ) {
        val selected = children.minBy { child =>
          KMeans.fastSquaredDistance(newClusterCenters(child), v)
        }
        (selected, v)
        }else {
          (index, v)
        }
      } else {
        (index, v)
      }
    }
  }
{code}

*Original code* 
{code:title=BisectingkMeans.scala}
  private def updateAssignments(
      assignments: RDD[(Long, VectorWithNorm)],
      divisibleIndices: Set[Long],
      newClusterCenters: Map[Long, VectorWithNorm]): RDD[(Long, VectorWithNorm)] = {
    assignments.map { case (index, v) =>
      if (divisibleIndices.contains(index)) {
        val children = Seq(leftChildIndex(index), rightChildIndex(index))
        val selected = children.minBy { child =>
          KMeans.fastSquaredDistance(newClusterCenters(child), v)
        }
        (selected, v)
      } else {
        (index, v)
      }
    }
  }
{code}","20/Dec/16 18:33;imatiach;I will start a pull request for the change.  I would like to add a test case that verifies the bug is fixed though.  Maybe you can send the sample dataset through github, and I can take a look?","20/Dec/16 18:37;apachespark;User 'imatiach-msft' has created a pull request for this issue:
https://github.com/apache/spark/pull/16355","20/Dec/16 18:38;imatiach;I've added a pull request here:
https://github.com/apache/spark/pull/16355

It would be nice to add a test case in spark itself to verify the code fix.",20/Dec/16 19:02;imatiach;If you could put the sample dataset on google drive or one drive and send me the link that would be great.  Putting the dataset on github would work too.  How large is the dataset?,20/Dec/16 19:26;imatiach;Do you have a smaller dataset than the one in the description which can reproduce the bug?,24/Jan/17 11:30;srowen;Resolved by https://github.com/apache/spark/pull/16355,"17/Aug/17 03:09;podongfeng;If the {{sparseDataset}} in  {{BisectingKMeansSuite}} is cached, then test case {{SPARK-16473: Verify Bisecting K-Means does not fail in edge case whereone cluster is empty after split}} always fails.
See PR https://github.com/apache/spark/pull/16763/files#diff-beaf4409631709a875704e6a4d0a1c13R37 ","05/Oct/17 21:04;imatiach;[~podongfeng] interesting - it looks like the dataset representation is somehow changing when it is cached?  My guess is that the row order may be changing or the numeric values may be changing?  The test failure itself is ok if the number of clusters is equal to k (which is actually perfectly fine for the algorithm), it just means that the dataset was not generated correctly to hit the very special edge case I was looking for, where one cluster is empty after a split in bisecting k-means.  I can't seem to see the test failure error message in your PR, could you run another build and post it here?  We may need to add some debugging/print statements everywhere to determine how the data is changing when you cache it - this doesn't mean there is any bug in the algorithm, it just means the test needs to be changed so that the test data, even after caching, is the same as the original one.",,,,,,,,,,,
Undeleted broadcast variables in Word2Vec causing OoM for long runs ,SPARK-16440,12987775,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,anthony-truchet,anthony-truchet,anthony-truchet,08/Jul/16 12:45,28/Feb/17 10:18,15/Aug/18 23:03,13/Jul/16 10:40,1.6.0,1.6.1,1.6.2,2.0.0,,,,,,,,,,,,1.6.3,2.0.1,,,,,MLlib,,,,,0,,,,,"Three broadcast variables created at the beginning of {{Word2Vec.fit()}} are never deleted nor unpersisted. This seems to cause excessive memory consumption on the driver for a job running hundreds of successive training.

They are 
{code}
    val expTable = sc.broadcast(createExpTable())
    val bcVocab = sc.broadcast(vocab)
    val bcVocabHash = sc.broadcast(vocabHash)
{code}

",,anthony-truchet,apachespark,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,SPARK-11898,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-07-08 12:50:06.5,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 28 10:18:03 UTC 2017,,,,,0|i30p9r:,9223372036854775807,,,,,,,,,,,,,"08/Jul/16 12:48;anthony-truchet;Hello Spark developers,

I'm preparing a patch for this issue. This will be my first contribution to Spark. I'll strive to follow the contribution guidelines, but please do not hesitate to tell me how to do it better if required :-)

","08/Jul/16 12:50;srowen;Yeah it would be fine to unpersist these at the end of the method. 
I suppose I'm surprised that the Broadcast vars don't unpersist themselves when they're out of scope and garbage collected via finalize?","12/Jul/16 12:09;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14153","13/Jul/16 10:40;srowen;Issue resolved by pull request 14153
[https://github.com/apache/spark/pull/14153]","19/Jul/16 11:19;anthony-truchet;Thanks for such a quick fix [~srowen] : I was off-line for the past week that's why I couldn't submit the patch quickly enough.

I would have {{destroyed}} the variable instead of {{unpersisting}} them though as the issues was memory consumption on the *driver* side: what am I missing which made you choose the later over the former ?","19/Jul/16 11:22;srowen;Oh, may be better still indeed. Feel free to submit a follow up associated to this same JIRA. ","19/Jul/16 13:07;anthony-truchet;I will, as well as putting this is a try finally to ensure proper deletion even in case of errors.","19/Jul/16 13:21;srowen;Yeah it seems good to destroy even in case of errors, but in practice, an error here means lots of things are wrong. Actually using try-finally to destroy every RDD/variable would make the code a mess. I think many cases where it plausibly won't matter, we don't. If there's a decent argument that errors here are common for some reason, OK, but not sure that's true.","19/Jul/16 18:04;apachespark;User 'AnthonyTruchet' has created a pull request for this issue:
https://github.com/apache/spark/pull/14268","21/Jul/16 08:12;anthony-truchet;Regarding the try finally: we are computing numerous learning from within a same spark context and some with vocabulary so large that they fail (yes we do try to filter out too big ones, but too big is difficult to define).

So we are in a context where we do care about resource cleaning in case of error in order to enable thousands of successive learnings some of with expected to fail.

As for core readability we can try to refactor the function to reduce the nesting or find a ""nice"" scala solution: I'll propose a patch and I'll welcome any feedback on it.","28/Feb/17 10:18;apachespark;User 'AnthonyTruchet' has created a pull request for this issue:
https://github.com/apache/spark/pull/14299",,,,,,,,,,,,,
IsotonicRegression produces NaNs with certain data,SPARK-16426,12987590,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nseggert,nseggert,nseggert,07/Jul/16 19:48,15/Jul/16 11:31,15/Aug/18 23:03,15/Jul/16 11:30,1.3.1,1.4.1,1.5.2,1.6.2,,,,,,,,,,,,2.1.0,,,,,,MLlib,,,,,0,,,,,"{code}
val r = sc.parallelize(Seq[(Double, Double, Double)]((2, 1, 1), (1, 1, 1), (0, 2, 1), (1, 2, 1), (0.5, 3, 1), (0, 3, 1)), 2)
val i = new IsotonicRegression().run(r)

scala> i.predict(3.0)
res12: Double = NaN

scala> i.predictions
res13: Array[Double] = Array(0.75, 0.75, NaN, NaN)
{code}

I believe I understand the problem so I'll submit a PR shortly.

The problem happens when rows with the same feature value but different labels end up on different partitions. The merge function in poolAdjacentViolators introduces 0-weight points to be used for linear interpolation. This works fine, as long as they are always next to a non-0-weight point, but in the above case, you can end up with two 0-weight points  with the same feature value, which end up next to each other in the final PAV step. If these points are pooled, it creates a NaN.

One solution to this is to ensure that the all points with identical feature values end up on the same partition. This is the solution I intend to submit a PR for. Another option would be to try to get rid of the 0-weight points, but that seems trickier to me.",,apachespark,dougb,nseggert,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-07-11 21:24:04.773,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 15 11:30:36 UTC 2016,,,,,0|i30o5r:,9223372036854775807,,,,,,,,,,,,,"11/Jul/16 21:24;apachespark;User 'neggert' has created a pull request for this issue:
https://github.com/apache/spark/pull/14140","15/Jul/16 11:30;srowen;Issue resolved by pull request 14140
[https://github.com/apache/spark/pull/14140]",,,,,,,,,,,,,,,,,,,,,,
Spark MLlib: MultilayerPerceptronClassifier - error while training,SPARK-16377,12986773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Cannot Reproduce,,mshiryae,mshiryae,05/Jul/16 10:29,21/Nov/16 10:55,15/Aug/18 23:03,21/Nov/16 08:42,1.5.2,,,,,,,,,,,,,,,,,,,,,ML,MLlib,,,,0,,,,,"Hi, 

I am trying to train model by MultilayerPerceptronClassifier. 

It works on sample data from data/mllib/sample_multiclass_classification_data.txt with 4 features, 3 classes and layers [4, 4, 3]. 
But when I try to use other input files with other features and classes (from here for example: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html) 
then I get errors. 

Example: 
Input file aloi (128 features, 1000 classes, layers [128, 128, 1000]): 


with block size = 1: 
ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to Infinity 
ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed 
ERROR LBFGS: Failure again! Giving up and returning. Maybe the objective is just poorly behaved? 


with default block size = 128: 
 java.lang.ArrayIndexOutOfBoundsException 
  at java.lang.System.arraycopy(Native Method) 
  at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:629) 
  at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:628) 
   at scala.collection.immutable.List.foreach(List.scala:381) 
   at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3.apply(Layer.scala:628) 
   at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3.apply(Layer.scala:624) 



Even if I modify sample_multiclass_classification_data.txt file (rename all 4-th features to 5-th) and run with layers [5, 5, 3] then I also get the same errors as for file above. 


So to resume: 
I can't run training with default block size and with more than 4 features. 
If I set  block size to 1 then some actions are happened but I get errors from LBFGS. 
It is reproducible with Spark 1.5.2 and from master branch on github (from 4-th July). 

Did somebody already met with such behavior? 
Is there bug in MultilayerPerceptronClassifier or I use it incorrectly? 

Thanks.",,mlnick,mshiryae,WeichenXu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-07-05 10:32:29.602,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 21 10:55:09 UTC 2016,,,,,0|i30j47:,9223372036854775807,yanboliang,,,,,,,,,,,,"05/Jul/16 10:32;srowen;Can you try vs master? 1.5.2 is old to report bugs against. You're sure you have set the number of features correctly and so on, I take it.","05/Jul/16 10:40;mshiryae;It is reproducible on master too (I just didn't find such option in versions list).
I am passing features and classes corresponding to input files, I have checked it manually, I even reduced count of rows in input file to one line and in that case again got error.","05/Jul/16 12:28;WeichenXu123;hi,

the exception:
java.lang.ArrayIndexOutOfBoundsException 
at java.lang.System.arraycopy(Native Method) 
at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:629) 
at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3$$anonfun$apply$4.apply(Layer.scala:628) 
at scala.collection.immutable.List.foreach(List.scala:381) 
at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3.apply(Layer.scala:628) 
at org.apache.spark.ml.ann.DataStacker$$anonfun$3$$anonfun$apply$3.apply(Layer.scala:624)

is not a problem.

You need to set input data featureNum such as:
spark.read.format(""libsvm"").options(""numFeatures"", ""128"").load(""..."");

otherwise it will automatically determin the feature number and use the example input file it will be 4, 
and if use such as Layer(128,128,1000) it will surely cause index-out-of-bounds error.

thanks!
","05/Jul/16 12:36;WeichenXu123;This exception still exists on master code.
ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to Infinity 
ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed 
ERROR LBFGS: Failure again! Giving up and returning. Maybe the objective is just poorly behaved?

it seems to be some numerical calculating problem?
I'll work on it and check what cause the problem. Thanks!","05/Jul/16 12:38;WeichenXu123;And I test on master version, also encounter the following exception when BlockSize with 128(not only blockSize=1):

16/07/05 05:20:49 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5
16/07/05 05:20:49 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25
16/07/05 05:20:49 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5
16/07/05 05:20:49 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25
16/07/05 05:20:49 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125
16/07/05 05:20:49 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0625
16/07/05 05:20:49 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.03125
16/07/05 05:20:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.015625
16/07/05 05:20:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5
16/07/05 05:20:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25
16/07/05 05:20:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125
16/07/05 05:20:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0625
16/07/05 05:20:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0703125
16/07/05 05:20:51 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed
....
16/07/05 05:20:58 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5
16/07/05 05:20:58 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25
16/07/05 05:20:58 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125
16/07/05 05:20:58 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0625
16/07/05 05:20:58 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.03125
16/07/05 05:20:58 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.03515625
16/07/05 05:20:59 ERROR LBFGS: Failure again! Giving up and returning. Maybe the objective is just poorly behaved?

it seems there are some problems in calculating objective function value and gradient, 
I'll work on it.
","05/Jul/16 23:25;mshiryae;The original issue with ArrayIndexOutOfBoundsException was due to bug in my code (inconsistency between layers and real features count).
And issue with ""ERROR StrongWolfeLineSearch"" isn't reproducible yet.
Sorry for taking your time and thank you for quick responses.","21/Nov/16 04:21;mlnick;Is this still a bug? As per your above comment seems we can close this as ""Not an Issue""?","21/Nov/16 10:55;mshiryae;Yes, you can close it.
The original problem has been resolved.
Thank you.",,,,,,,,,,,,,,,,
Retag RDD to tallSkinnyQR of RowMatrix,SPARK-16372,12986684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yinxusen,yinxusen,yinxusen,04/Jul/16 22:53,14/Jul/16 16:50,15/Aug/18 23:03,07/Jul/16 10:28,,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"The following Java code because of type erasing:

{code}
JavaRDD<Vector> rows = jsc.parallelize(...);
RowMatrix mat = new RowMatrix(rows.rdd());
QRDecomposition<RowMatrix, Matrix> result = mat.tallSkinnyQR(true);
{code}

We should use retag to restore the type to prevent the following exception:

{code}
java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lorg.apache.spark.mllib.linalg.Vector;
{code}",,apachespark,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13015,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-07-05 00:07:04.216,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 07 10:28:46 UTC 2016,,,,,0|i30ikf:,9223372036854775807,,,,,,,,,,,,,04/Jul/16 22:54;yinxusen;SPARK-11497 fixed this for PySpark.,"05/Jul/16 00:07;apachespark;User 'yinxusen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14051","07/Jul/16 10:28;srowen;Issue resolved by pull request 14051
[https://github.com/apache/spark/pull/14051]",,,,,,,,,,,,,,,,,,,,,
tallSkinnyQR of RowMatrix should aware of empty partition,SPARK-16369,12986673,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yinxusen,yinxusen,yinxusen,04/Jul/16 21:11,14/Jul/16 16:50,15/Aug/18 23:03,08/Jul/16 13:24,,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"tallSkinnyQR of RowMatrix should aware of empty partition, which could cause exception from Breeze qr decomposition.

See the [archived dev mail|https://mail-archives.apache.org/mod_mbox/spark-dev/201510.mbox/%3CCAF7ADNrycvPL3qX-VZJhq4OYmiUUhoscut_tkOm63Cm18iK1tQ@mail.gmail.com%3E] for more details.",,apachespark,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13015,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-07-04 22:28:04.948,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 08 13:24:23 UTC 2016,,,,,0|i30ii7:,9223372036854775807,,,,,,,,,,,,,"04/Jul/16 22:28;apachespark;User 'yinxusen' has created a pull request for this issue:
https://github.com/apache/spark/pull/14049","08/Jul/16 13:24;srowen;Issue resolved by pull request 14049
[https://github.com/apache/spark/pull/14049]",,,,,,,,,,,,,,,,,,,,,,
RowMatrıx Covariance,SPARK-16156,12982104,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,volkanagun@gmail.com,volkanagun@gmail.com,22/Jun/16 21:21,07/Jul/16 20:20,15/Aug/18 23:03,23/Jun/16 06:06,2.0.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,Spark doesn't provide a good solution of covariance for RowMatrix with large columns. This can be fixed with using efficient stable computations and approximating to the true mean.    ,Spark MLLIB,volkanagun@gmail.com,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,SPARK-14533,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-06-23 06:06:44.466,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 23 06:06:44 UTC 2016,,,,,0|i2zxmv:,9223372036854775807,,,,,,,,,,,,,"23/Jun/16 06:06;srowen;[~volkanagun@gmail.com] many things wrong with this JIRA, please read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

But this looks like a duplicate of https://issues.apache.org/jira/browse/SPARK-14533",,,,,,,,,,,,,,,,,,,,,,,
The SparseVector parser fails checking for valid end parenthesis,SPARK-16035,12980362,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,andreapasqua,andreapasqua,andreapasqua,18/Jun/16 00:51,18/Jun/16 05:42,15/Aug/18 23:03,18/Jun/16 05:41,1.6.1,2.0.0,,,,,,,,,,,,,,1.6.2,2.0.0,,,,,MLlib,PySpark,,,,0,,,,,"Running
                      SparseVector.parse(' (4, [0,1 ],[ 4.0,5.0] ')
will not raise an exception as expected, although it parses it as if there was an end parenthesis.

This can be fixed by replacing

                      if start == -1:
                               raise ValueError(""Tuple should end with ')'"")

with
                     if end == -1:
                               raise ValueError(""Tuple should end with ')'"")

Please see posted PR",,andreapasqua,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-06-18 01:11:03.59,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 18 05:41:47 UTC 2016,,,,,0|i2znp3:,9223372036854775807,,,,,,,,,,,,,"18/Jun/16 01:11;apachespark;User 'andreapasqua' has created a pull request for this issue:
https://github.com/apache/spark/pull/13750","18/Jun/16 05:41;mengxr;Issue resolved by pull request 13750
[https://github.com/apache/spark/pull/13750]",,,,,,,,,,,,,,,,,,,,,,
SVMWithSGD requires LabeledPoint of Regression,SPARK-15986,12979736,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,volkanagun@gmail.com,volkanagun@gmail.com,16/Jun/16 09:29,16/Jun/16 17:39,15/Aug/18 23:03,16/Jun/16 17:39,2.0.0,,,,,,,,,,,,,,,,,,,,,ML,MLlib,,,,0,,,,,"SVMWithSGD uses the LabelledPoint in mllib-regression however a classification routine that extends ml's Predictor class uses LabelledPoint of ml.feature. So basically any rotutine, for ex:  a classifier in ml pipeline cannot uses SVMWithSGD. This consistency problem can be solved by removing LabelledPoint in regression. Two different labelled points (one uses linalg.Vector, and other ml.linalg.Vector) is also confusing.         ",Spark 2.0 Snapshot,josephkb,volkanagun@gmail.com,,,,,,,,,,,,,,,,1814400,1814400,,0%,1814400,1814400,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-06-16 17:39:01.026,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 16 17:39:01 UTC 2016,,,,,0|i2zjtz:,9223372036854775807,,,,,,,,,,,,,16/Jun/16 17:39;josephkb;Thanks for reporting this; it's being worked on!,,,,,,,,,,,,,,,,,,,,,,,
BlockMatrix to IndexedRowMatrix throws an error,SPARK-15922,12978163,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,chaz2505,chaz2505,13/Jun/16 14:11,16/Jun/16 21:03,15/Aug/18 23:03,16/Jun/16 21:03,2.0.0,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,1,,,,,"{code}
import org.apache.spark.mllib.linalg.distributed._
import org.apache.spark.mllib.linalg._

val rows = IndexedRow(0L, new DenseVector(Array(1,2,3))) :: IndexedRow(1L, new DenseVector(Array(1,2,3))):: IndexedRow(2L, new DenseVector(Array(1,2,3))):: Nil
val rdd = sc.parallelize(rows)
val matrix = new IndexedRowMatrix(rdd, 3, 3)
val bmat = matrix.toBlockMatrix

val imat = bmat.toIndexedRowMatrix
imat.rows.collect // this throws an error - Caused by: java.lang.IllegalArgumentException: requirement failed: Vectors must be the same length!
{code}",,apachespark,chaz2505,dongjoon,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-06-13 18:39:15.78,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 16 21:03:16 UTC 2016,,,,,0|i2zdsv:,9223372036854775807,,,,,,,,,,,,,"13/Jun/16 18:39;dongjoon;Hi, [~chaz2505].
This is due to `toIndexedRowMatrix` bug. 
I'll make a PR for this.","13/Jun/16 18:40;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/13643","15/Jun/16 07:22;dongjoon;Hi, [~mengxr].
Could you review the PR on this bug?","16/Jun/16 21:03;srowen;Issue resolved by pull request 13643
[https://github.com/apache/spark/pull/13643]",,,,,,,,,,,,,,,,,,,,
Constructing FPGrowth fails when no numPartitions specified in pyspark,SPARK-15750,12975391,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zjffdu,zjffdu,zjffdu,03/Jun/16 06:32,07/May/18 21:49,15/Aug/18 23:03,07/May/18 21:49,2.0.0,,,,,,,,,,,,,,,2.4.0,,,,,,MLlib,PySpark,,,,0,,,,,"{code}
>>> model1 = FPGrowth.train(rdd, 0.6)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/jzhang/github/spark-2/python/pyspark/mllib/fpm.py"", line 96, in train
    model = callMLlibFunc(""trainFPGrowthModel"", data, float(minSupport), int(numPartitions))
  File ""/Users/jzhang/github/spark-2/python/pyspark/mllib/common.py"", line 130, in callMLlibFunc
    return callJavaFunc(sc, api, *args)
  File ""/Users/jzhang/github/spark-2/python/pyspark/mllib/common.py"", line 123, in callJavaFunc
    return _java2py(sc, func(*args))
  File ""/Users/jzhang/github/spark-2/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py"", line 933, in __call__
  File ""/Users/jzhang/github/spark-2/python/pyspark/sql/utils.py"", line 79, in deco
    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.IllegalArgumentException: u'requirement failed: Number of partitions must be positive but got -1'
{code}",,apachespark,josephkb,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-06-03 06:36:05.352,,false,,,,,,,,,,,,,9223372036854775807,,,Mon May 07 21:49:58 UTC 2018,,,,,0|i2yxg7:,9223372036854775807,josephkb,,,,,,,,,,,,"03/Jun/16 06:36;apachespark;User 'zjffdu' has created a pull request for this issue:
https://github.com/apache/spark/pull/13493","07/May/18 21:49;josephkb;Issue resolved by pull request 13493
[https://github.com/apache/spark/pull/13493]",,,,,,,,,,,,,,,,,,,,,,
"Word2VecSuite ""big model load / save"" caused OOM in maven jenkins builds",SPARK-15740,12975323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tmnd91,mengxr,mengxr,02/Jun/16 22:16,14/Jul/16 16:50,15/Aug/18 23:03,06/Jul/16 19:57,2.0.0,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"[~andrewor14] noticed some OOM errors caused by ""test big model load / save"" in Word2VecSuite, e.g., https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-master-test-maven-hadoop-2.2/1168/consoleFull. It doesn't show up in the test result because it was OOMed.

I'm going to disable the test first and leave this open for a proper fix.

cc [~tmnd91]",,apachespark,josephkb,mengxr,tmnd91,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11994,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-06-02 22:20:04.382,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 06 19:57:08 UTC 2016,,,,,0|i2yx13:,9223372036854775807,,,,,,,,,,,,,"02/Jun/16 22:20;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/13478","02/Jun/16 22:24;mengxr;[~tmnd91] Could you run the test and estimate how much ram does it need? Btw, we should set spark.kryoserializer.buffer.max to a small value instead of creating a big array for the test. Do you have time to look into this issue?",03/Jun/16 06:50;tmnd91;Looking into it right now.,"03/Jun/16 08:15;tmnd91;In order to set a small partition size w/r/t spark.kryoserializer.buffer.max I need to read that conf in the first place and change the word2vecserializer code to be adaptive. I think i can retrieve the conf with:
```
spark.conf.get(""spark.kryoserializer.buffer.max"")
```
But then how could i convert the string in a format like ""64m"" to the number of bytes? Is there a utile somewhere to take advantage of?

I purpose to make the partitioning adaptive to that value and to set a small spark.kryoserializer.buffer.max to a small size only for the test, therefore we'll need a small array avoiding OOMs.
Let me know what do you think about it.",03/Jun/16 21:08;mengxr;The proposal looks good to me. Please also try to measure the memory requirement so we can easily tell whether the issue is fixed or not. Triggering Jenkins maven builds is not convenient.,"04/Jun/16 08:39;tmnd91;As of now the memory requirement would be something like 2k.
I made a pull request named SPARK-15740. I was able to run it successfully, but I don't know how to measure the memory req. of the test. Can you point me to some resource where I can learn how to?
","04/Jun/16 08:43;apachespark;User 'tmnd1991' has created a pull request for this issue:
https://github.com/apache/spark/pull/13509","06/Jul/16 19:57;josephkb;Issue resolved by pull request 13509
[https://github.com/apache/spark/pull/13509]",,,,,,,,,,,,,,,,
Replace FileSystem.get(conf) with path.getFileSystem(conf) when removing CheckpointFile in MLlib,SPARK-15664,12974155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lianhuiwang,lianhuiwang,lianhuiwang,31/May/16 09:09,01/Jun/16 13:31,15/Aug/18 23:03,01/Jun/16 13:30,,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"if sparkContext.set CheckpointDir to another Dir that is not default FileSystem, it will throw exception when removing CheckpointFile in MLlib.",,apachespark,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-05-31 10:02:05.846,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 01 13:30:55 UTC 2016,,,,,0|i2yq6n:,9223372036854775807,,,,,,,,,,,,,"31/May/16 10:02;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/13408","01/Jun/16 13:30;srowen;Issue resolved by pull request 13408
[https://github.com/apache/spark/pull/13408]",,,,,,,,,,,,,,,,,,,,,,
DecisionTreeClassificationModel can't be saved within in  Pipeline caused by not implement Writable ,SPARK-15497,12972271,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,licl,licl,24/May/16 03:25,24/May/16 20:32,15/Aug/18 23:03,24/May/16 20:32,1.6.1,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"Here is my code
{code}
SQLContext sqlContext = getSQLContext();
		DataFrame data = sqlContext.read().format(""libsvm"").load(""file:///E:/workspace-mars/bigdata/sparkjob/data/mllib/sample_libsvm_data.txt"");
		// Index labels, adding metadata to the label column.
		// Fit on whole dataset to include all labels in index.
		StringIndexerModel labelIndexer = new StringIndexer()
		  .setInputCol(""label"")
		  .setOutputCol(""indexedLabel"")
		  .fit(data);
		// Automatically identify categorical features, and index them.
		VectorIndexerModel featureIndexer = new VectorIndexer()
		  .setInputCol(""features"")
		  .setOutputCol(""indexedFeatures"")
		  .setMaxCategories(4) // features with > 4 distinct values are treated as continuous
		  .fit(data);

		// Split the data into training and test sets (30% held out for testing)
		DataFrame[] splits = data.randomSplit(new double[]{0.7, 0.3});
		DataFrame trainingData = splits[0];
		DataFrame testData = splits[1];

		// Train a DecisionTree model.
		DecisionTreeClassifier dt = new DecisionTreeClassifier()
		  .setLabelCol(""indexedLabel"")
		  .setFeaturesCol(""indexedFeatures"");

		// Convert indexed labels back to original labels.
		IndexToString labelConverter = new IndexToString()
		  .setInputCol(""prediction"")
		  .setOutputCol(""predictedLabel"")
		  .setLabels(labelIndexer.labels());

		// Chain indexers and tree in a Pipeline
		Pipeline pipeline = new Pipeline()
		  .setStages(new PipelineStage[]{labelIndexer, featureIndexer, dt, labelConverter});

		// Train model.  This also runs the indexers.
		PipelineModel model = pipeline.fit(trainingData);
		model.save(""file:///e:/tmpmodel"");
{code}

and here is the exception
{code}
Exception in thread ""main"" java.lang.UnsupportedOperationException: Pipeline write will fail on this Pipeline because it contains a stage which does not implement Writable. Non-Writable stage: dtc_7bdeae1c4fb8 of type class org.apache.spark.ml.classification.DecisionTreeClassificationModel
	at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$validateStages$1.apply(Pipeline.scala:218)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$validateStages$1.apply(Pipeline.scala:215)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.validateStages(Pipeline.scala:215)
	at org.apache.spark.ml.PipelineModel$PipelineModelWriter.<init>(Pipeline.scala:325)
	at org.apache.spark.ml.PipelineModel.write(Pipeline.scala:309)
	at org.apache.spark.ml.util.MLWritable$class.save(ReadWrite.scala:131)
	at org.apache.spark.ml.PipelineModel.save(Pipeline.scala:280)
	at com.bjdv.spark.job.Testjob.main(Testjob.java:142)
{code}

sample_libsvm_data.txt is included in the 1.6.1 release tar",,bryanc,licl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-05-24 20:32:15.486,,false,,,,,,,,,,,,,9223372036854775807,,,Tue May 24 20:32:15 UTC 2016,,,,,0|i2yekn:,9223372036854775807,,,,,,,,,,,,,24/May/16 20:32;bryanc;This was added in SPARK-11888 and will be in Spark 2.0.,,,,,,,,,,,,,,,,,,,,,,,
LinearRegressionWithSGD fails on files more than 12Mb data ,SPARK-15403,12971102,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Duplicate,,rain_dev,rain_dev,19/May/16 10:22,20/May/16 11:16,15/Aug/18 23:03,20/May/16 11:16,1.6.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,1,,,,,"I parse my json-like data, passing by DataFrame and SparkSql facilities and then scale one numerical feature and create dummy variables for categorical features. So far from initial 14 keys of my json-like file I get about 200-240 features in the final LabeledPoint. The final data is sparse and every file contains as minimum 50000 of observations. I try to run two types of algorithms on data :  LinearRegressionWithSGD or LassoWithSGD, since the data is sparse and regularization might be required.  For data larger than 11MB  LinearRegressionWithSGD fails with the following error:
{quote} org.apache.spark.SparkException: Job aborted due to stage failure: Task 58 in stage 346.0 failed 1 times, most recent failure: Lost task 58.0 in stage 346.0 (TID 18140, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 179307 ms {quote}

I tried to reproduce this bug with bug in smaller example, and I suppose that something wrong could be with LinearRegressionWithSGD on large sets of data. I notices that while using StandardScaler on preprocessing step and counts on Linear Regression step, collect() method is perform, that can cause the bug. So the possibility to scale Linear regression is questioned, cause, as I far as I understand it, collect() performs on driver and so the sens of scaled calculations is lost. 

{code:scala}
import java.io.{File}

import org.apache.spark.mllib.linalg.{Vectors}
import org.apache.spark.mllib.regression.{LabeledPoint, LassoWithSGD}
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{SQLContext}

import scala.language.postfixOps


object Main2 {

  def main(args: Array[String]): Unit = {


    // Spark configuration is defined for execution on local computer, 4 cores 8Mb Ram
    val conf = new SparkConf()
      .setMaster(s""local[*]"")
      .setAppName(""spark_linear_regression_bug_report"")
      //multiple configurations were tried for driver/executor memories, including default configurations
      .set(""spark.driver.memory"", ""3g"")
      .set(""spark.executor.memory"", ""3g"")
      .set(""spark.executor.heartbeatInterval"", ""30s"")

    // Spark context and SQL context definitions
    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)

    val countFeatures = 500
    val countList = 500000

    val features = sc.broadcast(1 to countFeatures)

    val rdd: RDD[LabeledPoint] = sc.range(1, countList).map { i =>
      LabeledPoint(
        label = i.toDouble,
        features = Vectors.dense(features.value.map(_ => scala.util.Random.nextInt(2).toDouble).toArray)
      )
    }.persist()

    val numIterations = 1000
    val stepSize = 0.3
    val algorithm = new LassoWithSGD() //LassoWithSGD() 
    algorithm.setIntercept(true)
    algorithm.optimizer
      .setNumIterations(numIterations)
      .setStepSize(stepSize)
    val model = algorithm.run(rdd)

  }
}
{code}

the complete Error of the bug :
{quote}  [info] Running Main 
WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
WARN  org.apache.spark.util.Utils - Your hostname, julien-ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.0.49 instead (on interface wlan0)
WARN  org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address
INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
INFO  Remoting - Starting remoting
INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.0.49:59897]
INFO  org.spark-project.jetty.server.Server - jetty-8.y.z-SNAPSHOT
INFO  org.spark-project.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
WARN  com.github.fommil.netlib.BLAS - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
WARN  com.github.fommil.netlib.BLAS - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
[Stage 51:===========================================>              (3 + 1) / 4]ERROR org.apache.spark.util.Utils - Uncaught exception in thread driver-heartbeater
java.io.IOException: java.lang.ClassNotFoundException: org.apache.spark.storage.BroadcastBlockId
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1207) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.TaskMetrics.readObject(TaskMetrics.scala:219) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source) ~[na:na]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_91]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_91]
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373) ~[na:1.8.0_91]
	at org.apache.spark.util.Utils$.deserialize(Utils.scala:92) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1$$anonfun$apply$6.apply(Executor.scala:437) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1$$anonfun$apply$6.apply(Executor.scala:427) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at scala.Option.foreach(Option.scala:257) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1.apply(Executor.scala:427) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$reportHeartBeat$1.apply(Executor.scala:425) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at scala.collection.Iterator$class.foreach(Iterator.scala:742) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1194) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:425) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:470) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:470) [spark-core_2.11-1.6.1.jar:1.6.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_91]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_91]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_91]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_91]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_91]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_91]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
Caused by: java.lang.ClassNotFoundException: org.apache.spark.storage.BroadcastBlockId
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_91]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_91]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_91]
	at java.lang.Class.forName0(Native Method) ~[na:1.8.0_91]
	at java.lang.Class.forName(Class.java:348) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:628) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373) ~[na:1.8.0_91]
	at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[na:na]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_91]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_91]
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018) ~[na:1.8.0_91]
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:503) ~[na:1.8.0_91]
	at org.apache.spark.executor.TaskMetrics$$anonfun$readObject$1.apply$mcV$sp(TaskMetrics.scala:220) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1204) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	... 32 common frames omitted
WARN  org.apache.spark.HeartbeatReceiver - Removing executor driver with no recent heartbeats: 175339 ms exceeds timeout 120000 ms
ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor driver on localhost: Executor heartbeat timed out after 175339 ms
WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 105.0 (TID 420, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 175339 ms
ERROR org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 105.0 failed 1 times; aborting job
WARN  org.apache.spark.SparkContext - Killing executors is only supported in coarse-grained mode
[error] (run-main-0) org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 105.0 failed 1 times, most recent failure: Lost task 1.0 in stage 105.0 (TID 420, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 175339 ms
[error] Driver stacktrace:
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 105.0 failed 1 times, most recent failure: Lost task 1.0 in stage 105.0 (TID 420, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 175339 ms
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1127)
	at org.apache.spark.mllib.optimization.GradientDescent$.runMiniBatchSGD(GradientDescent.scala:227)
	at org.apache.spark.mllib.optimization.GradientDescent.optimize(GradientDescent.scala:128)
	at org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:308)
	at org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:229)
	at Main$.main(Main.scala:85)
	at Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
[trace] Stack trace suppressed: run last compile:run for the full output.
ERROR org.apache.spark.ContextCleaner - Error in cleaning thread
java.lang.InterruptedException: null
	at java.lang.Object.wait(Native Method) ~[na:1.8.0_91]
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143) ~[na:1.8.0_91]
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:176) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1180) [spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:173) [spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:68) [spark-core_2.11-1.6.1.jar:1.6.1]
ERROR org.apache.spark.util.Utils - uncaught error in thread SparkListenerBus, stopping SparkContext
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998) ~[na:1.8.0_91]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304) ~[na:1.8.0_91]
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312) ~[na:1.8.0_91]
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:66) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) ~[scala-library-2.11.7.jar:1.0.0-M1]
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1180) ~[spark-core_2.11-1.6.1.jar:1.6.1]
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63) [spark-core_2.11-1.6.1.jar:1.6.1]
java.lang.RuntimeException: Nonzero exit code: 1
	at scala.sys.package$.error(package.scala:27)
[trace] Stack trace suppressed: run last compile:run for the full output.
[error] (compile:run) Nonzero exit code: 1 {quote}","Ubuntu 14.04 with 8 Gb Ram, scala 2.11.7 with following memory settings for my project:  JAVA_OPTS=""-Xmx8G -Xms2G"" .",rain_dev,yotsumi,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10722,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-05-20 01:08:40.353,,false,,,,,Important,,,,,,,,9223372036854775807,,,Fri May 20 11:16:08 UTC 2016,,,,,0|i2y7d3:,9223372036854775807,,,,,,,,,,,,,"20/May/16 01:08;srowen;This looks like some problem in your Spark environment, like mismatched or conflicting builds of Spark. ClassNotFoundException wouldn't be something to do with data.","20/May/16 07:49;rain_dev;We tried this code in several environment, including a production cluster. The code itself works fine with less data or less feature, but doesn't seems to scale correctly.",20/May/16 11:16;srowen;(Don't set Blocker; read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark) Looks like this is a duplicate,,,,,,,,,,,,,,,,,,,,,
Wrong equation is used in the method of org.apache.spark.mllib.clustering.KMeans,SPARK-15399,12971043,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,William_lei,William_lei,19/May/16 06:44,19/May/16 09:06,15/Aug/18 23:03,19/May/16 09:06,1.6.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"the method is in org.apache.spark.mllib.clustering.KMeans
the equation |a-b|=||a|-|b|| is wrong when a and b are vector. but it is used in the spark-1.6.1.
private[mllib] def findClosest(
      centers: TraversableOnce[VectorWithNorm],
      point: VectorWithNorm): (Int, Double) = {
    var bestDistance = Double.PositiveInfinity
    var bestIndex = 0
    var i = 0
    centers.foreach { center =>
      // Since `\|a - b\| \geq |\|a\| - \|b\||`, we can use this lower bound to avoid unnecessary
      // distance computation.
      var lowerBoundOfSqDist = center.norm - point.norm
      lowerBoundOfSqDist = lowerBoundOfSqDist * lowerBoundOfSqDist
      if (lowerBoundOfSqDist < bestDistance) {
        val distance: Double = fastSquaredDistance(center, point)
        if (distance < bestDistance) {
          bestDistance = distance
          bestIndex = i
        }
      }
      i += 1
    }
    (bestIndex, bestDistance)
  }
the center and the point in the source code are vector. and I suggest the code is that
private[mllib] def findClosest(
      centers: TraversableOnce[VectorWithNorm],
      point: VectorWithNorm): (Int, Double) = {
    var bestDistance = Double.PositiveInfinity
    var bestIndex = 0
    var i = 0
    centers.foreach { center =>
      // distance computation.
      val distance: Double = fastSquaredDistance(center, point)
      if (distance < bestDistance) {
        bestDistance = distance
        bestIndex = i
      }
      i += 1
    }
    (bestIndex, bestDistance)
  }",windows 64bit,William_lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-05-19 09:06:26.478,,false,,,,,,,,,,,,,9223372036854775807,,,Thu May 19 09:06:26 UTC 2016,,,,,0|i2y6zz:,9223372036854775807,,,,,,,,,,,,,19/May/16 09:06;srowen;That's not what the code says. It says |a-b| >= ||a|-|b||. Please read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark first,,,,,,,,,,,,,,,,,,,,,,,
Fix and re-enable flaky test: mllib.stat.JavaStatisticsSuite.testCorr,SPARK-15043,12964108,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,srowen,joshrosen,joshrosen,30/Apr/16 23:07,25/May/16 06:00,15/Aug/18 23:03,25/May/16 06:00,2.0.0,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"It looks like the {{mllib.stat.JavaStatisticsSuite.testCorr}} test has become flaky:

https://spark-tests.appspot.com/tests/org.apache.spark.mllib.stat.JavaStatisticsSuite/testCorr

The first observed failure was in https://spark-tests.appspot.com/builds/spark-master-test-maven-hadoop-2.6/816

{code}
java.lang.AssertionError: expected:<0.9986422261219262> but was:<0.9986422261219272>
	at org.apache.spark.mllib.stat.JavaStatisticsSuite.testCorr(JavaStatisticsSuite.java:75)
{code}

I'm going to ignore this test now, but we need to come back and fix it.",,apachespark,joshrosen,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-05-01 07:54:47.999,,false,,,,,,,,,,,,,9223372036854775807,,,Wed May 25 06:00:37 UTC 2016,,,,,0|i2x0bz:,9223372036854775807,,,,,,2.0.0,,,,,,,01/May/16 07:54;srowen;I'll take a look and try to fix it. I didn't see that one while testing but might be some odd order of evaluation in the RDD that makes this tiny difference.,"01/May/16 08:49;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/12821",25/May/16 06:00;mengxr;Fixed as part of SPARK-15030.,,,,,,,,,,,,,,,,,,,,,
RankingMetrics.ndcgAt  throw  java.lang.ArrayIndexOutOfBoundsException,SPARK-14886,12962129,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,licl,licl,25/Apr/16 03:05,29/Apr/16 07:24,15/Aug/18 23:03,29/Apr/16 07:22,1.6.1,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"{code} 
@Since(""1.2.0"")
  def ndcgAt(k: Int): Double = {
    require(k > 0, ""ranking position k should be positive"")
    predictionAndLabels.map { case (pred, lab) =>
      val labSet = lab.toSet

      if (labSet.nonEmpty) {
        val labSetSize = labSet.size
        val n = math.min(math.max(pred.length, labSetSize), k)
        var maxDcg = 0.0
        var dcg = 0.0
        var i = 0
        while (i < n) {
          val gain = 1.0 / math.log(i + 2)
          if (labSet.contains(pred(i))) {
            dcg += gain
          }
          if (i < labSetSize) {
            maxDcg += gain
          }
          i += 1
        }
        dcg / maxDcg
      } else {
        logWarning(""Empty ground truth set, check input data"")
        0.0
      }
    }.mean()
  }
{code}

""if (labSet.contains(pred(i)))"" will throw ArrayIndexOutOfBoundsException when pred's size less then k.
That meas the true relevant documents has less size then the param k.
just try this with sample_movielens_data.txt
for example set pred.size to 5,labSetSize to 10,k to 20,then the n is 10. pred[10] not exists;
precisionAt is ok just because it has         
val n = math.min(pred.length, k)


",,apachespark,licl,mlnick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-04-25 07:29:52.226,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 29 07:22:00 UTC 2016,,,,,0|i2wo5r:,9223372036854775807,,,,,,,,,,,,,25/Apr/16 07:29;srowen;(Please format your code),"25/Apr/16 07:34;srowen;Yes I think {{if (labSet.contains(pred(i))) {}} needs to include the condition {{i < pred.length}}. The case here is that the pred.length < labelSet.size <= k, in which case predictions are evaluated through to labelSet.size. But anything after the end of the predictions should be considered ""not in the label set"". Go ahead and make a PR.","25/Apr/16 08:26;mlnick;Are you saying that the ""maxDCG"" should still be evaluated through the full label set (up to ""k"", at least)?","25/Apr/16 08:55;srowen;No I don't think this concerns maxDCG in particular. It's evaluated through to the label set size (or k, if k is smaller of course). It does affect dcg, because the number of predictions may be less than the label set size. The rest of the predictions conceptually don't exist and do not match the label set, and so don't add to dcg.","28/Apr/16 15:13;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/12756","29/Apr/16 07:22;mlnick;Issue resolved by pull request 12756
[https://github.com/apache/spark/pull/12756]",,,,,,,,,,,,,,,,,,
Error while encoding: java.lang.ClassCastException with LibSVMRelation,SPARK-14843,12961583,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,mlnick,mlnick,22/Apr/16 09:25,22/Apr/16 18:39,15/Aug/18 23:03,22/Apr/16 17:12,,,,,,,,,,,,,,,,2.0.0,,,,,,ML,MLlib,SQL,,,0,,,,,"While trying to run some example ML linear regression code, I came across the following. In fact this error occurs when doing {{./bin/run-example ml.LinearRegressionWithElasticNetExample}}.

{code}
scala> import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.regression.LinearRegression

scala> import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.mllib.linalg.Vector

scala> import org.apache.spark.sql.Row
import org.apache.spark.sql.Row

scala> val data = sqlContext.read.format(""libsvm"").load(""data/mllib/sample_linear_regression_data.txt"")
data: org.apache.spark.sql.DataFrame = [label: double, features: vector]

scala> val model = lr.fit(data)
{code}
Stack trace:
{code}
Driver stacktrace:
...
  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1276)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:357)
  at org.apache.spark.rdd.RDD.take(RDD.scala:1250)
  at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1290)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:357)
  at org.apache.spark.rdd.RDD.first(RDD.scala:1289)
  at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:165)
  at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:69)
  at org.apache.spark.ml.Predictor.fit(Predictor.scala:90)
  ... 48 elided
Caused by: java.lang.RuntimeException: Error while encoding: java.lang.ClassCastException: java.lang.Double cannot be cast to org.apache.spark.mllib.linalg.Vector
if (input[0, org.apache.spark.sql.Row].isNullAt) null else newInstance(class org.apache.spark.mllib.linalg.VectorUDT).serialize
:- input[0, org.apache.spark.sql.Row].isNullAt
:  :- input[0, org.apache.spark.sql.Row]
:  +- 0
:- null
+- newInstance(class org.apache.spark.mllib.linalg.VectorUDT).serialize
   :- newInstance(class org.apache.spark.mllib.linalg.VectorUDT)
   +- input[0, org.apache.spark.sql.Row].get
      :- input[0, org.apache.spark.sql.Row]
      +- 0

  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:230)
  at org.apache.spark.ml.source.libsvm.DefaultSource$$anonfun$buildReader$1$$anonfun$8.apply(LibSVMRelation.scala:209)
  at org.apache.spark.ml.source.libsvm.DefaultSource$$anonfun$buildReader$1$$anonfun$8.apply(LibSVMRelation.scala:207)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.next(FileScanRDD.scala:90)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegen$$anonfun$7$$anon$1.hasNext(WholeStageCodegen.scala:362)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
  at org.apache.spark.scheduler.Task.run(Task.scala:85)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:254)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: java.lang.Double cannot be cast to org.apache.spark.mllib.linalg.Vector
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:227)
  ... 17 more
{code}

The error is triggered by L163 of {{LinearRegression}}:
{code}
    val numFeatures = dataset.select(col($(featuresCol))).limit(1).rdd.map {
      case Row(features: Vector) => features.size
    }.first()
{code}

Using the above example, the following works:
{code}
scala> data.select(""label"").rdd.map { case Row(d: Double) => d }.first
res49: Double = -9.490009878824548
{code}
But this triggers the exception:
{code}
scala> data.select(""features"").rdd.map { case Row(d: Vector) => d }.first
16/04/22 11:25:20 ERROR Executor: Exception in task 0.0 in stage 87.0 (TID 98)
java.lang.RuntimeException: Error while encoding: java.lang.ClassCastException: java.lang.Double cannot be cast to org.apache.spark.mllib.linalg.Vector
...
{code}",,apachespark,lian cheng,mlnick,ygh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-04-22 13:51:03.857,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 22 17:12:07 UTC 2016,,,,,0|i2wkvz:,9223372036854775807,,,,,,,,,,,,,"22/Apr/16 13:51;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/12611","22/Apr/16 17:12;lian cheng;Issue resolved by pull request 12611
[https://github.com/apache/spark/pull/12611]",,,,,,,,,,,,,,,,,,,,,,
CrossValidatorModel.bestModel does not include hyper-parameters,SPARK-14740,12960186,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,pshearer,pshearer,20/Apr/16 00:53,20/Apr/16 22:48,15/Aug/18 23:03,20/Apr/16 22:48,1.6.1,,,,,,,,,,,,,,,,,,,,,MLlib,PySpark,,,,0,,,,,"If you tune hyperparameters using a CrossValidator object in PySpark, you may not be able to extract the parameter values of the best model.

{noformat}
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.mllib.linalg import Vectors
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

dataset = sqlContext.createDataFrame(
    [(Vectors.dense([0.0]), 0.0),
     (Vectors.dense([0.4]), 1.0),
     (Vectors.dense([0.5]), 0.0),
     (Vectors.dense([0.6]), 1.0),
     (Vectors.dense([1.0]), 1.0)] * 10,
    [""features"", ""label""])
lr = LogisticRegression()
grid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01, 0.001, 0.0001]).build()
evaluator = BinaryClassificationEvaluator()
cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)
cvModel = cv.fit(dataset)
{noformat}

I can get the regression coefficient out, but I can't get the regularization parameter

{noformat}
In [3]: cvModel.bestModel.coefficients
Out[3]: DenseVector([3.1573])

In [4]: cvModel.bestModel.explainParams()
Out[4]: ''

In [5]: cvModel.bestModel.extractParamMap()
Out[5]: {}

In [15]: cvModel.params
Out[15]: []

In [36]: cvModel.bestModel.params
Out[36]: []
{noformat}

For the original issue raised on StackOverflow please see http://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark

",,josephkb,pshearer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-04-20 22:48:27.287,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 20 22:48:27 UTC 2016,,,,,0|i2wcq7:,9223372036854775807,,,,,,,,,,,,,"20/Apr/16 15:04;pshearer;It appears it's accessible through cvModel.bestModel._java_obj.getRegParam(), just need to get this in the right place in the code.",20/Apr/16 22:48;josephkb;Thanks for reporting this.  I'm linking an existing issue which covers this.,,,,,,,,,,,,,,,,,,,,,,
Vectors.parse doesn't handle dense vectors of size 0 and sparse vectors with no indices,SPARK-14739,12960167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,arashpa@gmail.com,zero323,zero323,19/Apr/16 23:08,21/Apr/16 10:31,15/Aug/18 23:03,21/Apr/16 10:30,1.6.0,2.0.0,,,,,,,,,,,,,,1.6.2,2.0.0,,,,,MLlib,PySpark,,,,2,,,,,"DenseVector:

{code}
Vectors.parse(str(Vectors.dense([])))
## ValueError                                Traceback (most recent call last)
## .. 
## ValueError: Unable to parse values from
{code}

SparseVector:

{code}
Vectors.parse(str(Vectors.sparse(5, [], [])))
## ValueError                                Traceback (most recent call last)
##  ... 
## ValueError: Unable to parse indices from .
{code}",,apachespark,arashpa,josephkb,vishnu667,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-04-19 23:58:03.316,,false,,,,http://stackoverflow.com/q/36730727,,,,,,,,,9223372036854775807,,,Thu Apr 21 10:30:13 UTC 2016,,,,,0|i2wclz:,9223372036854775807,,,,,,,,,,,,,"19/Apr/16 23:58;apachespark;User 'arashpa' has created a pull request for this issue:
https://github.com/apache/spark/pull/12510","19/Apr/16 23:58;arashpa;Thanks for posting the ticket on Jira, I created the PR here:
https://github.com/apache/spark/pull/12510
","20/Apr/16 00:10;zero323;This solves only small part of the problem. Right now both Sparse and Dense vector parsing is broken, not to mention corresponding tests are dead code.","20/Apr/16 00:11;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/12511","20/Apr/16 00:23;apachespark;User 'vishnu667' has created a pull request for this issue:
https://github.com/apache/spark/pull/12512","20/Apr/16 00:28;arashpa;[~zero323] sure I can adjust my PR (move the tests), but since I found the bug do you think I should be getting the fix in? ","20/Apr/16 00:34;apachespark;User 'vishnu667' has created a pull request for this issue:
https://github.com/apache/spark/pull/12513","20/Apr/16 00:46;zero323;Sure, but your latest PR still doesn't resolve problem with dead tests. Instead of copying you could actually pull changes from my repo.

",20/Apr/16 01:03;zero323;I extracted relevant test fixes and made PR against your branch. ,"20/Apr/16 01:21;apachespark;User 'arashpa' has created a pull request for this issue:
https://github.com/apache/spark/pull/12515",20/Apr/16 01:21;arashpa;Sorry wasn't able to pull from your branch. I submitted a new PR with proper updates. Please let me know how it looks.,20/Apr/16 01:22;vishnu667;I've merged your PR with your test fixes. Thank you,"20/Apr/16 01:27;apachespark;User 'arashpa' has created a pull request for this issue:
https://github.com/apache/spark/pull/12516",20/Apr/16 22:46;josephkb;Note this may be lower priority as we move linear algebra to mllib-local in [SPARK-13944].  But it would be good to fix.,"21/Apr/16 10:30;srowen;Issue resolved by pull request 12516
[https://github.com/apache/spark/pull/12516]",,,,,,,,,
mllib DenseMatrix toArray could use the internal values,SPARK-14697,12959604,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Done,,jamborta,jamborta,18/Apr/16 10:07,18/Apr/16 18:02,15/Aug/18 23:03,18/Apr/16 18:02,1.6.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"When toArray is called from an mllib Densematrix, it calls the function from the superclass Matrix, where it creates a new array (and iterates through all the values).

This is not necessary as values are stored in DenseMatrix, in exactly the format that this function would require. Why not just call that?

",,jamborta,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-04-18 10:13:26.158,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 18 18:02:11 UTC 2016,,,,,0|i2w94v:,9223372036854775807,,,,,,,,,,,,,"18/Apr/16 10:13;srowen;I think the problem is that you're then accessing the internal representation, and that ends up being generally problematic. At least, I think returning a defensive copy is on purpose.","18/Apr/16 18:02;josephkb;This does point to something which has been bugging me: We don't do a good job of distinguishing between light vs. heavy operations for constructing vectors and matrices and for accessing their internals, which is important for cheap conversions between linear algebra libraries.  I just created a separate issue for this: [SPARK-14707]

I'll close this issue, but thanks for bringing this up.",,,,,,,,,,,,,,,,,,,,,,
Update RDD.treeAggregate not to use reduce,SPARK-14408,12956227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,josephkb,josephkb,josephkb,05/Apr/16 18:03,09/Jun/17 07:53,15/Aug/18 23:03,09/Jun/17 07:53,,,,,,,,,,,,,,,,2.3.0,,,,,,ML,MLlib,Spark Core,,,0,,,,,"**Issue**
In MLlib, we have assumed that {{RDD.treeAggregate}} allows the {{seqOp}} and {{combOp}} functions to modify and return their first argument, just like {{RDD.aggregate}}.  However, it is not documented that way.

I started to add docs to this effect, but then noticed that {{treeAggregate}} uses {{reduceByKey}} and {{reduce}} in its implementation, neither of which technically allows the seq/combOps to modify and return their first arguments.

**Question**: Is the implementation safe, or does it need to be updated?

**Decision**: Avoid using reduce.  Use fold instead.",,apachespark,dbtsai,holdenk,josephkb,mengxr,mlnick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-04-05 19:13:04.996,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 09 07:53:38 UTC 2017,,,,,0|i2vobj:,9223372036854775807,,,,,,,,,,,,,"05/Apr/16 19:13;srowen;Is it right-er to use the fold methods? I had thought this was one of the key differences, yes, that the fold methods were definitely allowed to modify their first args. In practice, it might happen to be fine for reduce too.","05/Apr/16 20:36;josephkb;fold seems better to me.  PairRDDFunctions.foldByKey does not explicitly say the function can modify the first argument, but comparing its implementation with aggregateByKey, I would guess it is fine.  What do you think?

RDD.fold should be fine, based on its docs.

This seems like a difficult thing to cover in unit tests...","05/Apr/16 20:46;srowen;Yeah it's right-er at least. There's a test for foldByKey with a mutable type: ""foldByKey with mutable result type"", though maybe it could be tweaked to use multiple partitions explicitly. I suppose you could assert this explicitly in the docs since apparently it's tested for.","06/Apr/16 20:58;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/12217","07/Apr/16 20:26;josephkb;After a bit of a scare (b/c of the confounding issue of StandardScaler not matching sklearn), it's definitely an issue with my initial PR to ""fix"" treeAggregate's implementation.  That said, I'm still having a hard time figuring out the right way to fix the implementation.  I'll comment more on the PR.","07/Apr/16 21:22;josephkb;Note on StandardScaler: MLlib's StandardScaler uses the unbiased sample std to rescale, whereas sklearn uses the biased sample std.
* [sklearn.preprocessing.StandardScaler | http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html] uses biased sample std.  R's [scale package | https://stat.ethz.ch/R-manual/R-devel/library/base/html/scale.html] uses the unbiased sample std.  I'm used to seeing the biased sample std used in ML, probably because it is handy for proofs to know columns have L2 norm 1. 
* [~mengxr] reports that glmnet uses the biased sample std.
* *Q*: Should we change StandardScaler to use unbiased sample std?","08/Apr/16 08:35;dbtsai;I remember that when we implemented the scaler, we had similar discussion. We ended up following R's scale package which is unbiased std. How about we add extra flag to StandardScaler to make it biased but default to unbiased? 

I remember that when I implemented LOR/LiR in Spark, there are packages in R using unbiased std to scale the features, and most of the time, when the # of samples are huge, this will not be a concern. So I ended up just using the standard scaler in mllib.","05/Jun/17 04:31;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18198","09/Jun/17 07:53;srowen;Issue resolved by pull request 18198
[https://github.com/apache/spark/pull/18198]",,,,,,,,,,,,,,,
Use treeAggregate instead of reduce in OnlineLDAOptimizer,SPARK-14322,12955140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuhaoyan,josephkb,josephkb,31/Mar/16 23:25,06/Apr/16 18:39,15/Aug/18 23:03,06/Apr/16 18:39,1.3.1,1.4.1,1.5.2,1.6.1,2.0.0,,,,,,,,,,,1.5.3,1.6.2,2.0.0,,,,ML,MLlib,,,,0,,,,,"OnlineLDAOptimizer uses {{RDD.reduce}} in two places where it could use treeAggregate.  This can cause scalability issues.  This should be an easy fix.

This is also a bug since it modifies the first argument to reduce, so we should use aggregate or treeAggregate.

See this line: [https://github.com/apache/spark/blob/f12f11e578169b47e3f8b18b299948c0670ba585/mllib/src/main/scala/org/apache/spark/mllib/clustering/LDAOptimizer.scala#L452]
and a few lines below it.",,apachespark,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-04-01 03:24:04.8,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 01 03:24:04 UTC 2016,,,,,0|i2vhmf:,9223372036854775807,josephkb,,,,,1.5.3,1.6.2,2.0.0,,,,,"01/Apr/16 03:24;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/12106",,,,,,,,,,,,,,,,,,,,,,,
LDA should support disable checkpoint,SPARK-14298,12954987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yanboliang,yanboliang,yanboliang,31/Mar/16 14:54,11/Apr/16 23:19,15/Aug/18 23:03,11/Apr/16 23:19,1.5.2,1.6.1,2.0.0,,,,,,,,,,,,,1.5.3,1.6.2,2.0.0,,,,ML,MLlib,,,,0,,,,,LDA should support disable checkpoint by setting checkpointInterval = -1,,apachespark,glenn.strycker@gmail.com,josephkb,michaelmalak,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-03-31 14:59:03.785,,false,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 10 08:51:03 UTC 2016,,,,,0|i2vgof:,9223372036854775807,josephkb,,,,,1.5.3,1.6.2,2.0.0,,,,,"31/Mar/16 14:59;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/12089","08/Apr/16 18:50;josephkb;Issue resolved by pull request 12089
[https://github.com/apache/spark/pull/12089]","08/Apr/16 18:50;josephkb;Reopening for backports, pending unit test PR","10/Apr/16 08:51;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/12286",,,,,,,,,,,,,,,,,,,,
Incorrect use of binarysearch in SparseMatrix,SPARK-14187,12953896,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,luckyrand@gmail.com,luckyrand@gmail.com,luckyrand@gmail.com,28/Mar/16 04:37,28/Mar/16 16:13,15/Aug/18 23:03,28/Mar/16 15:35,1.2.2,1.3.1,1.4.1,1.5.2,1.6.1,2.0.0,,,,,,,,,,1.5.3,1.6.2,2.0.0,,,,MLlib,,,,,0,,,,,"{{SparseMatrix}} use binarySearch to find {{index}}. 

{quote}
binarySearch returns index of the search key, if it is contained in the array within the specified range; otherwise, (-(insertion point) - 1).
{quote}

On the other hand, {{update}} method only compare the returned index with -1 to decide whether it is found or not.",,apachespark,luckyrand@gmail.com,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-03-28 05:05:03.647,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 28 15:35:20 UTC 2016,,,,,0|i2v9y7:,9223372036854775807,,,,,,1.5.3,1.6.2,2.0.0,,,,,"28/Mar/16 05:05;apachespark;User 'luckyrandom' has created a pull request for this issue:
https://github.com/apache/spark/pull/11992","28/Mar/16 15:35;mengxr;Issue resolved by pull request 11992
[https://github.com/apache/spark/pull/11992]",,,,,,,,,,,,,,,,,,,,,,
Add a constructor parameter `regParam` to (Streaming)LinearRegressionWithSGD,SPARK-13686,12947095,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,04/Mar/16 21:25,14/Mar/16 19:47,15/Aug/18 23:03,14/Mar/16 19:47,,,,,,,,,,,,,,,,2.0.0,,,,,,DStreams,MLlib,,,,0,,,,,"`LinearRegressionWithSGD` and `StreamingLinearRegressionWithSGD` does not have `regParam` as their constructor arguments. They just depends on GradientDescent's default reqParam values. 

To be consistent with other algorithms, we had better add them.",,apachespark,dongjoon,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-03-04 21:32:05.359,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 14 19:47:10 UTC 2016,,,,,0|i2u6ef:,9223372036854775807,,,,,,,,,,,,,"04/Mar/16 21:32;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/11527","09/Mar/16 21:15;dongjoon;Hi, [~mengxr].

Could you take a look at this issue when you have some time?","14/Mar/16 19:47;mengxr;Issue resolved by pull request 11527
[https://github.com/apache/spark/pull/11527]",,,,,,,,,,,,,,,,,,,,,
Fix mismatched default values for regParam in LogisticRegression,SPARK-13676,12946958,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,04/Mar/16 11:50,04/Mar/16 16:27,15/Aug/18 23:03,04/Mar/16 16:25,,,,,,,,,,,,,,,,2.0.0,,,,,,ML,MLlib,,,,0,,,,,"The default value of regularization parameter for `LogisticRegression` algorithm is different in Scala and Python. We should provide the same value.

{code:title=Scala|borderStyle=solid}
scala> new org.apache.spark.ml.classification.LogisticRegression().getRegParam
res0: Double = 0.0
{code}

{code:title=Python|borderStyle=solid}
>>> from pyspark.ml.classification import LogisticRegression
>>> LogisticRegression().getRegParam()
0.1
{code}",,apachespark,dongjoon,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-03-04 12:03:04.69,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 04 16:25:49 UTC 2016,,,,,0|i2u5jz:,9223372036854775807,,,,,,2.0.0,,,,,,,"04/Mar/16 12:03;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/11519","04/Mar/16 16:25;mengxr;Issue resolved by pull request 11519
[https://github.com/apache/spark/pull/11519]",,,,,,,,,,,,,,,,,,,,,,
Use approxQuantile from DataFrame stats in QuantileDiscretizer,SPARK-13600,12945902,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ocp,ocp,ocp,01/Mar/16 18:46,11/Apr/16 19:03,15/Aug/18 23:03,11/Apr/16 19:03,1.6.0,2.0.0,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"For consistency and code reuse, QuantileDiscretizer should use approxQuantile to find splits in the data rather than implement it's own method.

Additionally, making this change should remedy a bug where QuantileDiscretizer fails to calculate the correct splits in certain circumstances, resulting in an incorrect number of buckets/bins.

E.g.

val df = sc.parallelize(1.0 to 10.0 by 1.0).map(Tuple1.apply).toDF(""x"")
val discretizer = new QuantileDiscretizer().setInputCol(""x"").setOutputCol(""y"").setNumBuckets(5)
discretizer.fit(df).getSplits

gives:
Array(-Infinity, 2.0, 4.0, 6.0, 8.0, 10.0, Infinity)
which corresponds to 6 buckets (not 5).
",,apachespark,josephkb,mengxr,mlnick,ocp,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10785,,,,,SPARK-10785,,,,,0.0,,,,,,,,,,,,,,,,2016-03-02 18:08:54.931,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 11 19:03:08 UTC 2016,,,,,0|i2tzkv:,9223372036854775807,mlnick,,,,,,,,,,,,02/Mar/16 18:08;mlnick;[~ocp] do you plan to submit a PR? Since you worked on SPARK-13444?,"02/Mar/16 22:11;ocp;Yeah, you can assign this to me.  However, it may be a few days or even a week before I can get a PR together.  I was hoping to get people's opinion on reimplementing findSplitCandidates using another [method|https://en.wikipedia.org/wiki/Quantile#Estimating_quantiles_from_a_sample] (see the second paragraph of that section). I believe it's done this way in Numpy/Scipy. [~srowen][~mengxr][~yinxusen]",03/Mar/16 07:13;yinxusen;Vote for the new method.,"07/Mar/16 03:13;apachespark;User 'oliverpierson' has created a pull request for this issue:
https://github.com/apache/spark/pull/11553","08/Mar/16 09:34;mlnick;[~ocp] Could you update this ticket with something about the approach taken of using DataFrame {{approxQuantile}} - perhaps even update the title. Or it may make sense to create a new JIRA for the change to using {{approxQuantile}}, since it is quite an important change.","08/Mar/16 14:20;ocp;Sure thing.  I think the best way to fix the bug is to use approxQuantile so I've updated the ticket rather than open a new one.  If you'd prefer a new JIRA instead, let me know and I'll open it.  Thanks.","09/Mar/16 07:11;mlnick;Thanks, that's fine","11/Apr/16 19:03;mengxr;Issue resolved by pull request 11553
[https://github.com/apache/spark/pull/11553]",,,,,,,,,,,,,,,,
Fix the wrong parameter in R code comment in AssociationRulesSuite ,SPARK-13506,12944813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,podongfeng,podongfeng,podongfeng,26/Feb/16 08:12,29/Feb/16 14:52,15/Aug/18 23:03,29/Feb/16 14:51,,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"The following R Snippet in AssociationRulesSuite is wrong:

    /* Verify results using the `R` code:
       transactions = as(sapply(
         list(""r z h k p"",
              ""z y x w v u t s"",
              ""s x o n r"",
              ""x z y m t s q e"",
              ""z"",
              ""x z y r q t p""),
         FUN=function(x) strsplit(x,"" "",fixed=TRUE)),
         ""transactions"")
       ars = apriori(transactions,
                     parameter = list(support = 0.0, confidence = 0.5, target=""rules"", minlen=2))
       arsDF = as(ars, ""data.frame"")
       arsDF$support = arsDF$support * length(transactions)
       names(arsDF)[names(arsDF) == ""support""] = ""freq""
       > nrow(arsDF)
       [1] 23
       > sum(arsDF$confidence == 1)
       [1] 23
     */

The real outputs are:
> nrow(arsDF)
[1] 441838
> sum(arsDF$confidence == 1)
[1] 441592

It is found that the parameters in apriori function were wrong.",,apachespark,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-02-26 08:16:03.248,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 29 14:51:39 UTC 2016,,,,,0|i2tsvb:,9223372036854775807,,,,,,,,,,,,,"26/Feb/16 08:16;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/11387","29/Feb/16 14:51;srowen;Issue resolved by pull request 11387
[https://github.com/apache/spark/pull/11387]",,,,,,,,,,,,,,,,,,,,,,
QuantileDiscretizer chooses bad splits on large DataFrames,SPARK-13444,12941270,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ocp,ocp,ocp,23/Feb/16 03:49,07/Mar/16 09:48,15/Aug/18 23:03,25/Feb/16 13:27,1.6.0,2.0.0,,,,,,,,,,,,,,1.6.2,2.0.0,,,,,MLlib,,,,,0,,,,,"In certain circumstances, QuantileDiscretizer fails to calculate the correct splits and will instead split data into two bins regardless of the value specified in numBuckets.

For example, supposed dataset.count is 200 million.  And we do

val discretizer = new QuantileDiscretizer().setNumBuckets(10)
  ... set output and input columns ...
val dataWithBins = discretizer.fit(dataset).transform(dataset)

In this case, dataWithBins will have only two distinct bins versus the expected 10.

Problem is in line 113 and 114 of QuantileDiscretizer.scala and can be fixed by changing line 113 like so:
before: val requiredSamples = math.max(numBins * numBins, 10000)
after: val requiredSamples = math.max(numBins * numBins, 10000.0)

",,apachespark,mengxr,ocp,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-02-23 04:06:03.032,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 26 21:18:04 UTC 2016,,,,,0|i2t6zz:,9223372036854775807,,,,,,,,,,,,,"23/Feb/16 03:50;ocp;I've made the changes in my local fork and can put together a PR tomorrow.
","23/Feb/16 04:06;apachespark;User 'oliverpierson' has created a pull request for this issue:
https://github.com/apache/spark/pull/11319",23/Feb/16 04:06;ocp;PR submitted,"25/Feb/16 13:27;srowen;Issue resolved by pull request 11319
[https://github.com/apache/spark/pull/11319]","25/Feb/16 22:52;apachespark;User 'oliverpierson' has created a pull request for this issue:
https://github.com/apache/spark/pull/11377","26/Feb/16 21:18;apachespark;User 'oliverpierson' has created a pull request for this issue:
https://github.com/apache/spark/pull/11402",,,,,,,,,,,,,,,,,,
MLlib LogisticRegressionWithLBFGS swaps L1 and L2 incorrectly ,SPARK-13379,12940370,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,19/Feb/16 03:00,22/Feb/16 04:37,15/Aug/18 23:03,22/Feb/16 04:23,,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"We should correct MLlib LogisticRegressionWithLBFGS regularization map as:
SquaredL2Updater -> ""elasticNetParam = 0.0""
L1Updater -> ""elasticNetParam = 1.0""",,apachespark,dbtsai,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-02-19 03:06:03.695,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 04:37:17 UTC 2016,,,,,0|i2t1g7:,9223372036854775807,,,,,,,,,,,,,"19/Feb/16 03:06;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/11258","19/Feb/16 10:25;srowen;(You can make these more descriptive -- it swaps L1 and L2 regularization right? ""sets incorrectly"" doesn't say much at all about the problem)","22/Feb/16 04:23;dbtsai;Issue resolved by pull request 11258
[https://github.com/apache/spark/pull/11258]",22/Feb/16 04:37;dbtsai;[~srowen] Thanks. I updated the title as you suggested. ,,,,,,,,,,,,,,,,,,,,
Replace GraphImpl.fromExistingRDDs by Graph,SPARK-13355,12939678,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,17/Feb/16 01:46,03/Nov/16 23:48,15/Aug/18 23:03,23/Feb/16 07:55,1.3.1,1.4.1,1.5.2,1.6.0,2.0.0,,,,,,,,,,,1.4.2,1.5.3,1.6.1,2.0.0,,,ML,MLlib,,,,0,,,,,`GraphImpl.fromExistingRDDs` expects preprocessed vertex RDD as input. We call it in LDA without validating this requirement. So it might introduce errors. Replacing it by `Gpaph.apply` would be safer and more proper because it is a public API. ,,apachespark,mengxr,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-12488,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-02-17 01:54:04.736,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 17 01:54:04 UTC 2016,,,,,0|i2sx6n:,9223372036854775807,,,,,,1.5.3,1.6.1,2.0.0,,,,,"17/Feb/16 01:54;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/11226",,,,,,,,,,,,,,,,,,,,,,,
[ML] Allow setting 'degree' parameter to 1 for PolynomialExpansion,SPARK-13338,12939481,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,grzegorz.chilkiewicz,grzegorz.chilkiewicz,grzegorz.chilkiewicz,16/Feb/16 12:10,23/Feb/16 18:30,15/Aug/18 23:03,23/Feb/16 18:30,1.6.0,2.0.0,,,,,,,,,,,,,,2.0.0,,,,,,ML,MLlib,,,,0,,,,,"PolynomialExpansion has bug in validation of 'degree' parameter.
It does not allow setting degree to 1",,apachespark,grzegorz.chilkiewicz,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-02-16 12:27:05.498,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 23 18:30:09 UTC 2016,,,,,0|i2svz3:,9223372036854775807,,,,,,2.0.0,,,,,,,"16/Feb/16 12:27;apachespark;User 'grzegorz-chilkiewicz' has created a pull request for this issue:
https://github.com/apache/spark/pull/11216","23/Feb/16 18:30;mengxr;Issue resolved by pull request 11216
[https://github.com/apache/spark/pull/11216]",,,,,,,,,,,,,,,,,,,,,,
Word2Vec generate infinite distances when numIterations>5,SPARK-13289,12938623,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,flysjy,daiqi5477,daiqi5477,11/Feb/16 21:11,01/May/16 15:35,15/Aug/18 23:03,30/Apr/16 09:16,1.6.0,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,features,,,,"I recently ran some word2vec experiments on a cluster with 50 executors on some large text dataset but find out that when number of iterations is larger than 5 the distance between words will be all infinite. My code looks like this:

val text = sc.textFile(""/project/NLP/1_biliion_words/train"").map(_.split("" "").toSeq)
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
val word2vec = new Word2Vec().setMinCount(25).setVectorSize(96).setNumPartitions(99).setNumIterations(10).setWindowSize(5)
val model = word2vec.fit(text)
val synonyms = model.findSynonyms(""who"", 40)
for((synonym, cosineSimilarity) <- synonyms) {
  println(s""$synonym $cosineSimilarity"")
}

The results are: 
to Infinity
and Infinity
that Infinity
with Infinity
said Infinity
it Infinity
by Infinity
be Infinity
have Infinity
he Infinity
has Infinity
his Infinity
an Infinity
) Infinity
not Infinity
who Infinity
I Infinity
had Infinity
their Infinity
were Infinity
they Infinity
but Infinity
been Infinity

I tried many different datasets and different words for finding synonyms.","Linux, Scala",apachespark,daiqi5477,flysjy,michaelmalak,mlnick,rspitzer,wziyong,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-02-12 10:33:41.4,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 30 09:16:38 UTC 2016,,,,,0|i2sqpj:,9223372036854775807,mlnick,,,,,,,,,,,,12/Feb/16 10:33;srowen;THere isn't enough info to reproduce anything here. We don't have your data. You'd have to supply a much more specific lead on what the issue is.,"12/Feb/16 15:16;daiqi5477;I'm using the ""One Billion Words Language Modeling"" dataset available at: http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz

As for the code, it's very simple and you can run it in spark-shell:
val text = sc.textFile(""/project/NLP/1_biliion_words/train"").map(_.split("" "").toSeq)
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
val word2vec = new Word2Vec().setMinCount(25).setVectorSize(96).setNumPartitions(99).setNumIterations(10).setWindowSize(5)
val model = word2vec.fit(text)
val synonyms = model.findSynonyms(""who"", 40)
for((synonym, cosineSimilarity) <- synonyms)
{ println(s""$synonym $cosineSimilarity"") }

For environment, I believe you can reproduce it with any cluster that can run spark 1.6 (I didn't try local mode). 

Please note, the problem happens with multiple iterations. When you are trying to reproduce it, please set iterations>5, e.g. 10 ","16/Feb/16 16:34;daiqi5477;Hi Sean, Are you able to reproduce the issue? Do you need any other details? (Maybe the reply I made few days ago didn't send to you. If so, could you take a look at the details I provided below?) I tried some other parameters. It looks like it's more likely to fail with larger dataset, more partitions and more iterations. ",16/Feb/16 16:41;srowen;I haven't run it. It'd be great if you can run with this and propose a fix?,16/Feb/16 17:09;daiqi5477;I'm not familiar with the algorithm and implementation. Maybe we need to wait for some other people in the community who involved in the implementation to take a look at the issue.,"16/Feb/16 17:11;srowen;That's not usually how it works; I wouldn't expect others to debug for you. Just step through the code?
","16/Feb/16 17:18;daiqi5477;Yes, just download the data from the url. Unzip it and change the path the the training data folder. Then, step through the rest parts and the issue should be able to reproduced. (probably it's better to run on a cluster because the dataset is big) ","22/Feb/16 11:16;mlnick;[~daiqi5477] could you try your experiments again against the latest master, and see if you run into the same issue?
","22/Feb/16 18:10;daiqi5477;Do you have a runable distribution of the latest master somewhere? I tried to build it but didn't get through. It failed at building spark-catalyst_2.11 with the following error: Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:2.4.3:shade (default) on project spark-catalyst_2.11: Error creating shaded jar: Method code too large! -> [Help 1] 

I was building with ""build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean package""

I tried ""export MAVEN_OPTS=""-Xmx4g -XX:MaxPermSize=4g -XX:ReservedCodeCacheSize=2g"""" and ""export MAVEN_OPTS=""-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"""" 

I'm using java 1.7.80 on Ubuntu 14.04.3","23/Feb/16 02:23;wziyong;I pull the latest code,and I also met the problem like yours.Did you solve it?Thanks!",23/Feb/16 03:18;rspitzer;same,23/Feb/16 06:10;vectorijk;same. Spark-13431 mentioned this.,23/Feb/16 07:18;mlnick;Yes the master build is currently failing as detailed in SPARK-13431,26/Feb/16 07:54;mlnick;Master branch should be building now. Can you try again?,"02/Mar/16 19:27;daiqi5477;I tried ""build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean package"". I looks successful but I can't run it in yarn-client mode. Then I turned to ""./make-distribution.sh --name spark210 --tgz -Psparkr -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn"" but it can't go through. I also tried the compiled one at: http://people.apache.org/~pwendell/spark-nightly/spark-master-bin/latest/ and it also can't run with yarn-client mode. It showed some error related with yarn:

16/03/02 14:22:52 ERROR SparkContext: Error initializing SparkContext.
java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.history.YarnHistoryService
        at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:348)
        at org.apache.spark.util.Utils$.classForName(Utils.scala:174)
        at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5$$anonfun$apply$4.apply(SchedulerExtensionService.scala:111)
        at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5$$anonfun$apply$4.apply(SchedulerExtensionService.scala:110)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
        at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:110)
        at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:108)
        at scala.Option.map(Option.scala:146)
        at org.apache.spark.scheduler.cluster.SchedulerExtensionServices.start(SchedulerExtensionService.scala:108)
        at org.apache.spark.scheduler.cluster.YarnSchedulerBackend.start(YarnSchedulerBackend.scala:80)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:61)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:143)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:501)
        at org.apache.spark.repl.Main$.createSparkContext(Main.scala:98)
        at $line3.$read$$iw$$iw.<init>(<console>:12)
        at $line3.$read$$iw.<init>(<console>:22)
        at $line3.$read.<init>(<console>:24)
        at $line3.$read$.<init>(<console>:28)
        at $line3.$read$.<clinit>(<console>)
        at $line3.$eval$.$print$lzycompute(<console>:7)
        at $line3.$eval$.$print(<console>:6)
        at $line3.$eval.$print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:784)
        at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1039)
        at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:636)
        at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:635)
        at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
        at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
        at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:635)
        at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:567)
        at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:563)
        at scala.tools.nsc.interpreter.ILoop.reallyInterpret$1(ILoop.scala:802)
        at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:836)
        at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:694)
        at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:404)
        at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply$mcZ$sp(SparkILoop.scala:39)
        at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:38)
        at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:38)
        at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:213)
        at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:38)
        at org.apache.spark.repl.SparkILoop.loadFiles(SparkILoop.scala:95)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:922)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:911)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:911)
        at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)
        at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:911)
        at org.apache.spark.repl.Main$.doMain(Main.scala:64)
        at org.apache.spark.repl.Main$.main(Main.scala:47)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:734)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/03/02 14:22:52 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
16/03/02 14:22:52 WARN MetricsSystem: Stopping a MetricsSystem that is not running
java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.history.YarnHistoryService
  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  at java.lang.Class.forName0(Native Method)
  at java.lang.Class.forName(Class.java:348)
  at org.apache.spark.util.Utils$.classForName(Utils.scala:174)
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5$$anonfun$apply$4.apply(SchedulerExtensionService.scala:111)
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5$$anonfun$apply$4.apply(SchedulerExtensionService.scala:110)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:110)
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:108)
  at scala.Option.map(Option.scala:146)
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices.start(SchedulerExtensionService.scala:108)
  at org.apache.spark.scheduler.cluster.YarnSchedulerBackend.start(YarnSchedulerBackend.scala:80)
  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:61)
  at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:143)
  at org.apache.spark.SparkContext.<init>(SparkContext.scala:501)
  at org.apache.spark.repl.Main$.createSparkContext(Main.scala:98)
  ... 48 elided
java.lang.NullPointerException
  at org.apache.spark.sql.SQLContext$.createListenerAndUI(SQLContext.scala:1033)
  at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:88)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
  at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
  at org.apache.spark.repl.Main$.createSQLContext(Main.scala:108)
  ... 48 elided
<console>:13: error: not found: value sqlContext
       import sqlContext.implicits._
              ^
<console>:13: error: not found: value sqlContext
       import sqlContext.sql
              ^
Probably it's better to wait for the newer version released. ","18/Mar/16 03:51;apachespark;User 'flyjy' has created a pull request for this issue:
https://github.com/apache/spark/pull/11812","18/Mar/16 03:58;flysjy;This PR gives the distance values between 0 and 1.

scala> model.findSynonyms(""who"", 10)
res0: Array[(String, Double)] = Array((Harvard-educated,0.5253688097000122), (ex-SAS,0.5213794708251953), (McMutrie,0.5187736749649048), (fellow,0.5166833400726318), (businessman,0.5145374536514282), (American-born,0.5127736330032349), (British-born,0.5062344074249268), (gray-bearded,0.5047978162765503), (American-educated,0.5035858750343323), (mentored,0.49849334359169006))

scala> model.findSynonyms(""king"", 10)
res1: Array[(String, Double)] = Array((queen,0.6787897944450378), (prince,0.6786158084869385), (monarch,0.659771203994751), (emperor,0.6490438580513), (goddess,0.643266499042511), (dynasty,0.635733425617218), (sultan,0.6166239380836487), (pharaoh,0.6150713562965393), (birthplace,0.6143025159835815), (empress,0.6109727025032043))

scala> model.findSynonyms(""queen"", 10)
res2: Array[(String, Double)] = Array((princess,0.7670737504959106), (godmother,0.6982434988021851), (raven-haired,0.6877717971801758), (swan,0.684934139251709), (hunky,0.6816608309745789), (Titania,0.6808111071586609), (heroine,0.6794036030769348), (king,0.6787897944450378), (diva,0.67848801612854), (lip-synching,0.6731793284416199))
","21/Mar/16 19:57;daiqi5477;I'm trying to test it with the current master branch and nightly build with yarn, but spark always fail to start with the java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.history.YarnHistoryService issue. Does anyone have any idea about this? Looks like no one is reporting this issue. Should I raise another new issue about this?

The stack is like this:
java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.history.YarnHistoryService
  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  at java.lang.Class.forName0(Native Method)
  at java.lang.Class.forName(Class.java:348)
  at org.apache.spark.util.Utils$.classForName(Utils.scala:177)
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:109)
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:108)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:74)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices.start(SchedulerExtensionService.scala:108)
  at org.apache.spark.scheduler.cluster.YarnSchedulerBackend.start(YarnSchedulerBackend.scala:81)
  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)
  at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144)
  at org.apache.spark.SparkContext.<init>(SparkContext.scala:501)
  at org.apache.spark.repl.Main$.createSparkContext(Main.scala:89)
  ... 48 elided
java.lang.NullPointerException
  at org.apache.spark.sql.SQLContext$.createListenerAndUI(SQLContext.scala:1036)
  at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:91)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
  at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
  at org.apache.spark.repl.Main$.createSQLContext(Main.scala:99)
  ... 48 elided
<console>:13: error: not found: value sqlContext
       import sqlContext.implicits._
              ^
<console>:13: error: not found: value sqlContext
       import sqlContext.sql
              ^","23/Mar/16 15:59;flysjy;Can you run the SparkContext? The above logs show that sqlContext is not good, but, you should be able to run the word2vec scripts.",23/Mar/16 16:07;daiqi5477;sc also failed to start,"28/Mar/16 04:12;daiqi5477;I tested this commit on the ""One Billion Words Language Modeling"" dataset with 72 partitions and 15 iterations. It works well. (word2vec.scala has been changed recently. This PR might need to be updated accordingly.)",28/Mar/16 04:14;daiqi5477;I figured out the issue. It's caused by the SPARK_CONF set in my .bashrc for the 1.x spark.,"21/Apr/16 06:58;flysjy;The latest PR should have fixed the issue, and is ready to be merged. Please let me know if there is anything that I can do.","30/Apr/16 09:16;srowen;Issue resolved by pull request 11812
[https://github.com/apache/spark/pull/11812]"
RandomForest is stuck at computing same stage over and over,SPARK-13115,12935706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,tanwanirahul,tanwanirahul,01/Feb/16 18:42,02/Feb/16 12:09,15/Aug/18 23:03,02/Feb/16 03:09,1.5.2,,,,,,,,,,,,,,,,,,,,,ML,MLlib,Spark Core,,,0,,,,,"While running the RandomForest regression, the algorithm keeps computing the same stage and does not proceed any further. I have observed the same stage being computed for more than 11 hours. Attached are some of the captures from Spark WebUI.

Also, the spark event logs for this model run could be fetched from Spark Event Logs (https://s3.amazonaws.com/com.tookitaki.public.logs/spark-event-logs). I am running spark-1.5.2 in the standalone local mode. Also, I wanted to know why any stage is marked skipped? 

Let me know if you would need more information.",,tanwanirahul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Feb/16 18:45;tanwanirahul;Stage details.png;https://issues.apache.org/jira/secure/attachment/12785568/Stage+details.png,01/Feb/16 18:44;tanwanirahul;Stages.png;https://issues.apache.org/jira/secure/attachment/12785567/Stages.png,01/Feb/16 18:46;tanwanirahul;Task details.png;https://issues.apache.org/jira/secure/attachment/12785569/Task+details.png,3.0,,,,,,,,,,,,,,,,2016-02-02 03:09:08.218,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 02 12:09:55 UTC 2016,,,,,0|i2s8z3:,9223372036854775807,,,,,,,,,,,,,"02/Feb/16 03:09;srowen;Without more info, this didn't necessarily sound like a problem.  Anything can be slow without enough resource. You would have to narrow down why you think something is stuck and ideally propose a change. ","02/Feb/16 03:47;tanwanirahul;Computing the same stage over and over again is different than progressing at a slower rate. I have run the same dataset with 40 cores and 100 GB Memory in the cluster mode but the issue remains. Furthermore, even the bigger and wider datasets than this particular dataset has completed in very finite amount of time.

Could you suggest what more information help us to find and digg into the issue?","02/Feb/16 12:09;srowen;Failing repeatedly is still likely to be a function of the rest of your app, the input, or the cluster. The question is what exactly is the failure and why is it a problem in RF itself?",,,,,,,,,,,,,,,,,,,,,
HashTF dosn't count TF correctly,SPARK-13103,12935354,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Cannot Reproduce,,louisliutw,louisliutw,30/Jan/16 14:33,07/Feb/16 07:38,15/Aug/18 23:03,07/Feb/16 07:38,1.6.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"I wrote a Python program to calculate frequencies of n-gram sequences with HashTF.
But it generate a strange output. It found more ""一一下嗎"" than ""一一下"".

HashTF gets words' index with hash()
But hashes of some Chinese words are negative.
Ex:
>>> hash('一一下嗎')
-6433835193350070115
>>> hash('一一下')
-5938108283593463272","Ubuntu 14.04
Python 3.4.3",holdenk,louisliutw,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-01-30 15:47:22.276,,false,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 07 07:38:54 UTC 2016,,,,,0|i2s6sv:,9223372036854775807,,,,,,,,,,,,,"30/Jan/16 15:47;yuhaoyan;Thanks for finding this. I'm not sure what's the historical reason, yet it's not common that HashingTF in Python was implemented independently from the Scala version.





","01/Feb/16 21:59;holdenk;I don't think this is unique to chinese (e.g. the hash of the word string is negative in python 2, 3 , and scala).
However the modulu operator used will return positive numbers (default behaviour in python and explicitly used in scala).
Do you have a small code sample which illustrates the problem more clearly?","02/Feb/16 12:27;srowen;Yes, I doubt it has anything to do with the hash code since the hash is not related to the count. [~louisliutw] what is some text that would show this result? What's your code? ","03/Feb/16 10:18;louisliutw;I'm sorry, you are right. The negative numbers doesn't matter.

Those code shall explain the problem:

>>> from pyspark.mllib.feature import HashingTF, IDF
>>> hashtf = HashingTF()
>>> hash('的問題哦')
-234244945207099392
>>> hash('豪們都把')
8689153874407194624
>>> hashtf.indexOf('的問題哦')
0 
>>> hashtf.indexOf('豪們都把')
0",03/Feb/16 19:36;holdenk;I think that is expected behavior - if your finding too large a number of collisions you can change the number of features with setNumFeatures ,"07/Feb/16 07:38;srowen;I can't reproduce this:

{code}
>>> from pyspark.mllib.feature import HashingTF, IDF
>>> hashtf = HashingTF()
>>> hashtf.indexOf('的問題哦')
594182
>>> hashtf.indexOf('豪們都把')
227158
{code}

Can you try the latest master just to double check?

Your idea is a good one Holden but the default # of features is 2^20. That shouldn't be an issue here.",,,,,,,,,,,,,,,,,,
EMLDAOptimizer deletes dependent checkpoint of DistributedLDAModel,SPARK-13048,12934567,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,jvstein,jvstein,27/Jan/16 20:08,08/Apr/16 02:48,15/Aug/18 23:03,08/Apr/16 02:48,1.5.2,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"In EMLDAOptimizer, all checkpoints are deleted before returning the DistributedLDAModel.

The most recent checkpoint is still necessary for operations on the DistributedLDAModel under a couple scenarios:
- The graph doesn't fit in memory on the worker nodes (e.g. very large data set).
- Late worker failures that require reading the now-dependent checkpoint.

I ran into this problem running a 10M record LDA model in a memory starved environment. The model consistently failed in either the {{collect at LDAModel.scala:528}} stage (when converting to a LocalLDAModel) or in the {{reduce at LDAModel.scala:563}} stage (when calling ""describeTopics"" on the model). In both cases, a FileNotFoundException is thrown attempting to access a checkpoint file.

I'm not sure what the correct fix is here; it might involve a class signature change. An alternative simple fix is to leave the last checkpoint around and expect the user to clean the checkpoint directory themselves.

{noformat}
java.io.FileNotFoundException: File does not exist: /hdfs/path/to/checkpoints/c8bd2b4e-27dd-47b3-84ec-3ff0bac04587/rdd-635/part-00071
{noformat}

Relevant code is included below.

LDAOptimizer.scala:
{noformat}
  override private[clustering] def getLDAModel(iterationTimes: Array[Double]): LDAModel = {
    require(graph != null, ""graph is null, EMLDAOptimizer not initialized."")
    this.graphCheckpointer.deleteAllCheckpoints()
    // The constructor's default arguments assume gammaShape = 100 to ensure equivalence in
    // LDAModel.toLocal conversion
    new DistributedLDAModel(this.graph, this.globalTopicTotals, this.k, this.vocabSize,
      Vectors.dense(Array.fill(this.k)(this.docConcentration)), this.topicConcentration,
      iterationTimes)
  }
{noformat}

PeriodicCheckpointer.scala

{noformat}
  /**
   * Call this at the end to delete any remaining checkpoint files.
   */
  def deleteAllCheckpoints(): Unit = {
    while (checkpointQueue.nonEmpty) {
      removeCheckpointFile()
    }
  }

  /**
   * Dequeue the oldest checkpointed Dataset, and remove its checkpoint files.
   * This prints a warning but does not fail if the files cannot be removed.
   */
  private def removeCheckpointFile(): Unit = {
    val old = checkpointQueue.dequeue()
    // Since the old checkpoint is not deleted by Spark, we manually delete it.
    val fs = FileSystem.get(sc.hadoopConfiguration)
    getCheckpointFiles(old).foreach { checkpointFile =>
      try {
        fs.delete(new Path(checkpointFile), true)
      } catch {
        case e: Exception =>
          logWarning(""PeriodicCheckpointer could not remove old checkpoint file: "" +
            checkpointFile)
      }
    }
  }
{noformat}",Standalone Spark cluster,apachespark,holdenk,josephkb,jvstein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14420,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-02-01 22:43:05.031,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 08 02:48:51 UTC 2016,,,,,0|i2s1y7:,9223372036854775807,,,,,,2.0.0,,,,,,,"01/Feb/16 22:43;holdenk;This sounds useful, although we probably wouldn't want to always leave the last checkpoint around (and we also need to provide a way for the user to cleanup the last check point).

We could make a getCheckPointedLDAModel or offer a param to the current method and then add a cleanup function to the resulting LDAModel for the user to call. Any thoughts [~josephkb] or [~mengxr]? Also an interesting question would be how to expose something similar in the pipelines API.","20/Feb/16 00:25;jvstein;As an aside, the code in the clustering namespace violates the [open/closed principle](https://en.wikipedia.org/wiki/Open/closed_principle).
  - LDAOptimizer is unnecessarily a sealed trait (I understand it's a developer api, but I'm a developer...)
  - EMLDAOptimizer is final
  - Lots of private[clustering]

All of this meant that writing a decent workaround for the bug took a lot more code than I would have hoped.","04/Mar/16 17:49;josephkb;I'd say the best fix would be to add an option to LDA to not delete the last checkpoint.  I'd prefer to expose this as a Param in the spark.ml API, but it could be added to the spark.mllib API as well if necessary.

[~holdenk]  I agree we need to figure out how to handle/control caching and checkpointing within Pipelines, but that will have to wait for after 2.0.

[~jvstein]  We try to minimize the public API.  Although I agree with you about opening up APIs in principal, it have proven dangerous in practice.  Even when we mark things DeveloperApi, many users still use those APIs, making it difficult to change them in the future.",05/Apr/16 00:11;josephkb;I'll send a PR for this.  I'd prefer to fix this in 2.0 only since it will require a public API change (adding a Param saying not to delete the last checkpoint).,"05/Apr/16 01:46;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/12166",05/Apr/16 01:47;josephkb;[~jvstein] Would you be able to test whether the patch I just sent takes care of the problem you encountered?,"05/Apr/16 17:15;jvstein;Sure, I'll take a look. It'll be a few days before I can get around to it.","08/Apr/16 02:48;josephkb;Issue resolved by pull request 12166
[https://github.com/apache/spark/pull/12166]",,,,,,,,,,,,,,,,
Fix pydoc warnings in mllib/regression.py,SPARK-12986,12933926,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nampham,mengxr,mengxr,25/Jan/16 20:08,08/Feb/16 19:06,15/Aug/18 23:03,08/Feb/16 19:06,2.0.0,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,PySpark,,,,0,,,,,"Got those warnings by running ""make html"" under ""python/docs/"":

{code}
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.LinearRegressionWithSGD:3: ERROR: Unexpected indentation.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.LinearRegressionWithSGD:4: WARNING: Block quote ends without a blank line; unexpected unindent.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.RidgeRegressionWithSGD:3: ERROR: Unexpected indentation.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.RidgeRegressionWithSGD:4: WARNING: Block quote ends without a blank line; unexpected unindent.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.LassoWithSGD:3: ERROR: Unexpected indentation.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.LassoWithSGD:4: WARNING: Block quote ends without a blank line; unexpected unindent.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.IsotonicRegression:7: ERROR: Unexpected indentation.
/Users/meng/src/spark/python/pyspark/mllib/regression.py:docstring of pyspark.mllib.regression.IsotonicRegression:12: ERROR: Unexpected indentation.
{code}",,apachespark,bryanc,holdenk,mengxr,nampham,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-01-25 21:25:52.36,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 08 19:06:49 UTC 2016,,,,,0|i2rxzr:,9223372036854775807,,,,,,2.0.0,,,,,,,25/Jan/16 21:25;holdenk;Do you think we should maybe add make html to the lint python script and fail if there are warnings caused?,"25/Jan/16 21:56;bryanc;It looks like this is caused by an indented line not being preceded with a blank line.  For example the warnings for LinearRegressionWithSGD class description

{noformat}
    """"""
    Train a linear regression model with no regularization using Stochastic Gradient Descent.
    This solves the least squares regression formulation
                 f(weights) = 1/n ||A weights-y||^2^
    (which is the mean squared error).
    Here the data matrix has n rows, and the input RDD holds the set of rows of A, each with
    its corresponding right hand side label y.
    See also the documentation for the precise formulation.

    .. versionadded:: 0.9.0
    """"""
{noformat}

are fixed with this

{noformat}
     """"""
    Train a linear regression model with no regularization using Stochastic Gradient Descent.
    This solves the least squares regression formulation

                 f(weights) = 1/n ||A weights-y||^2^

    (which is the mean squared error).
    Here the data matrix has n rows, and the input RDD holds the set of rows of A, each with
    its corresponding right hand side label y.
    See also the documentation for the precise formulation.

    .. versionadded:: 0.9.0
    """"""
{noformat}",02/Feb/16 09:49;nampham;[~bryanc] was right. I have fixed the warnings and made a pull request.,"02/Feb/16 09:50;apachespark;User 'nampham2' has created a pull request for this issue:
https://github.com/apache/spark/pull/11025","03/Feb/16 01:23;mengxr;[~holdenk] I think it is useful to check the pydoc warnings at builds. ScalaDoc is quite noise, but Python warnings are usually pointing out real problems. Could you make a JIRA and ping Josh there?",03/Feb/16 04:20;holdenk;Sure :),"08/Feb/16 19:06;mengxr;Issue resolved by pull request 11025
[https://github.com/apache/spark/pull/11025]",,,,,,,,,,,,,,,,,
EMLDAOptimizer initialize should return EMLDAOptimizer other than its parent class,SPARK-12952,12933000,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,yinxusen,yinxusen,yinxusen,21/Jan/16 07:22,26/Jan/16 21:18,15/Aug/18 23:03,26/Jan/16 21:18,,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"EMLDAOptimizer initialize should return EMLDAOptimizer other than its parent class, like OnlineLDAOptimizer.",,apachespark,josephkb,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-01-21 07:30:04.978,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 26 21:18:13 UTC 2016,,,,,0|i2rsbb:,9223372036854775807,,,,,,,,,,,,,"21/Jan/16 07:30;apachespark;User 'yinxusen' has created a pull request for this issue:
https://github.com/apache/spark/pull/10863","26/Jan/16 21:18;josephkb;Issue resolved by pull request 10863
[https://github.com/apache/spark/pull/10863]",,,,,,,,,,,,,,,,,,,,,,
Fix LinearRegression.train for the case when label is constant and fitIntercept=false,SPARK-12732,12928820,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,iyounus,iyounus,iyounus,09/Jan/16 02:06,03/Feb/16 04:39,15/Aug/18 23:03,03/Feb/16 04:39,,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,1,,,,,"If the target variable is constant, then the linear regression must check if the fitIntercept is true or false, and handle these two cases separately.

If the fitIntercept is true, then there is no training needed and we set the intercept equal to the mean of y.

But if the fit intercept is false, then the model should still train.

Currently, LinearRegression handles both cases in the same way. It doesn't train the model and sets the intercept equal to the mean of y. Which, means that it returns a non-zero intercept even when the user forces the regression through the origin.",,Agent007,apachespark,dbtsai,iyounus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-01-09 12:40:27.897,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 03 04:39:08 UTC 2016,,,,,0|i2r2jb:,9223372036854775807,dbtsai,,,,,2.0.0,,,,,,,"09/Jan/16 02:15;iyounus;I would propose to do something like this for simplicity:

if (yStd == 0.0) {
    if ($(fitIntercept)) {
...
...  
} else {  
    yStd = 1.0
}

Since the model is trained in scaled space, setting `yStd = 1` will simply not scale `y` at all, and will produce the results as expected. Otherwise, we'll have to fix division by zero at several places in the code.","09/Jan/16 12:40;srowen;Sounds good [~iyounus], feel free to submit a PR.","11/Jan/16 19:45;apachespark;User 'iyounus' has created a pull request for this issue:
https://github.com/apache/spark/pull/10702","03/Feb/16 04:39;dbtsai;Issue resolved by pull request 10702
[https://github.com/apache/spark/pull/10702]",,,,,,,,,,,,,,,,,,,,
ML StopWordsRemover does not protect itself from column name duplication,SPARK-12711,12928629,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,gzes,gzes,gzes,08/Jan/16 12:43,02/Feb/16 19:18,15/Aug/18 23:03,02/Feb/16 19:17,1.6.0,,,,,,,,,,,,,,,1.6.1,2.0.0,,,,,ML,MLlib,,,,0,ml,mllib,newbie,suggestion,"At work we were 'taking a closer look' at ML transformers&estimators and I spotted that anomally.
On first look, resolution looks simple:
Add to StopWordsRemover.transformSchema line (as is done in e.g. PCA.transformSchema, StandardScaler.transformSchema, OneHotEncoder.transformSchema):
{code}
require(!schema.fieldNames.contains($(outputCol)), s""Output column ${$(outputCol)} already exists."")
{code}

Am I correct? Is that a bug?    If yes - I am willing to prepare an appropriate pull request.
Maybe a better idea is to make use of super.transformSchema in StopWordsRemover (and possibly in all other places)?


Links to files at github, mentioned above:
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/StopWordsRemover.scala#L147
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/Transformer.scala#L109-L111
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/StandardScaler.scala#L101-L102
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/PCA.scala#L138-L139
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/OneHotEncoder.scala#L75-L76
",,apachespark,gzes,josephkb,wjur,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-01-08 20:07:40.093,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 02 19:17:01 UTC 2016,,,,,0|i2r1d3:,9223372036854775807,,,,,,1.6.1,2.0.0,,,,,,"08/Jan/16 20:07;josephkb;You're correct that it should prevent column name duplication.  I'd recommend using SchemaUtils.appendColumn: [https://github.com/apache/spark/blob/00d9261724feb48d358679efbae6889833e893e0/mllib/src/main/scala/org/apache/spark/ml/util/SchemaUtils.scala#L54]

That will be great if you can send a PR to fix this.  Thanks!

By the way, please don't set the target version; committers or component maintainers will set that.","09/Jan/16 10:11;wjur;[~josephkb]Is there any particular reason why StopWordsRemover is not a UnaryTransformer? As the docs say, the UnaryTransformer is an ""Abstract class for transformers that take one input column, apply transformation, and output the result as a new column."" which is the case. Moreover, UnaryTransformer implementation checks whether the output column already exists or not. Then, Making StopWordsRemover a UnaryTransformer would solve the issue. Talking about UnaryTransformers candidates, I think StringIndexer is a similar case (and probably, there are other Transformers that could be UnaryTransformers). It doesn't check whether the output column exists in the input DataFrame (it has the same flaw). Making StringIndexer a UnaryTransformer would solve the flaw, too. What do you think?","11/Jan/16 21:14;josephkb;You're right that these transformers could be UnaryTransformers.  The main problem is that their transform() methods involve a little initialization, which is not supported well by the UnaryTransformer abstraction.

Relatedly, I'm starting to work on a design doc for Params for MLlib 2.0 which should help handle some of these issues.  Essentially, I'm working on making it so that these checks don't have to be implemented separately for each class and can be handled in a generic way by an abstraction.  So this JIRA may not be an issue for 2.0+.","13/Jan/16 15:52;apachespark;User 'grzegorz-chilkiewicz' has created a pull request for this issue:
https://github.com/apache/spark/pull/10741","02/Feb/16 19:17;josephkb;Issue resolved by pull request 10741
[https://github.com/apache/spark/pull/10741]",,,,,,,,,,,,,,,,,,,
word2vec trainWordsCount gets overflow,SPARK-12685,12928229,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,07/Jan/16 04:28,13/Jan/16 19:54,15/Aug/18 23:03,13/Jan/16 19:54,1.6.0,,,,,,,,,,,,,,,1.4.2,1.5.3,1.6.1,2.0.0,,,MLlib,,,,,0,,,,,"the log of word2vec  reports 
trainWordsCount = -785727483
during computation over a large dataset.

I'll also add vocabsize to the log.

Update the priority as it will affects the computation process.
alpha =
learningRate * (1 - numPartitions * wordCount.toDouble / (trainWordsCount + 1))",,apachespark,josephkb,michaelmalak,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-01-07 04:36:04.019,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 13 19:54:35 UTC 2016,,,,,0|i2qyw7:,9223372036854775807,josephkb,,,,,1.4.2,1.5.3,1.6.1,2.0.0,,,,"07/Jan/16 04:32;yuhaoyan;Update the priority as it will affects the computation process.
alpha =
                learningRate * (1 - numPartitions * wordCount.toDouble / (trainWordsCount + 1))","07/Jan/16 04:36;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/10627","08/Jan/16 19:44;josephkb;This affects earlier versions too.  Would you mind checking to see which ones, and also how hard it'd be to backport to them?  Thanks!","11/Jan/16 22:47;josephkb;(edit) It did not cherry-pick cleanly to previous Spark versions.  Would you mind sending backport PRs for branch-1.4, 1.5, and 1.6?  Thank you!",11/Jan/16 22:52;josephkb;Reopening so I don't forget backports,12/Jan/16 01:59;yuhaoyan;Sorry to miss that. I'll start on it.,"12/Jan/16 02:50;yuhaoyan;Hi [~josephkb] I suppose it will require 3 different PRs, right?","12/Jan/16 07:12;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/10721","13/Jan/16 19:54;josephkb;Issue resolved by pull request 10721
[https://github.com/apache/spark/pull/10721]",,,,,,,,,,,,,,,
"Loading Word2Vec model in pyspark gives ""ValueError: too many values to unpack"" in  findSynonyms",SPARK-12680,12928119,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,sloane.simmons,sloane.simmons,06/Jan/16 20:20,07/Jan/16 05:16,15/Aug/18 23:03,07/Jan/16 01:26,1.5.2,,,,,,,,,,,,,,,,,,,,,MLlib,PySpark,,,,0,,,,,"I can train a model with Word2Vec and then persist it with Word2VecModel#save.  If I load the saved model in pyspark (using python 2.7.10), I get the following error (model.transform included to show that other methods work).
{code}
In [3]: from pyspark.mllib.feature import Word2VecModel

In [4]: model = Word2VecModel.load(sc,""word_vec_from_cleaned_query.model"")
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
16/01/06 12:36:11 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
16/01/06 12:36:11 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS

In [5]: model.findSynonyms('white',10)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-6105c004edd9> in <module>()
----> 1 model.findSynonyms('white',10)

/usr/local/Cellar/apache-spark/1.5.2/libexec/python/pyspark/mllib/feature.pyc in findSynonyms(self, word, num)
    448         if not isinstance(word, basestring):
    449             word = _convert_to_vector(word)
--> 450         words, similarity = self.call(""findSynonyms"", word, num)
    451         return zip(words, similarity)
    452 

ValueError: too many values to unpack

In [6]: model.transform('white')
Out[6]: DenseVector([-0.0213, 0.2292, -0.2012, 0.107, -0.1475, 0.0578, 0.0731, -0.098, -0.1528, 0.1077, 0.0158, -0.0155, -0.1487, 0.0343, 0.2244, 0.0447, 0.2362, -0.1767, 0.064, -0.0148, -0.1291, -0.0171, -0.0642, -0.0754, 0.0417, 0.1547, 0.2745, -0.1178, -0.2895, -0.1314, 0.1023, -0.11, 0.0142, 0.0156, 0.1102, 0.0785, -0.0981, 0.0504, -0.0627, -0.0773, 0.0023, 0.1826, 0.1759, -0.1581, 0.3913, 0.0829, 0.0728, 0.1478, -0.0123, -0.1745, 0.2762, 0.0312, 0.138, 0.0786, -0.0546, 0.5123, 0.237, -0.0241, 0.1594, -0.0645, -0.0425, 0.1265, 0.0305, -0.3164, 0.0601, 0.0565, 0.0066, -0.0818, -0.384, -0.1513, 0.0775, -0.2278, -0.1478, -0.0659, -0.0778, 0.3194, -0.1931, -0.2991, 0.1629, 0.1018, -0.0603, 0.1091, -0.0334, -0.0513, 0.1067, 0.1273, 0.1187, 0.0461, 0.0407, 0.0515, 0.0958, 0.0498, -0.1561, 0.1726, -0.006, -0.0262, -0.0106, 0.1623, 0.1477, -0.0509])

In [7]: 
{code}

I think that this is a pyspark-specific error, since I can load the trained model in the scala spark-shell and use findSynonyms:
{code}
scala> import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}

scala> val model = Word2VecModel.load(sc,""word_vec_from_cleaned_query.model"")
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
16/01/06 14:17:14 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
16/01/06 14:17:14 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
model: org.apache.spark.mllib.feature.Word2VecModel = org.apache.spark.mllib.feature.Word2VecModel@2e7da886

scala> model.findSynonyms(""white"",10)
res0: Array[(String, Double)] = Array((stylish,0.8347662648041679), (shirt,0.7721922530954246), (stripe,0.7311193884955149), (striped,0.7033047124091971), (buttons,0.6891310548525095), (womens,0.671437501511924), (zaful,0.6659281321485323), (dorateymur,0.6654344754707424), (womenns,0.6637001786899768), (long,0.6573707323598634))

scala> 
{code}",,josephkb,michaelmalak,sloane.simmons,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-12016,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-01-07 01:02:53.556,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 07 05:16:35 UTC 2016,,,,,0|i2qy7z:,9223372036854775807,,,,,,,,,,,,,"07/Jan/16 01:02;josephkb;I've having trouble reproducing this on Spark 1.6.  Are you able to try it with 1.6?

(I'm testing on 1.5 now.)",07/Jan/16 01:17;josephkb;It fails on 1.5 for me too.  Will investigate...,"07/Jan/16 01:25;josephkb;Oh, I forgot; this was found before.  I'll try to backport the fix to 1.5","07/Jan/16 05:16;sloane.simmons;Sorry, I hadn't found SPARK-12016 when I was searching earlier - thanks for the quick investigation!",,,,,,,,,,,,,,,,,,,,
mllib deprecation messages mention non-existent version 1.7.0,SPARK-12651,12927322,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Duplicate,,vanzin,vanzin,05/Jan/16 17:44,25/Jan/16 23:57,15/Aug/18 23:03,25/Jan/16 23:57,2.0.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"Might be a problem in 1.6 also?

{code}
  @Since(""1.4.0"")
  @deprecated(""Support for runs is deprecated. This param will have no effect in 1.7.0."", ""1.6.0"")
  def getRuns: Int = runs
{code}",,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-12618,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2016-01-05 18:01:23.904,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 18:01:23 UTC 2016,,,,,0|i2qtb3:,9223372036854775807,,,,,,2.0.0,,,,,,,05/Jan/16 18:01;srowen;I've got this covered in SPARK-12618 / https://github.com/apache/spark/pull/10570 already,,,,,,,,,,,,,,,,,,,,,,,
Fix minor issues found by Findbugs,SPARK-12489,12923848,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,22/Dec/15 21:53,28/Dec/15 23:02,15/Aug/18 23:03,28/Dec/15 23:02,1.6.0,,,,,,,,,,,,,,,1.6.1,2.0.0,,,,,MLlib,Spark Core,SQL,,,0,,,,,"Just used FindBugs to scan the codes and fixed some real issues:

1. Close `java.sql.Statement`
2. Fix incorrect `asInstanceOf`.
3. Remove unnecessary `synchronized` and `ReentrantLock`.",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,9223372036854775807,,,2015-12-22 21:53:32.0,,,,,0|i2q8c7:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LDA describeTopics() Generates Invalid Term IDs,SPARK-12488,12923839,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,ilganeli,ilganeli,22/Dec/15 21:23,03/Nov/16 23:47,15/Aug/18 23:03,03/Nov/16 23:47,1.5.2,,,,,,,,,,,,,,,1.4.2,1.5.3,1.6.1,2.0.0,,,MLlib,,,,,0,,,,,"When running the LDA model, and using the describeTopics function, invalid values appear in the termID list that is returned:

The below example generates 10 topics on a data set with a vocabulary of 685.

{code}

    // Set LDA parameters
    val numTopics = 10
    val lda = new LDA().setK(numTopics).setMaxIterations(10)

    val ldaModel = lda.run(docTermVector)
    val distModel = ldaModel.asInstanceOf[org.apache.spark.mllib.clustering.DistributedLDAModel]
{code}

{code}
scala> ldaModel.describeTopics()(0)._1.sorted.reverse
res40: Array[Int] = Array(2064860663, 2054149956, 1991041659, 1986948613, 1962816105, 1858775243, 1842920256, 1799900935, 1792510791, 1792371944, 1737877485, 1712816533, 1690397927, 1676379181, 1664181296, 1501782385, 1274389076, 1260230987, 1226545007, 1213472080, 1068338788, 1050509279, 714524034, 678227417, 678227086, 624763822, 624623852, 618552479, 616917682, 551612860, 453929488, 371443786, 183302140, 58762039, 42599819, 9947563, 617, 616, 615, 612, 603, 597, 596, 595, 594, 593, 592, 591, 590, 589, 588, 587, 586, 585, 584, 583, 582, 581, 580, 579, 578, 577, 576, 575, 574, 573, 572, 571, 570, 569, 568, 567, 566, 565, 564, 563, 562, 561, 560, 559, 558, 557, 556, 555, 554, 553, 552, 551, 550, 549, 548, 547, 546, 545, 544, 543, 542, 541, 540, 539, 538, 537, 536, 535, 534, 533, 532, 53...
{code}

{code}
scala> ldaModel.describeTopics()(0)._1.sorted
res41: Array[Int] = Array(-2087809139, -2001127319, -1979718998, -1833443915, -1811530305, -1765302237, -1668096260, -1527422175, -1493838005, -1452770216, -1452508395, -1452502074, -1452277147, -1451720206, -1450928740, -1450237612, -1448730073, -1437852514, -1420883015, -1418557080, -1397997340, -1397995485, -1397991169, -1374921919, -1360937376, -1360533511, -1320627329, -1314475604, -1216400643, -1210734882, -1107065297, -1063529036, -1062984222, -1042985412, -1009109620, -951707740, -894644371, -799531743, -627436045, -586317106, -563544698, -326546674, -174108802, -155900771, -80887355, -78916591, -26690004, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 4...
{code}",,bryanc,holdenk,jonbates,josephkb,lkhamsurenl,yanboliang,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-12-24 09:06:16.025,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 03 23:46:35 UTC 2016,,,,,0|i2q8a7:,9223372036854775807,,,,,,,,,,,,,22/Dec/15 21:31;ilganeli;[~josephkb] Would love your feedback here. Thanks!,22/Dec/15 22:27;ilganeli;Further investigation identifies the issue as stemming from the docTermVector containing zero-vectors (as in no words from the vocabulary present in the document).,24/Dec/15 09:06;yuhaoyan;I cannot repro the issue even with empty vector in documents. Can you please share the corpus?,"28/Dec/15 22:04;josephkb;I don't see what could be causing this yet, but I'd really like it fixed if we can reproduce it.  I'll watch the JIRA.","28/Dec/15 22:56;ilganeli;I'll submit a dataset that causes this when I have a moment. Thanks!



Thank you,
Ilya Ganelin



","29/Dec/15 19:23;josephkb;Just saw this: [http://apache-spark-developers-list.1001551.n3.nabble.com/running-lda-in-spark-throws-exception-td15821.html]
It sounds like it might be the same problem.

I'm guessing it happens when the EM LDAOptimizer's indices or graph are created, but I haven't found the issue yet.","08/Jan/16 22:51;josephkb;It was just reported on this thread that it might be a YARN issue since it did not appear in local or standalone mode.  CC: [~andrewor14] Any thoughts, and are there others I should ping?

Also, is there anyone with a YARN cluster who can try it on Spark 1.6 or master?","04/Apr/16 18:29;josephkb;It's possible this was caused by incorrect Graph creation, fixed in [SPARK-13355].

Could you retry your dataset using the current master to see if the problem is fixed?  Thanks!",22/Apr/16 00:43;josephkb;Ping [~ilganeli] I hope this is fixed now!,"03/Nov/16 23:46;josephkb;I'm going to close this since it seems like it has been fixed by [SPARK-13355].  If anyone sees this is versions which include that patch, please report!  Thanks all.",,,,,,,,,,,,,,
MLLib should use existing SQLContext instead create new one,SPARK-12380,12922519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,16/Dec/15 22:00,24/Dec/15 13:24,15/Aug/18 23:03,16/Dec/15 23:48,,,,,,,,,,,,,,,,1.6.0,2.0.0,,,,,MLlib,PySpark,,,,0,,,,,,,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-12-16 22:13:05.097,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 16 23:48:40 UTC 2015,,,,,0|i2q05b:,9223372036854775807,,,,,,1.6.1,2.0.0,,,,,,"16/Dec/15 22:13;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/10338","16/Dec/15 23:48;davies;Issue resolved by pull request 10338
[https://github.com/apache/spark/pull/10338]",,,,,,,,,,,,,,,,,,,,,,
PowerIterationClustering test case failed if we deprecated KMeans.setRuns,SPARK-12363,12922307,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,yanboliang,yanboliang,16/Dec/15 09:30,26/Feb/16 05:16,15/Aug/18 23:03,23/Feb/16 07:44,1.3.1,1.4.1,1.5.2,1.6.0,2.0.0,,,,,,,,,,,1.3.2,1.4.2,1.5.3,1.6.1,2.0.0,,MLlib,,,,,0,backport-needed,,,,"We plan to deprecated `runs` of KMeans, PowerIterationClustering will leverage KMeans to train model.
I removed `setRuns` used in PowerIterationClustering, but one of the test cases failed.",,apachespark,josephkb,mengxr,michaelmalak,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13355,SPARK-11559,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-12-16 21:06:28.158,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 23 07:46:47 UTC 2016,,,,,0|i2pyu7:,9223372036854775807,mengxr,,,,,1.3.2,1.4.2,1.5.3,1.6.1,2.0.0,,,"16/Dec/15 09:35;yanboliang;This bug is very easy to reproduce. After I removed [this line|https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/clustering/PowerIterationClustering.scala#L388], one of the test cases failed.
I pasted the test cases here. It's very strange that the following two cases are based on the same dataset, but one success and the other failed.
Another clue is that when use `setInitializationMode(""degree"")` to train the PIC model, both the following two cases can pass. But if we use `setInitializationMode(""random"")`, the second test case will failed.
{code}
test(""power iteration clustering"") {
    /*
     We use the following graph to test PIC. All edges are assigned similarity 1.0 except 0.1 for
     edge (3, 4).
     15-14 -13 -12
     |           |
     4 . 3 - 2  11
     |   | x |   |
     5   0 - 1  10
     |           |
     6 - 7 - 8 - 9
     */

    val similarities = Seq[(Long, Long, Double)]((0, 1, 1.0), (0, 2, 1.0), (0, 3, 1.0), (1, 2, 1.0),
      (1, 3, 1.0), (2, 3, 1.0), (3, 4, 0.1), // (3, 4) is a weak edge
      (4, 5, 1.0), (4, 15, 1.0), (5, 6, 1.0), (6, 7, 1.0), (7, 8, 1.0), (8, 9, 1.0), (9, 10, 1.0),
      (10, 11, 1.0), (11, 12, 1.0), (12, 13, 1.0), (13, 14, 1.0), (14, 15, 1.0))
    val model = new PowerIterationClustering()
      .setK(2)
      .run(sc.parallelize(similarities, 2))
    val predictions = Array.fill(2)(mutable.Set.empty[Long])
    model.assignments.collect().foreach { a =>
      predictions(a.cluster) += a.id
    }
    assert(predictions.toSet == Set((0 to 3).toSet, (4 to 15).toSet))

    val model2 = new PowerIterationClustering()
      .setK(2)
      .setInitializationMode(""degree"")
      .run(sc.parallelize(similarities, 2))
    val predictions2 = Array.fill(2)(mutable.Set.empty[Long])
    model2.assignments.collect().foreach { a =>
      predictions2(a.cluster) += a.id
    }
    assert(predictions2.toSet == Set((0 to 3).toSet, (4 to 15).toSet))
  }

  test(""power iteration clustering on graph"") {
    /*
     We use the following graph to test PIC. All edges are assigned similarity 1.0 except 0.1 for
     edge (3, 4).
     15-14 -13 -12
     |           |
     4 . 3 - 2  11
     |   | x |   |
     5   0 - 1  10
     |           |
     6 - 7 - 8 - 9
     */

    val similarities = Seq[(Long, Long, Double)]((0, 1, 1.0), (0, 2, 1.0), (0, 3, 1.0), (1, 2, 1.0),
      (1, 3, 1.0), (2, 3, 1.0), (3, 4, 0.1), // (3, 4) is a weak edge
      (4, 5, 1.0), (4, 15, 1.0), (5, 6, 1.0), (6, 7, 1.0), (7, 8, 1.0), (8, 9, 1.0), (9, 10, 1.0),
      (10, 11, 1.0), (11, 12, 1.0), (12, 13, 1.0), (13, 14, 1.0), (14, 15, 1.0))

    val edges = similarities.flatMap { case (i, j, s) =>
      if (i != j) {
        Seq(Edge(i, j, s), Edge(j, i, s))
      } else {
        None
      }
    }
    val graph = Graph.fromEdges(sc.parallelize(edges, 2), 0.0)

    val model = new PowerIterationClustering()
      .setK(2)
      .run(graph)
    val predictions = Array.fill(2)(mutable.Set.empty[Long])
    model.assignments.collect().foreach { a =>
      predictions(a.cluster) += a.id
    }
    assert(predictions.toSet == Set((0 to 3).toSet, (4 to 15).toSet))

    val model2 = new PowerIterationClustering()
      .setK(2)
      .setInitializationMode(""degree"")
      .run(sc.parallelize(similarities, 2))
    val predictions2 = Array.fill(2)(mutable.Set.empty[Long])
    model2.assignments.collect().foreach { a =>
      predictions2(a.cluster) += a.id
    }
    assert(predictions2.toSet == Set((0 to 3).toSet, (4 to 15).toSet))
  }
{code}",16/Dec/15 09:44;yanboliang;cc [~mengxr] [~josephkb] [~viirya] Would you mind to take a look at this issue?,"16/Dec/15 21:06;josephkb;Thanks for identifying this.  What do the predictions look like?  Does it improve if you increase the number of iterations KMeans runs for when called from PIC?

It seems like an intuitively reasonable test, but I could see it failing for bad initial cluster centers or if KMeans needs to run for more iterations.",16/Dec/15 21:07;josephkb;Setting priority to Minor since we'll notice this bug when it becomes a bug.,"17/Dec/15 09:12;yanboliang;{quote}
Does it improve if you increase the number of iterations KMeans runs for when called from PIC?
{quote}
No, it converged in 1 iterations in any condition.

I'm confused that we have two test case:
{code}
test(""power iteration clustering"")
test(""power iteration clustering on graph"")
{code}
They use the same input data, but call different train API. 
It should get same result theoretically, but the first test case succeed and the second one failed.


[~josephkb] ","14/Feb/16 00:02;mengxr;This bug was fixed by https://github.com/apache/spark/pull/10539 and the fix was merged into master, branch-1.6, and branch-1.5. I left the JIRA open for branch-1.4 and branch-1.3 backports.",14/Feb/16 13:06;srowen;Is it realistic to expect another 1.3 or 1.4 release? I am not even sure 1.5.3 will be formally released,"19/Feb/16 08:33;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/11264","19/Feb/16 08:54;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/11265","23/Feb/16 07:44;mengxr;Issue resolved by pull request 11264
[https://github.com/apache/spark/pull/11264]",23/Feb/16 07:46;mengxr;I think it is still useful to backport the fix. People may maintain their own releases and there are definitely many Spark 1.4 users.,,,,,,,,,,,,,
spark.mllib should use SQLContext.getOrCreate,SPARK-12160,12919475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,05/Dec/15 23:53,24/Dec/15 13:26,15/Aug/18 23:03,08/Dec/15 07:37,1.4.1,1.5.2,1.6.0,,,,,,,,,,,,,1.5.3,1.6.0,,,,,MLlib,,,,,0,,,,,"spark.mllib currently uses {{new SQLContext(...)}} in several places, especially the model save/load methods.  It should use SQLContext.getOrCreate, which was introduced in Spark 1.4.

This can cause some problems, depending on how the user/system manages contexts.",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8870,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-12-05 23:57:13.322,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 08 07:37:35 UTC 2015,,,,,0|i2phwv:,9223372036854775807,,,,,,1.5.3,1.6.1,,,,,,"05/Dec/15 23:57;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/10161","08/Dec/15 00:55;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/10183","08/Dec/15 07:37;mengxr;Issue resolved by pull request 10183
[https://github.com/apache/spark/pull/10183]",,,,,,,,,,,,,,,,,,,,,
"Word2Vec uses a fixed length for sentences which is not reasonable for reality, and similarity functions and fields are not accessible",SPARK-12153,12919131,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ygcao,ygcao,ygcao,05/Dec/15 00:14,22/Feb/16 09:49,15/Aug/18 23:03,22/Feb/16 09:49,1.5.2,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,0,,,,,"sentence boundary matters for sliding window, we shouldn't train model from a window across sentences. 
the current 1000 word as a hard split for sentences doesn't really make sense which is not consistent with both original c version or other implementation like deeplearning4j etc.
the max sentence length is fixed and not tunable. Made it tunable as well.

I made changes to address above issues.
here is the pull request: https://github.com/apache/spark/pull/10152",,apachespark,ygcao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-12-05 22:13:06.668,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 09:49:59 UTC 2016,,,,,0|i2pfsf:,9223372036854775807,,,,,,,,,,,,,"05/Dec/15 22:13;apachespark;User 'ygcao' has created a pull request for this issue:
https://github.com/apache/spark/pull/10152",06/Dec/15 09:37;srowen;(I don't think this can be considered major),"22/Feb/16 09:49;srowen;Issue resolved by pull request 10152
[https://github.com/apache/spark/pull/10152]",,,,,,,,,,,,,,,,,,,,,
StringIndexer failing with Unseen label exception on test data ,SPARK-12092,12917492,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Won't Fix,,soniclavier,soniclavier,02/Dec/15 07:41,03/Dec/15 00:33,15/Aug/18 23:03,03/Dec/15 00:33,1.5.2,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"StringIndexer fails with exception
{code}
Caused by: org.apache.spark.SparkException: Unseen label
{code} 

StringIndexer which is *fit()* on train data, throws unseen label exception during *transform()* of test data, if the column in test data is having a value which was not seen during train stage.

The workaround is to have the train data and test data before fitting the StringIndexer, which is not always practical.

{code}
org.apache.spark.SparkException: Unseen label: new_value
	at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:139)
	at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:134)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:75)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:74)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:964)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.Project$$anonfun$1$$anonfun$apply$2.apply(basicOperators.scala:55)
	at org.apache.spark.sql.execution.Project$$anonfun$1$$anonfun$apply$2.apply(basicOperators.scala:53)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:909)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:909)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

",,soniclavier,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-12-02 07:58:17.747,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 02 07:58:17 UTC 2015,,,,,0|i2p5on:,9223372036854775807,,,,,,,,,,,,,02/Dec/15 07:58;zjffdu;Looks like it is been resolved in SPARK-8764,,,,,,,,,,,,,,,,,,,,,,,
ChiSqTest gets slower and slower over time when number of features is large,SPARK-12026,12916624,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuhaoyan,retnuH,retnuH,27/Nov/15 12:32,14/Jan/16 01:44,15/Aug/18 23:03,14/Jan/16 01:44,1.5.2,,,,,,,,,,,,,,,1.6.1,2.0.0,,,,,MLlib,,,,,0,mllib,stats,,,"I've been running a ChiSqTest to pick features for feature reduction.  My understanding is that internally it creates jobs to run on batches of 1000 features at a time.

I was under the impression that the features are treated as independant, but this does not appear to be the case.  When the number of features is large (160k in my case), each batch gets slower and slower.  As an example, running on 25 m3.2xlarges on Amazon EMR, it started at just over 1 minute per batch.  By the end, batches were taking over 30 minutes per batch.",,apachespark,josephkb,retnuH,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,27/Nov/15 14:55;retnuH;First Stages.png;https://issues.apache.org/jira/secure/attachment/12774632/First+Stages.png,27/Nov/15 14:56;retnuH;Latest Stages.png;https://issues.apache.org/jira/secure/attachment/12774633/Latest+Stages.png,,2.0,,,,,,,,,,,,,,,,2015-11-27 12:47:11.768,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 14 01:44:00 UTC 2016,,,,,0|i2p0bz:,9223372036854775807,,,,,,1.6.1,2.0.0,,,,,,"27/Nov/15 12:47;srowen;Can you narrow down what's slow? this isn't much to go on and doesn't rule out cluster issues. Like, what stages are taking longer? is the data size increasing? do you have a proposed PR?","27/Nov/15 14:39;retnuH;Here's the bumpf from the details button on the UI:

org.apache.spark.rdd.RDD.countByValue(RDD.scala:1156)
org.apache.spark.mllib.stat.test.ChiSqTest$.chiSquaredFeatures(ChiSqTest.scala:117)
org.apache.spark.mllib.stat.Statistics$.chiSqTest(Statistics.scala:178)
org.apache.spark.mllib.stat.Statistics$.chiSqTest(Statistics.scala:183)
org.apache.spark.mllib.stat.Statistics.chiSqTest(Statistics.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:606)
clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
clojure.lang.Reflector.invokeStaticMethod(Reflector.java:207)
bow.optimized$chi_square_exploration.doInvoke(optimized.clj:99)
clojure.lang.RestFn.applyTo(RestFn.java:137)
clojure.core$apply.invoke(core.clj:630)
util.cli$_main.doInvoke(cli.clj:63)
clojure.lang.RestFn.applyTo(RestFn.java:139)
util.cli.main(Unknown Source)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

Basically it seems to run 2 stages per batch (each batch of 1000 is it's own job, I believe).  The first countByValue is the one that gets slower; a second countByValue on the same line runs in a few seconds (as opposed to minutes).

I'll attach some screenshots from the SparkUI.","27/Nov/15 14:42;retnuH;As for cluster issues; I've done this several times over the past few weeks on different data sets and it exhibits the same behaviour.  Pretty sure it's algorithmic issue; not a networking one.  The Input, ShuffleRead and ShuffleWrite times are pretty consistent across the board.
",27/Nov/15 14:55;retnuH;Here's what the start of one of the jobs looks like,27/Nov/15 14:56;retnuH;Here's what the final stages of one of the jobs looks like,"04/Dec/15 14:49;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/10146","14/Jan/16 01:44;josephkb;Issue resolved by pull request 10146
[https://github.com/apache/spark/pull/10146]",,,,,,,,,,,,,,,,,
GaussianMixture.train crashes if an initial model is not None,SPARK-12006,12916338,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,26/Nov/15 01:50,07/Jan/16 18:33,15/Aug/18 23:03,07/Jan/16 18:33,1.4.0,1.5.0,1.6.0,,,,,,,,,,,,,1.4.2,1.5.3,1.6.1,2.0.0,,,MLlib,PySpark,,,,0,,,,,"Steps to reproduce :

{code}
from pyspark.mllib.clustering import GaussianMixture
from numpy import array

data = sc.textFile(""data/mllib/gmm_data.txt"")
parsedData = data.map(lambda line: array([float(x) for x in line.strip().split(' ')]))

gmm = GaussianMixture.train(parsedData, 2)
GaussianMixture.train(parsedData, 2, initialModel=gmm)
{code}

It looks like the source of the problem is [{{initialModelWeights}}|https://github.com/apache/spark/blob/branch-1.6/python/pyspark/mllib/clustering.py#L349] NumPy array. In 1.5 / 1.6 it leads to {{net.razorvine.pickle.PickleException}}, in 1.4 we get {{Method trainGaussianMixture(\[..., class org.apache.spark.mllib.linalg.DenseVector, class java.util.ArrayList, class java.util.ArrayList\]) does not exist}}
",,apachespark,josephkb,yhuai,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-11-26 02:01:02.718,,false,,,,http://stackoverflow.com/q/33912626/1560062,,,,,,,,,9223372036854775807,,,Thu Jan 07 18:33:56 UTC 2016,,,,,0|i2oykf:,9223372036854775807,josephkb,,,,,1.4.2,1.5.3,1.6.1,2.0.0,,,,"26/Nov/15 02:01;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/9986","06/Jan/16 19:59;josephkb;Issue resolved by pull request 9986
[https://github.com/apache/spark/pull/9986]","07/Jan/16 06:03;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/10632","07/Jan/16 06:07;yhuai;Sorry. I have reverted it from 1.4, 1.5, 1.6, and master (see https://github.com/apache/spark/pull/10632) because it breaks python style check. Since the original pr did not pass jenkins, I reverted it.","07/Jan/16 11:20;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/10644","07/Jan/16 18:33;josephkb;Issue resolved by pull request 10644
[https://github.com/apache/spark/pull/10644]",,,,,,,,,,,,,,,,,,
`sbt publishLocal` hits a Scala compiler bug caused by `Since` annotation,SPARK-12000,12916296,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mengxr,mengxr,mengxr,25/Nov/15 23:05,24/Dec/15 12:18,15/Aug/18 23:03,03/Dec/15 01:20,1.6.0,,,,,,,,,,,,,,,1.6.0,,,,,,Build,Documentation,MLlib,,,0,,,,,"Reported by [~josephkb]. Not sure what is the root cause, but this is the error message when I ran ""sbt publishLocal"":

{code}
[error] (launcher/compile:doc) javadoc returned nonzero exit code
[error] (mllib/compile:doc) scala.reflect.internal.FatalError:
[error]      while compiling: /Users/meng/src/spark/mllib/src/main/scala/org/apache/spark/mllib/util/modelSaveLoad.scala
[error]         during phase: global=terminal, atPhase=parser
[error]      library version: version 2.10.5
[error]     compiler version: version 2.10.5
[error]   reconstructed args: -Yno-self-type-checks -groups -classpath /Users/meng/src/spark/core/target/scala-2.10/classes:/Users/meng/src/spark/launcher/target/scala-2.10/classes:/Users/meng/src/spark/network/common/target/scala-2.10/classes:/Users/meng/src/spark/network/shuffle/target/scala-2.10/classes:/Users/meng/src/spark/unsafe/target/scala-2.10/classes:/Users/meng/src/spark/streaming/target/scala-2.10/classes:/Users/meng/src/spark/sql/core/target/scala-2.10/classes:/Users/meng/src/spark/sql/catalyst/target/scala-2.10/classes:/Users/meng/src/spark/graphx/target/scala-2.10/classes:/Users/meng/.ivy2/cache/org.spark-project.spark/unused/jars/unused-1.0.0.jar:/Users/meng/.ivy2/cache/com.google.guava/guava/bundles/guava-14.0.1.jar:/Users/meng/.ivy2/cache/io.netty/netty-all/jars/netty-all-4.0.29.Final.jar:/Users/meng/.ivy2/cache/org.fusesource.leveldbjni/leveldbjni-all/bundles/leveldbjni-all-1.8.jar:/Users/meng/.ivy2/cache/com.fasterxml.jackson.core/jackson-databind/bundles/jackson-databind-2.4.4.jar:/Users/meng/.ivy2/cache/com.fasterxml.jackson.core/jackson-annotations/bundles/jackson-annotations-2.4.4.jar:/Users/meng/.ivy2/cache/com.fasterxml.jackson.core/jackson-core/bundles/jackson-core-2.4.4.jar:/Users/meng/.ivy2/cache/com.twitter/chill_2.10/jars/chill_2.10-0.5.0.jar:/Users/meng/.ivy2/cache/com.twitter/chill-java/jars/chill-java-0.5.0.jar:/Users/meng/.ivy2/cache/com.esotericsoftware.kryo/kryo/bundles/kryo-2.21.jar:/Users/meng/.ivy2/cache/com.esotericsoftware.reflectasm/reflectasm/jars/reflectasm-1.07-shaded.jar:/Users/meng/.ivy2/cache/com.esotericsoftware.minlog/minlog/jars/minlog-1.2.jar:/Users/meng/.ivy2/cache/org.objenesis/objenesis/jars/objenesis-1.2.jar:/Users/meng/.ivy2/cache/org.apache.avro/avro-mapred/jars/avro-mapred-1.7.7-hadoop2.jar:/Users/meng/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7-tests.jar:/Users/meng/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7.jar:/Users/meng/.ivy2/cache/org.apache.avro/avro/jars/avro-1.7.7.jar:/Users/meng/.ivy2/cache/org.codehaus.jackson/jackson-core-asl/jars/jackson-core-asl-1.9.13.jar:/Users/meng/.ivy2/cache/org.codehaus.jackson/jackson-mapper-asl/jars/jackson-mapper-asl-1.9.13.jar:/Users/meng/.ivy2/cache/org.apache.commons/commons-compress/jars/commons-compress-1.4.1.jar:/Users/meng/.ivy2/cache/org.tukaani/xz/jars/xz-1.0.jar:/Users/meng/.ivy2/cache/org.slf4j/slf4j-api/jars/slf4j-api-1.7.10.jar:/Users/meng/.ivy2/cache/org.apache.xbean/xbean-asm5-shaded/bundles/xbean-asm5-shaded-4.4.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-client/jars/hadoop-client-2.2.0.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-common/jars/hadoop-common-2.2.0.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-annotations/jars/hadoop-annotations-2.2.0.jar:/Users/meng/.ivy2/cache/commons-cli/commons-cli/jars/commons-cli-1.2.jar:/Users/meng/.ivy2/cache/org.apache.commons/commons-math/jars/commons-math-2.1.jar:/Users/meng/.ivy2/cache/xmlenc/xmlenc/jars/xmlenc-0.52.jar:/Users/meng/.ivy2/cache/commons-httpclient/commons-httpclient/jars/commons-httpclient-3.1.jar:/Users/meng/.ivy2/cache/commons-net/commons-net/jars/commons-net-3.1.jar:/Users/meng/.ivy2/cache/log4j/log4j/bundles/log4j-1.2.17.jar:/Users/meng/.ivy2/cache/commons-lang/commons-lang/jars/commons-lang-2.5.jar:/Users/meng/.ivy2/cache/commons-configuration/commons-configuration/jars/commons-configuration-1.6.jar:/Users/meng/.ivy2/cache/commons-collections/commons-collections/jars/commons-collections-3.2.1.jar:/Users/meng/.ivy2/cache/commons-digester/commons-digester/jars/commons-digester-1.8.jar:/Users/meng/.ivy2/cache/commons-beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.jar:/Users/meng/.ivy2/cache/commons-beanutils/commons-beanutils-core/jars/commons-beanutils-core-1.8.0.jar:/Users/meng/.ivy2/cache/com.google.protobuf/protobuf-java/bundles/protobuf-java-2.5.0.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-auth/jars/hadoop-auth-2.2.0.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-hdfs/jars/hadoop-hdfs-2.2.0.jar:/Users/meng/.ivy2/cache/org.mortbay.jetty/jetty-util/jars/jetty-util-6.1.26.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-app/jars/hadoop-mapreduce-client-app-2.2.0.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-common/jars/hadoop-mapreduce-client-common-2.2.0.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-yarn-common/jars/hadoop-yarn-common-2.2.0.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-yarn-api/jars/hadoop-yarn-api-2.2.0.jar:/Users/meng/.ivy2/cache/com.google.inject/guice/jars/guice-3.0.jar:/Users/meng/.ivy2/cache/javax.inject/javax.inject/jars/javax.inject-1.jar:/Users/meng/.ivy2/cache/aopalliance/aopalliance/jars/aopalliance-1.0.jar:/Users/meng/.ivy2/cache/org.sonatype.sisu.inject/cglib/jars/cglib-2.2.1-v20090111.jar:/Users/meng/.ivy2/cache/com.sun.jersey.jersey-test-framework/jersey-test-framework-grizzly2/jars/jersey-test-framework-grizzly2-1.9.jar:/Users/meng/.ivy2/cache/com.sun.jersey/jersey-server/bundles/jersey-server-1.9.jar:/Users/meng/.ivy2/cache/asm/asm/jars/asm-3.2.jar:/Users/meng/.ivy2/cache/com.sun.jersey/jersey-json/bundles/jersey-json-1.9.jar:/Users/meng/.ivy2/cache/org.codehaus.jettison/jettison/bundles/jettison-1.1.jar:/Users/meng/.ivy2/cache/stax/stax-api/jars/stax-api-1.0.1.jar:/Users/meng/.ivy2/cache/org.codehaus.jackson/jackson-jaxrs/jars/jackson-jaxrs-1.8.8.jar:/Users/meng/.ivy2/cache/org.codehaus.jackson/jackson-xc/jars/jackson-xc-1.8.8.jar:/Users/meng/.ivy2/cache/com.sun.jersey.contribs/jersey-guice/jars/jersey-guice-1.9.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-yarn-client/jars/hadoop-yarn-client-2.2.0.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-core/jars/hadoop-mapreduce-client-core-2.2.0.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-yarn-server-common/jars/hadoop-yarn-server-common-2.2.0.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-shuffle/jars/hadoop-mapreduce-client-shuffle-2.2.0.jar:/Users/meng/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-jobclient/jars/hadoop-mapreduce-client-jobclient-2.2.0.jar:/Users/meng/.ivy2/cache/net.java.dev.jets3t/jets3t/jars/jets3t-0.7.1.jar:/Users/meng/.ivy2/cache/org.apache.curator/curator-recipes/bundles/curator-recipes-2.4.0.jar:/Users/meng/.ivy2/cache/org.apache.curator/curator-framework/bundles/curator-framework-2.4.0.jar:/Users/meng/.ivy2/cache/org.apache.curator/curator-client/bundles/curator-client-2.4.0.jar:/Users/meng/.ivy2/cache/org.apache.zookeeper/zookeeper/jars/zookeeper-3.4.5.jar:/Users/meng/.ivy2/cache/jline/jline/jars/jline-0.9.94.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty/jetty-plus/jars/jetty-plus-8.1.14.v20131031.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty.orbit/javax.transaction/orbits/javax.transaction-1.1.1.v201105210645.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty/jetty-webapp/jars/jetty-webapp-8.1.14.v20131031.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty/jetty-xml/jars/jetty-xml-8.1.14.v20131031.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty/jetty-util/jars/jetty-util-8.1.14.v20131031.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty/jetty-servlet/jars/jetty-servlet-8.1.14.v20131031.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty/jetty-security/jars/jetty-security-8.1.14.v20131031.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty/jetty-server/jars/jetty-server-8.1.14.v20131031.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty.orbit/javax.servlet/orbits/javax.servlet-3.0.0.v201112011016.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty/jetty-continuation/jars/jetty-continuation-8.1.14.v20131031.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty/jetty-http/jars/jetty-http-8.1.14.v20131031.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty/jetty-io/jars/jetty-io-8.1.14.v20131031.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty/jetty-jndi/jars/jetty-jndi-8.1.14.v20131031.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty.orbit/javax.mail.glassfish/orbits/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/meng/.ivy2/cache/org.eclipse.jetty.orbit/javax.activation/orbits/javax.activation-1.1.0.v201105071233.jar:/Users/meng/.ivy2/cache/org.apache.commons/commons-lang3/jars/commons-lang3-3.3.2.jar:/Users/meng/.ivy2/cache/org.apache.commons/commons-math3/jars/commons-math3-3.4.1.jar:/Users/meng/.ivy2/cache/org.slf4j/jul-to-slf4j/jars/jul-to-slf4j-1.7.10.jar:/Users/meng/.ivy2/cache/org.slf4j/jcl-over-slf4j/jars/jcl-over-slf4j-1.7.10.jar:/Users/meng/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.10.jar:/Users/meng/.ivy2/cache/com.ning/compress-lzf/bundles/compress-lzf-1.0.3.jar:/Users/meng/.ivy2/cache/org.xerial.snappy/snappy-java/bundles/snappy-java-1.1.2.jar:/Users/meng/.ivy2/cache/net.jpountz.lz4/lz4/jars/lz4-1.3.0.jar:/Users/meng/.ivy2/cache/org.roaringbitmap/RoaringBitmap/bundles/RoaringBitmap-0.5.11.jar:/Users/meng/.ivy2/cache/com.typesafe.akka/akka-remote_2.10/jars/akka-remote_2.10-2.3.11.jar:/Users/meng/.ivy2/cache/com.typesafe.akka/akka-actor_2.10/jars/akka-actor_2.10-2.3.11.jar:/Users/meng/.ivy2/cache/com.typesafe/config/bundles/config-1.2.1.jar:/Users/meng/.ivy2/cache/io.netty/netty/bundles/netty-3.8.0.Final.jar:/Users/meng/.ivy2/cache/org.uncommons.maths/uncommons-maths/jars/uncommons-maths-1.2.2a.jar:/Users/meng/.ivy2/cache/com.typesafe.akka/akka-slf4j_2.10/jars/akka-slf4j_2.10-2.3.11.jar:/Users/meng/.ivy2/cache/org.json4s/json4s-jackson_2.10/jars/json4s-jackson_2.10-3.2.10.jar:/Users/meng/.ivy2/cache/org.json4s/json4s-core_2.10/jars/json4s-core_2.10-3.2.10.jar:/Users/meng/.ivy2/cache/org.json4s/json4s-ast_2.10/jars/json4s-ast_2.10-3.2.10.jar:/Users/meng/.ivy2/cache/com.thoughtworks.paranamer/paranamer/jars/paranamer-2.6.jar:/Users/meng/.ivy2/cache/org.scala-lang/scalap/jars/scalap-2.10.0.jar:/Users/meng/.ivy2/cache/org.scala-lang/scala-compiler/jars/scala-compiler-2.10.0.jar:/Users/meng/.ivy2/cache/com.sun.jersey/jersey-core/bundles/jersey-core-1.9.jar:/Users/meng/.ivy2/cache/org.apache.mesos/mesos/jars/mesos-0.21.1-shaded-protobuf.jar:/Users/meng/.ivy2/cache/com.clearspring.analytics/stream/jars/stream-2.7.0.jar:/Users/meng/.ivy2/cache/io.dropwizard.metrics/metrics-core/bundles/metrics-core-3.1.2.jar:/Users/meng/.ivy2/cache/io.dropwizard.metrics/metrics-jvm/bundles/metrics-jvm-3.1.2.jar:/Users/meng/.ivy2/cache/io.dropwizard.metrics/metrics-json/bundles/metrics-json-3.1.2.jar:/Users/meng/.ivy2/cache/io.dropwizard.metrics/metrics-graphite/bundles/metrics-graphite-3.1.2.jar:/Users/meng/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-scala_2.10/bundles/jackson-module-scala_2.10-2.4.4.jar:/Users/meng/.ivy2/cache/com.google.code.findbugs/jsr305/jars/jsr305-2.0.1.jar:/Users/meng/.ivy2/cache/org.apache.ivy/ivy/jars/ivy-2.4.0.jar:/Users/meng/.ivy2/cache/oro/oro/jars/oro-2.0.8.jar:/Users/meng/.ivy2/cache/org.tachyonproject/tachyon-client/jars/tachyon-client-0.8.1.jar:/Users/meng/.ivy2/cache/commons-io/commons-io/jars/commons-io-2.4.jar:/Users/meng/.ivy2/cache/org.tachyonproject/tachyon-underfs-hdfs/jars/tachyon-underfs-hdfs-0.8.1.jar:/Users/meng/.ivy2/cache/org.tachyonproject/tachyon-underfs-s3/jars/tachyon-underfs-s3-0.8.1.jar:/Users/meng/.ivy2/cache/org.tachyonproject/tachyon-underfs-local/jars/tachyon-underfs-local-0.8.1.jar:/Users/meng/.ivy2/cache/net.razorvine/pyrolite/jars/pyrolite-4.9.jar:/Users/meng/.ivy2/cache/net.sf.py4j/py4j/jars/py4j-0.9.jar:/Users/meng/.ivy2/cache/org.scala-lang/scala-reflect/jars/scala-reflect-2.10.5.jar:/Users/meng/.ivy2/cache/org.codehaus.janino/janino/jars/janino-2.7.8.jar:/Users/meng/.ivy2/cache/org.codehaus.janino/commons-compiler/jars/commons-compiler-2.7.8.jar:/Users/meng/.ivy2/cache/org.apache.parquet/parquet-column/jars/parquet-column-1.7.0.jar:/Users/meng/.ivy2/cache/org.apache.parquet/parquet-common/jars/parquet-common-1.7.0.jar:/Users/meng/.ivy2/cache/org.apache.parquet/parquet-encoding/jars/parquet-encoding-1.7.0.jar:/Users/meng/.ivy2/cache/org.apache.parquet/parquet-generator/jars/parquet-generator-1.7.0.jar:/Users/meng/.ivy2/cache/commons-codec/commons-codec/jars/commons-codec-1.5.jar:/Users/meng/.ivy2/cache/org.apache.parquet/parquet-hadoop/jars/parquet-hadoop-1.7.0.jar:/Users/meng/.ivy2/cache/org.apache.parquet/parquet-format/jars/parquet-format-2.3.0-incubating.jar:/Users/meng/.ivy2/cache/org.apache.parquet/parquet-jackson/jars/parquet-jackson-1.7.0.jar:/Users/meng/.ivy2/cache/com.github.fommil.netlib/core/jars/core-1.1.2.jar:/Users/meng/.ivy2/cache/net.sourceforge.f2j/arpack_combined_all/jars/arpack_combined_all-0.1.jar:/Users/meng/.ivy2/cache/net.sourceforge.f2j/arpack_combined_all/jars/arpack_combined_all-0.1-javadoc.jar:/Users/meng/.ivy2/cache/org.scalanlp/breeze_2.10/jars/breeze_2.10-0.11.2.jar:/Users/meng/.ivy2/cache/org.scalanlp/breeze-macros_2.10/jars/breeze-macros_2.10-0.11.2.jar:/Users/meng/.ivy2/cache/net.sf.opencsv/opencsv/jars/opencsv-2.3.jar:/Users/meng/.ivy2/cache/com.github.rwl/jtransforms/jars/jtransforms-2.4.0.jar:/Users/meng/.ivy2/cache/org.spire-math/spire_2.10/jars/spire_2.10-0.7.4.jar:/Users/meng/.ivy2/cache/org.spire-math/spire-macros_2.10/jars/spire-macros_2.10-0.7.4.jar:/Users/meng/.ivy2/cache/org.scalamacros/quasiquotes_2.10/jars/quasiquotes_2.10-2.0.0.jar:/Users/meng/.ivy2/cache/org.jpmml/pmml-model/jars/pmml-model-1.1.15.jar:/Users/meng/.ivy2/cache/org.jpmml/pmml-agent/jars/pmml-agent-1.1.15.jar:/Users/meng/.ivy2/cache/org.jpmml/pmml-schema/jars/pmml-schema-1.1.15.jar:/Users/meng/.ivy2/cache/com.sun.xml.bind/jaxb-impl/jars/jaxb-impl-2.2.7.jar:/Users/meng/.ivy2/cache/com.sun.xml.bind/jaxb-core/jars/jaxb-core-2.2.7.jar:/Users/meng/.ivy2/cache/javax.xml.bind/jaxb-api/jars/jaxb-api-2.2.7.jar:/Users/meng/.ivy2/cache/com.sun.istack/istack-commons-runtime/jars/istack-commons-runtime-2.16.jar:/Users/meng/.ivy2/cache/com.sun.xml.fastinfoset/FastInfoset/jars/FastInfoset-1.2.12.jar:/Users/meng/.ivy2/cache/javax.xml.bind/jsr173_api/jars/jsr173_api-1.0.jar -skip-packages akka:org.apache.spark.api.python:org.apache.spark.network:org.apache.spark.deploy:org.apache.spark.util.collection -doc-title Spark 1.6.0 ScalaDoc -d /Users/meng/src/spark/mllib/target/scala-2.10/api -bootclasspath /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/meng/.ivy2/cache/org.scala-lang/scala-library/jars/scala-library-2.10.5.jar
[error]
[error]   last tree to typer: Literal(Constant(1.4.0))
[error]               symbol: null
[error]    symbol definition: null
[error]                  tpe: String(""1.4.0"")
[error]        symbol owners:
[error]       context owners: value <local clustering> -> package clustering
[error]
[error] == Enclosing template or block ==
[error]
[error] Apply(
[error]   new Since.""<init>""
[error]   ""1.4.0""
[error] )
[error]
[error] == Expanded type of tree ==
[error]
[error] ConstantType(value = Constant(1.4.0))
[error]
[error] no-symbol does not have an owner
[error] Total time: 64 s, completed Nov 25, 2015 2:12:09 PM
{code}",,apachespark,josephkb,joshrosen,lian cheng,mengxr,timhunter,yuhaoyan,yuu.ishikawa@gmail.com,,,,,,,,,,,,,,,,,,SPARK-11602,SPARK-11605,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-11-26 16:26:23.616,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 01:20:02 UTC 2015,,,,,0|i2oyb3:,9223372036854775807,,,,,,,,,,,,,"26/Nov/15 16:26;yuhaoyan;Met it with ""./build/sbt unidoc"" 

This is blocking 
11602 scala API, doc
11605 ML 1.6 QA: API: Java compatibility, docs

Is there a way to generate docs for MLlib successfully now ? Thanks.
",27/Nov/15 03:48;yuhaoyan;Scala API can be generated.,"28/Nov/15 08:26;lian cheng;Hit exactly the same issue and worked it around by switching to Java 8. Although {{unidoc}} still returned an (seemingly unharmful) error:
{noformat}
[info] Main Scala API documentation successful.
[error] (spark/javaunidoc:doc) javadoc returned nonzero exit code
[error] Total time: 248 s, completed Nov 28, 2015 4:24:31 PM
{noformat}",28/Nov/15 09:22;srowen;scaladoc + Java 1.8 javadoc doesn't work. The genjavadoc plugin generates invalid javadoc.,"28/Nov/15 10:25;lian cheng;Yeah, also found that the error message IS harmful. By switching to Java 8, scaladoc is OK while no javadoc pages are generated. If I run {{spark/javaunidoc:doc}} under SBT, it spills this:

{noformat}
[error] javadoc: error - invalid flag: -target
[info] Usage: javadoc [options] [packagenames] [sourcefiles] [@files]
[info]   -overview <file>                 Read overview documentation from HTML file
[info]   -public                          Show only public classes and members
[info]   -protected                       Show protected/public classes and members (default)
[info]   -package                         Show package/protected/public classes and members
[info]   -private                         Show all classes and members
[info]   -help                            Display command line options and exit
[info]   -doclet <class>                  Generate output via alternate doclet
[info]   -docletpath <path>               Specify where to find doclet class files
[info]   -sourcepath <pathlist>           Specify where to find source files
[info]   -classpath <pathlist>            Specify where to find user class files
[info]   -cp <pathlist>                   Specify where to find user class files
[info]   -exclude <pkglist>               Specify a list of packages to exclude
[info]   -subpackages <subpkglist>        Specify subpackages to recursively load
[info]   -breakiterator                   Compute first sentence with BreakIterator
[info]   -bootclasspath <pathlist>        Override location of class files loaded
[info]                                    by the bootstrap class loader
[info]   -source <release>                Provide source compatibility with specified release
[info]   -extdirs <dirlist>               Override location of installed extensions
[info]   -verbose                         Output messages about what Javadoc is doing
[info]   -locale <name>                   Locale to be used, e.g. en_US or en_US_WIN
[info]   -encoding <name>                 Source file encoding name
[info]   -quiet                           Do not display status messages
[info]   -J<flag>                         Pass <flag> directly to the runtime system
[info]   -X                               Print a synopsis of nonstandard options and exit
[info]
[info] Provided by Standard doclet:
[info]   -d <directory>                   Destination directory for output files
[info]   -use                             Create class and package usage pages
[info]   -version                         Include @version paragraphs
[info]   -author                          Include @author paragraphs
[info]   -docfilessubdirs                 Recursively copy doc-file subdirectories
[info]   -splitindex                      Split index into one file per letter
[info]   -windowtitle <text>              Browser window title for the documentation
[info]   -doctitle <html-code>            Include title for the overview page
[info]   -header <html-code>              Include header text for each page
[info]   -footer <html-code>              Include footer text for each page
[info]   -top    <html-code>              Include top text for each page
[info]   -bottom <html-code>              Include bottom text for each page
[info]   -link <url>                      Create links to javadoc output at <url>
[info]   -linkoffline <url> <url2>        Link to docs at <url> using package list at <url2>
[info]   -excludedocfilessubdir <name1>:.. Exclude any doc-files subdirectories with given name.
[info]   -group <name> <p1>:<p2>..        Group specified packages together in overview page
[info]   -nocomment                       Suppress description and tags, generate only declarations.
[info]   -nodeprecated                    Do not include @deprecated information
[info]   -noqualifier <name1>:<name2>:... Exclude the list of qualifiers from the output.
[info]   -nosince                         Do not include @since information
[info]   -notimestamp                     Do not include hidden time stamp
[info]   -nodeprecatedlist                Do not generate deprecated list
[info]   -notree                          Do not generate class hierarchy
[info]   -noindex                         Do not generate index
[info]   -nohelp                          Do not generate help link
[info]   -nonavbar                        Do not generate navigation bar
[info]   -serialwarn                      Generate warning about @serial tag
[info]   -tag <name>:<locations>:<header> Specify single argument custom tags
[info]   -taglet                          The fully qualified name of Taglet to register
[info]   -tagletpath                      The path to Taglets
[info]   -charset <charset>               Charset for cross-platform viewing of generated documentation.
[info]   -helpfile <file>                 Include file that help link links to
[info]   -linksource                      Generate source in HTML
[info]   -sourcetab <tab length>          Specify the number of spaces each tab takes up in the source
[info]   -keywords                        Include HTML meta tags with package, class and member info
[info]   -stylesheetfile <path>           File to change style of the generated documentation
[info]   -docencoding <name>              Specify the character encoding for the output
[info] 1 error
[error] (spark/javaunidoc:doc) javadoc returned nonzero exit code
{noformat}

By commenting out [this line|https://github.com/apache/spark/blob/308381420f51b6da1007ea09a02d740613a226e0/project/SparkBuild.scala#L164], it does generate valid javadoc. But TBH, I don't know why...","29/Nov/15 14:22;yuhaoyan;for 11605, I can use javap for now.","30/Nov/15 16:44;timhunter;Yes, I have this branch with some fixes, but I would need double review from someone more familiar with SBT to make sure it does not break something else:
https://github.com/apache/spark/compare/master...thunterdb:1511-java8?expand=1","30/Nov/15 22:01;apachespark;User 'thunterdb' has created a pull request for this issue:
https://github.com/apache/spark/pull/10048","30/Nov/15 22:44;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/10049","30/Nov/15 22:59;joshrosen;Switching to Java 8, by itself, is not a sufficient solution for fixing the release build, since we'd also have to modify our release publishing setup to ensure that a Java 7 class library is used during compilation. I'm going to try to see if there's a way to work around or fix this under Java 7.",30/Nov/15 22:59;joshrosen;This is fixed in my patch: https://github.com/apache/spark/pull/10049,"01/Dec/15 00:38;joshrosen;Issue resolved by pull request 10049
[https://github.com/apache/spark/pull/10049]","01/Dec/15 00:40;joshrosen;This ticket describes a few closely-related yet distinct doc publishing bugs, some of which are resolved by my patch and others which require changes to the build environment. The Javadoc {{-target}} issue and Javadoc + JDK8 bugs were fixed by my patch. It seems that the only easy way to fix the Scaladoc generation bug is to either compile with JDK8 or to use a newer Java 7 JDK. I was able to successfully generate the scaladocs while using Java version 1.7.0_79, which is the same version that we use in Jenkins. If you're affected by the Scaladoc issue, please update your JDK and report back here if that doesn't work.","01/Dec/15 01:34;joshrosen;Here's the full stacktrace of the compiler crash:

{code}
  last tree to typer: Literal(Constant(1.5.0))
              symbol: null
   symbol definition: null
                 tpe: String(""1.5.0"")
       symbol owners:
      context owners: value <local clustering> -> package clustering

== Enclosing template or block ==

Apply(
  new Since.""<init>""
  ""1.5.0""
)

== Expanded type of tree ==

ConstantType(value = Constant(1.5.0))

no-symbol does not have an owner
	at scala.reflect.internal.SymbolTable.abort(SymbolTable.scala:49)
	at scala.tools.nsc.Global.abort(Global.scala:254)
	at scala.reflect.internal.Symbols$NoSymbol.owner(Symbols.scala:3257)
	at scala.tools.nsc.symtab.classfile.ClassfileParser.addEnclosingTParams(ClassfileParser.scala:585)
	at scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfileParser.scala:530)
	at scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.scala:88)
	at scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoaders.scala:261)
	at scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.scala:194)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
	at scala.tools.nsc.doc.base.MemberLookupBase$$anonfun$cleanupBogusClasses$1$1.apply(MemberLookupBase.scala:153)
	at scala.tools.nsc.doc.base.MemberLookupBase$$anonfun$cleanupBogusClasses$1$1.apply(MemberLookupBase.scala:153)
	at scala.collection.TraversableLike$$anonfun$filter$1.apply(TraversableLike.scala:264)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.filter(TraversableLike.scala:263)
	at scala.collection.AbstractTraversable.filter(Traversable.scala:105)
	at scala.tools.nsc.doc.base.MemberLookupBase$class.cleanupBogusClasses$1(MemberLookupBase.scala:153)
	at scala.tools.nsc.doc.base.MemberLookupBase$class.lookupInTemplate(MemberLookupBase.scala:164)
	at scala.tools.nsc.doc.base.MemberLookupBase$class.scala$tools$nsc$doc$base$MemberLookupBase$$lookupInTemplate(MemberLookupBase.scala:128)
	at scala.tools.nsc.doc.base.MemberLookupBase$class.lookupInRootPackage(MemberLookupBase.scala:115)
	at scala.tools.nsc.doc.base.MemberLookupBase$class.memberLookup(MemberLookupBase.scala:52)
	at scala.tools.nsc.doc.DocFactory$$anon$1.memberLookup(DocFactory.scala:78)
	at scala.tools.nsc.doc.base.MemberLookupBase$$anon$1.link$lzycompute(MemberLookupBase.scala:27)
	at scala.tools.nsc.doc.base.MemberLookupBase$$anon$1.link(MemberLookupBase.scala:27)
	at scala.tools.nsc.doc.base.comment.EntityLink$.unapply(Body.scala:75)
	at scala.tools.nsc.doc.html.HtmlPage.inlineToHtml(HtmlPage.scala:126)
	at scala.tools.nsc.doc.html.HtmlPage$$anonfun$inlineToHtml$1.apply(HtmlPage.scala:115)
	at scala.tools.nsc.doc.html.HtmlPage$$anonfun$inlineToHtml$1.apply(HtmlPage.scala:115)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at scala.tools.nsc.doc.html.HtmlPage.inlineToHtml(HtmlPage.scala:115)
	at scala.tools.nsc.doc.html.HtmlPage$$anonfun$inlineToHtml$1.apply(HtmlPage.scala:115)
	at scala.tools.nsc.doc.html.HtmlPage$$anonfun$inlineToHtml$1.apply(HtmlPage.scala:115)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at scala.tools.nsc.doc.html.HtmlPage.inlineToHtml(HtmlPage.scala:115)
	at scala.tools.nsc.doc.html.HtmlPage.inlineToHtml(HtmlPage.scala:124)
	at scala.tools.nsc.doc.html.HtmlPage$$anonfun$inlineToHtml$1.apply(HtmlPage.scala:115)
	at scala.tools.nsc.doc.html.HtmlPage$$anonfun$inlineToHtml$1.apply(HtmlPage.scala:115)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at scala.tools.nsc.doc.html.HtmlPage.inlineToHtml(HtmlPage.scala:115)
	at scala.tools.nsc.doc.html.HtmlPage.blockToHtml(HtmlPage.scala:89)
	at scala.tools.nsc.doc.html.HtmlPage$$anonfun$bodyToHtml$1.apply(HtmlPage.scala:82)
	at scala.tools.nsc.doc.html.HtmlPage$$anonfun$bodyToHtml$1.apply(HtmlPage.scala:82)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at scala.tools.nsc.doc.html.HtmlPage.bodyToHtml(HtmlPage.scala:82)
	at scala.tools.nsc.doc.html.HtmlPage.commentToHtml(HtmlPage.scala:79)
	at scala.tools.nsc.doc.html.HtmlPage.commentToHtml(HtmlPage.scala:75)
	at scala.tools.nsc.doc.html.page.Template.memberToCommentBodyHtml(Template.scala:358)
	at scala.tools.nsc.doc.html.page.Template.memberToCommentHtml(Template.scala:320)
	at scala.tools.nsc.doc.html.page.Template.memberToHtml(Template.scala:296)
	at scala.tools.nsc.doc.html.page.Template$$anonfun$15.apply(Template.scala:220)
	at scala.tools.nsc.doc.html.page.Template$$anonfun$15.apply(Template.scala:220)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at scala.tools.nsc.doc.html.page.Template.<init>(Template.scala:220)
	at scala.tools.nsc.doc.html.HtmlFactory.scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1(HtmlFactory.scala:144)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at scala.tools.nsc.doc.html.HtmlFactory.scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at scala.tools.nsc.doc.html.HtmlFactory.scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at scala.tools.nsc.doc.html.HtmlFactory.scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at scala.tools.nsc.doc.html.HtmlFactory.scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at scala.tools.nsc.doc.html.HtmlFactory.scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory$$anonfun$scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1$2.apply(HtmlFactory.scala:146)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at scala.tools.nsc.doc.html.HtmlFactory.scala$tools$nsc$doc$html$HtmlFactory$$writeTemplate$1(HtmlFactory.scala:146)
	at scala.tools.nsc.doc.html.HtmlFactory.writeTemplates(HtmlFactory.scala:150)
	at scala.tools.nsc.doc.html.HtmlFactory.generate(HtmlFactory.scala:129)
	at scala.tools.nsc.doc.html.Doclet.generateImpl(Doclet.scala:16)
	at scala.tools.nsc.doc.doclet.Generator.generate(Generator.scala:24)
	at scala.tools.nsc.doc.DocFactory.generate$1(DocFactory.scala:131)
	at scala.tools.nsc.doc.DocFactory.document(DocFactory.scala:134)
	at xsbt.Runner.run(ScaladocInterface.scala:26)
	at xsbt.ScaladocInterface.run(ScaladocInterface.scala:10)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:101)
	at sbt.compiler.AnalyzingCompiler.doc(AnalyzingCompiler.scala:67)
	at sbt.compiler.AnalyzingCompiler.doc(AnalyzingCompiler.scala:62)
	at sbt.Doc$$anonfun$scaladoc$1.apply(Doc.scala:23)
	at sbt.Doc$$anonfun$scaladoc$1.apply(Doc.scala:23)
	at sbt.RawCompileLike$$anonfun$prepare$1.apply(RawCompileLike.scala:64)
	at sbt.RawCompileLike$$anonfun$prepare$1.apply(RawCompileLike.scala:56)
	at sbt.RawCompileLike$$anonfun$cached$1$$anonfun$2$$anonfun$apply$1.apply(RawCompileLike.scala:49)
	at sbt.RawCompileLike$$anonfun$cached$1$$anonfun$2$$anonfun$apply$1.apply(RawCompileLike.scala:47)
	at sbt.Tracked$$anonfun$outputChanged$1.apply(Tracked.scala:57)
	at sbt.Tracked$$anonfun$outputChanged$1.apply(Tracked.scala:52)
	at sbt.RawCompileLike$$anonfun$cached$1.apply(RawCompileLike.scala:54)
	at sbt.RawCompileLike$$anonfun$cached$1.apply(RawCompileLike.scala:39)
	at sbtunidoc.Plugin$Unidoc$.apply(Plugin.scala:134)
	at sbtunidoc.Plugin$$anonfun$baseCommonUnidocTasks$2.apply(Plugin.scala:27)
	at sbtunidoc.Plugin$$anonfun$baseCommonUnidocTasks$2.apply(Plugin.scala:27)
	at scala.Function1$$anonfun$compose$1.apply(Function1.scala:47)
	at sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:40)
	at sbt.std.Transform$$anon$4.work(System.scala:63)
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)
	at sbt.Execute.work(Execute.scala:235)
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159)
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

","01/Dec/15 19:59;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/10071","02/Dec/15 00:39;mengxr;I can confirm that this bug still shows up with JDK 1.7.0_79 but not with 1.8.0_65. Btw, it disappears if I revert commit 8a2336893a7ff610a6c4629dd567b85078730616. But I cannot see any suspicious code there.",02/Dec/15 00:51;mengxr;[~yuhaoyan] This doesn't block API doc checks because you can compile with JDK 8 on local.,"03/Dec/15 00:16;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/10114","03/Dec/15 01:20;mengxr;Issue resolved by pull request 10114
[https://github.com/apache/spark/pull/10114]",,,,,
Word2VecModel load and save cause SparkException when model is bigger than spark.kryoserializer.buffer.max,SPARK-11994,12916223,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tmnd91,tmnd91,tmnd91,25/Nov/15 18:02,02/Jun/16 22:16,15/Aug/18 23:03,05/Dec/15 15:42,1.4.1,1.5.1,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,1,kryo,mllib,,,"When loading a Word2VecModel of compressed size 58Mb using the Word2VecModel.load() method introduced in Spark 1.4.0 I get a `org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 0, required: 2` exception.
This happens because the model is saved as a unique file with no partitioning and the kryo buffer overflows when tries to serialize it all.
Increasing `spark.kryoserializer.buffer.max` works as a temporary solution but needs to increased again whenever we increase the model size. ",,apachespark,gprivitera,josephkb,michaelmalak,tmnd91,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6725,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-11-25 20:58:30.689,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 05 15:42:16 UTC 2015,,,,,0|i2oxuv:,9223372036854775807,,,,,,,,,,,,,25/Nov/15 20:58;josephkb;Good point; thanks for reporting this.  We should probably increase the partitioning adaptively based on model size in the save method.  Would you be willing to send a PR?,25/Nov/15 21:37;tmnd91;Sure.,"26/Nov/15 02:11;tmnd91;Since `spark.kryoserializer.buffer.max` defaults to 64MB, I decided to increase the number of partitions the model gets divided into at half that size (32MB).
One word2vec entry consists of an array of float of size vectorSize and a string, since the size of string is variable and considerably lower than the size of the array, I'm not going consider it in my size computation.
And obviously we have numWords entries. Stated that the size of a Float is 4 Bytes.
The number of partitions the model gets splitted into is given by the formulae: (4 * numWords * vectorSize / 33554432) + 1 
I added a test to verify that the save/load methods works when the file is splitted in 2 parts.
","28/Nov/15 10:27;apachespark;User 'tmnd1991' has created a pull request for this issue:
https://github.com/apache/spark/pull/9989","05/Dec/15 15:42;srowen;Issue resolved by pull request 9989
[https://github.com/apache/spark/pull/9989]",,,,,,,,,,,,,,,,,,,
Severl numbers in my spark shell (pyspark),SPARK-11992,12916216,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Invalid,,Bonsanto,Bonsanto,25/Nov/15 17:36,02/Dec/15 22:44,15/Aug/18 23:03,25/Nov/15 17:49,1.5.2,,,,,,,,,,,,,,,,,,,,,MLlib,PySpark,,,,0,newbie,,,,"The problem is very weird, I am currently trying to fit some classifiers from mllib library (SVM, LogisticRegression, RandomForest, DecisionTree and NaiveBayes), so they might classify the data properly, I am trying to compare their performances evaluating their predictions using my current validation data (the typical pipeline), and the problem is that when I try to fit any of those, my spark-shell console prints millions and millions of entries, and after that the fitting process gets stopped, you can see it [here|http://i.imgur.com/mohLnwr.png]

Some details:
- My data has around 15M of entries.
- I use LabeledPoints to represent each entry, where the features are SparseVectors and they have *104* features or dimensions.
- I don't show many things in the console, [log4j.properties|https://gist.github.com/Bonsanto/c487624db805f56882b8]
- The program is running locally in a computer with 16GB of RAM. 

I have already asked this, in StackOverflow, you can see it here [Crazy print|http://stackoverflow.com/questions/33807347/pyspark-shell-outputs-several-numbers-instead-of-the-loading-arrow]","Linux Ubuntu 14.04 LTS
Jupyter 
Spark 1.5.2",Bonsanto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-11-25 17:49:37.113,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 18:09:50 UTC 2015,,,,,0|i2oxtb:,9223372036854775807,,,,,,,,,,,,,"25/Nov/15 17:49;srowen;[~Bonsanto] only committers should set ""Blocker"" and a question can't be one. Despite the existence of this type, questions are for user@spark.apache.org rather than JIRA (or yes stackoverflow). I think you have to narrow down your problem more; it's not clear it's a Spark problem.","25/Nov/15 18:09;Bonsanto;[~srowen] Hello, I appreciate the time you spent by commenting my issue, this is my first time trying to ask and expose something in Jira, and I am seriously lost, is there any guide or something I can read, so I can formulate my question more properly, and avoid disturbing the busy Spark Community? ",,,,,,,,,,,,,,,,,,,,,,
pmml version attribute missing in the root node,SPARK-11582,12911442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fazlan.pera,fazlan.pera,fazlan.pera,09/Nov/15 06:10,09/Nov/15 16:59,15/Aug/18 23:03,09/Nov/15 16:59,1.4.1,1.5.0,1.5.1,,,,,,,,,,,,,1.6.0,,,,,,MLlib,,,,,0,,,,,The current pmml models generated do not specify the pmml version in its root node. This is a problem when using this pmml model in other tools because they expect the version attribute to be set explicitly. ,,apachespark,fazlan.pera,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-11-09 06:52:02.971,,false,,,,,Patch,,,,,,,,9223372036854775807,,,Mon Nov 09 16:59:16 UTC 2015,,,,,0|i2o4fr:,9223372036854775807,,,,,,1.6.0,,,,,,,"09/Nov/15 06:14;fazlan.pera;I have sent a pull request to fix the issue 

https://github.com/apache/spark/pull/9558","09/Nov/15 06:52;apachespark;User 'fazlan-nazeem' has created a pull request for this issue:
https://github.com/apache/spark/pull/9558",09/Nov/15 10:47;srowen;[~fazlan.pera] don't set Fix version yet as it's not fixed,"09/Nov/15 16:59;mengxr;Issue resolved by pull request 9558
[https://github.com/apache/spark/pull/9558]",,,,,,,,,,,,,,,,,,,,
Error thrown when using BlockMatrix.add,SPARK-11507,12910474,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,k.prophet@gmail.com,k.prophet@gmail.com,04/Nov/15 20:42,10/May/17 04:28,15/Aug/18 23:03,30/Mar/16 23:17,1.3.1,1.4.1,1.5.0,1.6.1,2.0.0,,,,,,,,,,,1.5.3,1.6.2,2.0.0,,,,MLlib,,,,,0,,,,,"In certain situations when adding two block matrices, I get an error regarding colPtr and the operation fails.  External issue URL includes full error and code for reproducing the problem.

","Mac/local machine, EC2
Scala",apachespark,josephkb,k.prophet@gmail.com,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20687,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-11-05 07:20:54.421,,false,,,,http://stackoverflow.com/questions/33528555/error-thrown-when-using-blockmatrix-add,,,,,,,,,9223372036854775807,,,Tue Mar 22 03:52:30 UTC 2016,,,,,0|i2nymf:,9223372036854775807,josephkb,,,,,1.5.3,1.6.2,2.0.0,,,,,05/Nov/15 07:20;yuhaoyan;Looking into it. Should be a bug. Breeze may remove the extra end in colPtr after addition. ,"06/Nov/15 02:51;yuhaoyan;Seems breeze will add extra 0 to value in CSCMatrix when adding two sparseMatrix and break the rule ""colPtr.last is always equal to rowIndices.length"". Invoking compact should resolve the issue. I will run a few tests and send a fix if confirmed.","06/Nov/15 09:50;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/9520",17/Jan/16 04:38;yuhaoyan;A fix has been merged into Breeze. https://github.com/scalanlp/breeze/commit/d255b66d7e7720f8447a49c78e762d21b18835c3. ,21/Mar/16 21:59;josephkb;Good to hear!  I am wondering though if it was a mistake to close your original PR (since the Breeze fix won't be put into Spark that quickly).  What do you think about re-opening your PR to get the bug fix into 2.0 and a few backports?,"22/Mar/16 03:52;yuhaoyan;Sure we can do it. About the fix, I assume we should copy first and invoke compact, right?",,,,,,,,,,,,,,,,,,
PySpark RowMatrix Constructor Has Type Erasure Issue,SPARK-11497,12910226,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dusenberrymw,dusenberrymw,dusenberrymw,04/Nov/15 02:18,04/Jul/16 22:54,15/Aug/18 23:03,11/Dec/15 22:23,1.5.0,1.5.1,1.6.0,,,,,,,,,,,,,1.5.3,1.6.0,2.0.0,,,,MLlib,PySpark,,,,0,,,,,"Implementing tallSkinnyQR in SPARK-9656 uncovered a bug with our PySpark RowMatrix constructor. As discussed on the dev list [here|http://apache-spark-developers-list.1001551.n3.nabble.com/K-Means-And-Class-Tags-td10038.html], there appears to be an issue with type erasure with RDDs coming from Java, and by extension from PySpark. Although we are attempting to construct a RowMatrix from an RDD[Vector] in [PythonMLlibAPI|https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/api/python/PythonMLLibAPI.scala#L1115], the Vector type is erased, resulting in an RDD[Object]. Thus, when calling Scala's tallSkinnyQR from PySpark, we get a Java ClassCastException in which an Object cannot be cast to a Spark Vector. As noted in the aforementioned dev list thread, this issue was also encountered with DecisionTrees, and the fix involved an explicit retag of the RDD with a Vector type. Thus, this PR will apply that fix to the createRowMatrix helper function in PythonMLlibAPI. IndexedRowMatrix and CoordinateMatrix do not appear to have this issue likely due to their related helper functions in PythonMLlibAPI creating the RDDs explicitly from DataFrames with pattern matching, thus preserving the types. 

The following reproduces this issue on the latest Git head, 1.5.1, and 1.5.0:

{code}
from pyspark.mllib.linalg.distributed import RowMatrix
rows = sc.parallelize([[3, -6], [4, -8], [0, 1]])
mat = RowMatrix(rows)
mat._java_matrix_wrapper.call(""tallSkinnyQR"", True)
{code}

Should result in the following exception:

{code}
java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lorg.apache.spark.mllib.linalg.Vector;
{code}",,apachespark,dusenberrymw,josephkb,,,,,,,,,,,,,,,,,,,,,,,SPARK-9656,,,,,,,SPARK-16372,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-11-04 02:39:03.343,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 11 22:23:11 UTC 2015,,,,,0|i2nx3r:,9223372036854775807,josephkb,,,,,1.5.3,1.6.1,2.0.0,,,,,"04/Nov/15 02:39;apachespark;User 'dusenberrymw' has created a pull request for this issue:
https://github.com/apache/spark/pull/9458","06/Nov/15 01:54;josephkb;Can you please paste code which reproduces this problem here?

Also, what version of Spark were you using?","06/Nov/15 18:43;dusenberrymw;Hey [~josephkb].  The following will reproduce this issue on the latest Git head, 1.5.1, and 1.5.0.  This came up as I was adding the PySpark wrapper for tallSkinnyQR in the PR for SPARK-9656.

{code}
from pyspark.mllib.linalg.distributed import RowMatrix
rows = sc.parallelize([[3, -6], [4, -8], [0, 1]])
mat = RowMatrix(rows)
mat._java_matrix_wrapper.call(""tallSkinnyQR"", True)
{code}

Should result in the following exception:

{code}
java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lorg.apache.spark.mllib.linalg.Vector;
{code}","07/Nov/15 19:39;josephkb;Oh, I see, I'd forgotten about that discussion!  Since this is a quick fix, I'll target it at 1.5, 1.6, and master.","07/Nov/15 21:51;josephkb;Correction: Does this affect any functionality in current versions of Spark?  Or is this only an issue with your wrapper for tallSkinnyQR?

I'm asking because I can still call things like computePrincipalComponents without this fix.  I can't call tallSkinnyQR, but that's also because of its return type not being converted to a Python object.

Do you have an example of current functionality which is not available?  If not, I might ask that we postpone this for 1.7 (unless it's needed for something targeted for 1.6).","09/Nov/15 03:42;dusenberrymw;So far, I've only encountered it as an issue with the tallSkinnyQR wrapper.  We can just postpone this fix and the wrapper for 1.7.","11/Dec/15 22:23;josephkb;Issue resolved by pull request 9458
[https://github.com/apache/spark/pull/9458]",,,,,,,,,,,,,,,,,
 Multivariate Gaussian Model with Covariance  matrix returns incorrect answer in some cases ,SPARK-11302,12907755,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,srowen,eyalsharon,eyalsharon,25/Oct/15 16:10,28/Oct/15 06:15,15/Aug/18 23:03,28/Oct/15 06:14,1.3.1,1.4.1,1.5.1,1.6.0,,,,,,,,,,,,1.3.2,1.4.2,1.5.2,1.6.0,,,MLlib,,,,,0,,,,,"I have been trying to apply an Anomaly Detection model  using Spark MLib. 

As an input, I feed the model with a mean vector and a Covariance matrix. ,assuming my features contain Co-variance.

Here are my input for the  model ,and the model returns zero for each data point for this input.

MU vector - 
1054.8, 1069.8, 1.3 ,1040.1
Cov' matrix - 
165496.0 , 167996.0,  11.0 , 163037.0  
167996.0,  170631.0,  19.0,  165405.0  
11.0,           19.0 ,         0.0,   2.0       
163037.0,   165405.0     2.0 ,  160707.0 



Conversely,  for the  non covariance case, represented by  this matrix ,the model is working and returns results as expected 
165496.0,  0.0 ,           0.0,   0.0                 
0.0,           170631.0,   0.0,   0.0                 
0.0 ,           0.0 ,           0.8,   0.0                 
0.0 ,           0.0,            0.0,  160594.2


",,apachespark,eyalsharon,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-10-25 16:15:37.943,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 28 06:14:55 UTC 2015,,,,,0|i2nhun:,9223372036854775807,,,,,,1.3.2,1.4.1,1.5.2,1.6.0,,,,25/Oct/15 16:15;srowen;It's not clear what you're trying to report. What code are you executing? what model?,"25/Oct/15 16:39;eyalsharon;Hi Sean,

Thanks for your reply. I will try to add more info

 - I'm using a Multivariate Gaussian for anomaly detection. I'm using this
source from Mlib -   MultivariateGaussian
<https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/stat/distribution/MultivariateGaussian.scala>

This library enables to create a Gaussian instance and to feed it with new
data point (which is a dense vector )  to return the probability.
Now ,when I run my code over, it always returns zero

- I checked my code using this example implantation I have found on GIT example
for anomaly detection  <https://github.com/vivanov/anomaly-detection>
Note the this example uses a *non covariance* matrix, If you run this code
with a full  covariance matrix, the PDF function will always return zero.

  To check to covariance case , here is a function which takes a  data set
(mat) with features and a corresponding mean  vector (mu) :

def createCovSigma(mat: DenseMatrix,mu: Vector) : DenseMatrix = {

  val rowsInArray = mat.transpose.toArray.grouped(mat.numCols).toArray
  val sigmaSubMU = rowsInArray.map(row => {(row.toList zip
mu.toArray).map(elem=>elem._1-elem._2)}.toArray )

  val checkArray = sigmaSubMU.flatMap(row=>row)

  val mat2 = new DenseMatrix(mat.numRows, mat.numCols,checkArray,true)
  val sigmaTmp: DenseMatrix = mat2.transpose.multiply(mat2)
  val sigmaTmpArray=sigmaTmp.toArray
  val sigmaMatrix: DenseMatrix =  new DenseMatrix(mat.numCols,
mat.numCols, sigmaTmpArray.flatMap(x=>List(x/mat.numRows)),true)

  sigmaMatrix
}


 If you need me to add more info I will

Thanks!

Eyal





-



























-- 


*This email and any files transmitted with it are confidential and intended 
solely for the use of the individual or entity to whom they are 
addressed. Please note that any disclosure, copying or distribution of the 
content of this information is strictly forbidden. If you have received 
this email message in error, please destroy it immediately and notify its 
sender.*
","26/Oct/15 10:06;srowen;In R, it notes that your covariance matrix isn't positive definite. It isn't -- it has negative eigenvalues. It doesn't mean it's wrong, but it could. Are you sure this is the right input and isn't maybe a victim of precision problems or rounding? in any event, I don't think MLlib is the problem here, since R won't compute this either. (I glanced at the implementation and it looked like what I expect to see too.)","26/Oct/15 13:20;eyalsharon;Hi Sean ,

Thanks for reaching out.

For convenience, when  added the Covariance matrix to the ticket I rounded
the numbers.

Below are the real values (should be organize in a 4x4 matrix ). The
covariance matrix, by math definition, is always *positive semi definite ( *and
not positive definite* )*
I checked this values in R with this function *
is.positive.semi.definite (*with
 a tolerance levels  of e-11,e-15,e-20) and it returns true for all cases .

401139.3599484815,387621.07664008765,73902.67897058972,314299.39550677023
,387621.07664008765,408594.15705509897,94234.19718534013,351268.39070671634
 ,73902.67897058972,94234.19718534013,969566.5912689088,125849.1446871119
,314299.39550677023,351268.39070671634,125849.1446871119,393043.68462620175


Best, Eyal




-- 


*This email and any files transmitted with it are confidential and intended 
solely for the use of the individual or entity to whom they are 
addressed. Please note that any disclosure, copying or distribution of the 
content of this information is strictly forbidden. If you have received 
this email message in error, please destroy it immediately and notify its 
sender.*
","26/Oct/15 14:25;srowen;Yeah I recognize that, but is this not the answer then? the covariance matrix is invalid. The covariance matrix you have here is very different.
What's mu? where are you computing the pdf? what is the log(pdf) -- that is, is it just not very very small? what does R say? I think there is still a lot of missing pieces here.","26/Oct/15 15:26;eyalsharon;Sure , I will try to elaborate more

MU  is the mean vector of my data set.

Here is the basic flow of my code with function I used. Each function runs
over the data set arranged in a matrix


*1- Create a  mu vector *

def createMU(mat: DenseMatrix): Vector = {

  val columnsInArray = toArrays(mat,false)

  Vectors.dense(columnsInArray.map(vector => vector.sum/vector.length ))

}


*2- create a cov matrix *

 def createCovSigma(mat: DenseMatrix,mu: Vector) : DenseMatrix = {


  val rowsInArray = toArrays(mat,true)
  val sigmaSubMU = rowsInArray.map(row => {(row.toList zip
mu.toArray).map(elem=>elem._1-elem._2)}.toArray )

  val checkArray = sigmaSubMU.flatMap(row=>row)

  println(""Matrix dimensions -  rows: "" + mat.numRows + "",cols: ""  +
mat.numCols)
  val mat2 = new DenseMatrix(mat.numRows, mat.numCols,checkArray,true)
  val sigmaTmp: DenseMatrix = mat2.transpose.multiply(mat2)
  val sigmaTmpArray=sigmaTmp.toArray
  val sigmaMatrix: DenseMatrix =  new DenseMatrix(mat.numCols,
mat.numCols, sigmaTmpArray.flatMap(x=>List(x/mat.numRows)),true)

  sigmaMatrix
}

* Note the I am using an auxiliary function toArrays, here is the definition:


def toArrays(mat: Matrix,byRow: Boolean): Array[Array[Double]]  = {

  val direction = if (byRow)  mat.numCols else mat.numRows
  mat.toArray.grouped(direction).toArray

}


*3- After having the mu and the sigma, I can no create an instance of the
gaussian *

val mg = new MultivariateGaussian(mu,sigma)


4- Now, I can create a projection using the PDF

E.g-    d3=mg.pdf(Vectors.dense(629,640,1.7188,618.19))

The model  returns zero for every data point



 4- For validation, I ran a gaussian implantation on Matlab and the results
are:

- For the case of *non covariance* matrix, the two models yield same result
exactly
- For the case of *covariance*, Matlab yields good result but Mlib doesn't.
( note that I feed the two models with the same input, concretely, the same
MU and covariance matrix  )


Best, Eyal





-- 


*This email and any files transmitted with it are confidential and intended 
solely for the use of the individual or entity to whom they are 
addressed. Please note that any disclosure, copying or distribution of the 
content of this information is strictly forbidden. If you have received 
this email message in error, please destroy it immediately and notify its 
sender.*
","26/Oct/15 15:33;srowen;I understand mu is the mean vector, but what is the vector? I'm trying to quickly reproduce this or not. It's good to share code here but I think even better would be just code that starts with your mu / sigma and shows it computing something you believe to be non-zero but isn't.  Right now this isn't a reproducible test case but it nearly is.

You show one data point but what else? what's the correct answer -- is it very small (like smaller than the smallest positive 64-bit float)? what about the result of logpdf?","26/Oct/15 17:26;eyalsharon;Cool , I will try although I hope  I fully captured all questions

1- Logpdf is also returns non reasonable value .   mg.logpdf(Vectors.dense(
629,640,1.7188,618.19))  =    -3891330.078277891 ( the exp is zero   )
2 -  my MU vector values -
 [1055.3910505836575,1070.489299610895,1.39020554474708,1040.5907503867697]
3 - the correct answer , as return from Matlab for this given data point
mg.pdf(Vectors.dense(629,640,1.7188,618.19))  is around e-05
4- when running the model with a non covariance matrix , model yields

 pdf - 7.293362507983666E-11, logpdf  -23.341471333876257 . Again , these
results match with the Matlab model


5- these are the printed values from my script

mu:
 [1055.3910505836575,1070.489299610895,1.39020554474708,1040.5907503867697]

sigma:

166769.00466698944  0.0                0.0                0.0

0.0                 172041.5670061245  0.0                0.0

0.0                 0.0                0.872524191943962  0.0

0.0                 0.0                0.0                161848.9196719207


sigmaCov:
 166769.00466698944  169336.6705268059   12.820670788921873
 164243.93314092053
169336.6705268059   172041.5670061245   21.62590020524533
166678.01075856484
12.820670788921873  21.62590020524533   0.872524191943962
4.283255814732373
164243.93314092053  166678.01075856484  4.283255814732373
161848.9196719207


I hope it helps.

Best, Eyal







-- 


*This email and any files transmitted with it are confidential and intended 
solely for the use of the individual or entity to whom they are 
addressed. Please note that any disclosure, copying or distribution of the 
content of this information is strictly forbidden. If you have received 
this email message in error, please destroy it immediately and notify its 
sender.*
","26/Oct/15 20:31;srowen;OK I reproduced all of this, thank you. This is roughly the code you can use to see the very large logpdf for this value:

{code}
import breeze.linalg.{DenseMatrix => BDM, Matrix => BM, DenseVector => BDV, SparseVector => BSV, Vector => BV, diag, max, eigSym}

val breezeMu = new BDV(Array(1055.3910505836575,1070.489299610895,1.39020554474708,1040.5907503867697))

val breezeSigma = new BDM(4, 4, Array(166769.00466698944, 169336.6705268059, 12.820670788921873, 164243.93314092053, 169336.6705268059, 172041.5670061245, 21.62590020524533, 166678.01075856484, 12.820670788921873, 21.62590020524533, 0.872524191943962, 4.283255814732373, 164243.93314092053, 166678.01075856484, 4.283255814732373, 161848.9196719207))

val EPSILON = {
    var eps = 1.0
    while ((1.0 + (eps / 2.0)) != 1.0) {
      eps /= 2.0
    }
    eps
  }

val eigSym.EigSym(d, u2) = eigSym(breezeSigma)
val tol = EPSILON * max(d) * d.length
val logPseudoDetSigma = d.activeValuesIterator.filter(_ > tol).map(math.log).sum
val pinvS = diag(new BDV(d.map(v => if (v > tol) math.sqrt(1.0 / v) else 0.0).toArray))

val (rootSigmaInv: BDM[Double], u: Double) = (pinvS * u2, -0.5 * (breezeMu.size * math.log(2.0 * math.Pi) + logPseudoDetSigma))

val x = new BDV(Array(629,640,1.7188,618.19))

val delta = x - breezeMu
val v = rootSigmaInv * delta
u + v.t * v * -0.5
{code}

The problem is the clever trick here to compute, well, delta' * inv(sigma) * delta by computing (inv(sigma) * delta)' * (inv(sigma) * delta). The square root bit loses too much precision in a case like this.

I think it's pretty easy to avoid entirely. There's no great reason not to return u and inv(sigma) directly and compute this in the straightforward way.","27/Oct/15 07:50;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/9293","27/Oct/15 20:50;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/9309","28/Oct/15 06:14;mengxr;Issue resolved by pull request 9309
[https://github.com/apache/spark/pull/9309]",,,,,,,,,,,,
__gettitem__ method throws IndexError exception when we try to access index after the last non-zero entry.,SPARK-10973,12903022,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,07/Oct/15 15:15,14/Oct/15 20:03,15/Aug/18 23:03,14/Oct/15 19:38,1.3.0,1.4.0,1.5.0,1.6.0,,,,,,,,,,,,1.3.2,1.4.2,1.5.2,1.6.0,,,MLlib,PySpark,,,,0,backport-needed,,,,"\_\_gettitem\_\_ method throws IndexError exception when we try to access  index  after the last non-zero entry.

{code}
from pyspark.mllib.linalg import Vectors
sv = Vectors.sparse(5, {1: 3})
sv[0]
## 0.0
sv[1]
## 3.0
sv[2]
## Traceback (most recent call last):
##   File ""<stdin>"", line 1, in <module>
##   File ""/python/pyspark/mllib/linalg/__init__.py"", line 734, in __getitem__
##     row_ind = inds[insert_index]
## IndexError: index out of bounds
{code}",,apachespark,josephkb,rxin,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-10-07 15:19:03.217,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 14 20:03:36 UTC 2015,,,,,0|i2mp07:,9223372036854775807,josephkb,,,,,1.3.2,1.4.2,1.5.2,1.6.0,,,,"07/Oct/15 15:19;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/9009","09/Oct/15 01:34;josephkb;Issue resolved by pull request 9009
[https://github.com/apache/spark/pull/9009]",09/Oct/15 01:35;josephkb;Reopening to backport,"10/Oct/15 16:57;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/9062","10/Oct/15 16:58;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/9063","10/Oct/15 16:58;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/9064",14/Oct/15 19:35;rxin;[~josephkb] this should be closed now right?,"14/Oct/15 20:03;josephkb;Yes, thanks!",,,,,,,,,,,,,,,,
RowMatrix.computeCovariance() result is not exactly symmetric,SPARK-10875,12901372,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,pnpritchard,pnpritchard,pnpritchard,29/Sep/15 17:46,09/Oct/15 05:22,15/Aug/18 23:03,09/Oct/15 05:22,1.5.0,,,,,,,,,,,,,,,1.6.0,,,,,,MLlib,,,,,0,,,,,"For some matrices, I have seen that the computed covariance matrix is not exactly symmetric, most likely due to some numerical rounding errors. This is problematic when trying to construct an instance of {{MultivariateGaussian}}, because it requires an exactly symmetric covariance matrix. See reproducible example below.

I would suggest modifying the implementation so that {{G(i, j)}} and {{G(j, i)}} are set at the same time, with the same value.

{code}
val rdd = RandomRDDs.normalVectorRDD(sc, 100, 10, 0, 0)
val matrix = new RowMatrix(rdd)
val mean = matrix.computeColumnSummaryStatistics().mean
val cov = matrix.computeCovariance()
val dist = new MultivariateGaussian(mean, cov) //throws breeze.linalg.MatrixNotSymmetricException
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-09-29 18:06:04.888,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 29 20:49:04 UTC 2015,,,,,0|i2mf0f:,9223372036854775807,mengxr,,,,,1.6.0,,,,,,,"29/Sep/15 18:06;srowen;Makes sense to me, do you want to try a PR?","29/Sep/15 20:49;apachespark;User 'pnpritchard' has created a pull request for this issue:
https://github.com/apache/spark/pull/8940",,,,,,,,,,,,,,,,,,,,,,
RandomForest serialization OOM during findBestSplits,SPARK-10821,12896287,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Done,,jluan,jluan,25/Sep/15 00:49,01/Oct/15 20:32,15/Aug/18 23:03,28/Sep/15 23:06,1.4.0,1.5.0,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,OOM,out-of-memory,,,"I am getting OOM during serialization for a relatively small dataset for a RandomForest. Even with spark.serializer.objectStreamReset at 1, It is still running out of memory when attempting to serialize my data.

Stack Trace:
Traceback (most recent call last):
  File ""/root/random_forest/random_forest_spark.py"", line 198, in <module>
    main()
  File ""/root/random_forest/random_forest_spark.py"", line 166, in main
    trainModel(dset)
  File ""/root/random_forest/random_forest_spark.py"", line 191, in trainModel
    impurity='gini', maxDepth=4, maxBins=32)
  File ""/root/spark/python/lib/pyspark.zip/pyspark/mllib/tree.py"", line 352, in trainClassifier
  File ""/root/spark/python/lib/pyspark.zip/pyspark/mllib/tree.py"", line 270, in _train
  File ""/root/spark/python/lib/pyspark.zip/pyspark/mllib/common.py"", line 130, in callMLlibFunc
  File ""/root/spark/python/lib/pyspark.zip/pyspark/mllib/common.py"", line 123, in callJavaFunc
  File ""/root/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
  File ""/root/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError15/09/25 00:44:41 DEBUG BlockManagerSlaveEndpoint: Done removing RDD 7, response is 0
15/09/25 00:44:41 DEBUG BlockManagerSlaveEndpoint: Sent response: 0 to AkkaRpcEndpointRef(Actor[akka://sparkDriver/temp/$Mj])
: An error occurred while calling o89.trainRandomForestModel.
: java.lang.OutOfMemoryError
        at java.io.ByteArrayOutputStream.hugeCapacity(ByteArrayOutputStream.java:123)
        at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:117)
        at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
        at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1876)
        at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1785)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1188)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
        at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
        at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:84)
        at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301)
        at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)
        at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)
        at org.apache.spark.SparkContext.clean(SparkContext.scala:2021)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:703)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:702)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
        at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:702)
        at org.apache.spark.mllib.tree.DecisionTree$.findBestSplits(DecisionTree.scala:625)
        at org.apache.spark.mllib.tree.RandomForest.run(RandomForest.scala:235)
        at org.apache.spark.mllib.tree.RandomForest$.trainClassifier(RandomForest.scala:291)
        at org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRandomForestModel(PythonMLLibAPI.scala:742)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
        at py4j.Gateway.invoke(Gateway.java:259)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:207)
        at java.lang.Thread.run(Thread.java:745)

Details:

My RDD is type MLLIB LabeledPoint objects, with each holding sparse vectors inside. This RDD has a total size of roughly 45MB. My sparse vector has a total length of ~15 million while only about 3000 or so are non-zeros. Works fine for up to sparse vector size 10 million. 

My cluster is setup on AWS such that my master is a r3.8xlarge along with two r3.4xlarge workers. Driver has ~190GB allocated to it while my RDD is ~45MB.

Configurations as follows:
spark version: 1.5.0 
----------------------------------- 
spark.executor.memory 32000m 
spark.driver.memory 230000m 
spark.driver.cores 10 
spark.executor.cores 5 
spark.executor.instances 17 
spark.driver.maxResultSize 0 
spark.storage.safetyFraction 1 
spark.storage.memoryFraction 0.9 
spark.storage.shuffleFraction 0.05 
spark.default.parallelism 128 
spark.serializer.objectStreamReset 1

My original code is in python which I tried on 1.4.0 and 1.5.0, so I thought that maybe running something in scala may resolve the problem. I wrote a toy scala example and tested it on the same system yielding the same errors. Note the test code will most likely eventually throw an error due to the fact certain features are always 0 and MLLIB currently errors out during this operation.



Running the following using spark-shell with my spark configuration gives me the OOM:
--------------------------------------------------------------------------
import scala.util.Random
import scala.collection.mutable.ArrayBuffer

import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.model.RandomForestModel
import org.apache.spark.mllib.util.MLUtils

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val r = Random

var size = 15000000
var count = 3000
val indptr = (1 to size by size/count).toArray
val data = Seq.fill(count)(r.nextDouble()).toArray

var dset = ArrayBuffer[LabeledPoint]()
for (i <- 1 to 10) {
	dset += LabeledPoint(r.nextInt(2), Vectors.sparse(size, indptr, data));
}

val distData = sc.parallelize(dset)
val splits = distData.randomSplit(Array(0.7, 0.3))
val (trainingData, testData) = (splits(0), splits(1))

// Train a RandomForest model.
//  Empty categoricalFeaturesInfo indicates all features are continuous.
val numClasses = 2
val categoricalFeaturesInfo = Map[Int, Int]()
val numTrees = 3 // Use more in practice.
val featureSubsetStrategy = ""auto"" // Let the algorithm choose.
val impurity = ""gini""
val maxDepth = 4
val maxBins = 32

val model = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)",Amazon EC2 Linux,jluan,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-09-28 23:06:02.324,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 01 20:32:41 UTC 2015,,,,,0|i2ljmv:,9223372036854775807,,,,,,,,,,,,,"28/Sep/15 23:06;josephkb;Hi, this is more a question for the user list, so I'll close it for now.  But a few comments:

The real problem is that MLlib decision trees are meant for a relatively small number of features.  They should work very well for a few thousand features, and could work for more but become fairly slow.

However, I'm working on a new implementation which should make training much faster with millions of features.

One suggestion: It sounds like your data are extremely sparse.  I'd suggest hashing your feature vector to maybe 1000 features and try again.","01/Oct/15 20:32;jluan;Thank you for the insight, do you know what the status of the new implementation of decision tree is or a possible ETA for when it will be ready? Maybe I can help with either testing or contributing to the code.",,,,,,,,,,,,,,,,,,,,,,
Gradient boosted trees: mapPartitions input size increasing ,SPARK-10629,12864429,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,Vimin_Wu,Vimin_Wu,16/Sep/15 02:16,19/Sep/15 07:19,15/Aug/18 23:03,19/Sep/15 07:19,1.4.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"First of all, I think my problem is quite different from https://issues.apache.org/jira/browse/SPARK-10433, which point that the input size increasing at each iteration.

My problem is the mapPartitions input size increase in one iteration. My training samples has 2958359 features in total. Within one iteration, 3 collectAsMap operation had been called. And here is a summary of each call.

| Stage Id |               Description                                | Duration  |   Input    | Shuffle Read | Shuffle Write |
|:----------:|:---------------------------------------------------:|:-----------:|:-----------:|:----------------:|:----------------:|
|      4      | mapPartitions at DecisionTree.scala:613 |  1.6 h      |710.2 MB |  	        | 	2.8 GB       |
|      5      | collectAsMap at DecisionTree.scala:642  |  1.8 min  |                |   	2.8 GB        |                      |
|      6      | mapPartitions at DecisionTree.scala:613 |  1.2 h      | 27.0 GB  |        |          5.6 GB |
|      7      | collectAsMap at DecisionTree.scala:642 | 2.0 min     |   |    5.6GB       |          |
|      8      | mapPartitions at DecisionTree.scala:613 |  1.2 h      | 26.5 GB  |        |          	11.1 GB |
|      9      | collectAsMap at DecisionTree.scala:642 | 2.0 min     |  |    8.3 GB      |          |

the mapPartitions operation took too long time! It's so strange! I wonder whether there is bug exits?",,josephkb,Vimin_Wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10433,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-09-16 07:31:42.183,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 07:31:42 UTC 2015,,,,,0|i2k89z:,9223372036854775807,,,,,,,,,,,,,"16/Sep/15 07:31;srowen;That sounds like the same issue in SPARK-10433; it's not clear that whatever it is only manifests as constantly increasing size. In any event, you should of course try 1.5+ to see if the fix fixes this anyway. I'd personally close this, given this information, until you are certain it happens now in the latest code, in which case it is something else.",,,,,,,,,,,,,,,,,,,,,,,
GradientBoostedTrees stuck with 2958359 features train data,SPARK-10616,12864224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,Vimin_Wu,Vimin_Wu,15/Sep/15 11:32,15/Sep/15 14:48,15/Aug/18 23:03,15/Sep/15 12:00,1.4.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"I run a job to train a Gradient Boosting Decision Tree. The code works well on a small part where the features is about 20 thousands. But, when I run it on one data data where there are 2958359 features in total. The job get stuck after finished ""collect at DecisionTree.scala:977"". 
Here is the statistics of my enviorment and logs, from the logs you can see it stop working at 15/09/15 18:50:57, 2 hours has passed no process at all, and I'm sure the memory is enough for this job. I hope someone can help me with this problem, thanks!
Storage
RDD Name	Storage Level	Cached Partitions	Fraction Cached	Size in Memory	Size in ExternalBlockStore	Size on Disk
MapPartitionsRDD 	Memory Deserialized 1x Replicated 	200 	100% 	771.7 MB 	0.0 B 	0.0 B
MapPartitionsRDD 	Memory Deserialized 1x Replicated 	200 	100% 	771.7 MB 	0.0 B 	0.0 B


Environment
Runtime Information
Name	Value
Java Home	/opt/jdk1.8.0_60/jre
Java Version	1.8.0_60 (Oracle Corporation)
Scala Version	version 2.10.4
Spark Properties
Name	Value
spark.akka.frameSize	100
spark.akka.threads	4
spark.app.id	application_1442215126745_0068
spark.app.name	GBDT_MODEL
spark.broadcast.compress	true
spark.broadcast.factory	org.apache.spark.broadcast.TorrentBroadcastFactory
spark.closure.serializer	org.apache.spark.serializer.JavaSerializer
spark.driver.extraJavaOptions	-Xmn2G -Xms4G -XX:MaxPermSize=200m -XX:+UseConcMarkSweepGC -XX:+PrintGC -XX:+PrintGCDetails -Xloggc:/tmp/spark.driver.gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp
spark.driver.extraLibraryPath	/usr/lib/hadoop/lib/native
spark.driver.host	10.47.1.38
spark.driver.memory	32g
spark.driver.port	29651
spark.dynamicAllocation.enabled	false
spark.dynamicAllocation.executorIdleTimeout	120
spark.dynamicAllocation.initialExecutors	10
spark.dynamicAllocation.maxExecutors	500
spark.dynamicAllocation.minExecutors	10
spark.dynamicAllocation.schedulerBacklogTimeout	5
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout	5
spark.eventLog.compress	true
spark.eventLog.dir	hdfs:///data/spark/logs/1.4
spark.eventLog.enabled	true
spark.executor.extraJavaOptions	-Xmn1G -XX:+UseG1GC
spark.executor.extraLibraryPath	/usr/lib/hadoop/lib/native
spark.executor.heartbeatInterval	10000
spark.executor.id	driver
spark.executor.instances	300
spark.executor.memory	16g
spark.externalBlockStore.folderName	spark-ff489341-5ce7-47bc-b0d3-a3011300f3fc
spark.file.transferTo	false
spark.fileserver.uri	http://10.47.1.38:42401
spark.io.compression.codec	org.apache.spark.io.LZFCompressionCodec
spark.master	yarn-cluster
spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOST	jt4dg.prod.mediav.com
spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASE	http://jt4dg.prod.mediav.com:9099/proxy/application_1442215126745_0068
spark.rdd.compress	true
spark.rpc.askTimeout	60
spark.rpc.retry.wait	3000
spark.rpc.timeout	300
spark.scheduler.maxRegisteredResourcesWaitingTime	3s
spark.scheduler.mode	FAIR
spark.serializer	org.apache.spark.serializer.KryoSerializer
spark.shuffle.service.enabled	true
spark.speculation	false
spark.sql.autoBroadcastJoinThreshold	104857600
spark.sql.codegen	false
spark.sql.hive.convertMetastoreParquet	true
spark.sql.inMemoryColumnarStorage.batchSize	1000
spark.sql.inMemoryColumnarStorage.compressed	true
spark.sql.parquet.binaryAsString	true
spark.sql.parquet.cacheMetadata	true
spark.sql.parquet.compression.codec	lzo
spark.sql.parquet.filterPushdown	false
spark.sql.parquet.useDataSourceApi	true
spark.sql.shuffle.partitions	200
spark.storage.blockManagerHeartBeatMs	120
spark.storage.memoryFraction	0.6
spark.storage.unrollFraction	0.2
spark.task.cpus	1
spark.task.maxFailures	8
spark.ui.filters	org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
spark.ui.killEnabled	true
spark.ui.port	0
spark.ui.retainedStages	1000
spark.yarn.am.extraJavaOptions	-Xmn1G -XX:+UseG1GC
spark.yarn.am.extraLibraryPath	/usr/lib/hadoop/lib/native
spark.yarn.am.memory	3G
spark.yarn.am.memoryOverhead	1024
spark.yarn.am.waitTime	300000
spark.yarn.app.attemptId	1
spark.yarn.app.container.log.dir	/hadoopdir13/yarn/logs/application_1442215126745_0068/container_1442215126745_0068_01_000001
spark.yarn.app.id	application_1442215126745_0068
spark.yarn.executor.memoryOverhead	1024
spark.yarn.historyServer.address	jt1dg.prod.mediav.com:18080
spark.yarn.report.interval	1000

Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=200m; support was removed in 8.0
15/09/15 18:50:12 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]
15/09/15 18:50:13 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1442215126745_0068_000001
15/09/15 18:50:13 INFO spark.SecurityManager: Changing view acls to: yarn,wuwm
15/09/15 18:50:13 INFO spark.SecurityManager: Changing modify acls to: yarn,wuwm
15/09/15 18:50:13 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, wuwm); users with modify permissions: Set(yarn, wuwm)
15/09/15 18:50:14 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
15/09/15 18:50:14 INFO yarn.ApplicationMaster: Waiting for spark context initialization
15/09/15 18:50:14 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 
15/09/15 18:50:14 INFO spark.SparkContext: Running Spark version 1.4.1
15/09/15 18:50:14 INFO spark.SecurityManager: Changing view acls to: yarn,wuwm
15/09/15 18:50:14 INFO spark.SecurityManager: Changing modify acls to: yarn,wuwm
15/09/15 18:50:14 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, wuwm); users with modify permissions: Set(yarn, wuwm)
15/09/15 18:50:14 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/09/15 18:50:14 INFO Remoting: Starting remoting
15/09/15 18:50:15 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.47.1.38:29651]
15/09/15 18:50:15 INFO util.Utils: Successfully started service 'sparkDriver' on port 29651.
15/09/15 18:50:15 INFO spark.SparkEnv: Registering MapOutputTracker
15/09/15 18:50:15 INFO spark.SparkEnv: Registering BlockManagerMaster
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir1/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-355c7a06-a8a1-406e-a2c1-fd0b48763e75
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir2/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-7c209d1d-f00f-4bd5-85b2-c636bdb631af
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir3/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-d81047b7-c58f-40df-9bfc-b6a5b024c4c5
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir4/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-4a71d2e3-72a3-4622-9d40-69943cc1cc80
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir5/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-1a94d11f-b6df-4c69-a971-08702bb97808
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir6/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-2c944d2c-24f4-49f5-90bf-56121419a059
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir7/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-b3a82a9b-5703-49b3-976e-70ccb5d43b0d
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir8/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-96e20bee-4ed0-4907-be62-1dc511484797
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir9/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-6129c0b6-c05d-417a-95c0-148e708686ee
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir10/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-d625fd54-3982-4b08-baf2-a2b5946f5954
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir11/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-504bb240-017e-4caa-89d0-81d60c38af4e
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir12/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-c17183b4-9ee6-4e1b-877e-cd78d96fefb8
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir13/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-75c6de8a-d443-47ae-9a7b-ad0a91a6ff2d
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir14/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-2b9ea7fb-7a24-4229-af58-43c5a511e2f0
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir15/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-17a6904a-7637-44ad-ad6c-c042d96617c3
15/09/15 18:50:15 INFO storage.DiskBlockManager: Created local directory at /hadoopdir16/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/blockmgr-fc2372bb-f6d2-4bf6-987c-84c3271024d8
15/09/15 18:50:15 INFO storage.MemoryStore: MemoryStore started with capacity 17.2 GB
15/09/15 18:50:15 INFO spark.HttpFileServer: HTTP File server directory is /hadoopdir1/yarn/local/usercache/wuwm/appcache/application_1442215126745_0068/httpd-812c9f01-8931-4fbc-a93a-685ed2619fe9
15/09/15 18:50:15 INFO spark.HttpServer: Starting HTTP Server
15/09/15 18:50:15 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/09/15 18:50:15 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:42401
15/09/15 18:50:15 INFO util.Utils: Successfully started service 'HTTP file server' on port 42401.
15/09/15 18:50:15 INFO spark.SparkEnv: Registering OutputCommitCoordinator
15/09/15 18:50:15 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
15/09/15 18:50:15 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/09/15 18:50:15 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:8925
15/09/15 18:50:15 INFO util.Utils: Successfully started service 'SparkUI' on port 8925.
15/09/15 18:50:15 INFO ui.SparkUI: Started SparkUI at http://10.47.1.38:8925
15/09/15 18:50:15 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler
15/09/15 18:50:15 INFO scheduler.FairSchedulableBuilder: Created default pool default, schedulingMode: FIFO, minShare: 0, weight: 1
15/09/15 18:50:16 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 18554.
15/09/15 18:50:16 INFO netty.NettyBlockTransferService: Server created on 18554
15/09/15 18:50:16 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/09/15 18:50:16 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.47.1.38:18554 with 17.2 GB RAM, BlockManagerId(driver, 10.47.1.38, 18554)
15/09/15 18:50:16 INFO storage.BlockManagerMaster: Registered BlockManager
15/09/15 18:50:16 INFO scheduler.EventLoggingListener: Logging events to hdfs:///data/spark/logs/1.4/application_1442215126745_0068_1.lzf
15/09/15 18:50:17 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka://sparkDriver/user/YarnAM#308423925])
15/09/15 18:50:17 INFO yarn.YarnRMClient: Registering the ApplicationMaster
15/09/15 18:50:17 INFO yarn.YarnRMClient: Connecting to ResourceManager at jt5dg.prod.mediav.com/10.47.1.103:8030
15/09/15 18:50:17 INFO yarn.YarnAllocator: Will allocate 300 executor containers, each with 17408 MB memory including 1024 MB overhead
15/09/15 18:50:17 INFO yarn.YarnAllocator: Allocating 300 executor containers with 17408 of memory each.
15/09/15 18:50:17 INFO yarn.YarnAllocator: ResourceRequest (host : *, num containers: 300, priority = 1 , capability : <memory:17408, vCores:1>)
15/09/15 18:50:17 INFO yarn.ApplicationMaster: Started progress reporter thread - sleep time : 5000
15/09/15 18:50:18 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 3000(ms)
15/09/15 18:50:18 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
15/09/15 18:50:19 INFO storage.MemoryStore: ensureFreeSpace(179816) called with curMem=0, maxMem=18438322913
15/09/15 18:50:19 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 175.6 KB, free 17.2 GB)
15/09/15 18:50:19 INFO storage.MemoryStore: ensureFreeSpace(16266) called with curMem=179816, maxMem=18438322913
15/09/15 18:50:19 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.9 KB, free 17.2 GB)
15/09/15 18:50:19 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.47.1.38:18554 (size: 15.9 KB, free: 17.2 GB)
15/09/15 18:50:19 INFO spark.SparkContext: Created broadcast 0 from textFile at MLUtils.scala:73
15/09/15 18:50:19 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
15/09/15 18:50:19 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 49753b4b5a029410c3bd91278c360c2241328387]
15/09/15 18:50:19 INFO mapred.FileInputFormat: Total input paths to process : 200
15/09/15 18:50:20 INFO spark.SparkContext: Starting job: reduce at MLUtils.scala:95
15/09/15 18:50:20 INFO scheduler.DAGScheduler: Got job 0 (reduce at MLUtils.scala:95) with 200 output partitions (allowLocal=false)
15/09/15 18:50:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 0(reduce at MLUtils.scala:95)
15/09/15 18:50:20 INFO scheduler.DAGScheduler: Parents of final stage: List()
15/09/15 18:50:20 INFO scheduler.DAGScheduler: Missing parents: List()
15/09/15 18:50:20 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at map at MLUtils.scala:93), which has no missing parents
15/09/15 18:50:20 INFO storage.MemoryStore: ensureFreeSpace(3872) called with curMem=196082, maxMem=18438322913
15/09/15 18:50:20 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 17.2 GB)
15/09/15 18:50:20 INFO storage.MemoryStore: ensureFreeSpace(2064) called with curMem=199954, maxMem=18438322913
15/09/15 18:50:20 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 17.2 GB)
15/09/15 18:50:20 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.47.1.38:18554 (size: 2.0 KB, free: 17.2 GB)
15/09/15 18:50:20 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874
15/09/15 18:50:20 INFO scheduler.DAGScheduler: Submitting 200 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at MLUtils.scala:93)
15/09/15 18:50:20 INFO cluster.YarnClusterScheduler: Adding task set 0.0 with 200 tasks
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000032 for on host hd274dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd274dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000014 for on host hd261dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd261dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000078 for on host hd277dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd277dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000162 for on host hd282dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd261dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd274dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd282dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd277dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000198 for on host hd255dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd282dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd255dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000020 for on host hd286dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd286dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd255dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000044 for on host hd260dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd286dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd260dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000204 for on host hd273dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd273dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd260dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000008 for on host hd291dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd291dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd273dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000062 for on host hd264dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd264dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd291dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000180 for on host hd272dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd264dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd272dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000174 for on host hd285dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd272dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd285dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000132 for on host hd258dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd258dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd285dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000186 for on host hd290dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd290dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd258dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000156 for on host hd263dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd263dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd290dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000084 for on host hd289dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd289dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd263dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000102 for on host hd276dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd276dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd289dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000068 for on host hd281dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd281dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd276dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000108 for on host hd254dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd281dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd254dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000114 for on host hd257dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd254dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd257dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000038 for on host hd288dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd257dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd288dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000002 for on host hd275dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd288dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd275dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000096 for on host hd280dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd275dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd280dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000210 for on host hd279dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd280dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd279dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000168 for on host hd293dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd279dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd293dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000050 for on host hd266dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd293dg.prod.mediav.com:8041
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd266dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000144 for on host hd271dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd271dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000192 for on host hd284dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd284dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000090 for on host hd262dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd262dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000138 for on host hd278dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd278dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000056 for on host hd292dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd292dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000150 for on host hd265dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd265dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000120 for on host hd270dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd270dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000126 for on host hd283dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd283dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000026 for on host hd256dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd256dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000074 for on host hd287dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd287dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000033 for on host hd274dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd274dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000015 for on host hd261dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd261dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000079 for on host hd277dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd277dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000163 for on host hd282dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd282dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000199 for on host hd255dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd255dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000021 for on host hd286dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd286dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000045 for on host hd260dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd260dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000205 for on host hd273dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd273dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000009 for on host hd291dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd291dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000063 for on host hd264dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd264dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000181 for on host hd272dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd272dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000175 for on host hd285dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd285dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000133 for on host hd258dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd258dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000187 for on host hd290dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd290dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000157 for on host hd263dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd263dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000085 for on host hd289dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd289dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000103 for on host hd276dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd276dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000069 for on host hd281dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd281dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000109 for on host hd254dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd254dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000115 for on host hd257dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd257dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000039 for on host hd288dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd288dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000003 for on host hd275dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd275dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000097 for on host hd280dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd280dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000211 for on host hd279dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd279dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000169 for on host hd293dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd293dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000051 for on host hd266dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd266dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000145 for on host hd271dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd271dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000193 for on host hd284dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd284dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000091 for on host hd262dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd262dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000139 for on host hd278dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd278dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000057 for on host hd292dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd292dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000151 for on host hd265dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd265dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000121 for on host hd270dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd270dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000127 for on host hd283dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd283dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000027 for on host hd256dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd256dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000075 for on host hd287dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd287dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000034 for on host hd274dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd274dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000016 for on host hd261dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd261dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000080 for on host hd277dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd277dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000164 for on host hd282dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd282dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000200 for on host hd255dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd255dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000022 for on host hd286dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd286dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000046 for on host hd260dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd260dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000206 for on host hd273dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd273dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000010 for on host hd291dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd291dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000064 for on host hd264dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd264dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000182 for on host hd272dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd272dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000176 for on host hd285dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd285dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000134 for on host hd258dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd258dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000188 for on host hd290dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd290dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000158 for on host hd263dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd263dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000086 for on host hd289dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd289dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000104 for on host hd276dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd276dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000070 for on host hd281dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd281dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000110 for on host hd254dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd254dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000116 for on host hd257dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd257dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000040 for on host hd288dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd288dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000004 for on host hd275dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd275dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000098 for on host hd280dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd280dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000212 for on host hd279dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd279dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000170 for on host hd293dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd293dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000052 for on host hd266dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd266dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000146 for on host hd271dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd271dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000194 for on host hd284dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd284dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000092 for on host hd262dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd262dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000140 for on host hd278dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd278dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000058 for on host hd292dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd292dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000152 for on host hd265dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd265dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000122 for on host hd270dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd270dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000128 for on host hd283dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd283dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000028 for on host hd256dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd256dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000076 for on host hd287dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd287dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000035 for on host hd274dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd274dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000017 for on host hd261dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd261dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000081 for on host hd277dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd277dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000165 for on host hd282dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd282dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000201 for on host hd255dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd255dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000023 for on host hd286dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd286dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000047 for on host hd260dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd260dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000207 for on host hd273dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd273dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000011 for on host hd291dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd291dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000065 for on host hd264dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd264dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000183 for on host hd272dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd272dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000177 for on host hd285dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd285dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000135 for on host hd258dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd258dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000189 for on host hd290dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd290dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000159 for on host hd263dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd263dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000087 for on host hd289dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd289dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000105 for on host hd276dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd276dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000071 for on host hd281dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd281dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000111 for on host hd254dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd254dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000117 for on host hd257dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd257dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000041 for on host hd288dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd288dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000005 for on host hd275dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd275dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000099 for on host hd280dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd280dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000213 for on host hd279dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd279dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000171 for on host hd293dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd293dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000053 for on host hd266dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd266dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000147 for on host hd271dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd271dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000195 for on host hd284dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd284dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000093 for on host hd262dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd262dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000141 for on host hd278dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd278dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000059 for on host hd292dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd292dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000153 for on host hd265dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd265dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000123 for on host hd270dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd270dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000129 for on host hd283dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd283dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000029 for on host hd256dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd256dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000036 for on host hd274dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd274dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000018 for on host hd261dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd261dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000082 for on host hd277dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd277dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000166 for on host hd282dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd282dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000202 for on host hd255dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd255dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000024 for on host hd286dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd286dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000048 for on host hd260dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd260dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000208 for on host hd273dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd273dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000012 for on host hd291dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd291dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000066 for on host hd264dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd264dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000184 for on host hd272dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd272dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000178 for on host hd285dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd285dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000136 for on host hd258dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd258dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000190 for on host hd290dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd290dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000160 for on host hd263dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd263dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000088 for on host hd289dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd289dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000106 for on host hd276dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd276dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000072 for on host hd281dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd281dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000112 for on host hd254dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd254dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000118 for on host hd257dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd257dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000042 for on host hd288dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd288dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000006 for on host hd275dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd275dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000100 for on host hd280dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd280dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000214 for on host hd279dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd279dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000172 for on host hd293dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd293dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000054 for on host hd266dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd266dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000148 for on host hd271dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd271dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000196 for on host hd284dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd284dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000094 for on host hd262dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd262dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000142 for on host hd278dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd278dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000060 for on host hd292dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd292dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000154 for on host hd265dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd265dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000124 for on host hd270dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd270dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000130 for on host hd283dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd283dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching container container_1442215126745_0068_01_000030 for on host hd256dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler,  executorHostname: hd256dg.prod.mediav.com
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd290dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000186/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd290dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000186/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd261dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000014/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd261dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000014/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd282dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000162/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd282dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000162/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd279dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000210/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd279dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000210/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd281dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000068/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd281dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000068/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 4, --hostname, hd282dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd260dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000044/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd260dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000044/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd258dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000132/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd258dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000132/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd272dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000180/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd272dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000180/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd275dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000002/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd275dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000002/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd277dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000078/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd277dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000078/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd291dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000008/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd291dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000008/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd274dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000032/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd274dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000032/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd257dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000114/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd257dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000114/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd288dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000038/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd288dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000038/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 20, --hostname, hd257dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 1, --hostname, hd274dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 9, --hostname, hd291dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 3, --hostname, hd277dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 22, --hostname, hd275dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 11, --hostname, hd272dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 13, --hostname, hd258dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 7, --hostname, hd260dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 18, --hostname, hd281dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 14, --hostname, hd290dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 2, --hostname, hd261dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 24, --hostname, hd279dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd293dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000168/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd293dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000168/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd255dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000198/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd255dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000198/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd289dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000084/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd289dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000084/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd263dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000156/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd263dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000156/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd286dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000020/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd286dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000020/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd264dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000062/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd264dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000062/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd280dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000096/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd280dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000096/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd285dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000174/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd285dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000174/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd276dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000102/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd276dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000102/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd273dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000204/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd273dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000204/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd254dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000108/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd254dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000108/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 8, --hostname, hd273dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 17, --hostname, hd276dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 12, --hostname, hd285dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 23, --hostname, hd280dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 10, --hostname, hd264dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 6, --hostname, hd286dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 15, --hostname, hd263dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 16, --hostname, hd289dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 5, --hostname, hd255dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 25, --hostname, hd293dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 21, --hostname, hd288dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:22 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 19, --hostname, hd254dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd271dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd274dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd283dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd261dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd270dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd287dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd256dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd278dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd265dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd262dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd282dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd255dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd286dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd284dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd260dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd272dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd264dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd292dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd273dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd291dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd277dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd266dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd285dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd258dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd286dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000021/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd286dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000021/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 42, --hostname, hd286dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd282dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000163/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd282dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000163/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 40, --hostname, hd282dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd261dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000015/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd261dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000015/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 38, --hostname, hd261dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd270dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000120/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd270dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000120/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 33, --hostname, hd270dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd271dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000144/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd271dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000144/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd272dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000181/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd272dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000181/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 27, --hostname, hd271dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 47, --hostname, hd272dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd262dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000090/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd262dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000090/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 29, --hostname, hd262dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd256dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000026/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd256dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000026/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd260dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000045/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd260dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000045/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd287dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000074/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd287dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000074/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd277dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000079/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd277dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000079/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd290dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 36, --hostname, hd287dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 43, --hostname, hd260dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 35, --hostname, hd256dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd265dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000150/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd265dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000150/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 39, --hostname, hd277dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd258dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000133/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd258dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000133/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 49, --hostname, hd258dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 32, --hostname, hd265dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd274dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000033/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd274dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000033/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd292dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000056/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd292dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000056/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 31, --hostname, hd292dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd291dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000009/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd291dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000009/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd284dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000192/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd284dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000192/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 45, --hostname, hd291dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd285dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000175/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd285dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000175/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 37, --hostname, hd274dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd255dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000199/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd255dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000199/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd283dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000126/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd283dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000126/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 41, --hostname, hd255dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd264dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000063/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd264dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000063/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd278dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000138/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd278dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000138/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 48, --hostname, hd285dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd266dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000050/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd266dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000050/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 28, --hostname, hd284dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 26, --hostname, hd266dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd273dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000205/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd273dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000205/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 30, --hostname, hd278dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 46, --hostname, hd264dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 34, --hostname, hd283dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 44, --hostname, hd273dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd263dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd289dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd276dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd281dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd275dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd288dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd293dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd279dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd254dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd280dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd290dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000187/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd290dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000187/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd266dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd257dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 50, --hostname, hd290dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd262dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd284dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd270dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd271dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd263dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000157/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd263dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000157/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd279dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000211/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd279dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000211/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd254dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000109/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd254dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000109/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd276dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000103/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd276dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000103/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd289dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000085/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd289dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000085/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd278dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd292dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd265dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd288dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000039/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd288dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000039/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 57, --hostname, hd288dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd262dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000091/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd262dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000091/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 52, --hostname, hd289dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd283dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 53, --hostname, hd276dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd280dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000097/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd280dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000097/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd281dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000069/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd281dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000069/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 55, --hostname, hd254dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 60, --hostname, hd279dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 51, --hostname, hd263dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd277dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 54, --hostname, hd281dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 59, --hostname, hd280dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd261dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 65, --hostname, hd262dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd284dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000193/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd284dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000193/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd287dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd256dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd274dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 64, --hostname, hd284dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd266dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000051/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd266dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000051/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 62, --hostname, hd266dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd255dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd257dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000115/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd257dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000115/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd270dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000121/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd270dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000121/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 69, --hostname, hd270dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd260dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd282dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 56, --hostname, hd257dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd286dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd273dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd293dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000169/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd293dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000169/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 61, --hostname, hd293dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd261dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000016/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd261dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000016/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd275dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000003/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd275dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000003/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 58, --hostname, hd275dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 74, --hostname, hd261dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd271dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000145/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd271dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000145/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 63, --hostname, hd271dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd265dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000151/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd265dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000151/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd292dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000057/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd292dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000057/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd278dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000139/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd278dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000139/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 66, --hostname, hd278dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd287dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000075/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd287dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000075/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 72, --hostname, hd287dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd277dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000080/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd277dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000080/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 68, --hostname, hd265dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd283dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000127/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd283dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000127/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd291dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 75, --hostname, hd277dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd274dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000034/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd274dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000034/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd256dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000027/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd256dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000027/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 67, --hostname, hd292dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 71, --hostname, hd256dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 73, --hostname, hd274dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 70, --hostname, hd283dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd286dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000022/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd286dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000022/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 78, --hostname, hd286dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd273dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000206/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd273dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000206/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd260dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000046/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd260dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000046/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 80, --hostname, hd273dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 79, --hostname, hd260dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd282dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000164/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd282dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000164/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd255dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000200/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd255dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000200/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 76, --hostname, hd282dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 77, --hostname, hd255dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd291dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000010/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd291dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000010/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 81, --hostname, hd291dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd264dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd285dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd272dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd258dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd290dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd289dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd263dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd276dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd281dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd254dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd257dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd275dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd288dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd280dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd264dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000064/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd264dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000064/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 82, --hostname, hd264dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd266dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd279dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd271dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd285dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000176/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd285dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000176/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd290dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000188/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd290dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000188/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd254dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000110/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd254dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000110/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd272dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000182/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd272dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000182/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 83, --hostname, hd272dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd293dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd258dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000134/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd258dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000134/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd289dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000086/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd289dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000086/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 91, --hostname, hd254dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd281dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000070/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd281dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000070/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 90, --hostname, hd281dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd276dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000104/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd276dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000104/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 86, --hostname, hd290dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd284dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd262dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd275dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000004/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd275dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000004/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 84, --hostname, hd285dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 94, --hostname, hd275dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd278dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd288dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000040/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd288dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000040/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd263dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000158/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd263dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000158/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 89, --hostname, hd276dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 85, --hostname, hd258dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 88, --hostname, hd289dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd271dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000146/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd271dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000146/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 87, --hostname, hd263dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 93, --hostname, hd288dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd279dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000212/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd279dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000212/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd257dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000116/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd257dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000116/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd284dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000194/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd284dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000194/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 96, --hostname, hd279dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 99, --hostname, hd271dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 100, --hostname, hd284dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 92, --hostname, hd257dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd262dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000092/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd262dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000092/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 101, --hostname, hd262dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd280dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000098/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd280dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000098/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 95, --hostname, hd280dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd266dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000052/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd266dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000052/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 98, --hostname, hd266dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd292dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd265dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd270dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd278dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000140/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd278dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000140/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 102, --hostname, hd278dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd283dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd293dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000170/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd293dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000170/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 97, --hostname, hd293dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd256dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd292dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000058/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd292dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000058/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd270dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000122/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd270dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000122/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 105, --hostname, hd270dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 103, --hostname, hd292dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd265dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000152/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd265dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000152/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 104, --hostname, hd265dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd283dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000128/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd283dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000128/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 106, --hostname, hd283dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd256dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000028/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd256dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000028/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 107, --hostname, hd256dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd287dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd274dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd261dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd277dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd282dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd255dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd286dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd260dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd261dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000017/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd261dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000017/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 110, --hostname, hd261dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd287dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000076/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd287dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000076/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 108, --hostname, hd287dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd277dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000081/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd277dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000081/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd274dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000035/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd274dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000035/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd282dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000165/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd282dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000165/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 111, --hostname, hd277dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 112, --hostname, hd282dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 109, --hostname, hd274dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd255dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000201/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd255dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000201/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 113, --hostname, hd255dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd273dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd260dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000047/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd260dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000047/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 115, --hostname, hd260dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd286dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000023/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd286dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000023/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 114, --hostname, hd286dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd291dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd273dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000207/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd273dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000207/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 116, --hostname, hd273dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd291dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000011/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd291dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000011/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 117, --hostname, hd291dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd264dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd272dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd285dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd258dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd290dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd263dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd289dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd276dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd264dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000065/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd264dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000065/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd272dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000183/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd272dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000183/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 118, --hostname, hd264dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd285dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000177/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd285dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000177/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 120, --hostname, hd285dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 119, --hostname, hd272dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd281dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd254dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd257dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd289dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000087/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd289dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000087/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 124, --hostname, hd289dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd263dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000159/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd263dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000159/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd290dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000189/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd290dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000189/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd258dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000135/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd258dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000135/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 122, --hostname, hd290dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 123, --hostname, hd263dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 121, --hostname, hd258dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd276dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000105/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd276dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000105/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 125, --hostname, hd276dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd288dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd275dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd257dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000117/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd257dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000117/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 128, --hostname, hd257dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd254dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000111/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd254dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000111/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 127, --hostname, hd254dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd281dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000071/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd281dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000071/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 126, --hostname, hd281dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd280dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd279dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd293dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd288dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000041/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd288dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000041/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd266dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 129, --hostname, hd288dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd275dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000005/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd275dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000005/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd271dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 130, --hostname, hd275dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd278dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd265dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd292dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd262dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd270dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd280dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000099/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd280dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000099/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 131, --hostname, hd280dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd266dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000053/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd266dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000053/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd271dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000147/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd271dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000147/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd270dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000123/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd270dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000123/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 141, --hostname, hd270dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd283dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd293dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000171/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd293dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000171/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 133, --hostname, hd293dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd292dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000059/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd292dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000059/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd284dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd265dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000153/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd265dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000153/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 139, --hostname, hd292dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 135, --hostname, hd271dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 134, --hostname, hd266dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 140, --hostname, hd265dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd279dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000213/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd279dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000213/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 132, --hostname, hd279dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd283dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000129/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd283dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000129/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 142, --hostname, hd283dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd262dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000093/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd262dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000093/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 137, --hostname, hd262dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd284dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000195/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd284dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000195/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 136, --hostname, hd284dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd278dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000141/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd278dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000141/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 138, --hostname, hd278dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd256dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd274dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd261dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd286dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd255dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd282dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd277dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd273dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd260dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd291dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd256dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000029/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd256dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000029/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 143, --hostname, hd256dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd286dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000024/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd286dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000024/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd274dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000036/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd274dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000036/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd261dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000018/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd261dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000018/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 145, --hostname, hd261dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd255dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000202/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd255dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000202/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 144, --hostname, hd274dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd272dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 149, --hostname, hd286dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 148, --hostname, hd255dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd264dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd285dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd258dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd260dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000048/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd260dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000048/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 150, --hostname, hd260dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd273dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000208/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd273dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000208/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd291dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000012/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd291dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000012/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 151, --hostname, hd273dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd282dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000166/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd282dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000166/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd277dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000082/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd277dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000082/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 147, --hostname, hd282dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 152, --hostname, hd291dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 146, --hostname, hd277dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd272dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000184/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd272dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000184/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 154, --hostname, hd272dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd285dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000178/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd285dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000178/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 155, --hostname, hd285dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd264dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000066/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd264dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000066/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 153, --hostname, hd264dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd290dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd258dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000136/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd258dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000136/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 156, --hostname, hd258dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd263dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd289dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd276dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd281dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd254dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd290dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000190/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd290dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000190/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd257dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 157, --hostname, hd290dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd288dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd275dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd280dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd279dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd263dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000160/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd263dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000160/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 158, --hostname, hd263dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd293dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd281dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000072/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd281dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000072/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 161, --hostname, hd281dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd266dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd289dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000088/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd289dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000088/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd257dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000118/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd257dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000118/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd288dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000042/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd288dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000042/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 163, --hostname, hd257dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 164, --hostname, hd288dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd276dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000106/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd276dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000106/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd254dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000112/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd254dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000112/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 159, --hostname, hd289dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 162, --hostname, hd254dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 160, --hostname, hd276dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd280dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000100/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd280dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000100/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd279dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000214/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd279dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000214/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 166, --hostname, hd280dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd275dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000006/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd275dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000006/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd293dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000172/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd293dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000172/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 167, --hostname, hd279dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 168, --hostname, hd293dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 165, --hostname, hd275dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd271dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd284dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd262dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd278dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd266dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000054/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd266dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000054/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 169, --hostname, hd266dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd292dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd284dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000196/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd284dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000196/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 171, --hostname, hd284dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd271dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000148/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd271dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000148/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 170, --hostname, hd271dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd265dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd278dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000142/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd278dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000142/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 173, --hostname, hd278dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd262dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000094/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd262dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000094/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 172, --hostname, hd262dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd292dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000060/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd292dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000060/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 174, --hostname, hd292dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd270dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd283dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd265dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000154/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd265dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000154/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 175, --hostname, hd265dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Starting Executor Container
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Connecting to ContainerManager at hd256dg.prod.mediav.com:8041
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Preparing Local resources
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd283dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000130/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd283dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000130/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__app__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar"", }, size: 453950, timestamp: 1442314210903, type: FILE, visibility: PRIVATE, , __spark__.jar -> resource {, scheme: ""hdfs"", host: ""prod-hadoop"", port: -1, file: ""/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar"", }, size: 129464792, timestamp: 1437055796697, type: FILE, visibility: PUBLIC, )
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd270dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000124/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd270dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000124/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 177, --hostname, hd283dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 176, --hostname, hd270dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with environment: Map(CLASSPATH -> $PWD:$PWD/__spark__.jar:$HADOOP_CONF_DIR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:$HADOOP_MAPRED_HOME/*:$HADOOP_MAPRED_HOME/lib/*:$YARN_HOME/*:$YARN_HOME/lib/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*, SPARK_LOG_URL_STDERR -> http://hd256dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000030/wuwm/stderr?start=0, SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1442215126745_0068, SPARK_YARN_CACHE_FILES_FILE_SIZES -> 129464792,453950, SPARK_USER -> wuwm, SPARK_YARN_CACHE_FILES_VISIBILITIES -> PUBLIC,PRIVATE, SPARK_YARN_MODE -> true, SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1437055796697,1442314210903, SPARK_LOG_URL_STDOUT -> http://hd256dg.prod.mediav.com:8042/node/containerlogs/container_1442215126745_0068_01_000030/wuwm/stdout?start=0, SPARK_YARN_CACHE_FILES -> hdfs://prod-hadoop/data/spark/lib/spark-assembly-1.4.1-hadoop2.0.0-cdh4.6.0.jar#__spark__.jar,hdfs://prod-hadoop/user/wuwm/.sparkStaging/application_1442215126745_0068/gbdt-training.jar#__app__.jar)
15/09/15 18:50:23 INFO yarn.ExecutorRunnable: Setting up executor with commands: List(LD_LIBRARY_PATH=""/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH"", $JAVA_HOME/bin/java, -server, -XX:OnOutOfMemoryError='kill %p', -Xms16384m, -Xmx16384m, '-Xmn1G', '-XX:+UseG1GC', -Djava.io.tmpdir=$PWD/tmp, '-Dspark.akka.threads=4', '-Dspark.driver.port=29651', '-Dspark.akka.frameSize=100', '-Dspark.ui.port=0', -Dspark.yarn.app.container.log.dir=<LOG_DIR>, org.apache.spark.executor.CoarseGrainedExecutorBackend, --driver-url, akka.tcp://sparkDriver@10.47.1.38:29651/user/CoarseGrainedScheduler, --executor-id, 178, --hostname, hd256dg.prod.mediav.com, --cores, 1, --app-id, application_1442215126745_0068, --user-class-path, file:$PWD/__app__.jar, 1>, <LOG_DIR>/stdout, 2>, <LOG_DIR>/stderr)
15/09/15 18:50:23 INFO scheduler.FairSchedulableBuilder: Added task set TaskSet_0 tasks to pool default
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd262dg.prod.mediav.com:3830
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd262dg.prod.mediav.com:34647
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd262dg.prod.mediav.com:48884
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd287dg.prod.mediav.com:23635
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd262dg.prod.mediav.com:22149
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd287dg.prod.mediav.com:28432
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd287dg.prod.mediav.com:55051
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd262dg.prod.mediav.com:11335
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd274dg.prod.mediav.com:30615
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd282dg.prod.mediav.com:5903
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd280dg.prod.mediav.com:38789
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd260dg.prod.mediav.com:37968
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd283dg.prod.mediav.com:30374
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd285dg.prod.mediav.com:47533
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd266dg.prod.mediav.com:55432
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd281dg.prod.mediav.com:62732
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd276dg.prod.mediav.com:43506
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd289dg.prod.mediav.com:12002
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd291dg.prod.mediav.com:32064
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd254dg.prod.mediav.com:36047
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd254dg.prod.mediav.com:5455
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd293dg.prod.mediav.com:60875
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd263dg.prod.mediav.com:2024
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd280dg.prod.mediav.com:54638
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd291dg.prod.mediav.com:39476
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd254dg.prod.mediav.com:14354
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd263dg.prod.mediav.com:5166
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd276dg.prod.mediav.com:40605
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd274dg.prod.mediav.com:11509
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd257dg.prod.mediav.com:35637
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd275dg.prod.mediav.com:2951
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd291dg.prod.mediav.com:49805
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd271dg.prod.mediav.com:51820
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd263dg.prod.mediav.com:23530
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd257dg.prod.mediav.com:57061
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd289dg.prod.mediav.com:32369
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd285dg.prod.mediav.com:64379
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd286dg.prod.mediav.com:14816
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd260dg.prod.mediav.com:32507
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd290dg.prod.mediav.com:31700
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd266dg.prod.mediav.com:16527
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd274dg.prod.mediav.com:12494
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd270dg.prod.mediav.com:64118
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd261dg.prod.mediav.com:46185
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd256dg.prod.mediav.com:9450
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd282dg.prod.mediav.com:37937
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd281dg.prod.mediav.com:10572
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd286dg.prod.mediav.com:40534
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd281dg.prod.mediav.com:25805
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd272dg.prod.mediav.com:51683
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd290dg.prod.mediav.com:21427
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd288dg.prod.mediav.com:4310
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd276dg.prod.mediav.com:6017
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd288dg.prod.mediav.com:17573
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd291dg.prod.mediav.com:29129
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd264dg.prod.mediav.com:12486
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd266dg.prod.mediav.com:19604
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd260dg.prod.mediav.com:60231
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd290dg.prod.mediav.com:6912
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd280dg.prod.mediav.com:40712
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd271dg.prod.mediav.com:53888
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd291dg.prod.mediav.com:20478
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd263dg.prod.mediav.com:61076
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd272dg.prod.mediav.com:46483
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd286dg.prod.mediav.com:57085
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd293dg.prod.mediav.com:11460
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd257dg.prod.mediav.com:30094
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd277dg.prod.mediav.com:5858
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd277dg.prod.mediav.com:60897
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd288dg.prod.mediav.com:32710
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd286dg.prod.mediav.com:12029
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd285dg.prod.mediav.com:22098
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd284dg.prod.mediav.com:3035
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd293dg.prod.mediav.com:19872
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd281dg.prod.mediav.com:12851
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd285dg.prod.mediav.com:25660
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd293dg.prod.mediav.com:16205
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd257dg.prod.mediav.com:47823
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd275dg.prod.mediav.com:35975
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd288dg.prod.mediav.com:52666
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd284dg.prod.mediav.com:1972
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd271dg.prod.mediav.com:12931
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd257dg.prod.mediav.com:2562
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd293dg.prod.mediav.com:25481
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd278dg.prod.mediav.com:57571
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd289dg.prod.mediav.com:61642
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd278dg.prod.mediav.com:50569
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd280dg.prod.mediav.com:37371
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd261dg.prod.mediav.com:21889
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd274dg.prod.mediav.com:42061
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd260dg.prod.mediav.com:16173
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd270dg.prod.mediav.com:24041
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd261dg.prod.mediav.com:56035
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd288dg.prod.mediav.com:49791
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd286dg.prod.mediav.com:2049
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd282dg.prod.mediav.com:30963
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd292dg.prod.mediav.com:31151
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd254dg.prod.mediav.com:30831
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd292dg.prod.mediav.com:54990
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd285dg.prod.mediav.com:63413
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd284dg.prod.mediav.com:4464
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd282dg.prod.mediav.com:29682
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd274dg.prod.mediav.com:42220
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd276dg.prod.mediav.com:27249
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd263dg.prod.mediav.com:30436
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd256dg.prod.mediav.com:30685
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd280dg.prod.mediav.com:5202
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd292dg.prod.mediav.com:2998
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd270dg.prod.mediav.com:45694
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd272dg.prod.mediav.com:11955
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd278dg.prod.mediav.com:56400
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd283dg.prod.mediav.com:44832
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd260dg.prod.mediav.com:20191
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd270dg.prod.mediav.com:10424
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd283dg.prod.mediav.com:33605
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd272dg.prod.mediav.com:25956
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd278dg.prod.mediav.com:63429
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd292dg.prod.mediav.com:30225
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd290dg.prod.mediav.com:28198
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd271dg.prod.mediav.com:12279
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd281dg.prod.mediav.com:4984
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd261dg.prod.mediav.com:12084
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd278dg.prod.mediav.com:25951
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd284dg.prod.mediav.com:27309
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd266dg.prod.mediav.com:31919
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd282dg.prod.mediav.com:28707
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd277dg.prod.mediav.com:56279
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd264dg.prod.mediav.com:59144
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd255dg.prod.mediav.com:28208
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd289dg.prod.mediav.com:38334
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd255dg.prod.mediav.com:40821
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd264dg.prod.mediav.com:64319
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd264dg.prod.mediav.com:29625
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd284dg.prod.mediav.com:12548
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd273dg.prod.mediav.com:49607
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd283dg.prod.mediav.com:49114
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd256dg.prod.mediav.com:13206
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd261dg.prod.mediav.com:51965
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd276dg.prod.mediav.com:44290
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd273dg.prod.mediav.com:35956
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd272dg.prod.mediav.com:37308
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd277dg.prod.mediav.com:63909
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd254dg.prod.mediav.com:23106
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd256dg.prod.mediav.com:22458
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd271dg.prod.mediav.com:33696
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd275dg.prod.mediav.com:16058
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd264dg.prod.mediav.com:31093
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd273dg.prod.mediav.com:1051
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd275dg.prod.mediav.com:43976
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd270dg.prod.mediav.com:59108
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd266dg.prod.mediav.com:34492
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd292dg.prod.mediav.com:28918
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd256dg.prod.mediav.com:10140
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd289dg.prod.mediav.com:60813
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd283dg.prod.mediav.com:38130
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd290dg.prod.mediav.com:60980
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd255dg.prod.mediav.com:38603
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd273dg.prod.mediav.com:3269
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd277dg.prod.mediav.com:4052
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd255dg.prod.mediav.com:18667
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd275dg.prod.mediav.com:49526
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd273dg.prod.mediav.com:54387
15/09/15 18:50:26 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd262dg.prod.mediav.com:44599/user/Executor#-286579772]) with ID 65
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd265dg.prod.mediav.com:33962
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd279dg.prod.mediav.com:47156
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd279dg.prod.mediav.com:35543
15/09/15 18:50:26 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 0.0 (TID 0, hd262dg.prod.mediav.com, RACK_LOCAL, 1425 bytes)
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd265dg.prod.mediav.com:59562
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd265dg.prod.mediav.com:37224
15/09/15 18:50:26 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd262dg.prod.mediav.com:46024/user/Executor#305996166]) with ID 101
15/09/15 18:50:26 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 0.0 (TID 1, hd262dg.prod.mediav.com, RACK_LOCAL, 1425 bytes)
15/09/15 18:50:26 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd262dg.prod.mediav.com:9343/user/Executor#-1261226495]) with ID 172
15/09/15 18:50:26 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 0.0 (TID 2, hd262dg.prod.mediav.com, RACK_LOCAL, 1425 bytes)
15/09/15 18:50:26 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd262dg.prod.mediav.com:34712/user/Executor#-240213204]) with ID 137
15/09/15 18:50:26 INFO scheduler.TaskSetManager: Starting task 112.0 in stage 0.0 (TID 3, hd262dg.prod.mediav.com, RACK_LOCAL, 1425 bytes)
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd258dg.prod.mediav.com:42541
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd258dg.prod.mediav.com:48564
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd279dg.prod.mediav.com:64784
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd279dg.prod.mediav.com:51620
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd279dg.prod.mediav.com:2891
15/09/15 18:50:26 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd265dg.prod.mediav.com:39926
15/09/15 18:50:27 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd258dg.prod.mediav.com:37599
15/09/15 18:50:27 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd265dg.prod.mediav.com:22169
15/09/15 18:50:27 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd258dg.prod.mediav.com:58582
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd262dg.prod.mediav.com:26036/user/Executor#-631528468]) with ID 29
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 129.0 in stage 0.0 (TID 4, hd262dg.prod.mediav.com, RACK_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd258dg.prod.mediav.com:30052
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd291dg.prod.mediav.com:42873/user/Executor#1927004066]) with ID 45
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 0.0 (TID 5, hd291dg.prod.mediav.com, RACK_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd287dg.prod.mediav.com:31412/user/Executor#1843642847]) with ID 72
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 60.0 in stage 0.0 (TID 6, hd287dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd281dg.prod.mediav.com:9842/user/Executor#1438294067]) with ID 54
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 137.0 in stage 0.0 (TID 7, hd281dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd287dg.prod.mediav.com:26691/user/Executor#2035640422]) with ID 108
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 104.0 in stage 0.0 (TID 8, hd287dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd282dg.prod.mediav.com:33258/user/Executor#-860497541]) with ID 76
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd280dg.prod.mediav.com:7297/user/Executor#-1826500061]) with ID 23
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 97.0 in stage 0.0 (TID 9, hd280dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd262dg.prod.mediav.com:35030 with 8.6 GB RAM, BlockManagerId(65, hd262dg.prod.mediav.com, 35030)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd287dg.prod.mediav.com:34921/user/Executor#-1444132110]) with ID 36
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd276dg.prod.mediav.com:53875/user/Executor#1844843451]) with ID 53
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 98.0 in stage 0.0 (TID 10, hd276dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd266dg.prod.mediav.com:61595/user/Executor#-1260960556]) with ID 62
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd262dg.prod.mediav.com:49994 with 8.6 GB RAM, BlockManagerId(101, hd262dg.prod.mediav.com, 49994)
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 139.0 in stage 0.0 (TID 11, hd266dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd260dg.prod.mediav.com:54790/user/Executor#-1525659401]) with ID 115
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd262dg.prod.mediav.com:40567 with 8.6 GB RAM, BlockManagerId(137, hd262dg.prod.mediav.com, 40567)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd285dg.prod.mediav.com:24488/user/Executor#681770952]) with ID 48
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd274dg.prod.mediav.com:5264/user/Executor#850594699]) with ID 1
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd283dg.prod.mediav.com:52786/user/Executor#375260350]) with ID 70
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd254dg.prod.mediav.com:39685/user/Executor#836663559]) with ID 55
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd289dg.prod.mediav.com:3993/user/Executor#-948255953]) with ID 52
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 115.0 in stage 0.0 (TID 12, hd289dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd254dg.prod.mediav.com:10583/user/Executor#-98105974]) with ID 91
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd287dg.prod.mediav.com:53041 with 8.6 GB RAM, BlockManagerId(108, hd287dg.prod.mediav.com, 53041)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd281dg.prod.mediav.com:2395 with 8.6 GB RAM, BlockManagerId(54, hd281dg.prod.mediav.com, 2395)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd280dg.prod.mediav.com:27479/user/Executor#806593419]) with ID 59
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd291dg.prod.mediav.com:31861 with 8.6 GB RAM, BlockManagerId(45, hd291dg.prod.mediav.com, 31861)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd287dg.prod.mediav.com:2547 with 8.6 GB RAM, BlockManagerId(36, hd287dg.prod.mediav.com, 2547)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd280dg.prod.mediav.com:1897 with 8.6 GB RAM, BlockManagerId(23, hd280dg.prod.mediav.com, 1897)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd276dg.prod.mediav.com:15025 with 8.6 GB RAM, BlockManagerId(53, hd276dg.prod.mediav.com, 15025)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd263dg.prod.mediav.com:36001/user/Executor#2015724387]) with ID 51
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd275dg.prod.mediav.com:59448/user/Executor#-743696016]) with ID 58
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd291dg.prod.mediav.com:14296/user/Executor#-322939320]) with ID 9
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd282dg.prod.mediav.com:47145 with 8.6 GB RAM, BlockManagerId(76, hd282dg.prod.mediav.com, 47145)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd293dg.prod.mediav.com:29486/user/Executor#355235889]) with ID 133
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd260dg.prod.mediav.com:20879 with 8.6 GB RAM, BlockManagerId(115, hd260dg.prod.mediav.com, 20879)
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 80.0 in stage 0.0 (TID 13, hd293dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd281dg.prod.mediav.com:19222/user/Executor#1443010090]) with ID 18
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd255dg.prod.mediav.com:53875/user/Executor#1153505479]) with ID 148
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd266dg.prod.mediav.com:60772 with 8.6 GB RAM, BlockManagerId(62, hd266dg.prod.mediav.com, 60772)
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 161.0 in stage 0.0 (TID 14, hd255dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd254dg.prod.mediav.com:14758/user/Executor#1639815949]) with ID 19
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd263dg.prod.mediav.com:16348/user/Executor#-819445026]) with ID 87
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd277dg.prod.mediav.com:59038/user/Executor#-481874934]) with ID 39
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd257dg.prod.mediav.com:48419/user/Executor#1277173238]) with ID 163
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd285dg.prod.mediav.com:15847/user/Executor#-759973001]) with ID 84
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd285dg.prod.mediav.com:7398 with 8.6 GB RAM, BlockManagerId(48, hd285dg.prod.mediav.com, 7398)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd274dg.prod.mediav.com:51526/user/Executor#2096899220]) with ID 37
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd291dg.prod.mediav.com:54651/user/Executor#-820252343]) with ID 152
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd290dg.prod.mediav.com:30012/user/Executor#-985909148]) with ID 157
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 102.0 in stage 0.0 (TID 15, hd290dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd266dg.prod.mediav.com:61823/user/Executor#-76787639]) with ID 98
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd272dg.prod.mediav.com:56601/user/Executor#-1282343332]) with ID 83
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd278dg.prod.mediav.com:57953/user/Executor#841437709]) with ID 66
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd256dg.prod.mediav.com:33466/user/Executor#1420351428]) with ID 71
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd255dg.prod.mediav.com:60243/user/Executor#-821655411]) with ID 41
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd261dg.prod.mediav.com:44258/user/Executor#-995187360]) with ID 110
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd254dg.prod.mediav.com:6401 with 8.6 GB RAM, BlockManagerId(55, hd254dg.prod.mediav.com, 6401)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd272dg.prod.mediav.com:7533/user/Executor#1797351652]) with ID 154
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd282dg.prod.mediav.com:48893/user/Executor#-193254225]) with ID 4
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd283dg.prod.mediav.com:19668 with 8.6 GB RAM, BlockManagerId(70, hd283dg.prod.mediav.com, 19668)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd256dg.prod.mediav.com:52901/user/Executor#-188708289]) with ID 143
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd274dg.prod.mediav.com:16654 with 8.6 GB RAM, BlockManagerId(1, hd274dg.prod.mediav.com, 16654)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd263dg.prod.mediav.com:1269/user/Executor#1162424859]) with ID 15
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd260dg.prod.mediav.com:35472/user/Executor#-1190050621]) with ID 150
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd271dg.prod.mediav.com:41012/user/Executor#1558950039]) with ID 135
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd270dg.prod.mediav.com:59921/user/Executor#1245200195]) with ID 176
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd288dg.prod.mediav.com:21820/user/Executor#1876535355]) with ID 129
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd289dg.prod.mediav.com:20447 with 8.6 GB RAM, BlockManagerId(52, hd289dg.prod.mediav.com, 20447)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd292dg.prod.mediav.com:10933/user/Executor#-1656233336]) with ID 174
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd255dg.prod.mediav.com:39363/user/Executor#-1938044900]) with ID 113
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd263dg.prod.mediav.com:41078/user/Executor#1676182649]) with ID 123
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd288dg.prod.mediav.com:41802/user/Executor#-243366962]) with ID 164
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd292dg.prod.mediav.com:38034/user/Executor#1250459028]) with ID 103
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd290dg.prod.mediav.com:36613/user/Executor#-1061372242]) with ID 50
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 133.0 in stage 0.0 (TID 16, hd290dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd275dg.prod.mediav.com:33978/user/Executor#1408606544]) with ID 94
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd264dg.prod.mediav.com:34852/user/Executor#884086869]) with ID 153
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd271dg.prod.mediav.com:12188/user/Executor#-1543984782]) with ID 27
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd293dg.prod.mediav.com:19339/user/Executor#-1057603292]) with ID 168
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd263dg.prod.mediav.com:58533/user/Executor#1477691403]) with ID 158
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd261dg.prod.mediav.com:63793/user/Executor#776697979]) with ID 145
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd289dg.prod.mediav.com:24623/user/Executor#-1241605049]) with ID 124
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd254dg.prod.mediav.com:14962 with 8.6 GB RAM, BlockManagerId(91, hd254dg.prod.mediav.com, 14962)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd286dg.prod.mediav.com:34890/user/Executor#577108799]) with ID 78
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd283dg.prod.mediav.com:55766/user/Executor#-2042900033]) with ID 177
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd271dg.prod.mediav.com:61387/user/Executor#492981340]) with ID 63
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd286dg.prod.mediav.com:49420/user/Executor#-448174640]) with ID 114
15/09/15 18:50:27 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hd255dg.prod.mediav.com:32501
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd277dg.prod.mediav.com:62893/user/Executor#1381264528]) with ID 75
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd288dg.prod.mediav.com:19666/user/Executor#1512148061]) with ID 93
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd276dg.prod.mediav.com:40655/user/Executor#1011208267]) with ID 17
15/09/15 18:50:27 INFO scheduler.TaskSetManager: Starting task 169.0 in stage 0.0 (TID 17, hd276dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd256dg.prod.mediav.com:7308/user/Executor#-1108086900]) with ID 35
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd277dg.prod.mediav.com:64994/user/Executor#1615019822]) with ID 146
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd283dg.prod.mediav.com:34645/user/Executor#35388606]) with ID 142
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd290dg.prod.mediav.com:9588/user/Executor#-1202363980]) with ID 122
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd280dg.prod.mediav.com:6093 with 8.6 GB RAM, BlockManagerId(59, hd280dg.prod.mediav.com, 6093)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd276dg.prod.mediav.com:64837/user/Executor#-1955977938]) with ID 125
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd291dg.prod.mediav.com:37581/user/Executor#-846661087]) with ID 117
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd261dg.prod.mediav.com:62093/user/Executor#-228862337]) with ID 2
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd262dg.prod.mediav.com:40567 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd262dg.prod.mediav.com:35030 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd262dg.prod.mediav.com:49994 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd282dg.prod.mediav.com:3334/user/Executor#1355572708]) with ID 112
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd271dg.prod.mediav.com:59456/user/Executor#-826749893]) with ID 99
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd257dg.prod.mediav.com:45022/user/Executor#1279638718]) with ID 20
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd255dg.prod.mediav.com:13770/user/Executor#1591652082]) with ID 5
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd288dg.prod.mediav.com:35894/user/Executor#434023115]) with ID 21
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd263dg.prod.mediav.com:57200 with 8.6 GB RAM, BlockManagerId(51, hd263dg.prod.mediav.com, 57200)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd275dg.prod.mediav.com:33761/user/Executor#1974679179]) with ID 130
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd261dg.prod.mediav.com:48615/user/Executor#1551458319]) with ID 74
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd271dg.prod.mediav.com:5327/user/Executor#-1603374292]) with ID 170
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd293dg.prod.mediav.com:25809 with 8.6 GB RAM, BlockManagerId(133, hd293dg.prod.mediav.com, 25809)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd275dg.prod.mediav.com:7744 with 8.6 GB RAM, BlockManagerId(58, hd275dg.prod.mediav.com, 7744)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd292dg.prod.mediav.com:59726/user/Executor#1169598787]) with ID 139
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd264dg.prod.mediav.com:44245/user/Executor#-2106606855]) with ID 118
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd287dg.prod.mediav.com:2270 with 8.6 GB RAM, BlockManagerId(72, hd287dg.prod.mediav.com, 2270)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd284dg.prod.mediav.com:48439/user/Executor#-1254065602]) with ID 136
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd293dg.prod.mediav.com:11946/user/Executor#37098678]) with ID 97
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd289dg.prod.mediav.com:43879/user/Executor#1063313186]) with ID 16
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd260dg.prod.mediav.com:12398/user/Executor#18063718]) with ID 7
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd257dg.prod.mediav.com:6403 with 8.6 GB RAM, BlockManagerId(163, hd257dg.prod.mediav.com, 6403)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd270dg.prod.mediav.com:21056/user/Executor#-223070665]) with ID 105
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd257dg.prod.mediav.com:13474/user/Executor#55288937]) with ID 128
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd291dg.prod.mediav.com:15183/user/Executor#959662111]) with ID 81
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd290dg.prod.mediav.com:48698 with 8.6 GB RAM, BlockManagerId(157, hd290dg.prod.mediav.com, 48698)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd285dg.prod.mediav.com:20373/user/Executor#-1912327117]) with ID 120
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd257dg.prod.mediav.com:41407/user/Executor#1413018663]) with ID 56
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd274dg.prod.mediav.com:58299 with 8.6 GB RAM, BlockManagerId(37, hd274dg.prod.mediav.com, 58299)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd280dg.prod.mediav.com:19459/user/Executor#635192475]) with ID 166
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd270dg.prod.mediav.com:58654/user/Executor#-83694677]) with ID 69
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd274dg.prod.mediav.com:42844/user/Executor#1398346558]) with ID 144
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd276dg.prod.mediav.com:44102/user/Executor#-1483544095]) with ID 160
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd260dg.prod.mediav.com:39810/user/Executor#775515676]) with ID 43
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd280dg.prod.mediav.com:21984/user/Executor#-658808002]) with ID 95
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd281dg.prod.mediav.com:24432 with 8.6 GB RAM, BlockManagerId(18, hd281dg.prod.mediav.com, 24432)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd284dg.prod.mediav.com:57715/user/Executor#-545500019]) with ID 100
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd286dg.prod.mediav.com:34161/user/Executor#-110331883]) with ID 149
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd291dg.prod.mediav.com:9728 with 8.6 GB RAM, BlockManagerId(152, hd291dg.prod.mediav.com, 9728)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd286dg.prod.mediav.com:42269/user/Executor#1224563203]) with ID 42
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd263dg.prod.mediav.com:19616 with 8.6 GB RAM, BlockManagerId(87, hd263dg.prod.mediav.com, 19616)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd285dg.prod.mediav.com:65268/user/Executor#1498262927]) with ID 12
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd281dg.prod.mediav.com:17354/user/Executor#-946460307]) with ID 126
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd255dg.prod.mediav.com:62882 with 8.6 GB RAM, BlockManagerId(148, hd255dg.prod.mediav.com, 62882)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd278dg.prod.mediav.com:16567 with 8.6 GB RAM, BlockManagerId(66, hd278dg.prod.mediav.com, 16567)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd283dg.prod.mediav.com:60344/user/Executor#854187994]) with ID 106
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd291dg.prod.mediav.com:47717 with 8.6 GB RAM, BlockManagerId(9, hd291dg.prod.mediav.com, 47717)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd254dg.prod.mediav.com:40888/user/Executor#-1008311359]) with ID 127
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd285dg.prod.mediav.com:15892 with 8.6 GB RAM, BlockManagerId(84, hd285dg.prod.mediav.com, 15892)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd277dg.prod.mediav.com:60781 with 8.6 GB RAM, BlockManagerId(39, hd277dg.prod.mediav.com, 60781)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd277dg.prod.mediav.com:50809/user/Executor#415495995]) with ID 3
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd266dg.prod.mediav.com:27698 with 8.6 GB RAM, BlockManagerId(98, hd266dg.prod.mediav.com, 27698)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd263dg.prod.mediav.com:28737 with 8.6 GB RAM, BlockManagerId(15, hd263dg.prod.mediav.com, 28737)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd284dg.prod.mediav.com:23204/user/Executor#-1401695663]) with ID 64
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd270dg.prod.mediav.com:64691/user/Executor#1005216395]) with ID 33
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd261dg.prod.mediav.com:18064/user/Executor#-481596015]) with ID 38
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd272dg.prod.mediav.com:13836 with 8.6 GB RAM, BlockManagerId(83, hd272dg.prod.mediav.com, 13836)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd288dg.prod.mediav.com:53123/user/Executor#-507858527]) with ID 57
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd280dg.prod.mediav.com:61771/user/Executor#484402847]) with ID 131
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd288dg.prod.mediav.com:64716 with 8.6 GB RAM, BlockManagerId(129, hd288dg.prod.mediav.com, 64716)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd256dg.prod.mediav.com:4157 with 8.6 GB RAM, BlockManagerId(143, hd256dg.prod.mediav.com, 4157)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd272dg.prod.mediav.com:63317 with 8.6 GB RAM, BlockManagerId(154, hd272dg.prod.mediav.com, 63317)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd270dg.prod.mediav.com:32108/user/Executor#-1783453245]) with ID 141
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd255dg.prod.mediav.com:8336 with 8.6 GB RAM, BlockManagerId(41, hd255dg.prod.mediav.com, 8336)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd264dg.prod.mediav.com:41047/user/Executor#-328929010]) with ID 82
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd271dg.prod.mediav.com:45534 with 8.6 GB RAM, BlockManagerId(135, hd271dg.prod.mediav.com, 45534)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd278dg.prod.mediav.com:39877/user/Executor#-578202831]) with ID 30
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd256dg.prod.mediav.com:5214 with 8.6 GB RAM, BlockManagerId(71, hd256dg.prod.mediav.com, 5214)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd281dg.prod.mediav.com:63392/user/Executor#282533120]) with ID 161
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd283dg.prod.mediav.com:52335/user/Executor#530828928]) with ID 34
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd270dg.prod.mediav.com:5548 with 8.6 GB RAM, BlockManagerId(176, hd270dg.prod.mediav.com, 5548)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd261dg.prod.mediav.com:16786 with 8.6 GB RAM, BlockManagerId(110, hd261dg.prod.mediav.com, 16786)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd281dg.prod.mediav.com:53462/user/Executor#-291312463]) with ID 90
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd260dg.prod.mediav.com:7955 with 8.6 GB RAM, BlockManagerId(150, hd260dg.prod.mediav.com, 7955)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd275dg.prod.mediav.com:37869 with 8.6 GB RAM, BlockManagerId(94, hd275dg.prod.mediav.com, 37869)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd293dg.prod.mediav.com:40882/user/Executor#-442822794]) with ID 61
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd257dg.prod.mediav.com:18954/user/Executor#193835535]) with ID 92
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd275dg.prod.mediav.com:47140/user/Executor#-134333072]) with ID 165
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd272dg.prod.mediav.com:42770/user/Executor#-1955345389]) with ID 119
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd271dg.prod.mediav.com:63849 with 8.6 GB RAM, BlockManagerId(27, hd271dg.prod.mediav.com, 63849)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd256dg.prod.mediav.com:31292/user/Executor#2134863987]) with ID 107
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd282dg.prod.mediav.com:42230/user/Executor#-707246617]) with ID 40
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd274dg.prod.mediav.com:40430/user/Executor#-14696554]) with ID 73
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd260dg.prod.mediav.com:23318/user/Executor#-1051712968]) with ID 79
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd282dg.prod.mediav.com:42292 with 8.6 GB RAM, BlockManagerId(4, hd282dg.prod.mediav.com, 42292)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd278dg.prod.mediav.com:36645/user/Executor#1481710153]) with ID 173
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd292dg.prod.mediav.com:45417/user/Executor#-880594536]) with ID 67
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd293dg.prod.mediav.com:23628 with 8.6 GB RAM, BlockManagerId(168, hd293dg.prod.mediav.com, 23628)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd264dg.prod.mediav.com:6701/user/Executor#-587248355]) with ID 10
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd256dg.prod.mediav.com:16697/user/Executor#1872173882]) with ID 178
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd292dg.prod.mediav.com:18520/user/Executor#1822675287]) with ID 31
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd292dg.prod.mediav.com:9515 with 8.6 GB RAM, BlockManagerId(174, hd292dg.prod.mediav.com, 9515)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd285dg.prod.mediav.com:45503/user/Executor#1635268644]) with ID 155
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd293dg.prod.mediav.com:55071/user/Executor#132595206]) with ID 25
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd292dg.prod.mediav.com:31959 with 8.6 GB RAM, BlockManagerId(103, hd292dg.prod.mediav.com, 31959)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd286dg.prod.mediav.com:22509 with 8.6 GB RAM, BlockManagerId(114, hd286dg.prod.mediav.com, 22509)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd286dg.prod.mediav.com:64735/user/Executor#960436890]) with ID 6
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd273dg.prod.mediav.com:46229/user/Executor#-621682693]) with ID 80
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd278dg.prod.mediav.com:60479/user/Executor#-752308431]) with ID 102
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd288dg.prod.mediav.com:58225 with 8.6 GB RAM, BlockManagerId(164, hd288dg.prod.mediav.com, 58225)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd261dg.prod.mediav.com:17918 with 8.6 GB RAM, BlockManagerId(145, hd261dg.prod.mediav.com, 17918)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd289dg.prod.mediav.com:48417 with 8.6 GB RAM, BlockManagerId(124, hd289dg.prod.mediav.com, 48417)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd288dg.prod.mediav.com:3459 with 8.6 GB RAM, BlockManagerId(93, hd288dg.prod.mediav.com, 3459)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd266dg.prod.mediav.com:10664/user/Executor#60318011]) with ID 26
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd290dg.prod.mediav.com:27328/user/Executor#230661053]) with ID 14
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd284dg.prod.mediav.com:41874/user/Executor#953855936]) with ID 171
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd283dg.prod.mediav.com:15357 with 8.6 GB RAM, BlockManagerId(177, hd283dg.prod.mediav.com, 15357)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd275dg.prod.mediav.com:41770/user/Executor#1426814105]) with ID 22
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd264dg.prod.mediav.com:38843 with 8.6 GB RAM, BlockManagerId(153, hd264dg.prod.mediav.com, 38843)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd266dg.prod.mediav.com:8634/user/Executor#974371141]) with ID 169
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd254dg.prod.mediav.com:60094/user/Executor#1965647154]) with ID 162
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd276dg.prod.mediav.com:65222 with 8.6 GB RAM, BlockManagerId(17, hd276dg.prod.mediav.com, 65222)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd291dg.prod.mediav.com:31861 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd281dg.prod.mediav.com:2395 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd286dg.prod.mediav.com:64184 with 8.6 GB RAM, BlockManagerId(78, hd286dg.prod.mediav.com, 64184)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd277dg.prod.mediav.com:50355/user/Executor#2016822248]) with ID 111
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd274dg.prod.mediav.com:39271/user/Executor#811706269]) with ID 109
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd290dg.prod.mediav.com:1034 with 8.6 GB RAM, BlockManagerId(50, hd290dg.prod.mediav.com, 1034)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd278dg.prod.mediav.com:12406/user/Executor#-1637512681]) with ID 138
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd289dg.prod.mediav.com:33137/user/Executor#977911863]) with ID 159
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd282dg.prod.mediav.com:10051/user/Executor#-2063918040]) with ID 147
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd289dg.prod.mediav.com:58218/user/Executor#-792536670]) with ID 88
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd273dg.prod.mediav.com:7925/user/Executor#-4635176]) with ID 44
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd283dg.prod.mediav.com:62866 with 8.6 GB RAM, BlockManagerId(142, hd283dg.prod.mediav.com, 62866)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd290dg.prod.mediav.com:58202/user/Executor#1446968348]) with ID 86
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd266dg.prod.mediav.com:47658/user/Executor#175417760]) with ID 134
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd261dg.prod.mediav.com:48502 with 8.6 GB RAM, BlockManagerId(74, hd261dg.prod.mediav.com, 48502)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd276dg.prod.mediav.com:45572/user/Executor#1487470075]) with ID 89
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd273dg.prod.mediav.com:36978/user/Executor#-1049030074]) with ID 116
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd257dg.prod.mediav.com:13672 with 8.6 GB RAM, BlockManagerId(20, hd257dg.prod.mediav.com, 13672)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd272dg.prod.mediav.com:16824/user/Executor#-1793400714]) with ID 11
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd284dg.prod.mediav.com:25270/user/Executor#654396108]) with ID 28
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd256dg.prod.mediav.com:65180 with 8.6 GB RAM, BlockManagerId(35, hd256dg.prod.mediav.com, 65180)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd271dg.prod.mediav.com:11439 with 8.6 GB RAM, BlockManagerId(63, hd271dg.prod.mediav.com, 11439)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd264dg.prod.mediav.com:50679/user/Executor#685270151]) with ID 46
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd277dg.prod.mediav.com:27612 with 8.6 GB RAM, BlockManagerId(75, hd277dg.prod.mediav.com, 27612)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd272dg.prod.mediav.com:32595/user/Executor#-599498795]) with ID 47
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd280dg.prod.mediav.com:1897 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd276dg.prod.mediav.com:21872 with 8.6 GB RAM, BlockManagerId(125, hd276dg.prod.mediav.com, 21872)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd273dg.prod.mediav.com:36509/user/Executor#1606966554]) with ID 8
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd282dg.prod.mediav.com:57776 with 8.6 GB RAM, BlockManagerId(112, hd282dg.prod.mediav.com, 57776)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd266dg.prod.mediav.com:60772 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd273dg.prod.mediav.com:22967/user/Executor#1775881363]) with ID 151
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd287dg.prod.mediav.com:53041 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd265dg.prod.mediav.com:60697/user/Executor#-33490033]) with ID 104
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd265dg.prod.mediav.com:26927/user/Executor#1748242505]) with ID 32
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd285dg.prod.mediav.com:52469 with 8.6 GB RAM, BlockManagerId(120, hd285dg.prod.mediav.com, 52469)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd262dg.prod.mediav.com:24194 with 8.6 GB RAM, BlockManagerId(172, hd262dg.prod.mediav.com, 24194)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd270dg.prod.mediav.com:3332 with 8.6 GB RAM, BlockManagerId(105, hd270dg.prod.mediav.com, 3332)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd265dg.prod.mediav.com:30493/user/Executor#1869772801]) with ID 175
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd264dg.prod.mediav.com:48358 with 8.6 GB RAM, BlockManagerId(118, hd264dg.prod.mediav.com, 48358)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd265dg.prod.mediav.com:45554/user/Executor#-839041598]) with ID 140
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd274dg.prod.mediav.com:25030 with 8.6 GB RAM, BlockManagerId(144, hd274dg.prod.mediav.com, 25030)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd258dg.prod.mediav.com:31985/user/Executor#1803292015]) with ID 121
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd284dg.prod.mediav.com:46214 with 8.6 GB RAM, BlockManagerId(100, hd284dg.prod.mediav.com, 46214)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd284dg.prod.mediav.com:33099 with 8.6 GB RAM, BlockManagerId(136, hd284dg.prod.mediav.com, 33099)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd279dg.prod.mediav.com:44656/user/Executor#1814026786]) with ID 167
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd281dg.prod.mediav.com:12163 with 8.6 GB RAM, BlockManagerId(126, hd281dg.prod.mediav.com, 12163)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd258dg.prod.mediav.com:56446/user/Executor#-1058219905]) with ID 49
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd276dg.prod.mediav.com:46848 with 8.6 GB RAM, BlockManagerId(160, hd276dg.prod.mediav.com, 46848)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd279dg.prod.mediav.com:52572/user/Executor#1274095730]) with ID 24
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd279dg.prod.mediav.com:1403/user/Executor#2094018840]) with ID 60
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd258dg.prod.mediav.com:27227/user/Executor#1584872503]) with ID 156
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd258dg.prod.mediav.com:59526/user/Executor#-1207197872]) with ID 13
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd265dg.prod.mediav.com:35269/user/Executor#997557489]) with ID 68
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd258dg.prod.mediav.com:52760/user/Executor#870980249]) with ID 85
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd279dg.prod.mediav.com:38410/user/Executor#1995283686]) with ID 132
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd279dg.prod.mediav.com:24917/user/Executor#-892212294]) with ID 96
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd264dg.prod.mediav.com:49664 with 8.6 GB RAM, BlockManagerId(82, hd264dg.prod.mediav.com, 49664)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd278dg.prod.mediav.com:1380 with 8.6 GB RAM, BlockManagerId(173, hd278dg.prod.mediav.com, 1380)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd289dg.prod.mediav.com:20447 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd276dg.prod.mediav.com:15025 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd278dg.prod.mediav.com:9221 with 8.6 GB RAM, BlockManagerId(30, hd278dg.prod.mediav.com, 9221)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd266dg.prod.mediav.com:7561 with 8.6 GB RAM, BlockManagerId(26, hd266dg.prod.mediav.com, 7561)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd291dg.prod.mediav.com:54360 with 8.6 GB RAM, BlockManagerId(117, hd291dg.prod.mediav.com, 54360)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd272dg.prod.mediav.com:63093 with 8.6 GB RAM, BlockManagerId(119, hd272dg.prod.mediav.com, 63093)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd275dg.prod.mediav.com:3649 with 8.6 GB RAM, BlockManagerId(22, hd275dg.prod.mediav.com, 3649)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd285dg.prod.mediav.com:57578 with 8.6 GB RAM, BlockManagerId(12, hd285dg.prod.mediav.com, 57578)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd273dg.prod.mediav.com:4400 with 8.6 GB RAM, BlockManagerId(80, hd273dg.prod.mediav.com, 4400)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd261dg.prod.mediav.com:35424 with 8.6 GB RAM, BlockManagerId(2, hd261dg.prod.mediav.com, 35424)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd272dg.prod.mediav.com:47434 with 8.6 GB RAM, BlockManagerId(11, hd272dg.prod.mediav.com, 47434)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd277dg.prod.mediav.com:17325 with 8.6 GB RAM, BlockManagerId(111, hd277dg.prod.mediav.com, 17325)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd292dg.prod.mediav.com:24852 with 8.6 GB RAM, BlockManagerId(139, hd292dg.prod.mediav.com, 24852)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd290dg.prod.mediav.com:48698 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd273dg.prod.mediav.com:51855 with 8.6 GB RAM, BlockManagerId(44, hd273dg.prod.mediav.com, 51855)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd256dg.prod.mediav.com:54569 with 8.6 GB RAM, BlockManagerId(178, hd256dg.prod.mediav.com, 54569)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd265dg.prod.mediav.com:65261 with 8.6 GB RAM, BlockManagerId(104, hd265dg.prod.mediav.com, 65261)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd273dg.prod.mediav.com:19800 with 8.6 GB RAM, BlockManagerId(116, hd273dg.prod.mediav.com, 19800)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd264dg.prod.mediav.com:28057 with 8.6 GB RAM, BlockManagerId(10, hd264dg.prod.mediav.com, 28057)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd265dg.prod.mediav.com:15593 with 8.6 GB RAM, BlockManagerId(175, hd265dg.prod.mediav.com, 15593)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd282dg.prod.mediav.com:54259 with 8.6 GB RAM, BlockManagerId(40, hd282dg.prod.mediav.com, 54259)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd262dg.prod.mediav.com:49994 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd293dg.prod.mediav.com:25809 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd265dg.prod.mediav.com:62939 with 8.6 GB RAM, BlockManagerId(32, hd265dg.prod.mediav.com, 62939)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd262dg.prod.mediav.com:35030 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd290dg.prod.mediav.com:1034 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd287dg.prod.mediav.com:2270 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hd255dg.prod.mediav.com:59546/user/Executor#-58897741]) with ID 77
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd262dg.prod.mediav.com:40567 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd292dg.prod.mediav.com:8313 with 8.6 GB RAM, BlockManagerId(67, hd292dg.prod.mediav.com, 8313)
15/09/15 18:50:27 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd255dg.prod.mediav.com:62882 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd258dg.prod.mediav.com:3750 with 8.6 GB RAM, BlockManagerId(121, hd258dg.prod.mediav.com, 3750)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd258dg.prod.mediav.com:44194 with 8.6 GB RAM, BlockManagerId(13, hd258dg.prod.mediav.com, 44194)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd258dg.prod.mediav.com:8532 with 8.6 GB RAM, BlockManagerId(85, hd258dg.prod.mediav.com, 8532)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd258dg.prod.mediav.com:30778 with 8.6 GB RAM, BlockManagerId(49, hd258dg.prod.mediav.com, 30778)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd280dg.prod.mediav.com:8788 with 8.6 GB RAM, BlockManagerId(166, hd280dg.prod.mediav.com, 8788)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd258dg.prod.mediav.com:61558 with 8.6 GB RAM, BlockManagerId(156, hd258dg.prod.mediav.com, 61558)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd289dg.prod.mediav.com:5555 with 8.6 GB RAM, BlockManagerId(16, hd289dg.prod.mediav.com, 5555)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd279dg.prod.mediav.com:49996 with 8.6 GB RAM, BlockManagerId(24, hd279dg.prod.mediav.com, 49996)
15/09/15 18:50:27 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd279dg.prod.mediav.com:34291 with 8.6 GB RAM, BlockManagerId(167, hd279dg.prod.mediav.com, 34291)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd279dg.prod.mediav.com:11149 with 8.6 GB RAM, BlockManagerId(132, hd279dg.prod.mediav.com, 11149)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd279dg.prod.mediav.com:34009 with 8.6 GB RAM, BlockManagerId(96, hd279dg.prod.mediav.com, 34009)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd279dg.prod.mediav.com:12678 with 8.6 GB RAM, BlockManagerId(60, hd279dg.prod.mediav.com, 12678)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd288dg.prod.mediav.com:30986 with 8.6 GB RAM, BlockManagerId(57, hd288dg.prod.mediav.com, 30986)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd276dg.prod.mediav.com:65222 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd254dg.prod.mediav.com:50339 with 8.6 GB RAM, BlockManagerId(162, hd254dg.prod.mediav.com, 50339)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd265dg.prod.mediav.com:41028 with 8.6 GB RAM, BlockManagerId(68, hd265dg.prod.mediav.com, 41028)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd262dg.prod.mediav.com:24194 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd255dg.prod.mediav.com:29011 with 8.6 GB RAM, BlockManagerId(5, hd255dg.prod.mediav.com, 29011)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd291dg.prod.mediav.com:31861 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd280dg.prod.mediav.com:1897 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd266dg.prod.mediav.com:60772 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd273dg.prod.mediav.com:6601 with 8.6 GB RAM, BlockManagerId(151, hd273dg.prod.mediav.com, 6601)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd257dg.prod.mediav.com:11991 with 8.6 GB RAM, BlockManagerId(128, hd257dg.prod.mediav.com, 11991)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd281dg.prod.mediav.com:2395 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd287dg.prod.mediav.com:53041 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd276dg.prod.mediav.com:15025 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd256dg.prod.mediav.com:22567 with 8.6 GB RAM, BlockManagerId(107, hd256dg.prod.mediav.com, 22567)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd270dg.prod.mediav.com:35197 with 8.6 GB RAM, BlockManagerId(141, hd270dg.prod.mediav.com, 35197)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd284dg.prod.mediav.com:55335 with 8.6 GB RAM, BlockManagerId(28, hd284dg.prod.mediav.com, 55335)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd289dg.prod.mediav.com:20447 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd281dg.prod.mediav.com:61667 with 8.6 GB RAM, BlockManagerId(90, hd281dg.prod.mediav.com, 61667)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd286dg.prod.mediav.com:20786 with 8.6 GB RAM, BlockManagerId(149, hd286dg.prod.mediav.com, 20786)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd285dg.prod.mediav.com:25860 with 8.6 GB RAM, BlockManagerId(155, hd285dg.prod.mediav.com, 25860)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd290dg.prod.mediav.com:48698 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd283dg.prod.mediav.com:5777 with 8.6 GB RAM, BlockManagerId(106, hd283dg.prod.mediav.com, 5777)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd293dg.prod.mediav.com:46714 with 8.6 GB RAM, BlockManagerId(97, hd293dg.prod.mediav.com, 46714)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd260dg.prod.mediav.com:38400 with 8.6 GB RAM, BlockManagerId(79, hd260dg.prod.mediav.com, 38400)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd266dg.prod.mediav.com:60871 with 8.6 GB RAM, BlockManagerId(169, hd266dg.prod.mediav.com, 60871)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd290dg.prod.mediav.com:6793 with 8.6 GB RAM, BlockManagerId(14, hd290dg.prod.mediav.com, 6793)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd290dg.prod.mediav.com:1034 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd282dg.prod.mediav.com:51731 with 8.6 GB RAM, BlockManagerId(147, hd282dg.prod.mediav.com, 51731)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd287dg.prod.mediav.com:2270 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd293dg.prod.mediav.com:25809 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd278dg.prod.mediav.com:27239 with 8.6 GB RAM, BlockManagerId(138, hd278dg.prod.mediav.com, 27239)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd291dg.prod.mediav.com:3890 with 8.6 GB RAM, BlockManagerId(81, hd291dg.prod.mediav.com, 3890)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd275dg.prod.mediav.com:52956 with 8.6 GB RAM, BlockManagerId(130, hd275dg.prod.mediav.com, 52956)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd262dg.prod.mediav.com:24194 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd255dg.prod.mediav.com:62882 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd276dg.prod.mediav.com:65222 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd257dg.prod.mediav.com:23974 with 8.6 GB RAM, BlockManagerId(92, hd257dg.prod.mediav.com, 23974)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd289dg.prod.mediav.com:22644 with 8.6 GB RAM, BlockManagerId(159, hd289dg.prod.mediav.com, 22644)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd272dg.prod.mediav.com:36193 with 8.6 GB RAM, BlockManagerId(47, hd272dg.prod.mediav.com, 36193)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd280dg.prod.mediav.com:29265 with 8.6 GB RAM, BlockManagerId(131, hd280dg.prod.mediav.com, 29265)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd276dg.prod.mediav.com:62943 with 8.6 GB RAM, BlockManagerId(89, hd276dg.prod.mediav.com, 62943)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd263dg.prod.mediav.com:5872 with 8.6 GB RAM, BlockManagerId(123, hd263dg.prod.mediav.com, 5872)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd288dg.prod.mediav.com:15935 with 8.6 GB RAM, BlockManagerId(21, hd288dg.prod.mediav.com, 15935)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd286dg.prod.mediav.com:5189 with 8.6 GB RAM, BlockManagerId(42, hd286dg.prod.mediav.com, 5189)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd292dg.prod.mediav.com:5167 with 8.6 GB RAM, BlockManagerId(31, hd292dg.prod.mediav.com, 5167)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd274dg.prod.mediav.com:44964 with 8.6 GB RAM, BlockManagerId(109, hd274dg.prod.mediav.com, 44964)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd265dg.prod.mediav.com:38596 with 8.6 GB RAM, BlockManagerId(140, hd265dg.prod.mediav.com, 38596)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd277dg.prod.mediav.com:27951 with 8.6 GB RAM, BlockManagerId(146, hd277dg.prod.mediav.com, 27951)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd262dg.prod.mediav.com:5957 with 8.6 GB RAM, BlockManagerId(29, hd262dg.prod.mediav.com, 5957)
15/09/15 18:50:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd290dg.prod.mediav.com:48176 with 8.6 GB RAM, BlockManagerId(86, hd290dg.prod.mediav.com, 48176)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd280dg.prod.mediav.com:4198 with 8.6 GB RAM, BlockManagerId(95, hd280dg.prod.mediav.com, 4198)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd271dg.prod.mediav.com:16160 with 8.6 GB RAM, BlockManagerId(99, hd271dg.prod.mediav.com, 16160)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd255dg.prod.mediav.com:22114 with 8.6 GB RAM, BlockManagerId(113, hd255dg.prod.mediav.com, 22114)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd257dg.prod.mediav.com:50217 with 8.6 GB RAM, BlockManagerId(56, hd257dg.prod.mediav.com, 50217)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd293dg.prod.mediav.com:18866 with 8.6 GB RAM, BlockManagerId(61, hd293dg.prod.mediav.com, 18866)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd266dg.prod.mediav.com:18054 with 8.6 GB RAM, BlockManagerId(134, hd266dg.prod.mediav.com, 18054)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd273dg.prod.mediav.com:35935 with 8.6 GB RAM, BlockManagerId(8, hd273dg.prod.mediav.com, 35935)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd270dg.prod.mediav.com:33090 with 8.6 GB RAM, BlockManagerId(33, hd270dg.prod.mediav.com, 33090)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd263dg.prod.mediav.com:30335 with 8.6 GB RAM, BlockManagerId(158, hd263dg.prod.mediav.com, 30335)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd254dg.prod.mediav.com:37252 with 8.6 GB RAM, BlockManagerId(127, hd254dg.prod.mediav.com, 37252)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd261dg.prod.mediav.com:51708 with 8.6 GB RAM, BlockManagerId(38, hd261dg.prod.mediav.com, 51708)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd264dg.prod.mediav.com:48038 with 8.6 GB RAM, BlockManagerId(46, hd264dg.prod.mediav.com, 48038)
15/09/15 18:50:29 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd262dg.prod.mediav.com:5957 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd281dg.prod.mediav.com:2780 with 8.6 GB RAM, BlockManagerId(161, hd281dg.prod.mediav.com, 2780)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd284dg.prod.mediav.com:65293 with 8.6 GB RAM, BlockManagerId(171, hd284dg.prod.mediav.com, 65293)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd278dg.prod.mediav.com:27815 with 8.6 GB RAM, BlockManagerId(102, hd278dg.prod.mediav.com, 27815)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd275dg.prod.mediav.com:27702 with 8.6 GB RAM, BlockManagerId(165, hd275dg.prod.mediav.com, 27702)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd283dg.prod.mediav.com:27671 with 8.6 GB RAM, BlockManagerId(34, hd283dg.prod.mediav.com, 27671)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd286dg.prod.mediav.com:15531 with 8.6 GB RAM, BlockManagerId(6, hd286dg.prod.mediav.com, 15531)
15/09/15 18:50:29 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd262dg.prod.mediav.com:5957 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd289dg.prod.mediav.com:48937 with 8.6 GB RAM, BlockManagerId(88, hd289dg.prod.mediav.com, 48937)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd270dg.prod.mediav.com:6944 with 8.6 GB RAM, BlockManagerId(69, hd270dg.prod.mediav.com, 6944)
15/09/15 18:50:29 INFO storage.BlockManagerInfo: Added rdd_4_97 in memory on hd280dg.prod.mediav.com:1897 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:29 INFO storage.BlockManagerInfo: Added rdd_4_19 in memory on hd291dg.prod.mediav.com:31861 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:29 INFO storage.BlockManagerInfo: Added rdd_4_137 in memory on hd281dg.prod.mediav.com:2395 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:29 INFO storage.BlockManagerInfo: Added rdd_4_139 in memory on hd266dg.prod.mediav.com:60772 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:29 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 0.0 (TID 5) in 2807 ms on hd291dg.prod.mediav.com (1/200)
15/09/15 18:50:29 INFO scheduler.TaskSetManager: Finished task 97.0 in stage 0.0 (TID 9) in 2785 ms on hd280dg.prod.mediav.com (2/200)
15/09/15 18:50:29 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 0.0 (TID 7) in 2813 ms on hd281dg.prod.mediav.com (3/200)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd290dg.prod.mediav.com:25045 with 8.6 GB RAM, BlockManagerId(122, hd290dg.prod.mediav.com, 25045)
15/09/15 18:50:29 INFO scheduler.TaskSetManager: Finished task 139.0 in stage 0.0 (TID 11) in 2770 ms on hd266dg.prod.mediav.com (4/200)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd260dg.prod.mediav.com:45170 with 8.6 GB RAM, BlockManagerId(7, hd260dg.prod.mediav.com, 45170)
15/09/15 18:50:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd274dg.prod.mediav.com:50461 with 8.6 GB RAM, BlockManagerId(73, hd274dg.prod.mediav.com, 50461)
15/09/15 18:50:29 INFO storage.BlockManagerInfo: Added rdd_4_115 in memory on hd289dg.prod.mediav.com:20447 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:29 INFO storage.BlockManagerInfo: Added rdd_4_98 in memory on hd276dg.prod.mediav.com:15025 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:29 INFO storage.BlockManagerInfo: Added rdd_4_112 in memory on hd262dg.prod.mediav.com:40567 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:29 INFO scheduler.TaskSetManager: Finished task 115.0 in stage 0.0 (TID 12) in 2805 ms on hd289dg.prod.mediav.com (5/200)
15/09/15 18:50:29 INFO storage.BlockManagerInfo: Added rdd_4_44 in memory on hd262dg.prod.mediav.com:49994 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 112.0 in stage 0.0 (TID 3) in 3016 ms on hd262dg.prod.mediav.com (6/200)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 98.0 in stage 0.0 (TID 10) in 2883 ms on hd276dg.prod.mediav.com (7/200)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 0.0 (TID 1) in 3050 ms on hd262dg.prod.mediav.com (8/200)
15/09/15 18:50:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd255dg.prod.mediav.com:24822 with 8.6 GB RAM, BlockManagerId(77, hd255dg.prod.mediav.com, 24822)
15/09/15 18:50:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd271dg.prod.mediav.com:33037 with 8.6 GB RAM, BlockManagerId(170, hd271dg.prod.mediav.com, 33037)
15/09/15 18:50:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd293dg.prod.mediav.com:60417 with 8.6 GB RAM, BlockManagerId(25, hd293dg.prod.mediav.com, 60417)
15/09/15 18:50:30 INFO storage.BlockManagerInfo: Added rdd_4_133 in memory on hd290dg.prod.mediav.com:1034 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:30 INFO storage.BlockManagerInfo: Added rdd_4_102 in memory on hd290dg.prod.mediav.com:48698 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 133.0 in stage 0.0 (TID 16) in 2733 ms on hd290dg.prod.mediav.com (9/200)
15/09/15 18:50:30 INFO storage.BlockManagerInfo: Added rdd_4_104 in memory on hd287dg.prod.mediav.com:53041 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 102.0 in stage 0.0 (TID 15) in 2824 ms on hd290dg.prod.mediav.com (10/200)
15/09/15 18:50:30 INFO storage.BlockManagerInfo: Added rdd_4_80 in memory on hd293dg.prod.mediav.com:25809 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:30 INFO storage.BlockManagerInfo: Added rdd_4_23 in memory on hd262dg.prod.mediav.com:35030 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 104.0 in stage 0.0 (TID 8) in 3127 ms on hd287dg.prod.mediav.com (11/200)
15/09/15 18:50:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd254dg.prod.mediav.com:28306 with 8.6 GB RAM, BlockManagerId(19, hd254dg.prod.mediav.com, 28306)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 80.0 in stage 0.0 (TID 13) in 2905 ms on hd293dg.prod.mediav.com (12/200)
15/09/15 18:50:30 INFO storage.BlockManagerInfo: Added rdd_4_161 in memory on hd255dg.prod.mediav.com:62882 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 0.0 (TID 0) in 3278 ms on hd262dg.prod.mediav.com (13/200)
15/09/15 18:50:30 INFO storage.BlockManagerInfo: Added rdd_4_60 in memory on hd287dg.prod.mediav.com:2270 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:30 INFO storage.BlockManagerInfo: Added rdd_4_169 in memory on hd276dg.prod.mediav.com:65222 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 161.0 in stage 0.0 (TID 14) in 2961 ms on hd255dg.prod.mediav.com (14/200)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 60.0 in stage 0.0 (TID 6) in 3245 ms on hd287dg.prod.mediav.com (15/200)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 169.0 in stage 0.0 (TID 17) in 2876 ms on hd276dg.prod.mediav.com (16/200)
15/09/15 18:50:30 INFO storage.BlockManagerInfo: Added rdd_4_47 in memory on hd262dg.prod.mediav.com:24194 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 0.0 (TID 2) in 3441 ms on hd262dg.prod.mediav.com (17/200)
15/09/15 18:50:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd277dg.prod.mediav.com:6547 with 8.6 GB RAM, BlockManagerId(3, hd277dg.prod.mediav.com, 6547)
15/09/15 18:50:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd260dg.prod.mediav.com:34432 with 8.6 GB RAM, BlockManagerId(43, hd260dg.prod.mediav.com, 34432)
15/09/15 18:50:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager hd284dg.prod.mediav.com:39393 with 8.6 GB RAM, BlockManagerId(64, hd284dg.prod.mediav.com, 39393)
15/09/15 18:50:30 INFO scheduler.TaskSetManager: Starting task 174.0 in stage 0.0 (TID 18, hd293dg.prod.mediav.com, RACK_LOCAL, 1425 bytes)
15/09/15 18:50:31 INFO storage.BlockManagerInfo: Added rdd_4_129 in memory on hd262dg.prod.mediav.com:5957 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:31 INFO scheduler.TaskSetManager: Finished task 129.0 in stage 0.0 (TID 4) in 4431 ms on hd262dg.prod.mediav.com (18/200)
15/09/15 18:50:31 INFO storage.BlockManagerInfo: Added rdd_4_174 in memory on hd293dg.prod.mediav.com:25809 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:31 INFO scheduler.TaskSetManager: Finished task 174.0 in stage 0.0 (TID 18) in 927 ms on hd293dg.prod.mediav.com (19/200)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 19, hd292dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 20, hd287dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 21, hd288dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 22, hd292dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 23, hd258dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 24, hd254dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 25, hd262dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 26, hd291dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 27, hd282dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 28, hd262dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 0.0 (TID 29, hd261dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 0.0 (TID 30, hd290dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 0.0 (TID 31, hd265dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 0.0 (TID 32, hd262dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 0.0 (TID 33, hd274dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 0.0 (TID 34, hd285dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 0.0 (TID 35, hd285dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 0.0 (TID 36, hd260dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 0.0 (TID 37, hd270dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 0.0 (TID 38, hd266dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 0.0 (TID 39, hd284dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 0.0 (TID 40, hd293dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 0.0 (TID 41, hd271dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 0.0 (TID 42, hd291dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 0.0 (TID 43, hd278dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 0.0 (TID 44, hd271dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 0.0 (TID 45, hd266dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 0.0 (TID 46, hd287dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 0.0 (TID 47, hd283dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 0.0 (TID 48, hd277dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 0.0 (TID 49, hd293dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 0.0 (TID 50, hd274dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 0.0 (TID 51, hd288dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 0.0 (TID 52, hd263dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 0.0 (TID 53, hd257dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 0.0 (TID 54, hd256dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 0.0 (TID 55, hd260dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 0.0 (TID 56, hd279dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 0.0 (TID 57, hd283dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 0.0 (TID 58, hd262dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 0.0 (TID 59, hd289dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 0.0 (TID 60, hd291dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 0.0 (TID 61, hd289dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 0.0 (TID 62, hd282dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 48.0 in stage 0.0 (TID 63, hd256dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 49.0 in stage 0.0 (TID 64, hd293dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 50.0 in stage 0.0 (TID 65, hd291dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 51.0 in stage 0.0 (TID 66, hd286dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 52.0 in stage 0.0 (TID 67, hd257dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 53.0 in stage 0.0 (TID 68, hd275dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 54.0 in stage 0.0 (TID 69, hd272dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 55.0 in stage 0.0 (TID 70, hd278dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 56.0 in stage 0.0 (TID 71, hd276dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 57.0 in stage 0.0 (TID 72, hd266dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 58.0 in stage 0.0 (TID 73, hd287dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 59.0 in stage 0.0 (TID 74, hd280dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 61.0 in stage 0.0 (TID 75, hd270dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 62.0 in stage 0.0 (TID 76, hd289dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 63.0 in stage 0.0 (TID 77, hd270dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 64.0 in stage 0.0 (TID 78, hd278dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 65.0 in stage 0.0 (TID 79, hd285dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 66.0 in stage 0.0 (TID 80, hd292dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 67.0 in stage 0.0 (TID 81, hd266dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 68.0 in stage 0.0 (TID 82, hd293dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 69.0 in stage 0.0 (TID 83, hd264dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 70.0 in stage 0.0 (TID 84, hd284dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 71.0 in stage 0.0 (TID 85, hd288dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 72.0 in stage 0.0 (TID 86, hd265dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 73.0 in stage 0.0 (TID 87, hd278dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 74.0 in stage 0.0 (TID 88, hd254dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 75.0 in stage 0.0 (TID 89, hd264dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 76.0 in stage 0.0 (TID 90, hd261dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 77.0 in stage 0.0 (TID 91, hd279dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 78.0 in stage 0.0 (TID 92, hd270dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 79.0 in stage 0.0 (TID 93, hd292dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 81.0 in stage 0.0 (TID 94, hd265dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 82.0 in stage 0.0 (TID 95, hd261dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 83.0 in stage 0.0 (TID 96, hd288dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 84.0 in stage 0.0 (TID 97, hd266dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 85.0 in stage 0.0 (TID 98, hd286dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 86.0 in stage 0.0 (TID 99, hd275dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 87.0 in stage 0.0 (TID 100, hd281dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 88.0 in stage 0.0 (TID 101, hd262dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 89.0 in stage 0.0 (TID 102, hd274dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 90.0 in stage 0.0 (TID 103, hd273dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 91.0 in stage 0.0 (TID 104, hd290dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 92.0 in stage 0.0 (TID 105, hd254dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 93.0 in stage 0.0 (TID 106, hd271dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 94.0 in stage 0.0 (TID 107, hd284dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 95.0 in stage 0.0 (TID 108, hd289dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 96.0 in stage 0.0 (TID 109, hd260dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 99.0 in stage 0.0 (TID 110, hd274dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 100.0 in stage 0.0 (TID 111, hd285dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 101.0 in stage 0.0 (TID 112, hd261dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 103.0 in stage 0.0 (TID 113, hd271dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 105.0 in stage 0.0 (TID 114, hd288dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 106.0 in stage 0.0 (TID 115, hd281dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 107.0 in stage 0.0 (TID 116, hd261dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 108.0 in stage 0.0 (TID 117, hd286dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 109.0 in stage 0.0 (TID 118, hd280dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 110.0 in stage 0.0 (TID 119, hd284dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 111.0 in stage 0.0 (TID 120, hd264dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 113.0 in stage 0.0 (TID 121, hd255dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 114.0 in stage 0.0 (TID 122, hd280dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 116.0 in stage 0.0 (TID 123, hd272dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 117.0 in stage 0.0 (TID 124, hd279dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 118.0 in stage 0.0 (TID 125, hd271dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 119.0 in stage 0.0 (TID 126, hd279dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 120.0 in stage 0.0 (TID 127, hd273dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 121.0 in stage 0.0 (TID 128, hd290dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 122.0 in stage 0.0 (TID 129, hd280dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 123.0 in stage 0.0 (TID 130, hd272dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 124.0 in stage 0.0 (TID 131, hd257dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 125.0 in stage 0.0 (TID 132, hd263dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 126.0 in stage 0.0 (TID 133, hd276dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 127.0 in stage 0.0 (TID 134, hd278dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 128.0 in stage 0.0 (TID 135, hd260dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 130.0 in stage 0.0 (TID 136, hd256dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 131.0 in stage 0.0 (TID 137, hd290dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 132.0 in stage 0.0 (TID 138, hd256dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 134.0 in stage 0.0 (TID 139, hd255dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 135.0 in stage 0.0 (TID 140, hd260dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 136.0 in stage 0.0 (TID 141, hd276dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 138.0 in stage 0.0 (TID 142, hd272dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 140.0 in stage 0.0 (TID 143, hd270dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 141.0 in stage 0.0 (TID 144, hd286dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 142.0 in stage 0.0 (TID 145, hd275dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 143.0 in stage 0.0 (TID 146, hd283dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 144.0 in stage 0.0 (TID 147, hd281dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 145.0 in stage 0.0 (TID 148, hd282dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 146.0 in stage 0.0 (TID 149, hd273dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 147.0 in stage 0.0 (TID 150, hd283dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 148.0 in stage 0.0 (TID 151, hd279dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 149.0 in stage 0.0 (TID 152, hd255dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 150.0 in stage 0.0 (TID 153, hd265dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 151.0 in stage 0.0 (TID 154, hd277dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 152.0 in stage 0.0 (TID 155, hd280dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 153.0 in stage 0.0 (TID 156, hd263dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 154.0 in stage 0.0 (TID 157, hd283dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 155.0 in stage 0.0 (TID 158, hd275dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 156.0 in stage 0.0 (TID 159, hd281dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 157.0 in stage 0.0 (TID 160, hd281dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 158.0 in stage 0.0 (TID 161, hd274dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 159.0 in stage 0.0 (TID 162, hd292dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 160.0 in stage 0.0 (TID 163, hd257dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 162.0 in stage 0.0 (TID 164, hd258dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 163.0 in stage 0.0 (TID 165, hd263dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 164.0 in stage 0.0 (TID 166, hd264dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 165.0 in stage 0.0 (TID 167, hd282dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 166.0 in stage 0.0 (TID 168, hd293dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 167.0 in stage 0.0 (TID 169, hd265dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 168.0 in stage 0.0 (TID 170, hd257dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 170.0 in stage 0.0 (TID 171, hd263dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 171.0 in stage 0.0 (TID 172, hd272dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 172.0 in stage 0.0 (TID 173, hd256dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 173.0 in stage 0.0 (TID 174, hd276dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 175.0 in stage 0.0 (TID 175, hd255dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 176.0 in stage 0.0 (TID 176, hd286dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 177.0 in stage 0.0 (TID 177, hd273dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 178.0 in stage 0.0 (TID 178, hd290dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 179.0 in stage 0.0 (TID 179, hd282dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 180.0 in stage 0.0 (TID 180, hd285dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 181.0 in stage 0.0 (TID 181, hd284dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 182.0 in stage 0.0 (TID 182, hd254dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 183.0 in stage 0.0 (TID 183, hd277dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 184.0 in stage 0.0 (TID 184, hd258dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 185.0 in stage 0.0 (TID 185, hd254dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 186.0 in stage 0.0 (TID 186, hd291dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 187.0 in stage 0.0 (TID 187, hd289dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 188.0 in stage 0.0 (TID 188, hd275dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 189.0 in stage 0.0 (TID 189, hd277dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 190.0 in stage 0.0 (TID 190, hd255dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 191.0 in stage 0.0 (TID 191, hd276dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 192.0 in stage 0.0 (TID 192, hd258dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 193.0 in stage 0.0 (TID 193, hd277dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 194.0 in stage 0.0 (TID 194, hd264dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 195.0 in stage 0.0 (TID 195, hd273dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:33 INFO scheduler.TaskSetManager: Starting task 196.0 in stage 0.0 (TID 196, hd258dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd260dg.prod.mediav.com:7955 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd285dg.prod.mediav.com:7398 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd280dg.prod.mediav.com:6093 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd290dg.prod.mediav.com:6793 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd280dg.prod.mediav.com:4198 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd290dg.prod.mediav.com:48176 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd282dg.prod.mediav.com:42292 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd285dg.prod.mediav.com:15892 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd260dg.prod.mediav.com:20879 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd271dg.prod.mediav.com:11439 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd278dg.prod.mediav.com:16567 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd274dg.prod.mediav.com:58299 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd257dg.prod.mediav.com:23974 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd261dg.prod.mediav.com:17918 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd291dg.prod.mediav.com:9728 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd257dg.prod.mediav.com:50217 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd278dg.prod.mediav.com:1380 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd272dg.prod.mediav.com:63093 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd270dg.prod.mediav.com:33090 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd261dg.prod.mediav.com:16786 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd282dg.prod.mediav.com:51731 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd254dg.prod.mediav.com:28306 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd276dg.prod.mediav.com:21872 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd284dg.prod.mediav.com:39393 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd274dg.prod.mediav.com:16654 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd293dg.prod.mediav.com:60417 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd271dg.prod.mediav.com:16160 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd290dg.prod.mediav.com:25045 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd274dg.prod.mediav.com:50461 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd288dg.prod.mediav.com:58225 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd288dg.prod.mediav.com:64716 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd292dg.prod.mediav.com:31959 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd266dg.prod.mediav.com:18054 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd272dg.prod.mediav.com:36193 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd286dg.prod.mediav.com:64184 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd283dg.prod.mediav.com:62866 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd291dg.prod.mediav.com:47717 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd277dg.prod.mediav.com:17325 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd285dg.prod.mediav.com:57578 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd275dg.prod.mediav.com:52956 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd283dg.prod.mediav.com:19668 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd254dg.prod.mediav.com:6401 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd271dg.prod.mediav.com:63849 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd264dg.prod.mediav.com:38843 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd291dg.prod.mediav.com:54360 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd261dg.prod.mediav.com:51708 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd284dg.prod.mediav.com:46214 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd289dg.prod.mediav.com:48417 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd284dg.prod.mediav.com:33099 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd261dg.prod.mediav.com:35424 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd278dg.prod.mediav.com:27239 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd289dg.prod.mediav.com:48937 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd266dg.prod.mediav.com:60871 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd285dg.prod.mediav.com:52469 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd282dg.prod.mediav.com:47145 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd255dg.prod.mediav.com:22114 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd260dg.prod.mediav.com:34432 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd286dg.prod.mediav.com:22509 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd277dg.prod.mediav.com:6547 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd271dg.prod.mediav.com:33037 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd282dg.prod.mediav.com:57776 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd292dg.prod.mediav.com:24852 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd288dg.prod.mediav.com:30986 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd257dg.prod.mediav.com:11991 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd266dg.prod.mediav.com:27698 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd286dg.prod.mediav.com:5189 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd281dg.prod.mediav.com:12163 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd271dg.prod.mediav.com:45534 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd283dg.prod.mediav.com:15357 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd260dg.prod.mediav.com:38400 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd270dg.prod.mediav.com:35197 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd264dg.prod.mediav.com:48358 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd254dg.prod.mediav.com:37252 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd275dg.prod.mediav.com:37869 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd256dg.prod.mediav.com:22567 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd289dg.prod.mediav.com:5555 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd260dg.prod.mediav.com:45170 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd263dg.prod.mediav.com:30335 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd292dg.prod.mediav.com:8313 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd277dg.prod.mediav.com:60781 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd286dg.prod.mediav.com:15531 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd293dg.prod.mediav.com:18866 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd255dg.prod.mediav.com:24822 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd266dg.prod.mediav.com:7561 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd270dg.prod.mediav.com:3332 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd263dg.prod.mediav.com:19616 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd293dg.prod.mediav.com:23628 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd280dg.prod.mediav.com:29265 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd264dg.prod.mediav.com:48038 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd255dg.prod.mediav.com:8336 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd264dg.prod.mediav.com:49664 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd264dg.prod.mediav.com:28057 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd293dg.prod.mediav.com:46714 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd276dg.prod.mediav.com:62943 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd288dg.prod.mediav.com:15935 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd263dg.prod.mediav.com:28737 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd275dg.prod.mediav.com:7744 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd275dg.prod.mediav.com:3649 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd263dg.prod.mediav.com:5872 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd273dg.prod.mediav.com:35935 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd281dg.prod.mediav.com:24432 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd279dg.prod.mediav.com:34291 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd272dg.prod.mediav.com:47434 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd275dg.prod.mediav.com:27702 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd279dg.prod.mediav.com:49996 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd273dg.prod.mediav.com:6601 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd256dg.prod.mediav.com:65180 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd289dg.prod.mediav.com:22644 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd273dg.prod.mediav.com:51855 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd273dg.prod.mediav.com:19800 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd263dg.prod.mediav.com:57200 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd257dg.prod.mediav.com:13672 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd292dg.prod.mediav.com:9515 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd279dg.prod.mediav.com:11149 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd279dg.prod.mediav.com:12678 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd276dg.prod.mediav.com:46848 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd284dg.prod.mediav.com:65293 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd285dg.prod.mediav.com:25860 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd283dg.prod.mediav.com:5777 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd279dg.prod.mediav.com:34009 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd286dg.prod.mediav.com:20786 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd277dg.prod.mediav.com:27951 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd274dg.prod.mediav.com:25030 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd292dg.prod.mediav.com:5167 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd270dg.prod.mediav.com:6944 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd283dg.prod.mediav.com:27671 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd257dg.prod.mediav.com:6403 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd261dg.prod.mediav.com:48502 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd277dg.prod.mediav.com:27612 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd274dg.prod.mediav.com:44964 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd278dg.prod.mediav.com:27815 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd272dg.prod.mediav.com:63317 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd256dg.prod.mediav.com:54569 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd254dg.prod.mediav.com:50339 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd273dg.prod.mediav.com:4400 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd288dg.prod.mediav.com:3459 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd281dg.prod.mediav.com:2780 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd284dg.prod.mediav.com:55335 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd254dg.prod.mediav.com:14962 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd281dg.prod.mediav.com:61667 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd291dg.prod.mediav.com:3890 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd256dg.prod.mediav.com:5214 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd270dg.prod.mediav.com:5548 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd282dg.prod.mediav.com:54259 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd256dg.prod.mediav.com:4157 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd272dg.prod.mediav.com:13836 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd265dg.prod.mediav.com:62939 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd258dg.prod.mediav.com:3750 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd258dg.prod.mediav.com:61558 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd258dg.prod.mediav.com:30778 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd258dg.prod.mediav.com:44194 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd265dg.prod.mediav.com:65261 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd258dg.prod.mediav.com:8532 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd265dg.prod.mediav.com:38596 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd265dg.prod.mediav.com:15593 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd265dg.prod.mediav.com:41028 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd287dg.prod.mediav.com:2547 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd280dg.prod.mediav.com:8788 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd293dg.prod.mediav.com:23628 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd260dg.prod.mediav.com:7955 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd290dg.prod.mediav.com:6793 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd282dg.prod.mediav.com:42292 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd290dg.prod.mediav.com:48176 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd271dg.prod.mediav.com:11439 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd278dg.prod.mediav.com:1380 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd280dg.prod.mediav.com:29265 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd280dg.prod.mediav.com:6093 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd285dg.prod.mediav.com:7398 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd278dg.prod.mediav.com:27239 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd288dg.prod.mediav.com:58225 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd278dg.prod.mediav.com:16567 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd255dg.prod.mediav.com:24822 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd293dg.prod.mediav.com:18866 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd282dg.prod.mediav.com:57776 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd289dg.prod.mediav.com:48417 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd281dg.prod.mediav.com:12163 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd284dg.prod.mediav.com:46214 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd260dg.prod.mediav.com:45170 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd254dg.prod.mediav.com:28306 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd275dg.prod.mediav.com:27702 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd260dg.prod.mediav.com:38400 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd285dg.prod.mediav.com:57578 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd291dg.prod.mediav.com:54360 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd288dg.prod.mediav.com:64716 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd271dg.prod.mediav.com:45534 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd270dg.prod.mediav.com:35197 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd275dg.prod.mediav.com:37869 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd260dg.prod.mediav.com:34432 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd284dg.prod.mediav.com:65293 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd291dg.prod.mediav.com:47717 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd263dg.prod.mediav.com:5872 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd286dg.prod.mediav.com:22509 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd266dg.prod.mediav.com:18054 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd292dg.prod.mediav.com:9515 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd290dg.prod.mediav.com:25045 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd257dg.prod.mediav.com:6403 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd286dg.prod.mediav.com:20786 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd274dg.prod.mediav.com:44964 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd266dg.prod.mediav.com:60871 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd264dg.prod.mediav.com:48358 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd266dg.prod.mediav.com:27698 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd270dg.prod.mediav.com:3332 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd284dg.prod.mediav.com:33099 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd284dg.prod.mediav.com:39393 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd277dg.prod.mediav.com:17325 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd274dg.prod.mediav.com:50461 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd282dg.prod.mediav.com:51731 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added rdd_4_22 in memory on hd293dg.prod.mediav.com:25809 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd261dg.prod.mediav.com:17918 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd273dg.prod.mediav.com:6601 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd257dg.prod.mediav.com:13672 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd283dg.prod.mediav.com:62866 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd278dg.prod.mediav.com:27815 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd272dg.prod.mediav.com:47434 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd291dg.prod.mediav.com:3890 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd263dg.prod.mediav.com:19616 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd275dg.prod.mediav.com:3649 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd272dg.prod.mediav.com:63317 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd277dg.prod.mediav.com:27951 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd256dg.prod.mediav.com:54569 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd270dg.prod.mediav.com:33090 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd271dg.prod.mediav.com:63849 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd274dg.prod.mediav.com:58299 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd261dg.prod.mediav.com:35424 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd264dg.prod.mediav.com:49664 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd286dg.prod.mediav.com:5189 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd266dg.prod.mediav.com:7561 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd293dg.prod.mediav.com:46714 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd274dg.prod.mediav.com:25030 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd288dg.prod.mediav.com:15935 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO scheduler.TaskSetManager: Starting task 197.0 in stage 0.0 (TID 197, hd293dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd254dg.prod.mediav.com:6401 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd274dg.prod.mediav.com:16654 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd292dg.prod.mediav.com:8313 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 0.0 (TID 40) in 813 ms on hd293dg.prod.mediav.com (20/200)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd257dg.prod.mediav.com:23974 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd286dg.prod.mediav.com:15531 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd283dg.prod.mediav.com:27671 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd277dg.prod.mediav.com:60781 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd276dg.prod.mediav.com:46848 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd276dg.prod.mediav.com:62943 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd281dg.prod.mediav.com:61667 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd289dg.prod.mediav.com:48937 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd281dg.prod.mediav.com:24432 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd292dg.prod.mediav.com:5167 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd283dg.prod.mediav.com:5777 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd272dg.prod.mediav.com:36193 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd256dg.prod.mediav.com:22567 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd263dg.prod.mediav.com:28737 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd292dg.prod.mediav.com:24852 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd271dg.prod.mediav.com:33037 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd273dg.prod.mediav.com:51855 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd283dg.prod.mediav.com:19668 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd261dg.prod.mediav.com:16786 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd293dg.prod.mediav.com:60417 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd286dg.prod.mediav.com:64184 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd292dg.prod.mediav.com:31959 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd270dg.prod.mediav.com:6944 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd271dg.prod.mediav.com:16160 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd277dg.prod.mediav.com:6547 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd288dg.prod.mediav.com:30986 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd291dg.prod.mediav.com:9728 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd275dg.prod.mediav.com:52956 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd289dg.prod.mediav.com:22644 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd257dg.prod.mediav.com:11991 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd282dg.prod.mediav.com:47145 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd254dg.prod.mediav.com:37252 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd256dg.prod.mediav.com:65180 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd288dg.prod.mediav.com:3459 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd275dg.prod.mediav.com:7744 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd273dg.prod.mediav.com:19800 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd273dg.prod.mediav.com:4400 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd287dg.prod.mediav.com:2547 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd264dg.prod.mediav.com:28057 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd256dg.prod.mediav.com:5214 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd263dg.prod.mediav.com:57200 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd258dg.prod.mediav.com:30778 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd261dg.prod.mediav.com:51708 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd272dg.prod.mediav.com:13836 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd282dg.prod.mediav.com:54259 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd276dg.prod.mediav.com:21872 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd283dg.prod.mediav.com:15357 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd261dg.prod.mediav.com:48502 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd285dg.prod.mediav.com:52469 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd255dg.prod.mediav.com:8336 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd277dg.prod.mediav.com:27612 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd284dg.prod.mediav.com:55335 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd281dg.prod.mediav.com:2780 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd264dg.prod.mediav.com:48038 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd279dg.prod.mediav.com:34009 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd280dg.prod.mediav.com:4198 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd258dg.prod.mediav.com:61558 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd265dg.prod.mediav.com:65261 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd258dg.prod.mediav.com:44194 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd258dg.prod.mediav.com:3750 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd258dg.prod.mediav.com:8532 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd279dg.prod.mediav.com:12678 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd285dg.prod.mediav.com:15892 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd265dg.prod.mediav.com:38596 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd279dg.prod.mediav.com:49996 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd265dg.prod.mediav.com:15593 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd260dg.prod.mediav.com:20879 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd279dg.prod.mediav.com:34291 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd265dg.prod.mediav.com:62939 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd279dg.prod.mediav.com:11149 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd264dg.prod.mediav.com:38843 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd270dg.prod.mediav.com:5548 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd265dg.prod.mediav.com:41028 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd257dg.prod.mediav.com:50217 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd254dg.prod.mediav.com:50339 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd273dg.prod.mediav.com:35935 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added rdd_4_41 in memory on hd262dg.prod.mediav.com:24194 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd256dg.prod.mediav.com:4157 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd272dg.prod.mediav.com:63093 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd280dg.prod.mediav.com:8788 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd285dg.prod.mediav.com:25860 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:34 INFO scheduler.TaskSetManager: Starting task 198.0 in stage 0.0 (TID 198, hd262dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:34 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 0.0 (TID 58) in 1035 ms on hd262dg.prod.mediav.com (21/200)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added rdd_4_131 in memory on hd290dg.prod.mediav.com:1034 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:34 INFO scheduler.TaskSetManager: Starting task 199.0 in stage 0.0 (TID 199, hd290dg.prod.mediav.com, ANY, 1425 bytes)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added rdd_4_106 in memory on hd281dg.prod.mediav.com:2395 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:34 INFO scheduler.TaskSetManager: Finished task 131.0 in stage 0.0 (TID 137) in 1045 ms on hd290dg.prod.mediav.com (22/200)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added rdd_4_186 in memory on hd291dg.prod.mediav.com:31861 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added rdd_4_6 in memory on hd262dg.prod.mediav.com:35030 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:34 INFO storage.BlockManagerInfo: Added rdd_4_152 in memory on hd280dg.prod.mediav.com:1897 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:34 INFO scheduler.TaskSetManager: Finished task 106.0 in stage 0.0 (TID 115) in 1069 ms on hd281dg.prod.mediav.com (23/200)
15/09/15 18:50:34 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 25) in 1095 ms on hd262dg.prod.mediav.com (24/200)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 152.0 in stage 0.0 (TID 155) in 1070 ms on hd280dg.prod.mediav.com (25/200)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 186.0 in stage 0.0 (TID 186) in 1065 ms on hd291dg.prod.mediav.com (26/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_88 in memory on hd262dg.prod.mediav.com:5957 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 88.0 in stage 0.0 (TID 101) in 1129 ms on hd262dg.prod.mediav.com (27/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd255dg.prod.mediav.com:22114 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_13 in memory on hd262dg.prod.mediav.com:40567 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_42 in memory on hd289dg.prod.mediav.com:20447 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_67 in memory on hd266dg.prod.mediav.com:60772 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 0.0 (TID 32) in 1198 ms on hd262dg.prod.mediav.com (28/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_56 in memory on hd276dg.prod.mediav.com:15025 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 0.0 (TID 59) in 1198 ms on hd289dg.prod.mediav.com (29/200)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 67.0 in stage 0.0 (TID 81) in 1201 ms on hd266dg.prod.mediav.com (30/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_91 in memory on hd290dg.prod.mediav.com:48698 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_126 in memory on hd276dg.prod.mediav.com:65222 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 56.0 in stage 0.0 (TID 71) in 1230 ms on hd276dg.prod.mediav.com (31/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_29 in memory on hd287dg.prod.mediav.com:2270 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 91.0 in stage 0.0 (TID 104) in 1231 ms on hd290dg.prod.mediav.com (32/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd278dg.prod.mediav.com:9221 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 126.0 in stage 0.0 (TID 133) in 1232 ms on hd276dg.prod.mediav.com (33/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hd255dg.prod.mediav.com:29011 (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 0.0 (TID 46) in 1265 ms on hd287dg.prod.mediav.com (34/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_1 in memory on hd287dg.prod.mediav.com:53041 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_9 in memory on hd262dg.prod.mediav.com:49994 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 20) in 1298 ms on hd287dg.prod.mediav.com (35/200)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 28) in 1307 ms on hd262dg.prod.mediav.com (36/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_197 in memory on hd293dg.prod.mediav.com:25809 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 197.0 in stage 0.0 (TID 197) in 584 ms on hd293dg.prod.mediav.com (37/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_113 in memory on hd255dg.prod.mediav.com:62882 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 113.0 in stage 0.0 (TID 121) in 1441 ms on hd255dg.prod.mediav.com (38/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_198 in memory on hd262dg.prod.mediav.com:24194 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 198.0 in stage 0.0 (TID 198) in 548 ms on hd262dg.prod.mediav.com (39/200)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd278dg.prod.mediav.com:9221 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd255dg.prod.mediav.com:29011 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd263dg.prod.mediav.com:30335 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd289dg.prod.mediav.com:5555 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hd254dg.prod.mediav.com:14962 (size: 15.9 KB, free: 8.6 GB)
15/09/15 18:50:35 INFO storage.BlockManagerInfo: Added rdd_4_199 in memory on hd290dg.prod.mediav.com:1034 (size: 3.6 MB, free: 8.6 GB)
15/09/15 18:50:35 INFO scheduler.TaskSetManager: Finished task 199.0 in stage 0.0 (TID 199) in 869 ms on hd290dg.prod.mediav.com (40/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_121 in memory on hd290dg.prod.mediav.com:6793 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 121.0 in stage 0.0 (TID 128) in 2687 ms on hd290dg.prod.mediav.com (41/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_59 in memory on hd280dg.prod.mediav.com:6093 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_156 in memory on hd281dg.prod.mediav.com:12163 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 59.0 in stage 0.0 (TID 74) in 2824 ms on hd280dg.prod.mediav.com (42/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_109 in memory on hd280dg.prod.mediav.com:29265 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_65 in memory on hd285dg.prod.mediav.com:7398 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 156.0 in stage 0.0 (TID 159) in 2821 ms on hd281dg.prod.mediav.com (43/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_11 in memory on hd290dg.prod.mediav.com:48176 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 65.0 in stage 0.0 (TID 79) in 2866 ms on hd285dg.prod.mediav.com (44/200)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 109.0 in stage 0.0 (TID 118) in 2864 ms on hd280dg.prod.mediav.com (45/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_17 in memory on hd260dg.prod.mediav.com:7955 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_178 in memory on hd290dg.prod.mediav.com:25045 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 0.0 (TID 30) in 2893 ms on hd290dg.prod.mediav.com (46/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_95 in memory on hd289dg.prod.mediav.com:48417 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_58 in memory on hd287dg.prod.mediav.com:2547 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_57 in memory on hd266dg.prod.mediav.com:7561 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_7 in memory on hd291dg.prod.mediav.com:54360 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 178.0 in stage 0.0 (TID 178) in 2893 ms on hd290dg.prod.mediav.com (47/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_34 in memory on hd288dg.prod.mediav.com:58225 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_89 in memory on hd274dg.prod.mediav.com:44964 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 0.0 (TID 36) in 2933 ms on hd260dg.prod.mediav.com (48/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_28 in memory on hd266dg.prod.mediav.com:18054 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 95.0 in stage 0.0 (TID 108) in 2925 ms on hd289dg.prod.mediav.com (49/200)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 58.0 in stage 0.0 (TID 73) in 2933 ms on hd287dg.prod.mediav.com (50/200)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 57.0 in stage 0.0 (TID 72) in 2933 ms on hd266dg.prod.mediav.com (51/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_20 in memory on hd266dg.prod.mediav.com:27698 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_164 in memory on hd264dg.prod.mediav.com:48358 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_166 in memory on hd293dg.prod.mediav.com:18866 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 89.0 in stage 0.0 (TID 102) in 2949 ms on hd274dg.prod.mediav.com (52/200)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 26) in 2971 ms on hd291dg.prod.mediav.com (53/200)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 0.0 (TID 51) in 2966 ms on hd288dg.prod.mediav.com (54/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_157 in memory on hd281dg.prod.mediav.com:2780 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 0.0 (TID 45) in 2978 ms on hd266dg.prod.mediav.com (55/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_49 in memory on hd293dg.prod.mediav.com:23628 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 0.0 (TID 38) in 2993 ms on hd266dg.prod.mediav.com (56/200)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 0.0 (TID 168) in 2973 ms on hd293dg.prod.mediav.com (57/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_85 in memory on hd286dg.prod.mediav.com:20786 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_5 in memory on hd254dg.prod.mediav.com:28306 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_144 in memory on hd281dg.prod.mediav.com:24432 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 164.0 in stage 0.0 (TID 166) in 2976 ms on hd264dg.prod.mediav.com (58/200)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 157.0 in stage 0.0 (TID 160) in 2992 ms on hd281dg.prod.mediav.com (59/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_87 in memory on hd281dg.prod.mediav.com:61667 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_27 in memory on hd271dg.prod.mediav.com:11439 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_168 in memory on hd257dg.prod.mediav.com:23974 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_45 in memory on hd289dg.prod.mediav.com:22644 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_100 in memory on hd285dg.prod.mediav.com:57578 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_73 in memory on hd278dg.prod.mediav.com:27815 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_64 in memory on hd278dg.prod.mediav.com:27239 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 49.0 in stage 0.0 (TID 64) in 3025 ms on hd293dg.prod.mediav.com (60/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_18 in memory on hd270dg.prod.mediav.com:6944 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 144.0 in stage 0.0 (TID 147) in 3016 ms on hd281dg.prod.mediav.com (61/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_50 in memory on hd291dg.prod.mediav.com:3890 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_127 in memory on hd278dg.prod.mediav.com:1380 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 85.0 in stage 0.0 (TID 98) in 3033 ms on hd286dg.prod.mediav.com (62/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_114 in memory on hd280dg.prod.mediav.com:4198 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_182 in memory on hd254dg.prod.mediav.com:6401 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 24) in 3060 ms on hd254dg.prod.mediav.com (63/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_62 in memory on hd289dg.prod.mediav.com:48937 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_151 in memory on hd277dg.prod.mediav.com:27951 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 87.0 in stage 0.0 (TID 100) in 3050 ms on hd281dg.prod.mediav.com (64/200)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 0.0 (TID 44) in 3066 ms on hd271dg.prod.mediav.com (65/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_55 in memory on hd278dg.prod.mediav.com:16567 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_70 in memory on hd284dg.prod.mediav.com:46214 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_3 in memory on hd292dg.prod.mediav.com:8313 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_191 in memory on hd276dg.prod.mediav.com:46848 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_0 in memory on hd292dg.prod.mediav.com:5167 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_83 in memory on hd288dg.prod.mediav.com:30986 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_82 in memory on hd261dg.prod.mediav.com:16786 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_118 in memory on hd271dg.prod.mediav.com:16160 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_159 in memory on hd292dg.prod.mediav.com:31959 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 100.0 in stage 0.0 (TID 111) in 3058 ms on hd285dg.prod.mediav.com (66/200)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 168.0 in stage 0.0 (TID 170) in 3049 ms on hd257dg.prod.mediav.com (67/200)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 0.0 (TID 37) in 3080 ms on hd270dg.prod.mediav.com (68/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_143 in memory on hd283dg.prod.mediav.com:15357 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 64.0 in stage 0.0 (TID 78) in 3071 ms on hd278dg.prod.mediav.com (69/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_53 in memory on hd275dg.prod.mediav.com:27702 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 0.0 (TID 61) in 3077 ms on hd289dg.prod.mediav.com (70/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_43 in memory on hd291dg.prod.mediav.com:9728 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 73.0 in stage 0.0 (TID 87) in 3073 ms on hd278dg.prod.mediav.com (71/200)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 50.0 in stage 0.0 (TID 65) in 3080 ms on hd291dg.prod.mediav.com (72/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_153 in memory on hd263dg.prod.mediav.com:5872 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_84 in memory on hd266dg.prod.mediav.com:60871 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 114.0 in stage 0.0 (TID 122) in 3071 ms on hd280dg.prod.mediav.com (73/200)
15/09/15 18:50:36 INFO storage.BlockManagerInfo: Added rdd_4_2 in memory on hd288dg.prod.mediav.com:15935 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:36 INFO scheduler.TaskSetManager: Finished task 182.0 in stage 0.0 (TID 182) in 3059 ms on hd254dg.prod.mediav.com (74/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_32 in memory on hd293dg.prod.mediav.com:60417 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_170 in memory on hd263dg.prod.mediav.com:28737 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_25 in memory on hd291dg.prod.mediav.com:47717 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_165 in memory on hd282dg.prod.mediav.com:57776 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_123 in memory on hd272dg.prod.mediav.com:63317 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_108 in memory on hd286dg.prod.mediav.com:22509 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 62.0 in stage 0.0 (TID 76) in 3090 ms on hd289dg.prod.mediav.com (75/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 151.0 in stage 0.0 (TID 154) in 3075 ms on hd277dg.prod.mediav.com (76/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_183 in memory on hd277dg.prod.mediav.com:60781 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_185 in memory on hd254dg.prod.mediav.com:37252 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 127.0 in stage 0.0 (TID 134) in 3084 ms on hd278dg.prod.mediav.com (77/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 70.0 in stage 0.0 (TID 84) in 3095 ms on hd284dg.prod.mediav.com (78/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 82.0 in stage 0.0 (TID 95) in 3096 ms on hd261dg.prod.mediav.com (79/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 83.0 in stage 0.0 (TID 96) in 3097 ms on hd288dg.prod.mediav.com (80/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 55.0 in stage 0.0 (TID 70) in 3103 ms on hd278dg.prod.mediav.com (81/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_158 in memory on hd274dg.prod.mediav.com:50461 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 118.0 in stage 0.0 (TID 125) in 3093 ms on hd271dg.prod.mediav.com (82/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_188 in memory on hd275dg.prod.mediav.com:3649 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_66 in memory on hd292dg.prod.mediav.com:24852 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_79 in memory on hd292dg.prod.mediav.com:9515 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_176 in memory on hd286dg.prod.mediav.com:15531 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_46 in memory on hd282dg.prod.mediav.com:42292 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_99 in memory on hd274dg.prod.mediav.com:25030 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 143.0 in stage 0.0 (TID 146) in 3096 ms on hd283dg.prod.mediav.com (83/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 159.0 in stage 0.0 (TID 162) in 3093 ms on hd292dg.prod.mediav.com (84/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 191.0 in stage 0.0 (TID 191) in 3087 ms on hd276dg.prod.mediav.com (85/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 53.0 in stage 0.0 (TID 68) in 3113 ms on hd275dg.prod.mediav.com (86/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 22) in 3128 ms on hd292dg.prod.mediav.com (87/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_94 in memory on hd284dg.prod.mediav.com:55335 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 0.0 (TID 60) in 3120 ms on hd291dg.prod.mediav.com (88/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 84.0 in stage 0.0 (TID 97) in 3112 ms on hd266dg.prod.mediav.com (89/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_189 in memory on hd277dg.prod.mediav.com:17325 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 19) in 3137 ms on hd292dg.prod.mediav.com (90/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_101 in memory on hd261dg.prod.mediav.com:35424 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_96 in memory on hd260dg.prod.mediav.com:34432 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 21) in 3141 ms on hd288dg.prod.mediav.com (91/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 0.0 (TID 49) in 3133 ms on hd293dg.prod.mediav.com (92/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 183.0 in stage 0.0 (TID 183) in 3105 ms on hd277dg.prod.mediav.com (93/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 123.0 in stage 0.0 (TID 130) in 3117 ms on hd272dg.prod.mediav.com (94/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 170.0 in stage 0.0 (TID 171) in 3112 ms on hd263dg.prod.mediav.com (95/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 153.0 in stage 0.0 (TID 156) in 3115 ms on hd263dg.prod.mediav.com (96/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_141 in memory on hd286dg.prod.mediav.com:5189 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 165.0 in stage 0.0 (TID 167) in 3115 ms on hd282dg.prod.mediav.com (97/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 0.0 (TID 42) in 3143 ms on hd291dg.prod.mediav.com (98/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_135 in memory on hd260dg.prod.mediav.com:45170 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 185.0 in stage 0.0 (TID 185) in 3114 ms on hd254dg.prod.mediav.com (99/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 108.0 in stage 0.0 (TID 117) in 3128 ms on hd286dg.prod.mediav.com (100/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_136 in memory on hd276dg.prod.mediav.com:21872 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 188.0 in stage 0.0 (TID 188) in 3117 ms on hd275dg.prod.mediav.com (101/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 158.0 in stage 0.0 (TID 161) in 3122 ms on hd274dg.prod.mediav.com (102/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_68 in memory on hd293dg.prod.mediav.com:46714 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 79.0 in stage 0.0 (TID 93) in 3144 ms on hd292dg.prod.mediav.com (103/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 66.0 in stage 0.0 (TID 80) in 3147 ms on hd292dg.prod.mediav.com (104/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_147 in memory on hd283dg.prod.mediav.com:27671 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 99.0 in stage 0.0 (TID 110) in 3142 ms on hd274dg.prod.mediav.com (105/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_194 in memory on hd264dg.prod.mediav.com:28057 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 189.0 in stage 0.0 (TID 189) in 3133 ms on hd277dg.prod.mediav.com (106/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 176.0 in stage 0.0 (TID 176) in 3135 ms on hd286dg.prod.mediav.com (107/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 101.0 in stage 0.0 (TID 112) in 3150 ms on hd261dg.prod.mediav.com (108/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 0.0 (TID 62) in 3166 ms on hd282dg.prod.mediav.com (109/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_36 in memory on hd257dg.prod.mediav.com:11991 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_33 in memory on hd274dg.prod.mediav.com:16654 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 94.0 in stage 0.0 (TID 107) in 3159 ms on hd284dg.prod.mediav.com (110/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_37 in memory on hd256dg.prod.mediav.com:5214 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 96.0 in stage 0.0 (TID 109) in 3161 ms on hd260dg.prod.mediav.com (111/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_105 in memory on hd288dg.prod.mediav.com:64716 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_122 in memory on hd280dg.prod.mediav.com:8788 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_40 in memory on hd283dg.prod.mediav.com:5777 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_71 in memory on hd288dg.prod.mediav.com:3459 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_171 in memory on hd272dg.prod.mediav.com:13836 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 141.0 in stage 0.0 (TID 144) in 3165 ms on hd286dg.prod.mediav.com (112/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 135.0 in stage 0.0 (TID 140) in 3167 ms on hd260dg.prod.mediav.com (113/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 136.0 in stage 0.0 (TID 141) in 3167 ms on hd276dg.prod.mediav.com (114/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_173 in memory on hd276dg.prod.mediav.com:62943 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_86 in memory on hd275dg.prod.mediav.com:37869 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 0.0 (TID 82) in 3184 ms on hd293dg.prod.mediav.com (115/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_35 in memory on hd263dg.prod.mediav.com:19616 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_69 in memory on hd264dg.prod.mediav.com:49664 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_30 in memory on hd283dg.prod.mediav.com:62866 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_15 in memory on hd285dg.prod.mediav.com:15892 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 147.0 in stage 0.0 (TID 150) in 3182 ms on hd283dg.prod.mediav.com (116/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 0.0 (TID 50) in 3207 ms on hd274dg.prod.mediav.com (117/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 194.0 in stage 0.0 (TID 194) in 3175 ms on hd264dg.prod.mediav.com (118/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 0.0 (TID 53) in 3210 ms on hd257dg.prod.mediav.com (119/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 0.0 (TID 54) in 3210 ms on hd256dg.prod.mediav.com (120/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_14 in memory on hd274dg.prod.mediav.com:58299 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 0.0 (TID 57) in 3214 ms on hd283dg.prod.mediav.com (121/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_111 in memory on hd264dg.prod.mediav.com:48038 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_132 in memory on hd256dg.prod.mediav.com:65180 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 122.0 in stage 0.0 (TID 129) in 3201 ms on hd280dg.prod.mediav.com (122/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_48 in memory on hd256dg.prod.mediav.com:22567 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_180 in memory on hd285dg.prod.mediav.com:52469 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 105.0 in stage 0.0 (TID 114) in 3208 ms on hd288dg.prod.mediav.com (123/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_21 in memory on hd284dg.prod.mediav.com:39393 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_124 in memory on hd257dg.prod.mediav.com:6403 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 171.0 in stage 0.0 (TID 172) in 3200 ms on hd272dg.prod.mediav.com (124/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 71.0 in stage 0.0 (TID 85) in 3218 ms on hd288dg.prod.mediav.com (125/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 0.0 (TID 99) in 3219 ms on hd275dg.prod.mediav.com (126/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 69.0 in stage 0.0 (TID 83) in 3226 ms on hd264dg.prod.mediav.com (127/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_103 in memory on hd271dg.prod.mediav.com:33037 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_154 in memory on hd283dg.prod.mediav.com:19668 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 0.0 (TID 47) in 3239 ms on hd283dg.prod.mediav.com (128/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 0.0 (TID 52) in 3237 ms on hd263dg.prod.mediav.com (129/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_31 in memory on hd277dg.prod.mediav.com:6547 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_128 in memory on hd260dg.prod.mediav.com:38400 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_93 in memory on hd271dg.prod.mediav.com:45534 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 173.0 in stage 0.0 (TID 174) in 3214 ms on hd276dg.prod.mediav.com (130/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_51 in memory on hd286dg.prod.mediav.com:64184 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 0.0 (TID 34) in 3249 ms on hd285dg.prod.mediav.com (131/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_181 in memory on hd284dg.prod.mediav.com:65293 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_63 in memory on hd270dg.prod.mediav.com:3332 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_54 in memory on hd272dg.prod.mediav.com:36193 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 111.0 in stage 0.0 (TID 120) in 3233 ms on hd264dg.prod.mediav.com (132/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 0.0 (TID 33) in 3255 ms on hd274dg.prod.mediav.com (133/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_116 in memory on hd272dg.prod.mediav.com:47434 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_184 in memory on hd258dg.prod.mediav.com:44194 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 132.0 in stage 0.0 (TID 138) in 3242 ms on hd256dg.prod.mediav.com (134/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 180.0 in stage 0.0 (TID 180) in 3233 ms on hd285dg.prod.mediav.com (135/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 0.0 (TID 39) in 3266 ms on hd284dg.prod.mediav.com (136/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 124.0 in stage 0.0 (TID 131) in 3246 ms on hd257dg.prod.mediav.com (137/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 48.0 in stage 0.0 (TID 63) in 3264 ms on hd256dg.prod.mediav.com (138/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 154.0 in stage 0.0 (TID 157) in 3252 ms on hd283dg.prod.mediav.com (139/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 103.0 in stage 0.0 (TID 113) in 3262 ms on hd271dg.prod.mediav.com (140/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 0.0 (TID 48) in 3279 ms on hd277dg.prod.mediav.com (141/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 128.0 in stage 0.0 (TID 135) in 3260 ms on hd260dg.prod.mediav.com (142/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_125 in memory on hd263dg.prod.mediav.com:57200 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_10 in memory on hd261dg.prod.mediav.com:48502 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_110 in memory on hd284dg.prod.mediav.com:33099 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 51.0 in stage 0.0 (TID 66) in 3280 ms on hd286dg.prod.mediav.com (143/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 181.0 in stage 0.0 (TID 181) in 3261 ms on hd284dg.prod.mediav.com (144/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 54.0 in stage 0.0 (TID 69) in 3286 ms on hd272dg.prod.mediav.com (145/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 63.0 in stage 0.0 (TID 77) in 3283 ms on hd270dg.prod.mediav.com (146/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 93.0 in stage 0.0 (TID 106) in 3278 ms on hd271dg.prod.mediav.com (147/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_74 in memory on hd254dg.prod.mediav.com:50339 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_172 in memory on hd256dg.prod.mediav.com:4157 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 116.0 in stage 0.0 (TID 123) in 3280 ms on hd272dg.prod.mediav.com (148/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_4 in memory on hd258dg.prod.mediav.com:30778 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_155 in memory on hd275dg.prod.mediav.com:7744 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_145 in memory on hd282dg.prod.mediav.com:54259 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_38 in memory on hd260dg.prod.mediav.com:20879 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_195 in memory on hd273dg.prod.mediav.com:51855 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 184.0 in stage 0.0 (TID 184) in 3279 ms on hd258dg.prod.mediav.com (149/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_193 in memory on hd277dg.prod.mediav.com:27612 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 110.0 in stage 0.0 (TID 119) in 3302 ms on hd284dg.prod.mediav.com (150/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_179 in memory on hd282dg.prod.mediav.com:51731 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_130 in memory on hd256dg.prod.mediav.com:54569 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 0.0 (TID 29) in 3326 ms on hd261dg.prod.mediav.com (151/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_78 in memory on hd270dg.prod.mediav.com:35197 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 125.0 in stage 0.0 (TID 132) in 3302 ms on hd263dg.prod.mediav.com (152/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_142 in memory on hd275dg.prod.mediav.com:52956 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 172.0 in stage 0.0 (TID 173) in 3304 ms on hd256dg.prod.mediav.com (153/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 74.0 in stage 0.0 (TID 88) in 3322 ms on hd254dg.prod.mediav.com (154/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_61 in memory on hd270dg.prod.mediav.com:5548 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 23) in 3349 ms on hd258dg.prod.mediav.com (155/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_8 in memory on hd282dg.prod.mediav.com:47145 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 0.0 (TID 55) in 3343 ms on hd260dg.prod.mediav.com (156/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_75 in memory on hd264dg.prod.mediav.com:38843 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 155.0 in stage 0.0 (TID 158) in 3322 ms on hd275dg.prod.mediav.com (157/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 145.0 in stage 0.0 (TID 148) in 3328 ms on hd282dg.prod.mediav.com (158/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 195.0 in stage 0.0 (TID 195) in 3320 ms on hd273dg.prod.mediav.com (159/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 130.0 in stage 0.0 (TID 136) in 3332 ms on hd256dg.prod.mediav.com (160/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 193.0 in stage 0.0 (TID 193) in 3323 ms on hd277dg.prod.mediav.com (161/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_192 in memory on hd258dg.prod.mediav.com:61558 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_196 in memory on hd258dg.prod.mediav.com:8532 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 142.0 in stage 0.0 (TID 145) in 3337 ms on hd275dg.prod.mediav.com (162/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 179.0 in stage 0.0 (TID 179) in 3333 ms on hd282dg.prod.mediav.com (163/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 78.0 in stage 0.0 (TID 92) in 3353 ms on hd270dg.prod.mediav.com (164/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_90 in memory on hd273dg.prod.mediav.com:4400 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_117 in memory on hd279dg.prod.mediav.com:34009 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_119 in memory on hd279dg.prod.mediav.com:49996 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 61.0 in stage 0.0 (TID 75) in 3370 ms on hd270dg.prod.mediav.com (165/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 27) in 3389 ms on hd282dg.prod.mediav.com (166/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 75.0 in stage 0.0 (TID 89) in 3373 ms on hd264dg.prod.mediav.com (167/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_16 in memory on hd285dg.prod.mediav.com:25860 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_162 in memory on hd258dg.prod.mediav.com:3750 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_146 in memory on hd273dg.prod.mediav.com:19800 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 192.0 in stage 0.0 (TID 192) in 3368 ms on hd258dg.prod.mediav.com (168/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 196.0 in stage 0.0 (TID 196) in 3367 ms on hd258dg.prod.mediav.com (169/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_120 in memory on hd273dg.prod.mediav.com:35935 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 90.0 in stage 0.0 (TID 103) in 3394 ms on hd273dg.prod.mediav.com (170/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_39 in memory on hd279dg.prod.mediav.com:11149 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_52 in memory on hd257dg.prod.mediav.com:13672 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 0.0 (TID 124) in 3400 ms on hd279dg.prod.mediav.com (171/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 0.0 (TID 126) in 3402 ms on hd279dg.prod.mediav.com (172/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 0.0 (TID 35) in 3432 ms on hd285dg.prod.mediav.com (173/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_138 in memory on hd272dg.prod.mediav.com:63093 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 146.0 in stage 0.0 (TID 149) in 3415 ms on hd273dg.prod.mediav.com (174/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_167 in memory on hd265dg.prod.mediav.com:41028 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 162.0 in stage 0.0 (TID 164) in 3411 ms on hd258dg.prod.mediav.com (175/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 120.0 in stage 0.0 (TID 127) in 3434 ms on hd273dg.prod.mediav.com (176/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 0.0 (TID 56) in 3450 ms on hd279dg.prod.mediav.com (177/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 0.0 (TID 67) in 3451 ms on hd257dg.prod.mediav.com (178/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_148 in memory on hd279dg.prod.mediav.com:34291 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_77 in memory on hd279dg.prod.mediav.com:12678 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_150 in memory on hd265dg.prod.mediav.com:65261 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 138.0 in stage 0.0 (TID 142) in 3457 ms on hd272dg.prod.mediav.com (179/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 167.0 in stage 0.0 (TID 169) in 3451 ms on hd265dg.prod.mediav.com (180/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_76 in memory on hd261dg.prod.mediav.com:51708 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_81 in memory on hd265dg.prod.mediav.com:38596 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 148.0 in stage 0.0 (TID 151) in 3475 ms on hd279dg.prod.mediav.com (181/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 77.0 in stage 0.0 (TID 91) in 3490 ms on hd279dg.prod.mediav.com (182/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_107 in memory on hd261dg.prod.mediav.com:17918 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 76.0 in stage 0.0 (TID 90) in 3508 ms on hd261dg.prod.mediav.com (183/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 150.0 in stage 0.0 (TID 153) in 3497 ms on hd265dg.prod.mediav.com (184/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_24 in memory on hd271dg.prod.mediav.com:63849 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 81.0 in stage 0.0 (TID 94) in 3515 ms on hd265dg.prod.mediav.com (185/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_12 in memory on hd265dg.prod.mediav.com:15593 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 107.0 in stage 0.0 (TID 116) in 3528 ms on hd261dg.prod.mediav.com (186/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 0.0 (TID 41) in 3572 ms on hd271dg.prod.mediav.com (187/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_160 in memory on hd257dg.prod.mediav.com:50217 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 0.0 (TID 31) in 3585 ms on hd265dg.prod.mediav.com (188/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 160.0 in stage 0.0 (TID 163) in 3596 ms on hd257dg.prod.mediav.com (189/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_177 in memory on hd273dg.prod.mediav.com:6601 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 177.0 in stage 0.0 (TID 177) in 3655 ms on hd273dg.prod.mediav.com (190/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_149 in memory on hd255dg.prod.mediav.com:22114 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_187 in memory on hd289dg.prod.mediav.com:5555 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 149.0 in stage 0.0 (TID 152) in 3760 ms on hd255dg.prod.mediav.com (191/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 187.0 in stage 0.0 (TID 187) in 3794 ms on hd289dg.prod.mediav.com (192/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_26 in memory on hd278dg.prod.mediav.com:9221 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_190 in memory on hd255dg.prod.mediav.com:24822 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 0.0 (TID 43) in 3876 ms on hd278dg.prod.mediav.com (193/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 190.0 in stage 0.0 (TID 190) in 3848 ms on hd255dg.prod.mediav.com (194/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_140 in memory on hd270dg.prod.mediav.com:33090 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_163 in memory on hd263dg.prod.mediav.com:30335 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_72 in memory on hd265dg.prod.mediav.com:62939 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 140.0 in stage 0.0 (TID 143) in 3971 ms on hd270dg.prod.mediav.com (195/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 163.0 in stage 0.0 (TID 165) in 3966 ms on hd263dg.prod.mediav.com (196/200)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 72.0 in stage 0.0 (TID 86) in 4026 ms on hd265dg.prod.mediav.com (197/200)
15/09/15 18:50:37 INFO storage.BlockManagerInfo: Added rdd_4_175 in memory on hd255dg.prod.mediav.com:8336 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:37 INFO scheduler.TaskSetManager: Finished task 175.0 in stage 0.0 (TID 175) in 4056 ms on hd255dg.prod.mediav.com (198/200)
15/09/15 18:50:38 INFO storage.BlockManagerInfo: Added rdd_4_92 in memory on hd254dg.prod.mediav.com:14962 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:38 INFO scheduler.TaskSetManager: Finished task 92.0 in stage 0.0 (TID 105) in 4159 ms on hd254dg.prod.mediav.com (199/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_4_134 in memory on hd255dg.prod.mediav.com:29011 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 134.0 in stage 0.0 (TID 139) in 5506 ms on hd255dg.prod.mediav.com (200/200)
15/09/15 18:50:39 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at MLUtils.scala:95) finished in 15.745 s
15/09/15 18:50:39 INFO cluster.YarnClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool default
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Job 0 finished: reduce at MLUtils.scala:95, took 19.436039 s
15/09/15 18:50:39 INFO gbdt.GBDTModelTrainingJob: Number of Iteration: 3
15/09/15 18:50:39 INFO gbdt.GBDTModelTrainingJob: Max Depth: 3
15/09/15 18:50:39 INFO gbdt.GBDTModelTrainingJob: ======================== GBDT Training Begin ===============================
15/09/15 18:50:39 INFO spark.SparkContext: Starting job: take at DecisionTreeMetadata.scala:110
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Got job 1 (take at DecisionTreeMetadata.scala:110) with 1 output partitions (allowLocal=true)
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Final stage: ResultStage 1(take at DecisionTreeMetadata.scala:110)
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Parents of final stage: List()
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Missing parents: List()
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at retag at RandomForest.scala:137), which has no missing parents
15/09/15 18:50:39 INFO storage.MemoryStore: ensureFreeSpace(4312) called with curMem=202018, maxMem=18438322913
15/09/15 18:50:39 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.2 KB, free 17.2 GB)
15/09/15 18:50:39 INFO storage.MemoryStore: ensureFreeSpace(2255) called with curMem=206330, maxMem=18438322913
15/09/15 18:50:39 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 17.2 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.47.1.38:18554 (size: 2.2 KB, free: 17.2 GB)
15/09/15 18:50:39 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at retag at RandomForest.scala:137)
15/09/15 18:50:39 INFO cluster.YarnClusterScheduler: Adding task set 1.0 with 1 tasks
15/09/15 18:50:39 INFO scheduler.FairSchedulableBuilder: Added task set TaskSet_1 tasks to pool default
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 200, hd292dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on hd292dg.prod.mediav.com:5167 (size: 2.2 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on hd292dg.prod.mediav.com:5167 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 200) in 181 ms on hd292dg.prod.mediav.com (1/1)
15/09/15 18:50:39 INFO cluster.YarnClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool default
15/09/15 18:50:39 INFO scheduler.DAGScheduler: ResultStage 1 (take at DecisionTreeMetadata.scala:110) finished in 0.182 s
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Job 1 finished: take at DecisionTreeMetadata.scala:110, took 0.201751 s
15/09/15 18:50:39 INFO spark.SparkContext: Starting job: count at DecisionTreeMetadata.scala:111
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Got job 2 (count at DecisionTreeMetadata.scala:111) with 200 output partitions (allowLocal=false)
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Final stage: ResultStage 2(count at DecisionTreeMetadata.scala:111)
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Parents of final stage: List()
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Missing parents: List()
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at retag at RandomForest.scala:137), which has no missing parents
15/09/15 18:50:39 INFO storage.MemoryStore: ensureFreeSpace(4152) called with curMem=208585, maxMem=18438322913
15/09/15 18:50:39 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.1 KB, free 17.2 GB)
15/09/15 18:50:39 INFO storage.MemoryStore: ensureFreeSpace(2171) called with curMem=212737, maxMem=18438322913
15/09/15 18:50:39 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 17.2 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.47.1.38:18554 (size: 2.1 KB, free: 17.2 GB)
15/09/15 18:50:39 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:874
15/09/15 18:50:39 INFO scheduler.DAGScheduler: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at retag at RandomForest.scala:137)
15/09/15 18:50:39 INFO cluster.YarnClusterScheduler: Adding task set 2.0 with 200 tasks
15/09/15 18:50:39 INFO scheduler.FairSchedulableBuilder: Added task set TaskSet_2 tasks to pool default
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 56.0 in stage 2.0 (TID 201, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 106.0 in stage 2.0 (TID 202, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 2.0 (TID 203, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 165.0 in stage 2.0 (TID 204, hd282dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 68.0 in stage 2.0 (TID 205, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 54.0 in stage 2.0 (TID 206, hd272dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 105.0 in stage 2.0 (TID 207, hd288dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 157.0 in stage 2.0 (TID 208, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 2.0 (TID 209, hd256dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 2.0 (TID 210, hd288dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 2.0 (TID 211, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 135.0 in stage 2.0 (TID 212, hd260dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 172.0 in stage 2.0 (TID 213, hd256dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 2.0 (TID 214, hd271dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 2.0 (TID 215, hd283dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 193.0 in stage 2.0 (TID 216, hd277dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 164.0 in stage 2.0 (TID 217, hd264dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 123.0 in stage 2.0 (TID 218, hd272dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 2.0 (TID 219, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 2.0 (TID 220, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 86.0 in stage 2.0 (TID 221, hd275dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 2.0 (TID 222, hd270dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 2.0 (TID 223, hd283dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 2.0 (TID 224, hd289dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 150.0 in stage 2.0 (TID 225, hd265dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 2.0 (TID 226, hd274dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 70.0 in stage 2.0 (TID 227, hd284dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 2.0 (TID 228, hd277dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 128.0 in stage 2.0 (TID 229, hd260dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 177.0 in stage 2.0 (TID 230, hd273dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 173.0 in stage 2.0 (TID 231, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 91.0 in stage 2.0 (TID 232, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 84.0 in stage 2.0 (TID 233, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 156.0 in stage 2.0 (TID 234, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 2.0 (TID 235, hd265dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 109.0 in stage 2.0 (TID 236, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 2.0 (TID 237, hd263dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 131.0 in stage 2.0 (TID 238, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 66.0 in stage 2.0 (TID 239, hd292dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 179.0 in stage 2.0 (TID 240, hd282dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 132.0 in stage 2.0 (TID 241, hd256dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 2.0 (TID 242, hd278dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 48.0 in stage 2.0 (TID 243, hd256dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 160.0 in stage 2.0 (TID 244, hd257dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 107.0 in stage 2.0 (TID 245, hd261dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 89.0 in stage 2.0 (TID 246, hd274dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 126.0 in stage 2.0 (TID 247, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 116.0 in stage 2.0 (TID 248, hd272dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 78.0 in stage 2.0 (TID 249, hd270dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 65.0 in stage 2.0 (TID 250, hd285dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 2.0 (TID 251, hd285dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 122.0 in stage 2.0 (TID 252, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 113.0 in stage 2.0 (TID 253, hd255dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 2.0 (TID 254, hd291dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 159.0 in stage 2.0 (TID 255, hd292dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 103.0 in stage 2.0 (TID 256, hd271dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 154.0 in stage 2.0 (TID 257, hd283dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 95.0 in stage 2.0 (TID 258, hd289dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 146.0 in stage 2.0 (TID 259, hd273dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 71.0 in stage 2.0 (TID 260, hd288dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 136.0 in stage 2.0 (TID 261, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 2.0 (TID 262, hd260dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 88.0 in stage 2.0 (TID 263, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 2.0 (TID 264, hd291dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 191.0 in stage 2.0 (TID 265, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 2.0 (TID 266, hd287dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 267, hd292dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 85.0 in stage 2.0 (TID 268, hd286dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 175.0 in stage 2.0 (TID 269, hd255dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 99.0 in stage 2.0 (TID 270, hd274dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 81.0 in stage 2.0 (TID 271, hd265dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 170.0 in stage 2.0 (TID 272, hd263dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 2.0 (TID 273, hd271dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 2.0 (TID 274, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 147.0 in stage 2.0 (TID 275, hd283dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 183.0 in stage 2.0 (TID 276, hd277dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 178.0 in stage 2.0 (TID 277, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 168.0 in stage 2.0 (TID 278, hd257dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 279, hd292dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 100.0 in stage 2.0 (TID 280, hd285dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 2.0 (TID 281, hd279dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 2.0 (TID 282, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 51.0 in stage 2.0 (TID 283, hd286dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 171.0 in stage 2.0 (TID 284, hd272dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 92.0 in stage 2.0 (TID 285, hd254dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 82.0 in stage 2.0 (TID 286, hd261dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 124.0 in stage 2.0 (TID 287, hd257dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 182.0 in stage 2.0 (TID 288, hd254dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 2.0 (TID 289, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 73.0 in stage 2.0 (TID 290, hd278dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 2.0 (TID 291, hd261dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 162.0 in stage 2.0 (TID 292, hd258dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 190.0 in stage 2.0 (TID 293, hd255dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 188.0 in stage 2.0 (TID 294, hd275dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 194.0 in stage 2.0 (TID 295, hd264dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 187.0 in stage 2.0 (TID 296, hd289dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 97.0 in stage 2.0 (TID 297, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 96.0 in stage 2.0 (TID 298, hd260dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 52.0 in stage 2.0 (TID 299, hd257dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 63.0 in stage 2.0 (TID 300, hd270dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 145.0 in stage 2.0 (TID 301, hd282dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 142.0 in stage 2.0 (TID 302, hd275dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 192.0 in stage 2.0 (TID 303, hd258dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 76.0 in stage 2.0 (TID 304, hd261dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 53.0 in stage 2.0 (TID 305, hd275dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 141.0 in stage 2.0 (TID 306, hd286dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 87.0 in stage 2.0 (TID 307, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 77.0 in stage 2.0 (TID 308, hd279dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 49.0 in stage 2.0 (TID 309, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 57.0 in stage 2.0 (TID 310, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 130.0 in stage 2.0 (TID 311, hd256dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 94.0 in stage 2.0 (TID 312, hd284dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 72.0 in stage 2.0 (TID 313, hd265dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 2.0 (TID 314, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 120.0 in stage 2.0 (TID 315, hd273dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 163.0 in stage 2.0 (TID 316, hd263dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 2.0 (TID 317, hd291dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 148.0 in stage 2.0 (TID 318, hd279dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 2.0 (TID 319, hd285dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 101.0 in stage 2.0 (TID 320, hd261dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 143.0 in stage 2.0 (TID 321, hd283dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 125.0 in stage 2.0 (TID 322, hd263dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 2.0 (TID 323, hd284dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 79.0 in stage 2.0 (TID 324, hd292dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 74.0 in stage 2.0 (TID 325, hd254dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 110.0 in stage 2.0 (TID 326, hd284dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 184.0 in stage 2.0 (TID 327, hd258dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 69.0 in stage 2.0 (TID 328, hd264dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 2.0 (TID 329, hd258dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 67.0 in stage 2.0 (TID 330, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 140.0 in stage 2.0 (TID 331, hd270dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 118.0 in stage 2.0 (TID 332, hd271dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 50.0 in stage 2.0 (TID 333, hd291dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 108.0 in stage 2.0 (TID 334, hd286dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 2.0 (TID 335, hd260dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 180.0 in stage 2.0 (TID 336, hd285dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 119.0 in stage 2.0 (TID 337, hd279dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 2.0 (TID 338, hd282dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 339, hd287dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 138.0 in stage 2.0 (TID 340, hd272dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 2.0 (TID 341, hd274dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 149.0 in stage 2.0 (TID 342, hd255dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 2.0 (TID 343, hd289dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 195.0 in stage 2.0 (TID 344, hd273dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 121.0 in stage 2.0 (TID 345, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 93.0 in stage 2.0 (TID 346, hd271dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 166.0 in stage 2.0 (TID 347, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 59.0 in stage 2.0 (TID 348, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 2.0 (TID 349, hd291dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 350, hd288dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 75.0 in stage 2.0 (TID 351, hd264dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 58.0 in stage 2.0 (TID 352, hd287dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 176.0 in stage 2.0 (TID 353, hd286dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 158.0 in stage 2.0 (TID 354, hd274dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 196.0 in stage 2.0 (TID 355, hd258dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 64.0 in stage 2.0 (TID 356, hd278dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 144.0 in stage 2.0 (TID 357, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 83.0 in stage 2.0 (TID 358, hd288dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 62.0 in stage 2.0 (TID 359, hd289dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 185.0 in stage 2.0 (TID 360, hd254dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 167.0 in stage 2.0 (TID 361, hd265dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 2.0 (TID 362, hd257dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 153.0 in stage 2.0 (TID 363, hd263dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 134.0 in stage 2.0 (TID 364, hd255dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 90.0 in stage 2.0 (TID 365, hd273dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 111.0 in stage 2.0 (TID 366, hd264dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 151.0 in stage 2.0 (TID 367, hd277dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 61.0 in stage 2.0 (TID 368, hd270dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 117.0 in stage 2.0 (TID 369, hd279dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 2.0 (TID 370, hd282dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 55.0 in stage 2.0 (TID 371, hd278dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 189.0 in stage 2.0 (TID 372, hd277dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 114.0 in stage 2.0 (TID 373, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 2.0 (TID 374, hd254dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 181.0 in stage 2.0 (TID 375, hd284dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 155.0 in stage 2.0 (TID 376, hd275dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 2.0 (TID 377, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 127.0 in stage 2.0 (TID 378, hd278dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd288dg.prod.mediav.com:64716 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd281dg.prod.mediav.com:2780 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd261dg.prod.mediav.com:48502 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd263dg.prod.mediav.com:19616 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd261dg.prod.mediav.com:17918 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd290dg.prod.mediav.com:1034 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd270dg.prod.mediav.com:6944 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd272dg.prod.mediav.com:47434 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd262dg.prod.mediav.com:49994 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd293dg.prod.mediav.com:25809 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd272dg.prod.mediav.com:36193 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd256dg.prod.mediav.com:5214 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd281dg.prod.mediav.com:2395 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd260dg.prod.mediav.com:34432 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd290dg.prod.mediav.com:48698 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd262dg.prod.mediav.com:40567 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd271dg.prod.mediav.com:33037 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd280dg.prod.mediav.com:29265 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd276dg.prod.mediav.com:62943 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd283dg.prod.mediav.com:5777 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd262dg.prod.mediav.com:35030 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd256dg.prod.mediav.com:65180 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd287dg.prod.mediav.com:2270 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd261dg.prod.mediav.com:35424 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd288dg.prod.mediav.com:58225 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd255dg.prod.mediav.com:24822 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd293dg.prod.mediav.com:18866 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd286dg.prod.mediav.com:5189 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd271dg.prod.mediav.com:11439 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd278dg.prod.mediav.com:27239 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd292dg.prod.mediav.com:5167 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd254dg.prod.mediav.com:28306 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd291dg.prod.mediav.com:47717 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd280dg.prod.mediav.com:8788 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd278dg.prod.mediav.com:1380 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd257dg.prod.mediav.com:23974 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd263dg.prod.mediav.com:5872 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd258dg.prod.mediav.com:8532 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd273dg.prod.mediav.com:4400 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 279) in 128 ms on hd292dg.prod.mediav.com (1/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_22 in memory on hd293dg.prod.mediav.com:25809 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_131 in memory on hd290dg.prod.mediav.com:1034 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_96 in memory on hd260dg.prod.mediav.com:34432 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_91 in memory on hd290dg.prod.mediav.com:48698 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 80.0 in stage 2.0 (TID 379, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_9 in memory on hd262dg.prod.mediav.com:49994 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 2.0 (TID 220) in 199 ms on hd293dg.prod.mediav.com (2/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_18 in memory on hd270dg.prod.mediav.com:6944 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_105 in memory on hd288dg.prod.mediav.com:64716 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_107 in memory on hd261dg.prod.mediav.com:17918 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_35 in memory on hd263dg.prod.mediav.com:19616 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 133.0 in stage 2.0 (TID 380, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 131.0 in stage 2.0 (TID 238) in 200 ms on hd290dg.prod.mediav.com (3/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 102.0 in stage 2.0 (TID 381, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_37 in memory on hd256dg.prod.mediav.com:5214 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 91.0 in stage 2.0 (TID 232) in 205 ms on hd290dg.prod.mediav.com (4/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_27 in memory on hd271dg.prod.mediav.com:11439 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_141 in memory on hd286dg.prod.mediav.com:5189 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 2.0 (TID 382, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_157 in memory on hd281dg.prod.mediav.com:2780 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_166 in memory on hd293dg.prod.mediav.com:18866 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 2.0 (TID 274) in 199 ms on hd262dg.prod.mediav.com (5/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_190 in memory on hd255dg.prod.mediav.com:24822 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 96.0 in stage 2.0 (TID 298) in 195 ms on hd260dg.prod.mediav.com (6/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_122 in memory on hd280dg.prod.mediav.com:8788 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_103 in memory on hd271dg.prod.mediav.com:33037 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_168 in memory on hd257dg.prod.mediav.com:23974 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_54 in memory on hd272dg.prod.mediav.com:36193 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 2.0 (TID 222) in 214 ms on hd270dg.prod.mediav.com (7/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 105.0 in stage 2.0 (TID 207) in 220 ms on hd288dg.prod.mediav.com (8/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 107.0 in stage 2.0 (TID 245) in 210 ms on hd261dg.prod.mediav.com (9/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_5 in memory on hd254dg.prod.mediav.com:28306 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_40 in memory on hd283dg.prod.mediav.com:5777 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_173 in memory on hd276dg.prod.mediav.com:62943 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 2.0 (TID 237) in 214 ms on hd263dg.prod.mediav.com (10/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 2.0 (TID 209) in 222 ms on hd256dg.prod.mediav.com (11/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_90 in memory on hd273dg.prod.mediav.com:4400 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_10 in memory on hd261dg.prod.mediav.com:48502 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 157.0 in stage 2.0 (TID 208) in 227 ms on hd281dg.prod.mediav.com (12/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_101 in memory on hd261dg.prod.mediav.com:35424 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_132 in memory on hd256dg.prod.mediav.com:65180 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 190.0 in stage 2.0 (TID 293) in 209 ms on hd255dg.prod.mediav.com (13/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 2.0 (TID 273) in 212 ms on hd271dg.prod.mediav.com (14/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 2.0 (TID 347) in 201 ms on hd293dg.prod.mediav.com (15/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 141.0 in stage 2.0 (TID 306) in 208 ms on hd286dg.prod.mediav.com (16/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_34 in memory on hd288dg.prod.mediav.com:58225 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_153 in memory on hd263dg.prod.mediav.com:5872 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 122.0 in stage 2.0 (TID 252) in 218 ms on hd280dg.prod.mediav.com (17/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_127 in memory on hd278dg.prod.mediav.com:1380 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_64 in memory on hd278dg.prod.mediav.com:27239 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_106 in memory on hd281dg.prod.mediav.com:2395 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 54.0 in stage 2.0 (TID 206) in 232 ms on hd272dg.prod.mediav.com (18/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 103.0 in stage 2.0 (TID 256) in 219 ms on hd271dg.prod.mediav.com (19/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 168.0 in stage 2.0 (TID 278) in 216 ms on hd257dg.prod.mediav.com (20/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 2.0 (TID 215) in 230 ms on hd283dg.prod.mediav.com (21/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd275dg.prod.mediav.com:37869 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 2.0 (TID 374) in 203 ms on hd254dg.prod.mediav.com (22/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 90.0 in stage 2.0 (TID 365) in 205 ms on hd273dg.prod.mediav.com (23/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_6 in memory on hd262dg.prod.mediav.com:35030 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 173.0 in stage 2.0 (TID 231) in 228 ms on hd276dg.prod.mediav.com (24/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_13 in memory on hd262dg.prod.mediav.com:40567 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_25 in memory on hd291dg.prod.mediav.com:47717 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_109 in memory on hd280dg.prod.mediav.com:29265 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 132.0 in stage 2.0 (TID 241) in 231 ms on hd256dg.prod.mediav.com (25/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_29 in memory on hd287dg.prod.mediav.com:2270 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 64.0 in stage 2.0 (TID 356) in 214 ms on hd278dg.prod.mediav.com (26/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 101.0 in stage 2.0 (TID 320) in 220 ms on hd261dg.prod.mediav.com (27/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_116 in memory on hd272dg.prod.mediav.com:47434 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 2.0 (TID 291) in 225 ms on hd261dg.prod.mediav.com (28/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 127.0 in stage 2.0 (TID 378) in 212 ms on hd278dg.prod.mediav.com (29/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_80 in memory on hd293dg.prod.mediav.com:25809 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 137.0 in stage 2.0 (TID 383, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 2.0 (TID 210) in 245 ms on hd288dg.prod.mediav.com (30/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 106.0 in stage 2.0 (TID 202) in 249 ms on hd281dg.prod.mediav.com (31/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 153.0 in stage 2.0 (TID 363) in 217 ms on hd263dg.prod.mediav.com (32/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 112.0 in stage 2.0 (TID 384, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 2.0 (TID 385, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 2.0 (TID 264) in 236 ms on hd291dg.prod.mediav.com (33/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 2.0 (TID 203) in 251 ms on hd262dg.prod.mediav.com (34/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 2.0 (TID 314) in 229 ms on hd262dg.prod.mediav.com (35/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 109.0 in stage 2.0 (TID 236) in 242 ms on hd280dg.prod.mediav.com (36/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 60.0 in stage 2.0 (TID 386, hd287dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 174.0 in stage 2.0 (TID 387, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 2.0 (TID 266) in 240 ms on hd287dg.prod.mediav.com (37/200)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 80.0 in stage 2.0 (TID 379) in 53 ms on hd293dg.prod.mediav.com (38/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_133 in memory on hd290dg.prod.mediav.com:1034 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Finished task 116.0 in stage 2.0 (TID 248) in 246 ms on hd272dg.prod.mediav.com (39/200)
15/09/15 18:50:39 INFO storage.BlockManagerInfo: Added rdd_7_44 in memory on hd262dg.prod.mediav.com:49994 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:39 INFO scheduler.TaskSetManager: Starting task 199.0 in stage 2.0 (TID 388, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 133.0 in stage 2.0 (TID 380) in 60 ms on hd290dg.prod.mediav.com (40/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 2.0 (TID 382) in 56 ms on hd262dg.prod.mediav.com (41/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_102 in memory on hd290dg.prod.mediav.com:48698 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 102.0 in stage 2.0 (TID 381) in 72 ms on hd290dg.prod.mediav.com (42/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_112 in memory on hd262dg.prod.mediav.com:40567 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd260dg.prod.mediav.com:45170 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_23 in memory on hd262dg.prod.mediav.com:35030 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_137 in memory on hd281dg.prod.mediav.com:2395 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_196 in memory on hd258dg.prod.mediav.com:8532 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_174 in memory on hd293dg.prod.mediav.com:25809 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_60 in memory on hd287dg.prod.mediav.com:2270 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 112.0 in stage 2.0 (TID 384) in 56 ms on hd262dg.prod.mediav.com (43/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 2.0 (TID 385) in 56 ms on hd262dg.prod.mediav.com (44/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 2.0 (TID 383) in 62 ms on hd281dg.prod.mediav.com (45/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_199 in memory on hd290dg.prod.mediav.com:1034 (size: 3.6 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Starting task 197.0 in stage 2.0 (TID 389, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 196.0 in stage 2.0 (TID 355) in 285 ms on hd258dg.prod.mediav.com (46/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 60.0 in stage 2.0 (TID 386) in 62 ms on hd287dg.prod.mediav.com (47/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 174.0 in stage 2.0 (TID 387) in 62 ms on hd293dg.prod.mediav.com (48/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd283dg.prod.mediav.com:27671 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd289dg.prod.mediav.com:5555 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd285dg.prod.mediav.com:7398 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd291dg.prod.mediav.com:54360 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd255dg.prod.mediav.com:8336 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 199.0 in stage 2.0 (TID 388) in 59 ms on hd290dg.prod.mediav.com (49/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd282dg.prod.mediav.com:42292 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_86 in memory on hd275dg.prod.mediav.com:37869 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 2.0 (TID 221) in 345 ms on hd275dg.prod.mediav.com (50/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_197 in memory on hd293dg.prod.mediav.com:25809 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 197.0 in stage 2.0 (TID 389) in 65 ms on hd293dg.prod.mediav.com (51/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_135 in memory on hd260dg.prod.mediav.com:45170 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_65 in memory on hd285dg.prod.mediav.com:7398 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 135.0 in stage 2.0 (TID 212) in 412 ms on hd260dg.prod.mediav.com (52/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_147 in memory on hd283dg.prod.mediav.com:27671 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_187 in memory on hd289dg.prod.mediav.com:5555 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_7 in memory on hd291dg.prod.mediav.com:54360 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 65.0 in stage 2.0 (TID 250) in 415 ms on hd285dg.prod.mediav.com (53/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 147.0 in stage 2.0 (TID 275) in 411 ms on hd283dg.prod.mediav.com (54/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_46 in memory on hd282dg.prod.mediav.com:42292 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 2.0 (TID 254) in 423 ms on hd291dg.prod.mediav.com (55/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 187.0 in stage 2.0 (TID 296) in 416 ms on hd289dg.prod.mediav.com (56/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_175 in memory on hd255dg.prod.mediav.com:8336 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 2.0 (TID 370) in 418 ms on hd282dg.prod.mediav.com (57/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 175.0 in stage 2.0 (TID 269) in 440 ms on hd255dg.prod.mediav.com (58/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd281dg.prod.mediav.com:12163 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_156 in memory on hd281dg.prod.mediav.com:12163 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 156.0 in stage 2.0 (TID 234) in 836 ms on hd281dg.prod.mediav.com (59/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd282dg.prod.mediav.com:57776 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd284dg.prod.mediav.com:46214 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd272dg.prod.mediav.com:63317 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd262dg.prod.mediav.com:5957 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd283dg.prod.mediav.com:62866 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd261dg.prod.mediav.com:16786 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd293dg.prod.mediav.com:46714 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd283dg.prod.mediav.com:19668 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd289dg.prod.mediav.com:22644 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd290dg.prod.mediav.com:48176 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd289dg.prod.mediav.com:48417 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd254dg.prod.mediav.com:6401 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd288dg.prod.mediav.com:3459 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd261dg.prod.mediav.com:51708 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd256dg.prod.mediav.com:4157 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd293dg.prod.mediav.com:60417 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd276dg.prod.mediav.com:15025 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd283dg.prod.mediav.com:15357 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd266dg.prod.mediav.com:60871 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd263dg.prod.mediav.com:30335 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd263dg.prod.mediav.com:28737 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd292dg.prod.mediav.com:9515 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd276dg.prod.mediav.com:46848 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd284dg.prod.mediav.com:33099 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd270dg.prod.mediav.com:33090 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd273dg.prod.mediav.com:35935 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd280dg.prod.mediav.com:6093 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd257dg.prod.mediav.com:50217 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd285dg.prod.mediav.com:52469 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd273dg.prod.mediav.com:51855 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd279dg.prod.mediav.com:49996 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd266dg.prod.mediav.com:60772 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd274dg.prod.mediav.com:16654 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd291dg.prod.mediav.com:9728 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd288dg.prod.mediav.com:30986 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd266dg.prod.mediav.com:7561 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd279dg.prod.mediav.com:34009 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd288dg.prod.mediav.com:15935 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd254dg.prod.mediav.com:37252 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd270dg.prod.mediav.com:5548 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd275dg.prod.mediav.com:7744 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd265dg.prod.mediav.com:41028 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd280dg.prod.mediav.com:4198 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd274dg.prod.mediav.com:50461 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd289dg.prod.mediav.com:48937 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd266dg.prod.mediav.com:18054 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd277dg.prod.mediav.com:17325 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd284dg.prod.mediav.com:65293 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd264dg.prod.mediav.com:48038 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd278dg.prod.mediav.com:16567 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd255dg.prod.mediav.com:29011 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_70 in memory on hd284dg.prod.mediav.com:46214 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_165 in memory on hd282dg.prod.mediav.com:57776 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_56 in memory on hd276dg.prod.mediav.com:15025 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_71 in memory on hd288dg.prod.mediav.com:3459 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_123 in memory on hd272dg.prod.mediav.com:63317 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_76 in memory on hd261dg.prod.mediav.com:51708 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_45 in memory on hd289dg.prod.mediav.com:22644 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_11 in memory on hd290dg.prod.mediav.com:48176 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_30 in memory on hd283dg.prod.mediav.com:62866 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_185 in memory on hd254dg.prod.mediav.com:37252 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_154 in memory on hd283dg.prod.mediav.com:19668 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_68 in memory on hd293dg.prod.mediav.com:46714 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 70.0 in stage 2.0 (TID 227) in 1202 ms on hd284dg.prod.mediav.com (60/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_143 in memory on hd283dg.prod.mediav.com:15357 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_172 in memory on hd256dg.prod.mediav.com:4157 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_191 in memory on hd276dg.prod.mediav.com:46848 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Starting task 98.0 in stage 2.0 (TID 390, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 165.0 in stage 2.0 (TID 204) in 1213 ms on hd282dg.prod.mediav.com (61/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 56.0 in stage 2.0 (TID 201) in 1217 ms on hd276dg.prod.mediav.com (62/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_79 in memory on hd292dg.prod.mediav.com:9515 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_114 in memory on hd280dg.prod.mediav.com:4198 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_59 in memory on hd280dg.prod.mediav.com:6093 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_95 in memory on hd289dg.prod.mediav.com:48417 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_110 in memory on hd284dg.prod.mediav.com:33099 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 2.0 (TID 224) in 1211 ms on hd289dg.prod.mediav.com (63/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 71.0 in stage 2.0 (TID 260) in 1204 ms on hd288dg.prod.mediav.com (64/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_140 in memory on hd270dg.prod.mediav.com:33090 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 76.0 in stage 2.0 (TID 304) in 1197 ms on hd261dg.prod.mediav.com (65/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 123.0 in stage 2.0 (TID 218) in 1214 ms on hd272dg.prod.mediav.com (66/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_82 in memory on hd261dg.prod.mediav.com:16786 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_181 in memory on hd284dg.prod.mediav.com:65293 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_160 in memory on hd257dg.prod.mediav.com:50217 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_61 in memory on hd270dg.prod.mediav.com:5548 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 185.0 in stage 2.0 (TID 360) in 1191 ms on hd254dg.prod.mediav.com (67/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_195 in memory on hd273dg.prod.mediav.com:51855 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_55 in memory on hd278dg.prod.mediav.com:16567 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_182 in memory on hd254dg.prod.mediav.com:6401 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 154.0 in stage 2.0 (TID 257) in 1209 ms on hd283dg.prod.mediav.com (68/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_120 in memory on hd273dg.prod.mediav.com:35935 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_33 in memory on hd274dg.prod.mediav.com:16654 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 2.0 (TID 205) in 1224 ms on hd293dg.prod.mediav.com (69/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 2.0 (TID 219) in 1220 ms on hd290dg.prod.mediav.com (70/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 2.0 (TID 223) in 1218 ms on hd283dg.prod.mediav.com (71/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_158 in memory on hd274dg.prod.mediav.com:50461 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_43 in memory on hd291dg.prod.mediav.com:9728 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_88 in memory on hd262dg.prod.mediav.com:5957 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 143.0 in stage 2.0 (TID 321) in 1201 ms on hd283dg.prod.mediav.com (72/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_111 in memory on hd264dg.prod.mediav.com:48038 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 191.0 in stage 2.0 (TID 265) in 1213 ms on hd276dg.prod.mediav.com (73/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 172.0 in stage 2.0 (TID 213) in 1225 ms on hd256dg.prod.mediav.com (74/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 95.0 in stage 2.0 (TID 258) in 1216 ms on hd289dg.prod.mediav.com (75/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_32 in memory on hd293dg.prod.mediav.com:60417 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 114.0 in stage 2.0 (TID 373) in 1197 ms on hd280dg.prod.mediav.com (76/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 59.0 in stage 2.0 (TID 348) in 1201 ms on hd280dg.prod.mediav.com (77/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 110.0 in stage 2.0 (TID 326) in 1206 ms on hd284dg.prod.mediav.com (78/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_180 in memory on hd285dg.prod.mediav.com:52469 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 140.0 in stage 2.0 (TID 331) in 1206 ms on hd270dg.prod.mediav.com (79/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 82.0 in stage 2.0 (TID 286) in 1213 ms on hd261dg.prod.mediav.com (80/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 79.0 in stage 2.0 (TID 324) in 1207 ms on hd292dg.prod.mediav.com (81/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_62 in memory on hd289dg.prod.mediav.com:48937 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 61.0 in stage 2.0 (TID 368) in 1202 ms on hd270dg.prod.mediav.com (82/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 181.0 in stage 2.0 (TID 375) in 1202 ms on hd284dg.prod.mediav.com (83/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Starting task 129.0 in stage 2.0 (TID 391, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_163 in memory on hd263dg.prod.mediav.com:30335 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 160.0 in stage 2.0 (TID 244) in 1225 ms on hd257dg.prod.mediav.com (84/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 55.0 in stage 2.0 (TID 371) in 1205 ms on hd278dg.prod.mediav.com (85/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 2.0 (TID 341) in 1209 ms on hd274dg.prod.mediav.com (86/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 88.0 in stage 2.0 (TID 263) in 1222 ms on hd262dg.prod.mediav.com (87/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 195.0 in stage 2.0 (TID 344) in 1210 ms on hd273dg.prod.mediav.com (88/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_155 in memory on hd275dg.prod.mediav.com:7744 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_134 in memory on hd255dg.prod.mediav.com:29011 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 2.0 (TID 317) in 1214 ms on hd291dg.prod.mediav.com (89/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 120.0 in stage 2.0 (TID 315) in 1215 ms on hd273dg.prod.mediav.com (90/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_189 in memory on hd277dg.prod.mediav.com:17325 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 158.0 in stage 2.0 (TID 354) in 1209 ms on hd274dg.prod.mediav.com (91/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 182.0 in stage 2.0 (TID 288) in 1220 ms on hd254dg.prod.mediav.com (92/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_83 in memory on hd288dg.prod.mediav.com:30986 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 111.0 in stage 2.0 (TID 366) in 1208 ms on hd264dg.prod.mediav.com (93/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_84 in memory on hd266dg.prod.mediav.com:60871 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 2.0 (TID 211) in 1240 ms on hd293dg.prod.mediav.com (94/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 180.0 in stage 2.0 (TID 336) in 1219 ms on hd285dg.prod.mediav.com (95/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_170 in memory on hd263dg.prod.mediav.com:28737 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 62.0 in stage 2.0 (TID 359) in 1217 ms on hd289dg.prod.mediav.com (96/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_2 in memory on hd288dg.prod.mediav.com:15935 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 163.0 in stage 2.0 (TID 316) in 1224 ms on hd263dg.prod.mediav.com (97/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 155.0 in stage 2.0 (TID 376) in 1217 ms on hd275dg.prod.mediav.com (98/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 83.0 in stage 2.0 (TID 358) in 1222 ms on hd288dg.prod.mediav.com (99/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 134.0 in stage 2.0 (TID 364) in 1221 ms on hd255dg.prod.mediav.com (100/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 189.0 in stage 2.0 (TID 372) in 1221 ms on hd277dg.prod.mediav.com (101/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_57 in memory on hd266dg.prod.mediav.com:7561 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 84.0 in stage 2.0 (TID 233) in 1246 ms on hd266dg.prod.mediav.com (102/200)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 170.0 in stage 2.0 (TID 272) in 1241 ms on hd263dg.prod.mediav.com (103/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_67 in memory on hd266dg.prod.mediav.com:60772 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_28 in memory on hd266dg.prod.mediav.com:18054 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 350) in 1235 ms on hd288dg.prod.mediav.com (104/200)
15/09/15 18:50:40 INFO storage.BlockManagerInfo: Added rdd_7_98 in memory on hd276dg.prod.mediav.com:15025 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:40 INFO scheduler.TaskSetManager: Finished task 57.0 in stage 2.0 (TID 310) in 1245 ms on hd266dg.prod.mediav.com (105/200)
15/09/15 18:50:41 INFO scheduler.TaskSetManager: Starting task 139.0 in stage 2.0 (TID 392, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:41 INFO scheduler.TaskSetManager: Finished task 67.0 in stage 2.0 (TID 330) in 1246 ms on hd266dg.prod.mediav.com (106/200)
15/09/15 18:50:41 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 2.0 (TID 377) in 1243 ms on hd266dg.prod.mediav.com (107/200)
15/09/15 18:50:41 INFO scheduler.TaskSetManager: Finished task 98.0 in stage 2.0 (TID 390) in 66 ms on hd276dg.prod.mediav.com (108/200)
15/09/15 18:50:41 INFO storage.BlockManagerInfo: Added rdd_7_129 in memory on hd262dg.prod.mediav.com:5957 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:41 INFO scheduler.TaskSetManager: Finished task 129.0 in stage 2.0 (TID 391) in 57 ms on hd262dg.prod.mediav.com (109/200)
15/09/15 18:50:41 INFO storage.BlockManagerInfo: Added rdd_7_119 in memory on hd279dg.prod.mediav.com:49996 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:41 INFO storage.BlockManagerInfo: Added rdd_7_117 in memory on hd279dg.prod.mediav.com:34009 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:41 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 2.0 (TID 337) in 1280 ms on hd279dg.prod.mediav.com (110/200)
15/09/15 18:50:41 INFO storage.BlockManagerInfo: Added rdd_7_167 in memory on hd265dg.prod.mediav.com:41028 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:41 INFO storage.BlockManagerInfo: Added rdd_7_139 in memory on hd266dg.prod.mediav.com:60772 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:41 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 2.0 (TID 369) in 1284 ms on hd279dg.prod.mediav.com (111/200)
15/09/15 18:50:41 INFO scheduler.TaskSetManager: Finished task 167.0 in stage 2.0 (TID 361) in 1292 ms on hd265dg.prod.mediav.com (112/200)
15/09/15 18:50:41 INFO scheduler.TaskSetManager: Finished task 139.0 in stage 2.0 (TID 392) in 57 ms on hd266dg.prod.mediav.com (113/200)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd280dg.prod.mediav.com:1897 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd275dg.prod.mediav.com:52956 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd275dg.prod.mediav.com:3649 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd282dg.prod.mediav.com:47145 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd272dg.prod.mediav.com:63093 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd293dg.prod.mediav.com:23628 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd284dg.prod.mediav.com:39393 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd285dg.prod.mediav.com:15892 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added rdd_7_188 in memory on hd275dg.prod.mediav.com:3649 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added rdd_7_15 in memory on hd285dg.prod.mediav.com:15892 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added rdd_7_142 in memory on hd275dg.prod.mediav.com:52956 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added rdd_7_8 in memory on hd282dg.prod.mediav.com:47145 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added rdd_7_138 in memory on hd272dg.prod.mediav.com:63093 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added rdd_7_49 in memory on hd293dg.prod.mediav.com:23628 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:43 INFO scheduler.TaskSetManager: Finished task 188.0 in stage 2.0 (TID 294) in 4206 ms on hd275dg.prod.mediav.com (114/200)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added rdd_7_21 in memory on hd284dg.prod.mediav.com:39393 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:43 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 2.0 (TID 319) in 4211 ms on hd285dg.prod.mediav.com (115/200)
15/09/15 18:50:43 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 2.0 (TID 338) in 4209 ms on hd282dg.prod.mediav.com (116/200)
15/09/15 18:50:43 INFO scheduler.TaskSetManager: Finished task 142.0 in stage 2.0 (TID 302) in 4214 ms on hd275dg.prod.mediav.com (117/200)
15/09/15 18:50:43 INFO scheduler.TaskSetManager: Finished task 49.0 in stage 2.0 (TID 309) in 4215 ms on hd293dg.prod.mediav.com (118/200)
15/09/15 18:50:43 INFO scheduler.TaskSetManager: Finished task 138.0 in stage 2.0 (TID 340) in 4212 ms on hd272dg.prod.mediav.com (119/200)
15/09/15 18:50:43 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 2.0 (TID 323) in 4220 ms on hd284dg.prod.mediav.com (120/200)
15/09/15 18:50:43 INFO storage.BlockManagerInfo: Added rdd_7_97 in memory on hd280dg.prod.mediav.com:1897 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:44 INFO scheduler.TaskSetManager: Starting task 152.0 in stage 2.0 (TID 393, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1425 bytes)
15/09/15 18:50:44 INFO scheduler.TaskSetManager: Finished task 97.0 in stage 2.0 (TID 297) in 4255 ms on hd280dg.prod.mediav.com (121/200)
15/09/15 18:50:44 INFO storage.BlockManagerInfo: Added rdd_7_152 in memory on hd280dg.prod.mediav.com:1897 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:44 INFO scheduler.TaskSetManager: Finished task 152.0 in stage 2.0 (TID 393) in 61 ms on hd280dg.prod.mediav.com (122/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd277dg.prod.mediav.com:27612 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd262dg.prod.mediav.com:24194 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd260dg.prod.mediav.com:38400 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd260dg.prod.mediav.com:20879 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd265dg.prod.mediav.com:38596 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd285dg.prod.mediav.com:25860 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd292dg.prod.mediav.com:24852 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd265dg.prod.mediav.com:65261 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd292dg.prod.mediav.com:31959 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd255dg.prod.mediav.com:62882 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd271dg.prod.mediav.com:63849 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd275dg.prod.mediav.com:27702 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd272dg.prod.mediav.com:13836 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd286dg.prod.mediav.com:20786 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd256dg.prod.mediav.com:22567 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd282dg.prod.mediav.com:51731 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd258dg.prod.mediav.com:44194 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd254dg.prod.mediav.com:50339 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd256dg.prod.mediav.com:54569 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd287dg.prod.mediav.com:53041 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd281dg.prod.mediav.com:24432 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd276dg.prod.mediav.com:65222 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd286dg.prod.mediav.com:22509 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd274dg.prod.mediav.com:25030 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd286dg.prod.mediav.com:15531 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd291dg.prod.mediav.com:3890 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd287dg.prod.mediav.com:2547 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd277dg.prod.mediav.com:27951 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 2.0 (TID 394, hd262dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Starting task 161.0 in stage 2.0 (TID 395, hd255dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Starting task 186.0 in stage 2.0 (TID 396, hd291dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Starting task 115.0 in stage 2.0 (TID 397, hd289dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Starting task 169.0 in stage 2.0 (TID 398, hd276dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Starting task 104.0 in stage 2.0 (TID 399, hd287dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd291dg.prod.mediav.com:31861 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Starting task 198.0 in stage 2.0 (TID 400, hd262dg.prod.mediav.com, NODE_LOCAL, 1425 bytes)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_41 in memory on hd262dg.prod.mediav.com:24194 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_130 in memory on hd256dg.prod.mediav.com:54569 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_1 in memory on hd287dg.prod.mediav.com:53041 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_24 in memory on hd271dg.prod.mediav.com:63849 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_66 in memory on hd292dg.prod.mediav.com:24852 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 2.0 (TID 282) in 8209 ms on hd262dg.prod.mediav.com (123/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_159 in memory on hd292dg.prod.mediav.com:31959 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_74 in memory on hd254dg.prod.mediav.com:50339 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_193 in memory on hd277dg.prod.mediav.com:27612 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_171 in memory on hd272dg.prod.mediav.com:13836 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 130.0 in stage 2.0 (TID 311) in 8211 ms on hd256dg.prod.mediav.com (124/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_38 in memory on hd260dg.prod.mediav.com:20879 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_179 in memory on hd282dg.prod.mediav.com:51731 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_144 in memory on hd281dg.prod.mediav.com:24432 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_58 in memory on hd287dg.prod.mediav.com:2547 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 2.0 (TID 214) in 8234 ms on hd271dg.prod.mediav.com (125/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_184 in memory on hd258dg.prod.mediav.com:44194 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 66.0 in stage 2.0 (TID 239) in 8231 ms on hd292dg.prod.mediav.com (126/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_48 in memory on hd256dg.prod.mediav.com:22567 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 339) in 8216 ms on hd287dg.prod.mediav.com (127/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_128 in memory on hd260dg.prod.mediav.com:38400 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 159.0 in stage 2.0 (TID 255) in 8229 ms on hd292dg.prod.mediav.com (128/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_16 in memory on hd285dg.prod.mediav.com:25860 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 74.0 in stage 2.0 (TID 325) in 8219 ms on hd254dg.prod.mediav.com (129/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_176 in memory on hd286dg.prod.mediav.com:15531 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 171.0 in stage 2.0 (TID 284) in 8226 ms on hd272dg.prod.mediav.com (130/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_85 in memory on hd286dg.prod.mediav.com:20786 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 193.0 in stage 2.0 (TID 216) in 8241 ms on hd277dg.prod.mediav.com (131/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_151 in memory on hd277dg.prod.mediav.com:27951 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 2.0 (TID 262) in 8231 ms on hd260dg.prod.mediav.com (132/200)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 179.0 in stage 2.0 (TID 240) in 8239 ms on hd282dg.prod.mediav.com (133/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_99 in memory on hd274dg.prod.mediav.com:25030 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 144.0 in stage 2.0 (TID 357) in 8219 ms on hd281dg.prod.mediav.com (134/200)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 58.0 in stage 2.0 (TID 352) in 8222 ms on hd287dg.prod.mediav.com (135/200)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 184.0 in stage 2.0 (TID 327) in 8227 ms on hd258dg.prod.mediav.com (136/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_150 in memory on hd265dg.prod.mediav.com:65261 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 48.0 in stage 2.0 (TID 243) in 8242 ms on hd256dg.prod.mediav.com (137/200)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 128.0 in stage 2.0 (TID 229) in 8246 ms on hd260dg.prod.mediav.com (138/200)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 2.0 (TID 251) in 8243 ms on hd285dg.prod.mediav.com (139/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_19 in memory on hd291dg.prod.mediav.com:31861 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 151.0 in stage 2.0 (TID 367) in 8225 ms on hd277dg.prod.mediav.com (140/200)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 85.0 in stage 2.0 (TID 268) in 8241 ms on hd286dg.prod.mediav.com (141/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_108 in memory on hd286dg.prod.mediav.com:22509 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_81 in memory on hd265dg.prod.mediav.com:38596 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 176.0 in stage 2.0 (TID 353) in 8231 ms on hd286dg.prod.mediav.com (142/200)
15/09/15 18:50:47 INFO storage.BlockManagerInfo: Added rdd_7_53 in memory on hd275dg.prod.mediav.com:27702 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 99.0 in stage 2.0 (TID 270) in 8248 ms on hd274dg.prod.mediav.com (143/200)
15/09/15 18:50:47 INFO scheduler.TaskSetManager: Finished task 150.0 in stage 2.0 (TID 225) in 8259 ms on hd265dg.prod.mediav.com (144/200)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_113 in memory on hd255dg.prod.mediav.com:62882 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_50 in memory on hd291dg.prod.mediav.com:3890 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 81.0 in stage 2.0 (TID 271) in 8254 ms on hd265dg.prod.mediav.com (145/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 108.0 in stage 2.0 (TID 334) in 8244 ms on hd286dg.prod.mediav.com (146/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 2.0 (TID 349) in 8246 ms on hd291dg.prod.mediav.com (147/200)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_126 in memory on hd276dg.prod.mediav.com:65222 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 53.0 in stage 2.0 (TID 305) in 8257 ms on hd275dg.prod.mediav.com (148/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 113.0 in stage 2.0 (TID 253) in 8274 ms on hd255dg.prod.mediav.com (149/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 50.0 in stage 2.0 (TID 333) in 8261 ms on hd291dg.prod.mediav.com (150/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 126.0 in stage 2.0 (TID 247) in 8284 ms on hd276dg.prod.mediav.com (151/200)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd292dg.prod.mediav.com:8313 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd278dg.prod.mediav.com:9221 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd279dg.prod.mediav.com:12678 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd264dg.prod.mediav.com:49664 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd290dg.prod.mediav.com:6793 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd266dg.prod.mediav.com:27698 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_104 in memory on hd287dg.prod.mediav.com:2270 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_115 in memory on hd289dg.prod.mediav.com:48937 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_186 in memory on hd291dg.prod.mediav.com:9728 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 104.0 in stage 2.0 (TID 399) in 248 ms on hd287dg.prod.mediav.com (152/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 115.0 in stage 2.0 (TID 397) in 255 ms on hd289dg.prod.mediav.com (153/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 186.0 in stage 2.0 (TID 396) in 255 ms on hd291dg.prod.mediav.com (154/200)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_121 in memory on hd290dg.prod.mediav.com:6793 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_47 in memory on hd262dg.prod.mediav.com:49994 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_198 in memory on hd262dg.prod.mediav.com:5957 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_77 in memory on hd279dg.prod.mediav.com:12678 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_26 in memory on hd278dg.prod.mediav.com:9221 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_20 in memory on hd266dg.prod.mediav.com:27698 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_161 in memory on hd255dg.prod.mediav.com:29011 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_69 in memory on hd264dg.prod.mediav.com:49664 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_3 in memory on hd292dg.prod.mediav.com:8313 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 121.0 in stage 2.0 (TID 345) in 8421 ms on hd290dg.prod.mediav.com (155/200)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_169 in memory on hd276dg.prod.mediav.com:15025 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 198.0 in stage 2.0 (TID 400) in 280 ms on hd262dg.prod.mediav.com (156/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 2.0 (TID 394) in 283 ms on hd262dg.prod.mediav.com (157/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 2.0 (TID 242) in 8439 ms on hd278dg.prod.mediav.com (158/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 77.0 in stage 2.0 (TID 308) in 8428 ms on hd279dg.prod.mediav.com (159/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 161.0 in stage 2.0 (TID 395) in 291 ms on hd255dg.prod.mediav.com (160/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 2.0 (TID 289) in 8438 ms on hd266dg.prod.mediav.com (161/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 267) in 8444 ms on hd292dg.prod.mediav.com (162/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 169.0 in stage 2.0 (TID 398) in 292 ms on hd276dg.prod.mediav.com (163/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 69.0 in stage 2.0 (TID 328) in 8434 ms on hd264dg.prod.mediav.com (164/200)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd273dg.prod.mediav.com:6601 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd258dg.prod.mediav.com:3750 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd278dg.prod.mediav.com:27815 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd265dg.prod.mediav.com:15593 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd290dg.prod.mediav.com:25045 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd264dg.prod.mediav.com:48358 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd279dg.prod.mediav.com:11149 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd270dg.prod.mediav.com:3332 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd270dg.prod.mediav.com:35197 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd279dg.prod.mediav.com:34291 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd277dg.prod.mediav.com:6547 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd265dg.prod.mediav.com:62939 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd257dg.prod.mediav.com:11991 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd254dg.prod.mediav.com:14962 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd274dg.prod.mediav.com:44964 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd264dg.prod.mediav.com:38843 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd271dg.prod.mediav.com:45534 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_164 in memory on hd264dg.prod.mediav.com:48358 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_178 in memory on hd290dg.prod.mediav.com:25045 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_75 in memory on hd264dg.prod.mediav.com:38843 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_148 in memory on hd279dg.prod.mediav.com:34291 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_63 in memory on hd270dg.prod.mediav.com:3332 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_162 in memory on hd258dg.prod.mediav.com:3750 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_12 in memory on hd265dg.prod.mediav.com:15593 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_92 in memory on hd254dg.prod.mediav.com:14962 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_31 in memory on hd277dg.prod.mediav.com:6547 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_89 in memory on hd274dg.prod.mediav.com:44964 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_72 in memory on hd265dg.prod.mediav.com:62939 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_73 in memory on hd278dg.prod.mediav.com:27815 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 178.0 in stage 2.0 (TID 277) in 9219 ms on hd290dg.prod.mediav.com (165/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 164.0 in stage 2.0 (TID 217) in 9232 ms on hd264dg.prod.mediav.com (166/200)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_39 in memory on hd279dg.prod.mediav.com:11149 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 75.0 in stage 2.0 (TID 351) in 9211 ms on hd264dg.prod.mediav.com (167/200)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_93 in memory on hd271dg.prod.mediav.com:45534 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 63.0 in stage 2.0 (TID 300) in 9221 ms on hd270dg.prod.mediav.com (168/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 148.0 in stage 2.0 (TID 318) in 9218 ms on hd279dg.prod.mediav.com (169/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 162.0 in stage 2.0 (TID 292) in 9222 ms on hd258dg.prod.mediav.com (170/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 2.0 (TID 235) in 9233 ms on hd265dg.prod.mediav.com (171/200)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_177 in memory on hd273dg.prod.mediav.com:6601 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 2.0 (TID 228) in 9238 ms on hd277dg.prod.mediav.com (172/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 92.0 in stage 2.0 (TID 285) in 9227 ms on hd254dg.prod.mediav.com (173/200)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_78 in memory on hd270dg.prod.mediav.com:35197 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 89.0 in stage 2.0 (TID 246) in 9236 ms on hd274dg.prod.mediav.com (174/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 72.0 in stage 2.0 (TID 313) in 9225 ms on hd265dg.prod.mediav.com (175/200)
15/09/15 18:50:48 INFO storage.BlockManagerInfo: Added rdd_7_36 in memory on hd257dg.prod.mediav.com:11991 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 73.0 in stage 2.0 (TID 290) in 9231 ms on hd278dg.prod.mediav.com (176/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 93.0 in stage 2.0 (TID 346) in 9224 ms on hd271dg.prod.mediav.com (177/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 2.0 (TID 281) in 9236 ms on hd279dg.prod.mediav.com (178/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 177.0 in stage 2.0 (TID 230) in 9251 ms on hd273dg.prod.mediav.com (179/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 78.0 in stage 2.0 (TID 249) in 9251 ms on hd270dg.prod.mediav.com (180/200)
15/09/15 18:50:48 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 2.0 (TID 362) in 9232 ms on hd257dg.prod.mediav.com (181/200)
15/09/15 18:50:51 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd260dg.prod.mediav.com:7955 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:51 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd286dg.prod.mediav.com:64184 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:51 INFO storage.BlockManagerInfo: Added rdd_7_17 in memory on hd260dg.prod.mediav.com:7955 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:51 INFO storage.BlockManagerInfo: Added rdd_7_51 in memory on hd286dg.prod.mediav.com:64184 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:51 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 2.0 (TID 335) in 12222 ms on hd260dg.prod.mediav.com (182/200)
15/09/15 18:50:51 INFO scheduler.TaskSetManager: Finished task 51.0 in stage 2.0 (TID 283) in 12236 ms on hd286dg.prod.mediav.com (183/200)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd274dg.prod.mediav.com:58299 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd276dg.prod.mediav.com:21872 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd277dg.prod.mediav.com:60781 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd271dg.prod.mediav.com:16160 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd255dg.prod.mediav.com:22114 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd289dg.prod.mediav.com:20447 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd281dg.prod.mediav.com:61667 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added rdd_7_42 in memory on hd289dg.prod.mediav.com:20447 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added rdd_7_136 in memory on hd276dg.prod.mediav.com:21872 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added rdd_7_14 in memory on hd274dg.prod.mediav.com:58299 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added rdd_7_118 in memory on hd271dg.prod.mediav.com:16160 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added rdd_7_183 in memory on hd277dg.prod.mediav.com:60781 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:54 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 2.0 (TID 343) in 15211 ms on hd289dg.prod.mediav.com (184/200)
15/09/15 18:50:54 INFO scheduler.TaskSetManager: Finished task 136.0 in stage 2.0 (TID 261) in 15231 ms on hd276dg.prod.mediav.com (185/200)
15/09/15 18:50:54 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 2.0 (TID 226) in 15242 ms on hd274dg.prod.mediav.com (186/200)
15/09/15 18:50:54 INFO scheduler.TaskSetManager: Finished task 183.0 in stage 2.0 (TID 276) in 15234 ms on hd277dg.prod.mediav.com (187/200)
15/09/15 18:50:54 INFO scheduler.TaskSetManager: Finished task 118.0 in stage 2.0 (TID 332) in 15226 ms on hd271dg.prod.mediav.com (188/200)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added rdd_7_87 in memory on hd281dg.prod.mediav.com:61667 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:54 INFO storage.BlockManagerInfo: Added rdd_7_149 in memory on hd255dg.prod.mediav.com:22114 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:55 INFO scheduler.TaskSetManager: Finished task 87.0 in stage 2.0 (TID 307) in 15248 ms on hd281dg.prod.mediav.com (189/200)
15/09/15 18:50:55 INFO scheduler.TaskSetManager: Finished task 149.0 in stage 2.0 (TID 342) in 15250 ms on hd255dg.prod.mediav.com (190/200)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd257dg.prod.mediav.com:13672 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd257dg.prod.mediav.com:6403 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd273dg.prod.mediav.com:19800 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd282dg.prod.mediav.com:54259 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd258dg.prod.mediav.com:61558 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd264dg.prod.mediav.com:28057 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added rdd_7_52 in memory on hd257dg.prod.mediav.com:13672 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added rdd_7_124 in memory on hd257dg.prod.mediav.com:6403 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added rdd_7_146 in memory on hd273dg.prod.mediav.com:19800 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added rdd_7_194 in memory on hd264dg.prod.mediav.com:28057 (size: 3.7 MB, free: 8.6 GB)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added rdd_7_192 in memory on hd258dg.prod.mediav.com:61558 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:55 INFO storage.BlockManagerInfo: Added rdd_7_145 in memory on hd282dg.prod.mediav.com:54259 (size: 3.8 MB, free: 8.6 GB)
15/09/15 18:50:55 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 2.0 (TID 299) in 16222 ms on hd257dg.prod.mediav.com (191/200)
15/09/15 18:50:55 INFO scheduler.TaskSetManager: Finished task 124.0 in stage 2.0 (TID 287) in 16228 ms on hd257dg.prod.mediav.com (192/200)
15/09/15 18:50:55 INFO scheduler.TaskSetManager: Finished task 146.0 in stage 2.0 (TID 259) in 16234 ms on hd273dg.prod.mediav.com (193/200)
15/09/15 18:50:55 INFO scheduler.TaskSetManager: Finished task 194.0 in stage 2.0 (TID 295) in 16231 ms on hd264dg.prod.mediav.com (194/200)
15/09/15 18:50:55 INFO scheduler.TaskSetManager: Finished task 192.0 in stage 2.0 (TID 303) in 16230 ms on hd258dg.prod.mediav.com (195/200)
15/09/15 18:50:55 INFO scheduler.TaskSetManager: Finished task 145.0 in stage 2.0 (TID 301) in 16232 ms on hd282dg.prod.mediav.com (196/200)
15/09/15 18:50:56 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd285dg.prod.mediav.com:57578 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:56 INFO storage.BlockManagerInfo: Added rdd_7_100 in memory on hd285dg.prod.mediav.com:57578 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:56 INFO scheduler.TaskSetManager: Finished task 100.0 in stage 2.0 (TID 280) in 16442 ms on hd285dg.prod.mediav.com (197/200)
15/09/15 18:50:56 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd284dg.prod.mediav.com:55335 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:56 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd258dg.prod.mediav.com:30778 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:56 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hd263dg.prod.mediav.com:57200 (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:56 INFO storage.BlockManagerInfo: Added rdd_7_94 in memory on hd284dg.prod.mediav.com:55335 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:56 INFO storage.BlockManagerInfo: Added rdd_7_125 in memory on hd263dg.prod.mediav.com:57200 (size: 3.9 MB, free: 8.6 GB)
15/09/15 18:50:56 INFO scheduler.TaskSetManager: Finished task 94.0 in stage 2.0 (TID 312) in 17227 ms on hd284dg.prod.mediav.com (198/200)
15/09/15 18:50:56 INFO scheduler.TaskSetManager: Finished task 125.0 in stage 2.0 (TID 322) in 17227 ms on hd263dg.prod.mediav.com (199/200)
15/09/15 18:50:56 INFO storage.BlockManagerInfo: Added rdd_7_4 in memory on hd258dg.prod.mediav.com:30778 (size: 4.0 MB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 2.0 (TID 329) in 17246 ms on hd258dg.prod.mediav.com (200/200)
15/09/15 18:50:57 INFO cluster.YarnClusterScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool default
15/09/15 18:50:57 INFO scheduler.DAGScheduler: ResultStage 2 (count at DecisionTreeMetadata.scala:111) finished in 17.275 s
15/09/15 18:50:57 INFO scheduler.DAGScheduler: Job 2 finished: count at DecisionTreeMetadata.scala:111, took 17.302142 s
15/09/15 18:50:57 INFO spark.SparkContext: Starting job: collect at DecisionTree.scala:977
15/09/15 18:50:57 INFO scheduler.DAGScheduler: Got job 3 (collect at DecisionTree.scala:977) with 200 output partitions (allowLocal=false)
15/09/15 18:50:57 INFO scheduler.DAGScheduler: Final stage: ResultStage 3(collect at DecisionTree.scala:977)
15/09/15 18:50:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
15/09/15 18:50:57 INFO scheduler.DAGScheduler: Missing parents: List()
15/09/15 18:50:57 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (PartitionwiseSampledRDD[9] at sample at DecisionTree.scala:977), which has no missing parents
15/09/15 18:50:57 INFO storage.MemoryStore: ensureFreeSpace(4880) called with curMem=214908, maxMem=18438322913
15/09/15 18:50:57 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 4.8 KB, free 17.2 GB)
15/09/15 18:50:57 INFO storage.MemoryStore: ensureFreeSpace(2503) called with curMem=219788, maxMem=18438322913
15/09/15 18:50:57 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.4 KB, free 17.2 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.47.1.38:18554 (size: 2.4 KB, free: 17.2 GB)
15/09/15 18:50:57 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:874
15/09/15 18:50:57 INFO scheduler.DAGScheduler: Submitting 200 missing tasks from ResultStage 3 (PartitionwiseSampledRDD[9] at sample at DecisionTree.scala:977)
15/09/15 18:50:57 INFO cluster.YarnClusterScheduler: Adding task set 3.0 with 200 tasks
15/09/15 18:50:57 INFO scheduler.FairSchedulableBuilder: Added task set TaskSet_3 tasks to pool default
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 3.0 (TID 401, hd283dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 3.0 (TID 402, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 58.0 in stage 3.0 (TID 403, hd287dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 83.0 in stage 3.0 (TID 404, hd288dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 193.0 in stage 3.0 (TID 405, hd277dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 113.0 in stage 3.0 (TID 406, hd255dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 77.0 in stage 3.0 (TID 407, hd279dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 122.0 in stage 3.0 (TID 408, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 3.0 (TID 409, hd291dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 3.0 (TID 410, hd270dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 56.0 in stage 3.0 (TID 411, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 3.0 (TID 412, hd265dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 123.0 in stage 3.0 (TID 413, hd272dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 145.0 in stage 3.0 (TID 414, hd282dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 84.0 in stage 3.0 (TID 415, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 120.0 in stage 3.0 (TID 416, hd273dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 3.0 (TID 417, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 3.0 (TID 418, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 179.0 in stage 3.0 (TID 419, hd282dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 3.0 (TID 420, hd291dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 132.0 in stage 3.0 (TID 421, hd256dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 175.0 in stage 3.0 (TID 422, hd255dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 192.0 in stage 3.0 (TID 423, hd258dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 3.0 (TID 424, hd285dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 183.0 in stage 3.0 (TID 425, hd277dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 3.0 (TID 426, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 130.0 in stage 3.0 (TID 427, hd256dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 189.0 in stage 3.0 (TID 428, hd277dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 429, hd287dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 155.0 in stage 3.0 (TID 430, hd275dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 164.0 in stage 3.0 (TID 431, hd264dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 71.0 in stage 3.0 (TID 432, hd288dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 74.0 in stage 3.0 (TID 433, hd254dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 188.0 in stage 3.0 (TID 434, hd275dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 3.0 (TID 435, hd271dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 165.0 in stage 3.0 (TID 436, hd282dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 3.0 (TID 437, hd257dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 97.0 in stage 3.0 (TID 438, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 3.0 (TID 439, hd274dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 3.0 (TID 440, hd256dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 111.0 in stage 3.0 (TID 441, hd264dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 55.0 in stage 3.0 (TID 442, hd278dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 3.0 (TID 443, hd284dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 75.0 in stage 3.0 (TID 444, hd264dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 127.0 in stage 3.0 (TID 445, hd278dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 103.0 in stage 3.0 (TID 446, hd271dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 140.0 in stage 3.0 (TID 447, hd270dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 70.0 in stage 3.0 (TID 448, hd284dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 3.0 (TID 449, hd292dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 172.0 in stage 3.0 (TID 450, hd256dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 92.0 in stage 3.0 (TID 451, hd254dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 119.0 in stage 3.0 (TID 452, hd279dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 3.0 (TID 453, hd278dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 167.0 in stage 3.0 (TID 454, hd265dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 66.0 in stage 3.0 (TID 455, hd292dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 73.0 in stage 3.0 (TID 456, hd278dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 96.0 in stage 3.0 (TID 457, hd260dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 50.0 in stage 3.0 (TID 458, hd291dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 149.0 in stage 3.0 (TID 459, hd255dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 93.0 in stage 3.0 (TID 460, hd271dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 64.0 in stage 3.0 (TID 461, hd278dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 53.0 in stage 3.0 (TID 462, hd275dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 3.0 (TID 463, hd279dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 88.0 in stage 3.0 (TID 464, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 65.0 in stage 3.0 (TID 465, hd285dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 185.0 in stage 3.0 (TID 466, hd254dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 187.0 in stage 3.0 (TID 467, hd289dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 121.0 in stage 3.0 (TID 468, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 194.0 in stage 3.0 (TID 469, hd264dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 3.0 (TID 470, hd277dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 106.0 in stage 3.0 (TID 471, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 173.0 in stage 3.0 (TID 472, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 116.0 in stage 3.0 (TID 473, hd272dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 63.0 in stage 3.0 (TID 474, hd270dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 125.0 in stage 3.0 (TID 475, hd263dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 184.0 in stage 3.0 (TID 476, hd258dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 144.0 in stage 3.0 (TID 477, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 62.0 in stage 3.0 (TID 478, hd289dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 89.0 in stage 3.0 (TID 479, hd274dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 166.0 in stage 3.0 (TID 480, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 126.0 in stage 3.0 (TID 481, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 3.0 (TID 482, hd254dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 105.0 in stage 3.0 (TID 483, hd288dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 142.0 in stage 3.0 (TID 484, hd275dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 3.0 (TID 485, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 177.0 in stage 3.0 (TID 486, hd273dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 160.0 in stage 3.0 (TID 487, hd257dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 59.0 in stage 3.0 (TID 488, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 141.0 in stage 3.0 (TID 489, hd286dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 131.0 in stage 3.0 (TID 490, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 48.0 in stage 3.0 (TID 491, hd256dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 3.0 (TID 492, hd260dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 72.0 in stage 3.0 (TID 493, hd265dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 134.0 in stage 3.0 (TID 494, hd255dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 3.0 (TID 495, hd288dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 117.0 in stage 3.0 (TID 496, hd279dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 128.0 in stage 3.0 (TID 497, hd260dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 94.0 in stage 3.0 (TID 498, hd284dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 178.0 in stage 3.0 (TID 499, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 110.0 in stage 3.0 (TID 500, hd284dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 3.0 (TID 501, hd289dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 154.0 in stage 3.0 (TID 502, hd283dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 190.0 in stage 3.0 (TID 503, hd255dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 54.0 in stage 3.0 (TID 504, hd272dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 3.0 (TID 505, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 506, hd288dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 67.0 in stage 3.0 (TID 507, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 90.0 in stage 3.0 (TID 508, hd273dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 51.0 in stage 3.0 (TID 509, hd286dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 57.0 in stage 3.0 (TID 510, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 3.0 (TID 511, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 3.0 (TID 512, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 69.0 in stage 3.0 (TID 513, hd264dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 150.0 in stage 3.0 (TID 514, hd265dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 3.0 (TID 515, hd263dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 3.0 (TID 516, hd260dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 81.0 in stage 3.0 (TID 517, hd265dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 138.0 in stage 3.0 (TID 518, hd272dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 3.0 (TID 519, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 76.0 in stage 3.0 (TID 520, hd261dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 3.0 (TID 521, hd283dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 99.0 in stage 3.0 (TID 522, hd274dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 82.0 in stage 3.0 (TID 523, hd261dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 118.0 in stage 3.0 (TID 524, hd271dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 100.0 in stage 3.0 (TID 525, hd285dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 95.0 in stage 3.0 (TID 526, hd289dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 156.0 in stage 3.0 (TID 527, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 108.0 in stage 3.0 (TID 528, hd286dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 153.0 in stage 3.0 (TID 529, hd263dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 3.0 (TID 530, hd287dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 176.0 in stage 3.0 (TID 531, hd286dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 157.0 in stage 3.0 (TID 532, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 195.0 in stage 3.0 (TID 533, hd273dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 91.0 in stage 3.0 (TID 534, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 49.0 in stage 3.0 (TID 535, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 79.0 in stage 3.0 (TID 536, hd292dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 85.0 in stage 3.0 (TID 537, hd286dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 78.0 in stage 3.0 (TID 538, hd270dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 107.0 in stage 3.0 (TID 539, hd261dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 124.0 in stage 3.0 (TID 540, hd257dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 136.0 in stage 3.0 (TID 541, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 171.0 in stage 3.0 (TID 542, hd272dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 68.0 in stage 3.0 (TID 543, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 3.0 (TID 544, hd258dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 3.0 (TID 545, hd289dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 3.0 (TID 546, hd261dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 547, hd292dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 163.0 in stage 3.0 (TID 548, hd263dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 143.0 in stage 3.0 (TID 549, hd283dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 191.0 in stage 3.0 (TID 550, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 3.0 (TID 551, hd274dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 162.0 in stage 3.0 (TID 552, hd258dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 168.0 in stage 3.0 (TID 553, hd257dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 147.0 in stage 3.0 (TID 554, hd283dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 148.0 in stage 3.0 (TID 555, hd279dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 180.0 in stage 3.0 (TID 556, hd285dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 182.0 in stage 3.0 (TID 557, hd254dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 3.0 (TID 558, hd291dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 3.0 (TID 559, hd282dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 61.0 in stage 3.0 (TID 560, hd270dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 170.0 in stage 3.0 (TID 561, hd263dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 86.0 in stage 3.0 (TID 562, hd275dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 151.0 in stage 3.0 (TID 563, hd277dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 158.0 in stage 3.0 (TID 564, hd274dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 3.0 (TID 565, hd291dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 87.0 in stage 3.0 (TID 566, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 101.0 in stage 3.0 (TID 567, hd261dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 3.0 (TID 568, hd282dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 3.0 (TID 569, hd285dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 181.0 in stage 3.0 (TID 570, hd284dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 114.0 in stage 3.0 (TID 571, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 135.0 in stage 3.0 (TID 572, hd260dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 3.0 (TID 573, hd271dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 159.0 in stage 3.0 (TID 574, hd292dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 196.0 in stage 3.0 (TID 575, hd258dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 109.0 in stage 3.0 (TID 576, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 146.0 in stage 3.0 (TID 577, hd273dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 52.0 in stage 3.0 (TID 578, hd257dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd278dg.prod.mediav.com:27239 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd287dg.prod.mediav.com:2547 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd256dg.prod.mediav.com:65180 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd278dg.prod.mediav.com:1380 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd276dg.prod.mediav.com:62943 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd278dg.prod.mediav.com:27815 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd284dg.prod.mediav.com:39393 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd262dg.prod.mediav.com:5957 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd282dg.prod.mediav.com:57776 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd288dg.prod.mediav.com:64716 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd279dg.prod.mediav.com:49996 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd256dg.prod.mediav.com:5214 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd262dg.prod.mediav.com:40567 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd254dg.prod.mediav.com:14962 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd275dg.prod.mediav.com:3649 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd290dg.prod.mediav.com:1034 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd291dg.prod.mediav.com:47717 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd288dg.prod.mediav.com:58225 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd272dg.prod.mediav.com:47434 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd278dg.prod.mediav.com:9221 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd285dg.prod.mediav.com:15892 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd270dg.prod.mediav.com:33090 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd284dg.prod.mediav.com:46214 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd256dg.prod.mediav.com:22567 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd256dg.prod.mediav.com:4157 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd274dg.prod.mediav.com:16654 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd290dg.prod.mediav.com:6793 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd254dg.prod.mediav.com:37252 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd270dg.prod.mediav.com:6944 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd287dg.prod.mediav.com:53041 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd255dg.prod.mediav.com:62882 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd276dg.prod.mediav.com:65222 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd291dg.prod.mediav.com:31861 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd284dg.prod.mediav.com:55335 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd286dg.prod.mediav.com:64184 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd274dg.prod.mediav.com:25030 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd255dg.prod.mediav.com:24822 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd254dg.prod.mediav.com:28306 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd279dg.prod.mediav.com:34009 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd266dg.prod.mediav.com:60772 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd272dg.prod.mediav.com:36193 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd290dg.prod.mediav.com:48176 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd264dg.prod.mediav.com:48038 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd265dg.prod.mediav.com:15593 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd288dg.prod.mediav.com:30986 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd276dg.prod.mediav.com:15025 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd270dg.prod.mediav.com:3332 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd291dg.prod.mediav.com:9728 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd260dg.prod.mediav.com:20879 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd289dg.prod.mediav.com:22644 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd272dg.prod.mediav.com:63093 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd279dg.prod.mediav.com:11149 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd287dg.prod.mediav.com:2270 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd254dg.prod.mediav.com:50339 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd278dg.prod.mediav.com:16567 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd255dg.prod.mediav.com:29011 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd275dg.prod.mediav.com:7744 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd275dg.prod.mediav.com:27702 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd285dg.prod.mediav.com:7398 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd290dg.prod.mediav.com:25045 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd293dg.prod.mediav.com:25809 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd271dg.prod.mediav.com:63849 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd280dg.prod.mediav.com:8788 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd258dg.prod.mediav.com:61558 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd258dg.prod.mediav.com:44194 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd262dg.prod.mediav.com:35030 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd283dg.prod.mediav.com:19668 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd262dg.prod.mediav.com:24194 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd270dg.prod.mediav.com:5548 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd276dg.prod.mediav.com:46848 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd276dg.prod.mediav.com:21872 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd283dg.prod.mediav.com:62866 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd289dg.prod.mediav.com:48417 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd273dg.prod.mediav.com:35935 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd293dg.prod.mediav.com:60417 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd274dg.prod.mediav.com:44964 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd279dg.prod.mediav.com:12678 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd289dg.prod.mediav.com:20447 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd281dg.prod.mediav.com:12163 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd289dg.prod.mediav.com:48937 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd281dg.prod.mediav.com:2395 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd271dg.prod.mediav.com:33037 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd255dg.prod.mediav.com:22114 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd265dg.prod.mediav.com:62939 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd264dg.prod.mediav.com:48358 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd261dg.prod.mediav.com:16786 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd264dg.prod.mediav.com:28057 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd277dg.prod.mediav.com:60781 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd279dg.prod.mediav.com:34291 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd260dg.prod.mediav.com:7955 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd257dg.prod.mediav.com:11991 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd256dg.prod.mediav.com:54569 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd271dg.prod.mediav.com:45534 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd281dg.prod.mediav.com:2780 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd265dg.prod.mediav.com:65261 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd275dg.prod.mediav.com:52956 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd282dg.prod.mediav.com:51731 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd282dg.prod.mediav.com:54259 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd270dg.prod.mediav.com:35197 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd289dg.prod.mediav.com:5555 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd266dg.prod.mediav.com:60871 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd285dg.prod.mediav.com:57578 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd265dg.prod.mediav.com:41028 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd257dg.prod.mediav.com:50217 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd254dg.prod.mediav.com:6401 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd284dg.prod.mediav.com:33099 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd260dg.prod.mediav.com:34432 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd265dg.prod.mediav.com:38596 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd293dg.prod.mediav.com:18866 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd271dg.prod.mediav.com:16160 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd266dg.prod.mediav.com:7561 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd260dg.prod.mediav.com:45170 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd288dg.prod.mediav.com:3459 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd273dg.prod.mediav.com:6601 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd286dg.prod.mediav.com:15531 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd286dg.prod.mediav.com:20786 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd257dg.prod.mediav.com:6403 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd258dg.prod.mediav.com:30778 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd275dg.prod.mediav.com:37869 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd292dg.prod.mediav.com:8313 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd263dg.prod.mediav.com:57200 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd263dg.prod.mediav.com:19616 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd292dg.prod.mediav.com:24852 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd263dg.prod.mediav.com:28737 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd286dg.prod.mediav.com:5189 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd264dg.prod.mediav.com:38843 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd266dg.prod.mediav.com:18054 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd261dg.prod.mediav.com:51708 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd277dg.prod.mediav.com:17325 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd263dg.prod.mediav.com:5872 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd255dg.prod.mediav.com:8336 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd263dg.prod.mediav.com:30335 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd261dg.prod.mediav.com:48502 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd283dg.prod.mediav.com:5777 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd260dg.prod.mediav.com:38400 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd277dg.prod.mediav.com:27612 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd272dg.prod.mediav.com:63317 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd258dg.prod.mediav.com:3750 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd280dg.prod.mediav.com:6093 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd285dg.prod.mediav.com:52469 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd277dg.prod.mediav.com:6547 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd286dg.prod.mediav.com:22509 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd280dg.prod.mediav.com:1897 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd273dg.prod.mediav.com:51855 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd264dg.prod.mediav.com:49664 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd290dg.prod.mediav.com:48698 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd273dg.prod.mediav.com:4400 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd281dg.prod.mediav.com:24432 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd292dg.prod.mediav.com:9515 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd282dg.prod.mediav.com:42292 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd274dg.prod.mediav.com:58299 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd283dg.prod.mediav.com:15357 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd257dg.prod.mediav.com:23974 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd291dg.prod.mediav.com:3890 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd293dg.prod.mediav.com:23628 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd266dg.prod.mediav.com:27698 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd261dg.prod.mediav.com:17918 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd262dg.prod.mediav.com:49994 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd273dg.prod.mediav.com:19800 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd283dg.prod.mediav.com:27671 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd272dg.prod.mediav.com:13836 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd292dg.prod.mediav.com:5167 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd257dg.prod.mediav.com:13672 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd288dg.prod.mediav.com:15935 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd284dg.prod.mediav.com:65293 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd282dg.prod.mediav.com:47145 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd261dg.prod.mediav.com:35424 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd293dg.prod.mediav.com:46714 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd258dg.prod.mediav.com:8532 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd291dg.prod.mediav.com:54360 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd281dg.prod.mediav.com:61667 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd280dg.prod.mediav.com:29265 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd277dg.prod.mediav.com:27951 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 112.0 in stage 3.0 (TID 579, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 129.0 in stage 3.0 (TID 580, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 3.0 (TID 426) in 130 ms on hd262dg.prod.mediav.com (1/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 3.0 (TID 581, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 133.0 in stage 3.0 (TID 582, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 3.0 (TID 583, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd274dg.prod.mediav.com:50461 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 60.0 in stage 3.0 (TID 584, hd287dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 58.0 in stage 3.0 (TID 403) in 146 ms on hd287dg.prod.mediav.com (2/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 3.0 (TID 402) in 147 ms on hd262dg.prod.mediav.com (3/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 88.0 in stage 3.0 (TID 464) in 136 ms on hd262dg.prod.mediav.com (4/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 3.0 (TID 417) in 145 ms on hd262dg.prod.mediav.com (5/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd292dg.prod.mediav.com:31959 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 3.0 (TID 443) in 147 ms on hd284dg.prod.mediav.com (6/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 127.0 in stage 3.0 (TID 445) in 146 ms on hd278dg.prod.mediav.com (7/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 105.0 in stage 3.0 (TID 483) in 142 ms on hd288dg.prod.mediav.com (8/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 131.0 in stage 3.0 (TID 490) in 141 ms on hd290dg.prod.mediav.com (9/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 102.0 in stage 3.0 (TID 585, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd271dg.prod.mediav.com:11439 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 3.0 (TID 512) in 143 ms on hd262dg.prod.mediav.com (10/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 48.0 in stage 3.0 (TID 491) in 148 ms on hd256dg.prod.mediav.com (11/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 3.0 (TID 530) in 143 ms on hd287dg.prod.mediav.com (12/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 3.0 (TID 453) in 154 ms on hd278dg.prod.mediav.com (13/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 3.0 (TID 501) in 151 ms on hd289dg.prod.mediav.com (14/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 3.0 (TID 495) in 153 ms on hd288dg.prod.mediav.com (15/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 64.0 in stage 3.0 (TID 461) in 159 ms on hd278dg.prod.mediav.com (16/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 126.0 in stage 3.0 (TID 481) in 158 ms on hd276dg.prod.mediav.com (17/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 3.0 (TID 424) in 168 ms on hd285dg.prod.mediav.com (18/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 199.0 in stage 3.0 (TID 586, hd290dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 3.0 (TID 420) in 170 ms on hd291dg.prod.mediav.com (19/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 429) in 169 ms on hd287dg.prod.mediav.com (20/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 104.0 in stage 3.0 (TID 587, hd287dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 3.0 (TID 588, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 70.0 in stage 3.0 (TID 448) in 237 ms on hd284dg.prod.mediav.com (21/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 53.0 in stage 3.0 (TID 462) in 241 ms on hd275dg.prod.mediav.com (22/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 178.0 in stage 3.0 (TID 499) in 236 ms on hd290dg.prod.mediav.com (23/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 113.0 in stage 3.0 (TID 406) in 251 ms on hd255dg.prod.mediav.com (24/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 116.0 in stage 3.0 (TID 473) in 245 ms on hd272dg.prod.mediav.com (25/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 3.0 (TID 401) in 259 ms on hd283dg.prod.mediav.com (26/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 73.0 in stage 3.0 (TID 456) in 249 ms on hd278dg.prod.mediav.com (27/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 94.0 in stage 3.0 (TID 498) in 243 ms on hd284dg.prod.mediav.com (28/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 506) in 246 ms on hd288dg.prod.mediav.com (29/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 66.0 in stage 3.0 (TID 455) in 254 ms on hd292dg.prod.mediav.com (30/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 91.0 in stage 3.0 (TID 534) in 244 ms on hd290dg.prod.mediav.com (31/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.47.1.38:18554 in memory (size: 2.1 KB, free: 17.2 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 547) in 246 ms on hd292dg.prod.mediav.com (32/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 144.0 in stage 3.0 (TID 477) in 257 ms on hd281dg.prod.mediav.com (33/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 3.0 (TID 568) in 244 ms on hd282dg.prod.mediav.com (34/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 79.0 in stage 3.0 (TID 536) in 251 ms on hd292dg.prod.mediav.com (35/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd262dg.prod.mediav.com:24194 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 96.0 in stage 3.0 (TID 457) in 265 ms on hd260dg.prod.mediav.com (36/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd262dg.prod.mediav.com:49994 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 109.0 in stage 3.0 (TID 576) in 248 ms on hd280dg.prod.mediav.com (37/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 195.0 in stage 3.0 (TID 533) in 254 ms on hd273dg.prod.mediav.com (38/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd266dg.prod.mediav.com:18054 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd281dg.prod.mediav.com:61667 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd291dg.prod.mediav.com:31861 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd278dg.prod.mediav.com:27239 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd261dg.prod.mediav.com:35424 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd257dg.prod.mediav.com:23974 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd290dg.prod.mediav.com:1034 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd271dg.prod.mediav.com:11439 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd283dg.prod.mediav.com:62866 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd284dg.prod.mediav.com:39393 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd276dg.prod.mediav.com:65222 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd260dg.prod.mediav.com:7955 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd258dg.prod.mediav.com:30778 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd280dg.prod.mediav.com:29265 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd272dg.prod.mediav.com:36193 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd264dg.prod.mediav.com:48038 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 147.0 in stage 3.0 (TID 554) in 254 ms on hd283dg.prod.mediav.com (39/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd261dg.prod.mediav.com:17918 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd257dg.prod.mediav.com:11991 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd257dg.prod.mediav.com:13672 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd292dg.prod.mediav.com:9515 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd254dg.prod.mediav.com:37252 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd292dg.prod.mediav.com:8313 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd284dg.prod.mediav.com:33099 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd258dg.prod.mediav.com:44194 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd288dg.prod.mediav.com:15935 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd274dg.prod.mediav.com:50461 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 150.0 in stage 3.0 (TID 514) in 261 ms on hd265dg.prod.mediav.com (40/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd261dg.prod.mediav.com:51708 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd262dg.prod.mediav.com:35030 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd272dg.prod.mediav.com:47434 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd292dg.prod.mediav.com:5167 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 145.0 in stage 3.0 (TID 414) in 277 ms on hd282dg.prod.mediav.com (41/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd290dg.prod.mediav.com:48698 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd273dg.prod.mediav.com:4400 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 3.0 (TID 437) in 273 ms on hd257dg.prod.mediav.com (42/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd287dg.prod.mediav.com:2270 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd290dg.prod.mediav.com:25045 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd282dg.prod.mediav.com:47145 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd254dg.prod.mediav.com:50339 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd286dg.prod.mediav.com:15531 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd275dg.prod.mediav.com:7744 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd285dg.prod.mediav.com:15892 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd273dg.prod.mediav.com:51855 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd261dg.prod.mediav.com:16786 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd278dg.prod.mediav.com:27815 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd281dg.prod.mediav.com:24432 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd258dg.prod.mediav.com:3750 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd284dg.prod.mediav.com:55335 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd260dg.prod.mediav.com:34432 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 82.0 in stage 3.0 (TID 523) in 264 ms on hd261dg.prod.mediav.com (43/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd287dg.prod.mediav.com:53041 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd278dg.prod.mediav.com:1380 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd258dg.prod.mediav.com:8532 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd260dg.prod.mediav.com:45170 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd258dg.prod.mediav.com:61558 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd280dg.prod.mediav.com:1897 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd254dg.prod.mediav.com:28306 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd293dg.prod.mediav.com:18866 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 152.0 in stage 3.0 (TID 589, hd280dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd289dg.prod.mediav.com:22644 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd275dg.prod.mediav.com:27702 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd289dg.prod.mediav.com:48937 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd277dg.prod.mediav.com:60781 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 115.0 in stage 3.0 (TID 590, hd289dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd282dg.prod.mediav.com:54259 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd265dg.prod.mediav.com:15593 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd264dg.prod.mediav.com:38843 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd260dg.prod.mediav.com:20879 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd265dg.prod.mediav.com:65261 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd273dg.prod.mediav.com:6601 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd264dg.prod.mediav.com:28057 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd255dg.prod.mediav.com:8336 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 141.0 in stage 3.0 (TID 489) in 271 ms on hd286dg.prod.mediav.com (44/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 183.0 in stage 3.0 (TID 425) in 281 ms on hd277dg.prod.mediav.com (45/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd287dg.prod.mediav.com:2547 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 76.0 in stage 3.0 (TID 520) in 267 ms on hd261dg.prod.mediav.com (46/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd257dg.prod.mediav.com:6403 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd288dg.prod.mediav.com:58225 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd274dg.prod.mediav.com:25030 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd282dg.prod.mediav.com:51731 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd292dg.prod.mediav.com:31959 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd256dg.prod.mediav.com:22567 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd260dg.prod.mediav.com:38400 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd284dg.prod.mediav.com:46214 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd270dg.prod.mediav.com:33090 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd255dg.prod.mediav.com:62882 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd286dg.prod.mediav.com:5189 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd265dg.prod.mediav.com:62939 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd283dg.prod.mediav.com:27671 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd293dg.prod.mediav.com:46714 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd282dg.prod.mediav.com:42292 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 72.0 in stage 3.0 (TID 493) in 277 ms on hd265dg.prod.mediav.com (47/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd292dg.prod.mediav.com:24852 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 192.0 in stage 3.0 (TID 423) in 287 ms on hd258dg.prod.mediav.com (48/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 3.0 (TID 578) in 267 ms on hd257dg.prod.mediav.com (49/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 124.0 in stage 3.0 (TID 540) in 273 ms on hd257dg.prod.mediav.com (50/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 179.0 in stage 3.0 (TID 419) in 291 ms on hd282dg.prod.mediav.com (51/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd278dg.prod.mediav.com:9221 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 177.0 in stage 3.0 (TID 486) in 286 ms on hd273dg.prod.mediav.com (52/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 3.0 (TID 559) in 276 ms on hd282dg.prod.mediav.com (53/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 3.0 (TID 480) in 288 ms on hd293dg.prod.mediav.com (54/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 54.0 in stage 3.0 (TID 504) in 284 ms on hd272dg.prod.mediav.com (55/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd288dg.prod.mediav.com:64716 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 196.0 in stage 3.0 (TID 575) in 278 ms on hd258dg.prod.mediav.com (56/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 3.0 (TID 581) in 164 ms on hd262dg.prod.mediav.com (57/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 75.0 in stage 3.0 (TID 444) in 297 ms on hd264dg.prod.mediav.com (58/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 3.0 (TID 492) in 291 ms on hd260dg.prod.mediav.com (59/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 133.0 in stage 3.0 (TID 582) in 166 ms on hd290dg.prod.mediav.com (60/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 101.0 in stage 3.0 (TID 567) in 284 ms on hd261dg.prod.mediav.com (61/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 194.0 in stage 3.0 (TID 469) in 298 ms on hd264dg.prod.mediav.com (62/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 135.0 in stage 3.0 (TID 572) in 284 ms on hd260dg.prod.mediav.com (63/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 3.0 (TID 412) in 310 ms on hd265dg.prod.mediav.com (64/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 3.0 (TID 583) in 171 ms on hd262dg.prod.mediav.com (65/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 60.0 in stage 3.0 (TID 584) in 174 ms on hd287dg.prod.mediav.com (66/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 162.0 in stage 3.0 (TID 552) in 297 ms on hd258dg.prod.mediav.com (67/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 168.0 in stage 3.0 (TID 553) in 297 ms on hd257dg.prod.mediav.com (68/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 3.0 (TID 543) in 299 ms on hd293dg.prod.mediav.com (69/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 3.0 (TID 544) in 301 ms on hd258dg.prod.mediav.com (70/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 184.0 in stage 3.0 (TID 476) in 312 ms on hd258dg.prod.mediav.com (71/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 107.0 in stage 3.0 (TID 539) in 304 ms on hd261dg.prod.mediav.com (72/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 102.0 in stage 3.0 (TID 585) in 167 ms on hd290dg.prod.mediav.com (73/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 158.0 in stage 3.0 (TID 564) in 301 ms on hd274dg.prod.mediav.com (74/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 3.0 (TID 516) in 311 ms on hd260dg.prod.mediav.com (75/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 159.0 in stage 3.0 (TID 574) in 304 ms on hd292dg.prod.mediav.com (76/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 90.0 in stage 3.0 (TID 508) in 313 ms on hd273dg.prod.mediav.com (77/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 176.0 in stage 3.0 (TID 531) in 312 ms on hd286dg.prod.mediav.com (78/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 110.0 in stage 3.0 (TID 500) in 318 ms on hd284dg.prod.mediav.com (79/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 3.0 (TID 588) in 160 ms on hd262dg.prod.mediav.com (80/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 3.0 (TID 573) in 309 ms on hd271dg.prod.mediav.com (81/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 199.0 in stage 3.0 (TID 586) in 163 ms on hd290dg.prod.mediav.com (82/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 104.0 in stage 3.0 (TID 587) in 164 ms on hd287dg.prod.mediav.com (83/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 87.0 in stage 3.0 (TID 566) in 314 ms on hd281dg.prod.mediav.com (84/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 74.0 in stage 3.0 (TID 433) in 334 ms on hd254dg.prod.mediav.com (85/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 111.0 in stage 3.0 (TID 441) in 336 ms on hd264dg.prod.mediav.com (86/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 3.0 (TID 418) in 339 ms on hd266dg.prod.mediav.com (87/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd289dg.prod.mediav.com:20447 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 185.0 in stage 3.0 (TID 466) in 334 ms on hd254dg.prod.mediav.com (88/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 3.0 (TID 449) in 336 ms on hd292dg.prod.mediav.com (89/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd262dg.prod.mediav.com:5957 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 198.0 in stage 3.0 (TID 591, hd262dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd256dg.prod.mediav.com:5214 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd285dg.prod.mediav.com:25860 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd285dg.prod.mediav.com:25860 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd288dg.prod.mediav.com:30986 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 155.0 in stage 3.0 (TID 430) in 341 ms on hd275dg.prod.mediav.com (90/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd291dg.prod.mediav.com:47717 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 3.0 (TID 482) in 335 ms on hd254dg.prod.mediav.com (91/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd283dg.prod.mediav.com:19668 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd286dg.prod.mediav.com:64184 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 97.0 in stage 3.0 (TID 438) in 342 ms on hd280dg.prod.mediav.com (92/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd279dg.prod.mediav.com:34009 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd264dg.prod.mediav.com:48358 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd266dg.prod.mediav.com:60772 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd254dg.prod.mediav.com:14962 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 139.0 in stage 3.0 (TID 592, hd266dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hd280dg.prod.mediav.com:4198 (size: 2.4 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd280dg.prod.mediav.com:4198 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd281dg.prod.mediav.com:12163 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd291dg.prod.mediav.com:9728 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 186.0 in stage 3.0 (TID 593, hd291dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 175.0 in stage 3.0 (TID 422) in 348 ms on hd255dg.prod.mediav.com (93/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd271dg.prod.mediav.com:33037 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 137.0 in stage 3.0 (TID 594, hd281dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd279dg.prod.mediav.com:49996 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd281dg.prod.mediav.com:2395 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 62.0 in stage 3.0 (TID 478) in 341 ms on hd289dg.prod.mediav.com (94/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 99.0 in stage 3.0 (TID 522) in 334 ms on hd274dg.prod.mediav.com (95/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd285dg.prod.mediav.com:52469 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd285dg.prod.mediav.com:7398 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd275dg.prod.mediav.com:3649 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd256dg.prod.mediav.com:65180 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd277dg.prod.mediav.com:27612 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd270dg.prod.mediav.com:35197 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd289dg.prod.mediav.com:5555 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 128.0 in stage 3.0 (TID 497) in 341 ms on hd260dg.prod.mediav.com (96/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd289dg.prod.mediav.com:48417 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd262dg.prod.mediav.com:40567 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 140.0 in stage 3.0 (TID 447) in 349 ms on hd270dg.prod.mediav.com (97/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd255dg.prod.mediav.com:24822 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 152.0 in stage 3.0 (TID 589) in 75 ms on hd280dg.prod.mediav.com (98/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd261dg.prod.mediav.com:48502 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 115.0 in stage 3.0 (TID 590) in 74 ms on hd289dg.prod.mediav.com (99/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd274dg.prod.mediav.com:16654 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd283dg.prod.mediav.com:5777 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd280dg.prod.mediav.com:8788 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 3.0 (TID 545) in 338 ms on hd289dg.prod.mediav.com (100/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd285dg.prod.mediav.com:57578 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd256dg.prod.mediav.com:4157 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd290dg.prod.mediav.com:6793 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd290dg.prod.mediav.com:48176 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 129.0 in stage 3.0 (TID 580) in 238 ms on hd262dg.prod.mediav.com (101/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd293dg.prod.mediav.com:23628 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd271dg.prod.mediav.com:63849 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd274dg.prod.mediav.com:58299 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd264dg.prod.mediav.com:49664 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd291dg.prod.mediav.com:54360 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd277dg.prod.mediav.com:6547 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 83.0 in stage 3.0 (TID 404) in 365 ms on hd288dg.prod.mediav.com (102/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd273dg.prod.mediav.com:35935 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd255dg.prod.mediav.com:29011 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 161.0 in stage 3.0 (TID 595, hd255dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd263dg.prod.mediav.com:19616 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd276dg.prod.mediav.com:46848 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 3.0 (TID 440) in 359 ms on hd256dg.prod.mediav.com (103/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd284dg.prod.mediav.com:65293 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd274dg.prod.mediav.com:44964 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 154.0 in stage 3.0 (TID 502) in 351 ms on hd283dg.prod.mediav.com (104/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd281dg.prod.mediav.com:2780 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd293dg.prod.mediav.com:60417 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd265dg.prod.mediav.com:38596 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd272dg.prod.mediav.com:63317 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd276dg.prod.mediav.com:21872 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd271dg.prod.mediav.com:16160 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd279dg.prod.mediav.com:12678 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd291dg.prod.mediav.com:3890 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd275dg.prod.mediav.com:37869 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd263dg.prod.mediav.com:28737 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 3.0 (TID 558) in 346 ms on hd291dg.prod.mediav.com (105/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd286dg.prod.mediav.com:22509 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 51.0 in stage 3.0 (TID 509) in 353 ms on hd286dg.prod.mediav.com (106/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd273dg.prod.mediav.com:19800 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd263dg.prod.mediav.com:30335 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd279dg.prod.mediav.com:34291 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd277dg.prod.mediav.com:27951 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd277dg.prod.mediav.com:17325 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd265dg.prod.mediav.com:41028 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 164.0 in stage 3.0 (TID 431) in 367 ms on hd264dg.prod.mediav.com (107/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd283dg.prod.mediav.com:15357 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd263dg.prod.mediav.com:57200 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd266dg.prod.mediav.com:7561 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd271dg.prod.mediav.com:45534 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 3.0 (TID 496) in 359 ms on hd279dg.prod.mediav.com (108/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 92.0 in stage 3.0 (TID 451) in 365 ms on hd254dg.prod.mediav.com (109/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd266dg.prod.mediav.com:27698 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd257dg.prod.mediav.com:50217 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 156.0 in stage 3.0 (TID 527) in 358 ms on hd281dg.prod.mediav.com (110/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 103.0 in stage 3.0 (TID 446) in 370 ms on hd271dg.prod.mediav.com (111/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 67.0 in stage 3.0 (TID 507) in 362 ms on hd266dg.prod.mediav.com (112/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 3.0 (TID 452) in 373 ms on hd279dg.prod.mediav.com (113/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd282dg.prod.mediav.com:57776 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 80.0 in stage 3.0 (TID 596, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd293dg.prod.mediav.com:25809 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 3.0 (TID 409) in 382 ms on hd291dg.prod.mediav.com (114/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 65.0 in stage 3.0 (TID 465) in 374 ms on hd285dg.prod.mediav.com (115/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 106.0 in stage 3.0 (TID 471) in 374 ms on hd281dg.prod.mediav.com (116/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd278dg.prod.mediav.com:16567 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd272dg.prod.mediav.com:63093 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd276dg.prod.mediav.com:62943 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 180.0 in stage 3.0 (TID 556) in 363 ms on hd285dg.prod.mediav.com (117/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 188.0 in stage 3.0 (TID 434) in 382 ms on hd275dg.prod.mediav.com (118/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 193.0 in stage 3.0 (TID 405) in 389 ms on hd277dg.prod.mediav.com (119/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 132.0 in stage 3.0 (TID 421) in 386 ms on hd256dg.prod.mediav.com (120/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd270dg.prod.mediav.com:3332 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd279dg.prod.mediav.com:11149 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd254dg.prod.mediav.com:6401 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd275dg.prod.mediav.com:52956 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 95.0 in stage 3.0 (TID 526) in 373 ms on hd289dg.prod.mediav.com (121/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd272dg.prod.mediav.com:13836 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 187.0 in stage 3.0 (TID 467) in 382 ms on hd289dg.prod.mediav.com (122/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 78.0 in stage 3.0 (TID 538) in 372 ms on hd270dg.prod.mediav.com (123/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd256dg.prod.mediav.com:54569 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 3.0 (TID 551) in 374 ms on hd274dg.prod.mediav.com (124/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd266dg.prod.mediav.com:60871 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 3.0 (TID 546) in 375 ms on hd261dg.prod.mediav.com (125/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd286dg.prod.mediav.com:20786 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 112.0 in stage 3.0 (TID 579) in 275 ms on hd262dg.prod.mediav.com (126/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 190.0 in stage 3.0 (TID 503) in 382 ms on hd255dg.prod.mediav.com (127/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd263dg.prod.mediav.com:5872 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd255dg.prod.mediav.com:22114 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd270dg.prod.mediav.com:6944 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd288dg.prod.mediav.com:3459 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 3.0 (TID 439) in 393 ms on hd274dg.prod.mediav.com (128/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 3.0 (TID 521) in 381 ms on hd283dg.prod.mediav.com (129/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 122.0 in stage 3.0 (TID 408) in 400 ms on hd280dg.prod.mediav.com (130/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd270dg.prod.mediav.com:5548 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 172.0 in stage 3.0 (TID 450) in 395 ms on hd256dg.prod.mediav.com (131/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 100.0 in stage 3.0 (TID 525) in 384 ms on hd285dg.prod.mediav.com (132/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 121.0 in stage 3.0 (TID 468) in 393 ms on hd290dg.prod.mediav.com (133/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 49.0 in stage 3.0 (TID 535) in 385 ms on hd293dg.prod.mediav.com (134/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 3.0 (TID 485) in 394 ms on hd290dg.prod.mediav.com (135/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 174.0 in stage 3.0 (TID 597, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 69.0 in stage 3.0 (TID 513) in 391 ms on hd264dg.prod.mediav.com (136/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 3.0 (TID 435) in 403 ms on hd271dg.prod.mediav.com (137/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 3.0 (TID 565) in 385 ms on hd291dg.prod.mediav.com (138/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 120.0 in stage 3.0 (TID 416) in 409 ms on hd273dg.prod.mediav.com (139/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 3.0 (TID 470) in 402 ms on hd277dg.prod.mediav.com (140/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 134.0 in stage 3.0 (TID 494) in 398 ms on hd255dg.prod.mediav.com (141/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 3.0 (TID 515) in 397 ms on hd263dg.prod.mediav.com (142/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 191.0 in stage 3.0 (TID 550) in 393 ms on hd276dg.prod.mediav.com (143/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 181.0 in stage 3.0 (TID 570) in 392 ms on hd284dg.prod.mediav.com (144/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 89.0 in stage 3.0 (TID 479) in 405 ms on hd274dg.prod.mediav.com (145/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 157.0 in stage 3.0 (TID 532) in 399 ms on hd281dg.prod.mediav.com (146/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 3.0 (TID 519) in 401 ms on hd293dg.prod.mediav.com (147/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 136.0 in stage 3.0 (TID 541) in 401 ms on hd276dg.prod.mediav.com (148/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 81.0 in stage 3.0 (TID 517) in 406 ms on hd265dg.prod.mediav.com (149/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 197.0 in stage 3.0 (TID 598, hd293dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 123.0 in stage 3.0 (TID 413) in 422 ms on hd272dg.prod.mediav.com (150/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 77.0 in stage 3.0 (TID 407) in 425 ms on hd279dg.prod.mediav.com (151/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 118.0 in stage 3.0 (TID 524) in 407 ms on hd271dg.prod.mediav.com (152/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 170.0 in stage 3.0 (TID 561) in 406 ms on hd263dg.prod.mediav.com (153/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 3.0 (TID 562) in 407 ms on hd275dg.prod.mediav.com (154/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 50.0 in stage 3.0 (TID 458) in 423 ms on hd291dg.prod.mediav.com (155/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 108.0 in stage 3.0 (TID 528) in 412 ms on hd286dg.prod.mediav.com (156/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 146.0 in stage 3.0 (TID 577) in 408 ms on hd273dg.prod.mediav.com (157/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 151.0 in stage 3.0 (TID 563) in 411 ms on hd277dg.prod.mediav.com (158/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 163.0 in stage 3.0 (TID 548) in 415 ms on hd263dg.prod.mediav.com (159/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 148.0 in stage 3.0 (TID 555) in 414 ms on hd279dg.prod.mediav.com (160/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 189.0 in stage 3.0 (TID 428) in 434 ms on hd277dg.prod.mediav.com (161/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 167.0 in stage 3.0 (TID 454) in 431 ms on hd265dg.prod.mediav.com (162/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 57.0 in stage 3.0 (TID 510) in 426 ms on hd266dg.prod.mediav.com (163/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 143.0 in stage 3.0 (TID 549) in 421 ms on hd283dg.prod.mediav.com (164/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 125.0 in stage 3.0 (TID 475) in 433 ms on hd263dg.prod.mediav.com (165/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 93.0 in stage 3.0 (TID 460) in 435 ms on hd271dg.prod.mediav.com (166/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 3.0 (TID 511) in 430 ms on hd266dg.prod.mediav.com (167/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 160.0 in stage 3.0 (TID 487) in 435 ms on hd257dg.prod.mediav.com (168/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 186.0 in stage 3.0 (TID 593) in 98 ms on hd291dg.prod.mediav.com (169/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 198.0 in stage 3.0 (TID 591) in 105 ms on hd262dg.prod.mediav.com (170/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 3.0 (TID 594) in 100 ms on hd281dg.prod.mediav.com (171/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 139.0 in stage 3.0 (TID 592) in 104 ms on hd266dg.prod.mediav.com (172/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 165.0 in stage 3.0 (TID 436) in 447 ms on hd282dg.prod.mediav.com (173/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 55.0 in stage 3.0 (TID 442) in 449 ms on hd278dg.prod.mediav.com (174/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 3.0 (TID 505) in 440 ms on hd293dg.prod.mediav.com (175/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 173.0 in stage 3.0 (TID 472) in 448 ms on hd276dg.prod.mediav.com (176/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 138.0 in stage 3.0 (TID 518) in 441 ms on hd272dg.prod.mediav.com (177/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 63.0 in stage 3.0 (TID 474) in 448 ms on hd270dg.prod.mediav.com (178/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 3.0 (TID 463) in 452 ms on hd279dg.prod.mediav.com (179/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 171.0 in stage 3.0 (TID 542) in 441 ms on hd272dg.prod.mediav.com (180/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 142.0 in stage 3.0 (TID 484) in 450 ms on hd275dg.prod.mediav.com (181/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 182.0 in stage 3.0 (TID 557) in 441 ms on hd254dg.prod.mediav.com (182/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 84.0 in stage 3.0 (TID 415) in 463 ms on hd266dg.prod.mediav.com (183/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 130.0 in stage 3.0 (TID 427) in 461 ms on hd256dg.prod.mediav.com (184/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 85.0 in stage 3.0 (TID 537) in 446 ms on hd286dg.prod.mediav.com (185/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 153.0 in stage 3.0 (TID 529) in 449 ms on hd263dg.prod.mediav.com (186/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 149.0 in stage 3.0 (TID 459) in 459 ms on hd255dg.prod.mediav.com (187/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 3.0 (TID 410) in 468 ms on hd270dg.prod.mediav.com (188/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 71.0 in stage 3.0 (TID 432) in 465 ms on hd288dg.prod.mediav.com (189/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 61.0 in stage 3.0 (TID 560) in 449 ms on hd270dg.prod.mediav.com (190/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 3.0 (TID 569) in 449 ms on hd285dg.prod.mediav.com (191/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 114.0 in stage 3.0 (TID 571) in 448 ms on hd280dg.prod.mediav.com (192/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 80.0 in stage 3.0 (TID 596) in 91 ms on hd293dg.prod.mediav.com (193/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 174.0 in stage 3.0 (TID 597) in 68 ms on hd293dg.prod.mediav.com (194/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd276dg.prod.mediav.com:15025 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 98.0 in stage 3.0 (TID 599, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 197.0 in stage 3.0 (TID 598) in 54 ms on hd293dg.prod.mediav.com (195/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 56.0 in stage 3.0 (TID 411) in 479 ms on hd276dg.prod.mediav.com (196/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on hd280dg.prod.mediav.com:6093 in memory (size: 2.1 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 59.0 in stage 3.0 (TID 488) in 472 ms on hd280dg.prod.mediav.com (197/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.47.1.38:18554 in memory (size: 2.2 KB, free: 17.2 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on hd292dg.prod.mediav.com:5167 in memory (size: 2.2 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Starting task 169.0 in stage 3.0 (TID 600, hd276dg.prod.mediav.com, PROCESS_LOCAL, 1534 bytes)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 98.0 in stage 3.0 (TID 599) in 34 ms on hd276dg.prod.mediav.com (198/200)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 169.0 in stage 3.0 (TID 600) in 26 ms on hd276dg.prod.mediav.com (199/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.47.1.38:18554 in memory (size: 2.0 KB, free: 17.2 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd261dg.prod.mediav.com:35424 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd266dg.prod.mediav.com:18054 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd280dg.prod.mediav.com:4198 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd258dg.prod.mediav.com:30778 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd270dg.prod.mediav.com:5548 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd276dg.prod.mediav.com:15025 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd288dg.prod.mediav.com:3459 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd275dg.prod.mediav.com:37869 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd278dg.prod.mediav.com:27239 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd283dg.prod.mediav.com:15357 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd283dg.prod.mediav.com:62866 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd264dg.prod.mediav.com:48358 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd258dg.prod.mediav.com:44194 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd266dg.prod.mediav.com:60772 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd271dg.prod.mediav.com:33037 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd264dg.prod.mediav.com:48038 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd262dg.prod.mediav.com:49994 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd271dg.prod.mediav.com:45534 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd274dg.prod.mediav.com:44964 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd284dg.prod.mediav.com:65293 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd266dg.prod.mediav.com:27698 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd291dg.prod.mediav.com:31861 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd289dg.prod.mediav.com:20447 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd293dg.prod.mediav.com:18866 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd286dg.prod.mediav.com:15531 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd275dg.prod.mediav.com:7744 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd254dg.prod.mediav.com:37252 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd285dg.prod.mediav.com:25860 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd277dg.prod.mediav.com:27951 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd263dg.prod.mediav.com:57200 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd260dg.prod.mediav.com:7955 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd271dg.prod.mediav.com:11439 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd257dg.prod.mediav.com:11991 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd278dg.prod.mediav.com:9221 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd281dg.prod.mediav.com:12163 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd284dg.prod.mediav.com:33099 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd261dg.prod.mediav.com:17918 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd273dg.prod.mediav.com:19800 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd286dg.prod.mediav.com:22509 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd257dg.prod.mediav.com:50217 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd254dg.prod.mediav.com:50339 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd292dg.prod.mediav.com:9515 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd292dg.prod.mediav.com:8313 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd272dg.prod.mediav.com:36193 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd292dg.prod.mediav.com:5167 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd276dg.prod.mediav.com:62943 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd263dg.prod.mediav.com:19616 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd281dg.prod.mediav.com:61667 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd256dg.prod.mediav.com:5214 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd280dg.prod.mediav.com:29265 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd270dg.prod.mediav.com:35197 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd274dg.prod.mediav.com:16654 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd282dg.prod.mediav.com:47145 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd274dg.prod.mediav.com:50461 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd261dg.prod.mediav.com:48502 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd287dg.prod.mediav.com:2270 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd293dg.prod.mediav.com:46714 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd263dg.prod.mediav.com:5872 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd257dg.prod.mediav.com:23974 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd263dg.prod.mediav.com:30335 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd262dg.prod.mediav.com:24194 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd281dg.prod.mediav.com:2395 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd274dg.prod.mediav.com:58299 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd264dg.prod.mediav.com:49664 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd273dg.prod.mediav.com:51855 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd271dg.prod.mediav.com:63849 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd257dg.prod.mediav.com:13672 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd265dg.prod.mediav.com:41028 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd254dg.prod.mediav.com:6401 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd276dg.prod.mediav.com:46848 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd286dg.prod.mediav.com:20786 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd271dg.prod.mediav.com:16160 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd287dg.prod.mediav.com:53041 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd262dg.prod.mediav.com:5957 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd265dg.prod.mediav.com:15593 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd289dg.prod.mediav.com:48937 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd288dg.prod.mediav.com:15935 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd255dg.prod.mediav.com:62882 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd276dg.prod.mediav.com:65222 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd280dg.prod.mediav.com:1897 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd284dg.prod.mediav.com:39393 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd255dg.prod.mediav.com:22114 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd285dg.prod.mediav.com:15892 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd272dg.prod.mediav.com:63317 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd263dg.prod.mediav.com:28737 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd277dg.prod.mediav.com:60781 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd270dg.prod.mediav.com:3332 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd285dg.prod.mediav.com:52469 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd290dg.prod.mediav.com:48698 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd273dg.prod.mediav.com:6601 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd280dg.prod.mediav.com:6093 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd283dg.prod.mediav.com:27671 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd282dg.prod.mediav.com:42292 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd289dg.prod.mediav.com:48417 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd266dg.prod.mediav.com:7561 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd260dg.prod.mediav.com:34432 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd260dg.prod.mediav.com:45170 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd282dg.prod.mediav.com:54259 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd280dg.prod.mediav.com:8788 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd273dg.prod.mediav.com:35935 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd270dg.prod.mediav.com:6944 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd275dg.prod.mediav.com:52956 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd261dg.prod.mediav.com:51708 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd272dg.prod.mediav.com:63093 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd290dg.prod.mediav.com:1034 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd265dg.prod.mediav.com:65261 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd288dg.prod.mediav.com:58225 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd277dg.prod.mediav.com:17325 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd262dg.prod.mediav.com:40567 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd293dg.prod.mediav.com:60417 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd277dg.prod.mediav.com:27612 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd279dg.prod.mediav.com:49996 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd258dg.prod.mediav.com:8532 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd288dg.prod.mediav.com:30986 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd291dg.prod.mediav.com:47717 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd265dg.prod.mediav.com:38596 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd293dg.prod.mediav.com:25809 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd273dg.prod.mediav.com:4400 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd283dg.prod.mediav.com:5777 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd285dg.prod.mediav.com:57578 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd272dg.prod.mediav.com:47434 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd260dg.prod.mediav.com:20879 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd281dg.prod.mediav.com:2780 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd256dg.prod.mediav.com:4157 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd255dg.prod.mediav.com:29011 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd286dg.prod.mediav.com:5189 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd285dg.prod.mediav.com:7398 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd289dg.prod.mediav.com:22644 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd283dg.prod.mediav.com:19668 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd255dg.prod.mediav.com:8336 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd279dg.prod.mediav.com:12678 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd254dg.prod.mediav.com:14962 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd292dg.prod.mediav.com:31959 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd282dg.prod.mediav.com:57776 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd264dg.prod.mediav.com:28057 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd258dg.prod.mediav.com:61558 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd256dg.prod.mediav.com:22567 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd279dg.prod.mediav.com:34291 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd256dg.prod.mediav.com:54569 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd261dg.prod.mediav.com:16786 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd256dg.prod.mediav.com:65180 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd279dg.prod.mediav.com:34009 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd260dg.prod.mediav.com:38400 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd278dg.prod.mediav.com:16567 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd293dg.prod.mediav.com:23628 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd290dg.prod.mediav.com:25045 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd264dg.prod.mediav.com:38843 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd282dg.prod.mediav.com:51731 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd276dg.prod.mediav.com:21872 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd278dg.prod.mediav.com:27815 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd291dg.prod.mediav.com:9728 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd258dg.prod.mediav.com:3750 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd275dg.prod.mediav.com:3649 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd275dg.prod.mediav.com:27702 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.TaskSetManager: Finished task 161.0 in stage 3.0 (TID 595) in 255 ms on hd255dg.prod.mediav.com (200/200)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd270dg.prod.mediav.com:33090 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO cluster.YarnClusterScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool default
15/09/15 18:50:57 INFO scheduler.DAGScheduler: ResultStage 3 (collect at DecisionTree.scala:977) finished in 0.624 s
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd274dg.prod.mediav.com:25030 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd284dg.prod.mediav.com:55335 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO scheduler.DAGScheduler: Job 3 finished: collect at DecisionTree.scala:977, took 0.644801 s
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd262dg.prod.mediav.com:35030 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd287dg.prod.mediav.com:2547 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd289dg.prod.mediav.com:5555 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd266dg.prod.mediav.com:60871 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd288dg.prod.mediav.com:64716 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd277dg.prod.mediav.com:6547 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd254dg.prod.mediav.com:28306 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd290dg.prod.mediav.com:6793 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd278dg.prod.mediav.com:1380 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd291dg.prod.mediav.com:54360 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd286dg.prod.mediav.com:64184 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd281dg.prod.mediav.com:24432 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd257dg.prod.mediav.com:6403 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd265dg.prod.mediav.com:62939 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd255dg.prod.mediav.com:24822 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd279dg.prod.mediav.com:11149 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd291dg.prod.mediav.com:3890 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd292dg.prod.mediav.com:24852 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd290dg.prod.mediav.com:48176 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd284dg.prod.mediav.com:46214 in memory (size: 2.0 KB, free: 8.6 GB)
15/09/15 18:50:57 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on hd272dg.prod.mediav.com:13836 in memory (size: 2.0 KB, free: 8.6 GB)



",,Vimin_Wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10433,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-09-15 12:00:59.381,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 14:48:08 UTC 2015,,,,,0|i2k72f:,9223372036854775807,,,,,,,,,,,,,"15/Sep/15 12:00;srowen;[~Vimin_Wu] Please search JIRA first, as this is almost certainly a duplicate of the issue I mentioned. Also, it's great that you try to provide log info, but you should probably narrow this down to the relevant info that focuses on what you believe the problem is.","15/Sep/15 14:22;Vimin_Wu;
Dear Mr Owen,

I'm sorry. I didn't narrow the logs, because I didn't know where is the problem. I test with a smaller data, it works well. However, it get stuck when I use a bigger data. 

I have searched the JIRA and didn't find any similar problem like this. Can you give a link of the similar problem.

 I will be grateful for anything you can help me on this matter!

Thank you!


wu_wenmin@126.com
 
From: Sean Owen (JIRA)
Date: 2015-09-15 20:01
To: wu_wenmin
Subject: [jira] [Resolved] (SPARK-10616) GradientBoostedTrees stuck with 2958359 features train data
 
     [ https://issues.apache.org/jira/browse/SPARK-10616?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]
 
Sean Owen resolved SPARK-10616.
-------------------------------
    Resolution: Duplicate
 
[~Vimin_Wu] Please search JIRA first, as this is almost certainly a duplicate of the issue I mentioned. Also, it's great that you try to provide log info, but you should probably narrow this down to the relevant info that focuses on what you believe the problem is.
 
 
 
 
--
This message was sent by Atlassian JIRA
(v6.3.4#6332)
",15/Sep/15 14:48;srowen;This JIRA is already resolved as a duplicate of https://issues.apache.org/jira/browse/SPARK-10433 -- that's what I mean.,,,,,,,,,,,,,,,,,,,,,
Decision tree binary classification with ordered categorical features: incorrect centroid,SPARK-10524,12862916,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,josephkb,josephkb,10/Sep/15 00:46,10/Feb/16 01:25,15/Aug/18 23:03,10/Feb/16 01:13,1.5.0,1.6.0,,,,,,,,,,,,,,1.6.1,2.0.0,,,,,ML,MLlib,,,,0,,,,,"In DecisionTree and RandomForest binary classification with ordered categorical features, we order categories' bins based on the hard prediction, but we should use the soft prediction.

Here are the 2 places in mllib and ml:
* [https://github.com/apache/spark/blob/45de518742446ddfbd4816c9d0f8501139f9bc2d/mllib/src/main/scala/org/apache/spark/mllib/tree/DecisionTree.scala#L887]
* [https://github.com/apache/spark/blob/45de518742446ddfbd4816c9d0f8501139f9bc2d/mllib/src/main/scala/org/apache/spark/ml/tree/impl/RandomForest.scala#L779]

The PR which fixes this should include a unit test which isolates this issue, ideally by directly calling binsToBestSplit.",,apachespark,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-09-13 15:31:02.164,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 10 01:25:17 UTC 2016,,,,,0|i2jz27:,9223372036854775807,josephkb,,,,,1.6.1,2.0.0,,,,,,"13/Sep/15 15:31;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/8734",10/Feb/16 01:25;josephkb;Note: I only backported to 1.6 since this may affect the results of user code (though it should improve it in general).,,,,,,,,,,,,,,,,,,,,,,
Gradient boosted trees: increasing input size in 1.4,SPARK-10433,12861575,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,srowen,srowen,03/Sep/15 13:51,24/Mar/16 11:52,15/Aug/18 23:03,24/Mar/16 11:52,1.4.1,,,,,,,,,,,,,,,1.5.0,,,,,,MLlib,,,,,1,,,,,"(Sorry to say I don't have any leads on a fix, but this was reported by three different people and I confirmed it at fairly close range, so think it's legitimate:)

This is probably best explained in the words from the mailing list thread at http://mail-archives.apache.org/mod_mbox/spark-user/201509.mbox/%3C55E84380.2000408%40gmail.com%3E . Matt Forbes says:

{quote}
I am training a boosted trees model on a couple million input samples (with around 300 features) and am noticing that the input size of each stage is increasing each iteration. For each new tree, the first step seems to be building the decision tree metadata, which does a .count() on the input data, so this is the step I've been using to track the input size changing. Here is what I'm seeing: 
{quote}

{code}
count at DecisionTreeMetadata.scala:111 
1. Input Size / Records: 726.1 MB / 1295620 
2. Input Size / Records: 106.9 GB / 64780816 
3. Input Size / Records: 160.3 GB / 97171224 
4. Input Size / Records: 214.8 GB / 129680959 
5. Input Size / Records: 268.5 GB / 162533424 
.... 
Input Size / Records: 1912.6 GB / 1382017686 
.... 
{code}

{quote}
This step goes from taking less than 10s up to 5 minutes by the 15th or so iteration. I'm not quite sure what could be causing this. I am passing a memory-only cached RDD[LabeledPoint] to GradientBoostedTrees.train 
{quote}

Johannes Bauer showed me a very similar problem.

Peter Rudenko offers this sketch of a reproduction:

{code}
val boostingStrategy = BoostingStrategy.defaultParams(""Classification"")
    boostingStrategy.setNumIterations(30)
    boostingStrategy.setLearningRate(1.0)
    boostingStrategy.treeStrategy.setMaxDepth(3)
    boostingStrategy.treeStrategy.setMaxBins(128)
    boostingStrategy.treeStrategy.setSubsamplingRate(1.0)
    boostingStrategy.treeStrategy.setMinInstancesPerNode(1)
    boostingStrategy.treeStrategy.setUseNodeIdCache(true)
    boostingStrategy.treeStrategy.setCategoricalFeaturesInfo(
      mapAsJavaMap(categoricalFeatures).asInstanceOf[java.util.Map[java.lang.Integer, java.lang.Integer]])

val model = GradientBoostedTrees.train(instances, boostingStrategy)
{code}
",,dbtsai,josephkb,prudenko,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6684,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-09-04 20:54:06.997,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 24 11:52:01 UTC 2016,,,,,0|i2jqxb:,9223372036854775807,,,,,,,,,,,,,"04/Sep/15 20:54;josephkb;Has this been reported on 1.5?  I've seen reports for 1.4, but was told by [~dbtsai] that 1.5 seems to have fixed this issue.  I believe that the caching (and optional checkpointing) added in 1.5 fix this issue, but it would be great to get confirmation.",04/Sep/15 22:28;dbtsai;[~sowen] I can confirm that this should be fixed in 1.5,"05/Sep/15 11:42;srowen;Oh that's great, do you know what change might have fixed it, or be related? I can resolve as a duplicate.","05/Sep/15 23:09;josephkb;I'm pretty sure it was this one: [https://issues.apache.org/jira/browse/SPARK-6684].  The only possible alternative would be a fix in Core, but I think it's this one.",06/Sep/15 08:37;srowen;Quite possible; would that have resulted in excessively large inputs to each stage? I was seeing that megabytes of input to the trees suddenly became gigabytes over many iterations. The number of records exploded for some reason. That itself doesn't seem like a problem of long lineage but I might miss the connection.,06/Sep/15 14:20;dbtsai;++1 I saw exactly the same symptom and I was wondering the same question.,"08/Sep/15 23:12;josephkb;I had seen the input size growing, but I missed the growth in number of records when I first glanced at this JIRA.  That's really strange.  I'll try to reproduce it in case it's a Spark Core bug.  It may have been inadvertently fixed by my caching/checkpointing patch in 1.5.","09/Sep/15 03:26;josephkb;Here's some minimal code to reproduce it:
{code}
from pyspark.ml.regression import GBTRegressor
from pyspark.mllib.util import MLUtils

df = MLUtils.loadLibSVMFile(sc, ""data/mllib/sample_libsvm_data.txt"").toDF()

gbt = GBTRegressor(maxIter=50)
model = gbt.fit(df)
{code}

If you run that with branch-1.4, the input size grows with each iteration.  However, if you add {{data.cache()}} after each time ""data"" is updated, then the bug is fixed.  ""data"" is defined here: [https://github.com/apache/spark/blob/8f82bb440879b82012b78563347d00b55547a964/mllib/src/main/scala/org/apache/spark/mllib/tree/GradientBoostedTrees.scala#L192]

[~andrewor14] This is the bug I mentioned.  It's weird that caching fixes it.  I'll next try using 1.4's version of GBT in 1.5 to see if it is fixed within 1.5.","09/Sep/15 04:43;josephkb;I tried removing this one patch: [https://github.com/jkbradley/spark/commit/be7be6d4c7d978c20e601d1f5f56ecb3479814cb] from the current master branch (i.e., removed caching on each iteration).  That re-introduced the bug.  Here's the branch: [https://github.com/jkbradley/spark/tree/gbt-debug]","21/Mar/16 22:20;josephkb;I'm closing this since it seems to have been fixed in 1.5, but please say if it has occurred again after that.","24/Mar/16 11:52;srowen;Just re-resolving as duplicate to reflect the resolution, thank you.",,,,,,,,,,,,,
DenseMatrix gives different hashcode even though equals returns true,SPARK-10414,12861232,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Duplicate,,vinodkc,vinodkc,02/Sep/15 06:57,05/Sep/15 03:11,15/Aug/18 23:03,04/Sep/15 20:50,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"hashcode implementation in DenseMatrix gives different result for same input

val dm = Matrices.dense(2, 2, Array(0.0, 1.0, 2.0, 3.0))
    val dm1 = Matrices.dense(2, 2, Array(0.0, 1.0, 2.0, 3.0))
    assert(dm1 === dm) // passed
    assert(dm1.hashCode === dm.hashCode) // Failed

This violates the hashCode/equals contract.",,apachespark,josephkb,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9919,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-09-02 07:02:07.346,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 05 03:11:47 UTC 2015,,,,,0|i2joun:,9223372036854775807,,,,,,,,,,,,,"02/Sep/15 07:02;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/8565","04/Sep/15 20:50;josephkb;This looks like a duplicate of an existing JIRA and PR.  [~vinodkc], could you please close this and help review the existing PR?  Thanks!","05/Sep/15 02:57;vinodkc;[~josephkb]
Could you please share me that existing JIRA id to review the PR
Thanks
","05/Sep/15 03:11;vinodkc;Thanks
Got the JIRA id
https://issues.apache.org/jira/browse/SPARK-9919",,,,,,,,,,,,,,,,,,,,
model.predictAll() fails at user_product.first(),SPARK-10361,12860619,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Cannot Reproduce,,atxvelu,atxvelu,31/Aug/15 07:22,03/Sep/15 08:33,15/Aug/18 23:03,03/Sep/15 08:33,1.3.1,1.4.1,1.5.0,,,,,,,,,,,,,,,,,,,MLlib,PySpark,,,,0,,,,,"This code, adapted from the documentation, fails when calling PredictAll() after an ALS.train()


15/08/31 00:11:45 ERROR PythonRDD: Python worker exited unexpectedly (crashed)
java.net.SocketException: Connection reset by peer: socket write error
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(Unknown Source)
	at java.net.SocketOutputStream.write(Unknown Source)
	at java.io.BufferedOutputStream.write(Unknown Source)
	at java.io.DataOutputStream.write(Unknown Source)
	at java.io.FilterOutputStream.write(Unknown Source)
	at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:413)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:425)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)
15/08/31 00:11:45 ERROR PythonRDD: This may have been caused by a prior exception:
java.net.SocketException: Connection reset by peer: socket write error
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(Unknown Source)
	at java.net.SocketOutputStream.write(Unknown Source)
	at java.io.BufferedOutputStream.write(Unknown Source)
	at java.io.DataOutputStream.write(Unknown Source)
	at java.io.FilterOutputStream.write(Unknown Source)
	at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:413)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:425)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)
15/08/31 00:11:45 ERROR Executor: Exception in task 0.0 in stage 187.0 (TID 85)
java.net.SocketException: Connection reset by peer: socket write error
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(Unknown Source)
	at java.net.SocketOutputStream.write(Unknown Source)
	at java.io.BufferedOutputStream.write(Unknown Source)
	at java.io.DataOutputStream.write(Unknown Source)
	at java.io.FilterOutputStream.write(Unknown Source)
	at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:413)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:425)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)
15/08/31 00:11:45 WARN TaskSetManager: Lost task 0.0 in stage 187.0 (TID 85, localhost): java.net.SocketException: Connection reset by peer: socket write error
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(Unknown Source)
	at java.net.SocketOutputStream.write(Unknown Source)
	at java.io.BufferedOutputStream.write(Unknown Source)
	at java.io.DataOutputStream.write(Unknown Source)
	at java.io.FilterOutputStream.write(Unknown Source)
	at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:413)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:425)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)

15/08/31 00:11:45 ERROR TaskSetManager: Task 0 in stage 187.0 failed 1 times; aborting job
15/08/31 00:11:45 INFO TaskSchedulerImpl: Removed TaskSet 187.0, whose tasks have all completed, from pool 
15/08/31 00:11:45 INFO TaskSchedulerImpl: Cancelling stage 187
15/08/31 00:11:45 INFO DAGScheduler: ResultStage 187 (runJob at PythonRDD.scala:366) failed in 0.427 s
15/08/31 00:11:45 INFO DAGScheduler: Job 16 failed: runJob at PythonRDD.scala:366, took 0.434198 s
Traceback (most recent call last):
  File ""C:\Program Files (x86)\JetBrains\PyCharm Community Edition 4.5.3\helpers\pydev\pydevd.py"", line 2358, in <module>
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Program Files (x86)\JetBrains\PyCharm Community Edition 4.5.3\helpers\pydev\pydevd.py"", line 1778, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:/Users/thirumalaiv1/PycharmProjects/MovieLensALS/MovieLensALS.py"", line 137, in <module>
    validationRmse = computeRmse(model, validation, numValidation)
  File ""C:/Users/thirumalaiv1/PycharmProjects/MovieLensALS/MovieLensALS.py"", line 49, in computeRmse
    predictions = model.predictAll(data).map(lambda x: ((x[0], x[1]), x[2]))
  File ""C:\spark-1.4.1\python\pyspark\mllib\recommendation.py"", line 126, in predictAll
    first = user_product.first()
  File ""C:\spark-1.4.1\python\pyspark\rdd.py"", line 1295, in first
    rs = self.take(1)
  File ""C:\spark-1.4.1\python\pyspark\rdd.py"", line 1277, in take
    res = self.context.runJob(self, takeUpToNumLeft, p, True)
  File ""C:\spark-1.4.1\python\pyspark\context.py"", line 897, in runJob
    allowLocal)
  File ""C:\Users\thirumalaiv1\PyCharmVirtualEnv\MovieLensALSVirtEnv\lib\site-packages\py4j\java_gateway.py"", line 813, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""C:\Users\thirumalaiv1\PyCharmVirtualEnv\MovieLensALSVirtEnv\lib\site-packages\py4j\protocol.py"", line 308, in get_return_value
    format(target_id, ""."", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 187.0 failed 1 times, most recent failure: Lost task 0.0 in stage 187.0 (TID 85, localhost): java.net.SocketException: Connection reset by peer: socket write error
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(Unknown Source)
	at java.net.SocketOutputStream.write(Unknown Source)
	at java.io.BufferedOutputStream.write(Unknown Source)
	at java.io.DataOutputStream.write(Unknown Source)
	at java.io.FilterOutputStream.write(Unknown Source)
	at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:413)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:425)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)","Windows 10, Python 2.7 and with all the three versions of Spark",atxvelu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-08-31 07:42:56.253,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 31 16:49:03 UTC 2015,,,,,0|i2jl53:,9223372036854775807,,,,,,,,,,,,,31/Aug/15 07:42;srowen;Doesn't this just appear to be a network problem or other problem specific to your cluster? The error is SocketException,"31/Aug/15 07:49;atxvelu;[~srowen] I'm running a standalone version of spark on windows.
I didn't see any process crash or anything suspicious in the firewall logs -- let me know if I'm missing something ?
",31/Aug/15 08:10;srowen;The error is right here though -- your Python and JVM processes aren't able to communicate. ,"31/Aug/15 16:19;atxvelu;Thanks [~srowen].

Is this a known issue, any suggestions ?","31/Aug/15 16:49;srowen;The tests are passing and I haven't heard anything like this. It does point to a local problem. At least, this stack trace is not the problem per se; the Python process wasn't able to connect to the JVM. You'd need to see why.",,,,,,,,,,,,,,,,,,,
MLlib BLAS gemm outputs wrong result when beta = 0.0 for transpose transpose matrix multiplication,SPARK-10353,12860543,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,30/Aug/15 05:30,02/Sep/15 20:34,15/Aug/18 23:03,02/Sep/15 20:33,1.3.1,1.4.1,1.5.0,,,,,,,,,,,,,1.3.2,1.4.2,1.5.0,,,,MLlib,,,,,0,,,,,"Basically 
{code}
if (beta != 0.0) {
  f2jBLAS.dscal(C.values.length, beta, C.values, 1)
}
{code}
should be
{code}
if (beta != 1.0) {
  f2jBLAS.dscal(C.values.length, beta, C.values, 1)
}
{code}",,apachespark,brkyvz,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-08-30 05:32:02.764,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 02 20:33:34 UTC 2015,,,,,0|i2jkon:,9223372036854775807,,,,,,,,,,,,,"30/Aug/15 05:32;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/8525",30/Aug/15 19:27;mengxr;Leave the JIRA open for 1.3.,"02/Sep/15 11:05;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/8572","02/Sep/15 20:33;mengxr;Issue resolved by pull request 8572
[https://github.com/apache/spark/pull/8572]",,,,,,,,,,,,,,,,,,,,
GeneralizedLinearModel doesn't unpersist cached data,SPARK-10182,12858287,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,wildfire,wildfire,wildfire,24/Aug/15 12:45,27/Aug/15 17:58,15/Aug/18 23:03,27/Aug/15 17:58,1.4.1,,,,,,,,,,,,,,,1.6.0,,,,,,MLlib,,,,,0,,,,,"The problem might be reproduced in spark-shell with following code snippet:

{code}
import org.apache.spark.SparkContext
import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val samples = Seq[LabeledPoint](
  LabeledPoint(1.0, Vectors.dense(1.0, 0.0)),
  LabeledPoint(1.0, Vectors.dense(0.0, 1.0)),
  LabeledPoint(0.0, Vectors.dense(1.0, 1.0)),
  LabeledPoint(0.0, Vectors.dense(0.0, 0.0))
)

val rdd = sc.parallelize(samples)

for (i <- 0 until 10) {
  val model = {
    new LogisticRegressionWithLBFGS()
      .setNumClasses(2)
      .run(rdd)
      .clearThreshold()
  }
}
{code}

After code execution there are 10 {{MapPartitionsRDD}} objects on ""Storage"" tab in Spark application UI.",,apachespark,wildfire,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-08-24 14:20:46.518,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 27 17:58:16 UTC 2015,,,,,0|i2ja8v:,9223372036854775807,,,,,,1.6.0,,,,,,,24/Aug/15 14:20;srowen;I can't reproduce this. I ran this in the spark-shell and saw no cached RDDs after. Can you try the latest to verify?,"24/Aug/15 14:25;wildfire;Yes, please see screenshot attached: !http://piqqin.com/img/945dea5edcb132d9f7eac9969595c660.png!

Actually, I have a fix for this issue & I'm preparing the pull request.",24/Aug/15 14:27;srowen;This is still vs 1.4.1; I'm running master. It's important to narrow that much down. Especially if you're opening a PR vs master.,"24/Aug/15 14:28;apachespark;User 'SlavikBaranov' has created a pull request for this issue:
https://github.com/apache/spark/pull/8395","24/Aug/15 15:01;wildfire;Sorry, it was wrong spark-shell. Behaviour in master is slightly different: It looks like RDDs are removed from cache on GC. I had to modify the code a bit to reproduce the issue:

{code}
import org.apache.spark.SparkContext
import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

for (i <- 0 until 100) {
  val samples = Seq[LabeledPoint](
    LabeledPoint(1.0, Vectors.dense(1.0, 0.0)),
    LabeledPoint(1.0, Vectors.dense(0.0, 1.0)),
    LabeledPoint(0.0, Vectors.dense(1.0, 1.0)),
    LabeledPoint(0.0, Vectors.dense(0.0, 0.0))
  )

  val rdd = sc.parallelize(samples)

  val model = {
    new LogisticRegressionWithLBFGS()
      .setNumClasses(2)
      .run(rdd)
      .clearThreshold()
  }

}

{code}

!http://piqqin.com/img/ea6c54a1bf414828a794ca6604436d78.png!

The number of cached RDDs decreases over time. However, on real-size data when building cross-validated models this is real problem: useful pre-cached datasets are dropped from memory and replaced with these {{MapPartitionsRDD}}'s. With the fix I've submitted behaviour is perfectly fine: only one RDD is cached at a time, so pre-cached data is untouched.","27/Aug/15 17:58;srowen;Issue resolved by pull request 8395
[https://github.com/apache/spark/pull/8395]",,,,,,,,,,,,,,,,,,
GMM bug: match error,SPARK-10164,12857994,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,josephkb,josephkb,josephkb,21/Aug/15 23:09,24/Aug/15 01:34,15/Aug/18 23:03,24/Aug/15 01:34,1.5.0,,,,,,,,,,,,,,,1.5.0,,,,,,MLlib,,,,,0,,,,,"GaussianMixture now distributes matrix decompositions for certain problem sizes.  Distributed computation actually fails, but this was not tested in unit tests.  This is a regression.

Here is an example failure:
{code}
Exception in thread ""main"" scala.MatchError: ArrayBuffer(0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000
001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.050
00000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.0500000000000
0001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001) (of class scala.collection.mutable.ArrayBuffer)
        at scala.runtime.ScalaRunTime$.array_apply(ScalaRunTime.scala:71)
        at scala.Array$.slowcopy(Array.scala:81)
        at scala.Array$.copy(Array.scala:107)
        at org.apache.spark.mllib.clustering.GaussianMixture.run(GaussianMixture.scala:215)
        at mllib.perf.clustering.GaussianMixtureTest.run(GaussianMixtureTest.scala:60)
        at mllib.perf.TestRunner$$anonfun$2.apply(TestRunner.scala:66)
        at mllib.perf.TestRunner$$anonfun$2.apply(TestRunner.scala:64)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.immutable.Range.foreach(Range.scala:141)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at mllib.perf.TestRunner$.main(TestRunner.scala:64)
        at mllib.perf.TestRunner.main(TestRunner.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/08/21 21:25:33 INFO spark.SparkContext: Invoking stop() from shutdown hook
{code}",,apachespark,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-08-21 23:33:02.571,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 24 01:34:26 UTC 2015,,,,,0|i2j8hj:,9223372036854775807,,,,,,1.5.0,,,,,,,"21/Aug/15 23:33;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/8370","24/Aug/15 01:34;josephkb;Issue resolved by pull request 8370
[https://github.com/apache/spark/pull/8370]",,,,,,,,,,,,,,,,,,,,,,
"Validate i, j in apply (Dense and Sparse Matrices)",SPARK-10082,12856857,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,MechCoder,MechCoder,MechCoder,18/Aug/15 08:54,20/Oct/15 23:42,15/Aug/18 23:03,20/Oct/15 23:36,,,,,,,,,,,,,,,,1.6.0,,,,,,MLlib,,,,,0,,,,,"Given row_ind should be less than the number of rows
Given col_ind should be less than the number of cols.

The current code in master gives unpredictable behavior for such cases.",,apachespark,josephkb,MechCoder,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-08-18 09:02:04.444,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 23:42:07 UTC 2015,,,,,0|i2j1lj:,9223372036854775807,,,,,,,,,,,,,"18/Aug/15 09:02;apachespark;User 'MechCoder' has created a pull request for this issue:
https://github.com/apache/spark/pull/8271","20/Oct/15 23:36;mengxr;Issue resolved by pull request 8271
[https://github.com/apache/spark/pull/8271]","20/Oct/15 23:42;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/9189",,,,,,,,,,,,,,,,,,,,,
Association Rules needs JavaItems,SPARK-9959,12856027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,fliang,fliang,fliang,14/Aug/15 02:02,17/Aug/15 16:58,15/Aug/18 23:03,17/Aug/15 16:58,,,,,,,,,,,,,,,,1.5.0,,,,,,MLlib,,,,,0,,,,,We need a java-friendly way to extract `List[Item]` out of `AssociationRule.Rule`.,,apachespark,fliang,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-08-14 16:36:02.555,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 17 16:58:49 UTC 2015,,,,,0|i2iwlr:,9223372036854775807,mengxr,,,,,1.5.0,,,,,,,"14/Aug/15 16:36;apachespark;User 'feynmanliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/8206","17/Aug/15 16:58;mengxr;Issue resolved by pull request 8206
[https://github.com/apache/spark/pull/8206]",,,,,,,,,,,,,,,,,,,,,,
SparseMatrix should override equals,SPARK-9750,12853050,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,fliang,fliang,fliang,07/Aug/15 20:52,01/Oct/15 21:30,15/Aug/18 23:03,11/Aug/15 19:50,,,,,,,,,,,,,,,,1.5.0,,,,,,MLlib,,,,,0,,,,,"[SparseMatrix|https://github.com/apache/spark/blob/9897cc5e3d6c70f7e45e887e2c6fc24dfa1adada/mllib/src/main/scala/org/apache/spark/mllib/linalg/Matrices.scala#L479] should override equals to ensure that two instances of the same matrix are equal.

This implementation should take into account the {{isTransposed}} flag and {{values}} may not be in the same order.",,apachespark,fliang,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10906,SPARK-9919,SPARK-9953,SPARK-9940,,,,,,,0.0,,,,,,,,,,,,,,,,2015-08-07 22:12:07.062,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 13 23:00:08 UTC 2015,,,,,0|i2ihgf:,9223372036854775807,josephkb,,,,,1.5.0,,,,,,,"07/Aug/15 22:12;apachespark;User 'feynmanliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/8042",07/Aug/15 22:12;josephkb;[~fliang] Are you working on this?,07/Aug/15 22:28;fliang;Yep.,"10/Aug/15 19:00;josephkb;This JIRA should probably be expanded to include DenseMatrix too.  Currently, DenseMatrix.equals always returns false for comparisons with SparseMatrix instances.","11/Aug/15 19:50;josephkb;Issue resolved by pull request 8042
[https://github.com/apache/spark/pull/8042]","13/Aug/15 05:22;mengxr;Btw, this duplicates SPARK-6364. I marked that one as duplicated since this was merged.",13/Aug/15 18:22;josephkb;Discussed with [~mengxr]: We're going to revert this for 1.5 and put it in 1.6 instead.,"13/Aug/15 22:15;josephkb;My understanding is as follows.  Does this sound right?

Terminology:
* literal equality: ignores semantics of tranpose, sparsity, etc., and simply compares objects for exact equality
* semantic equality: equality in linear algebra semantics, e.g., as Breeze does

(update) Scratching out my previous assessment since I did not realize default hash codes are sometimes implemented using references, not actual values.  See [~mengxr]'s list below instead.

-Status before this PR:-
* -Python-
** -Vectors: eq implements literal equality.  Default hash.-
** -Matrices: eq implements semantic equality.  Default hash. **-->This is a bug.**-
* -Scala-
** -Vectors: equals() implements semantic equality.  hashCode() uses first 16 elements.-
** -DenseMatrix: equals() implements semantic equality.  hashCode() uses all elements.-
** -SparseMatrix: default equals(), hashCode().-

-Thus, there is one bug which we could fix.  But it's probably not significant since it's been in MLlib for a while.-","13/Aug/15 22:17;josephkb;The plan therefore is to:
* Revert [https://github.com/apache/spark/pull/8042]
* Maybe fix the bug with Python matrices. (TBD)","13/Aug/15 22:20;josephkb;Notes on these things:
* Python: [https://docs.python.org/2/reference/datamodel.html#object.__hash__]
* Scala: [http://www.artima.com/lejava/articles/equality.html]","13/Aug/15 22:49;mengxr;This is really messy ... Using your terminology, this is what I found:

In 1.4 (which is the same as before this PR):

* Python
  * DenseVector: Semantic eq but only with `DenseVector`. Default hash. -> *bug*
  * SparseVector: Semantic eq but *wrong* (only with `SparseVector` and not handling explicit zeros). Default hash. -> *bug*
  * DenseMatrix: Semantic eq but only with `DenseMatrix`. Default hash. -> *bug*
  * SparseMatrix: Semantic eq but converting to dense. Default hash. -> *bug*
* Scala
  * Vectors: Semantic eq. Hash uses first 16 entries.
  * DenseMatrix: Semantic eq but *wrong* (only with `DenseMatrix` and not handling transpose). Guava Objects.hashCode (similar to default hash). -> *bug*
  * SparseMatrix: Literal eq. Default hash.

","13/Aug/15 23:00;josephkb;Discussed with [~mengxr]: The new plan is to NOT revert this JIRA's PR for 1.5.  It should be as safe as it was before (i.e., not safe at all to use these objects in collections).  And we can fix all of these items up for the 1.6 release.",,,,,,,,,,,,
.save() Procedure fails,SPARK-9638,12851659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,stijngeuens,stijngeuens,05/Aug/15 11:06,10/Aug/15 07:51,15/Aug/18 23:03,10/Aug/15 07:51,1.4.1,,,,,,,,,,,,,,,,,,,,,MLlib,PySpark,,,,0,,,,,"I am not able to save a MatrixFactorizationModel I created. 
Path ""./Models"" exists.

Working with pyspark in IPython notebook (spark version = 1.4.1, hadoop version = 2.6)

Error message:

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-14-28d4a0d852bb> in <module>()
----> 1 CFMFModel11.save(sc, ""./Models/CFMFModel11"")

C:\Users\s.geuens\Spark\spark-1.4.1-bin-hadoop2.6\python\pyspark\mllib\util.pyc in save(self, sc, path)
    202 
    203     def save(self, sc, path):
--> 204         self._java_model.save(sc._jsc.sc(), path)
    205 
    206 

C:\Users\s.geuens\Spark\spark-1.4.1-bin-hadoop2.6\python\lib\py4j-0.8.2.1-src.zip\py4j\java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

C:\Users\s.geuens\Spark\spark-1.4.1-bin-hadoop2.6\python\lib\py4j-0.8.2.1-src.zip\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o334.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1823.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1823.0 (TID 489, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1010)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:482)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:808)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:791)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:656)
	at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:490)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:462)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:428)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:801)
	at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:123)
	at org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:90)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1104)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",,josephkb,stijngeuens,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2356,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-08-05 11:20:19.061,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 05 21:25:26 UTC 2015,,,,,0|i2icyn:,9223372036854775807,,,,,,,,,,,,,05/Aug/15 11:20;srowen;I think this is because you are on Windows and you may not have Hadoop installed and/or HADOOP_HOME set. It needs some support binaries on windows to interact with the FS. That is I think this is the same as SPARK-2356 underneath.,"05/Aug/15 21:25;josephkb;If that seems to be the case, then I'll close this for now, but please reopen/comment if it's another issue.
(updated) I'll let someone else close it since I'm not familiar with Windows issues...",,,,,,,,,,,,,,,,,,,,,,
Erroneous result in Frequent Items (SQL) when merging FrequentItemCounters,SPARK-9616,12851512,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,brkyvz,brkyvz,brkyvz,04/Aug/15 21:47,06/Aug/15 17:31,15/Aug/18 23:03,06/Aug/15 17:31,1.5.0,,,,,,,,,,,,,,,1.5.0,,,,,,MLlib,SQL,,,,0,,,,,"Existing behavior with max size:
Partition A ->  Map(1 -> 3, 2 -> 3, 3 -> 4)
Partition B -> Map(4 -> 25)

Result -> Map()

Correct Behavior:
Partition A ->  Map(1 -> 3, 2 -> 3, 3 -> 4)
Partition B -> Map(4 -> 25)

Result -> Map(3 -> 1, 4 -> 22)",,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-08-04 21:53:13.862,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 04 21:53:13 UTC 2015,,,,,0|i2fqw7:,9223372036854775807,,,,,Spark 1.5 release,1.5.0,,,,,,,"04/Aug/15 21:53;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/7945",,,,,,,,,,,,,,,,,,,,,,,
Spelling error in Strategy.defaultStrategy,SPARK-9609,12851482,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,fliang,fliang,fliang,04/Aug/15 19:45,05/Aug/15 01:13,15/Aug/18 23:03,05/Aug/15 01:13,,,,,,,,,,,,,,,,1.5.0,,,,,,MLlib,,,,,0,,,,,"There is a misspelling of strategy as ""stategy"" which is exposed in the public API",,apachespark,fliang,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-08-04 19:47:05.002,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 05 01:13:38 UTC 2015,,,,,0|i2ibwf:,9223372036854775807,josephkb,,,,,1.5.0,,,,,,,04/Aug/15 19:45;fliang;Working on this,"04/Aug/15 19:47;apachespark;User 'feynmanliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/7941","05/Aug/15 01:13;josephkb;Issue resolved by pull request 7941
[https://github.com/apache/spark/pull/7941]",,,,,,,,,,,,,,,,,,,,,
SparseVector constructor must throw an error when declared number of elements less than array length,SPARK-9277,12848541,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,vykhand,vykhand,23/Jul/15 12:46,30/Jul/15 16:20,15/Aug/18 23:03,30/Jul/15 16:20,1.3.1,,,,,,,,,,,,,,,1.5.0,,,,,,MLlib,,,,,0,starter,,,,"I found that one can create SparseVector inconsistently and it will lead to an Java error in runtime, for example when training LogisticRegressionWithSGD.

Here is the test case:


In [2]:
sc.version
Out[2]:
u'1.3.1'
In [13]:
from pyspark.mllib.linalg import SparseVector
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.classification import LogisticRegressionWithSGD
In [3]:
x =  SparseVector(2, {1:1, 2:2, 3:3, 4:4, 5:5})
In [10]:
l = LabeledPoint(0, x)
In [12]:
r = sc.parallelize([l])
In [14]:
m = LogisticRegressionWithSGD.train(r)

Error:


Py4JJavaError: An error occurred while calling o86.trainLogisticRegressionModelWithSGD.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 11.0 failed 1 times, most recent failure: Lost task 7.0 in stage 11.0 (TID 47, localhost): java.lang.ArrayIndexOutOfBoundsException: 2


Attached is the notebook with the scenario and the full message",,apachespark,josephkb,MechCoder,mengxr,rikima,vykhand,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/Jul/15 12:47;vykhand;SparseVector test.html;https://issues.apache.org/jira/secure/attachment/12746794/SparseVector+test.html,23/Jul/15 12:47;vykhand;SparseVector test.ipynb;https://issues.apache.org/jira/secure/attachment/12746795/SparseVector+test.ipynb,,2.0,,,,,,,,,,,,,,,,2015-07-24 07:13:35.281,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 30 16:20:15 UTC 2015,,,,,0|i2hu4f:,9223372036854775807,,,,,,,,,,,,,"23/Jul/15 12:47;vykhand;
Attached is the notebook with the scenario and the full message:","24/Jul/15 07:13;MechCoder;I have labelled this as started. Will fix this in 4 days, if no one comes forward by then.","24/Jul/15 19:34;josephkb;Checking that the length of the array is < size sounds good.  Thanks for pointing this out!

Side note: We should not check the full array of indices for validity since Vector construction needs to be cheap (to facilitate translating to and from other vector types such as Breeze and numpy).","25/Jul/15 11:45;vykhand;Hi Joseph,

will it be too expensive performance wize to add the following check : 

max index in the array < size?

From the correctness perspective it is a better thing to do.
","25/Jul/15 21:40;vykhand;btw here is the case that shows that just checking that len(array) < size is unreliable:

In [4]:

x =  SparseVector(2, {1:1, 1:2, 1:3, 1:4, 5:5})
In [5]:

l = LabeledPoint(0, x)
In [6]:

r = sc.parallelize([l])
In [7]:

m = LogisticRegressionWithSGD.train(r)

Result :

Py4JJavaError: An error occurred while calling o38.trainLogisticRegressionModelWithSGD.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.ArrayIndexOutOfBoundsException: 5","26/Jul/15 07:37;srowen;Yes, it's clear this doesn't catch all cases. The cost of checking the entire input every time may be high, as this code path is used so heavily internally. This change is more of a fail-faster convenience for user code.","28/Jul/15 19:12;josephkb;I think a constant-time check is OK, but scanning the full array is too expensive.  We want construction to be constant-time (a simple copying of a reference).",29/Jul/15 07:37;srowen;[~MechCoder] Are you working on this?,30/Jul/15 09:19;MechCoder;I will not have access to a development environment till Saturday. Feel free to fix it. Thanks.,"30/Jul/15 11:32;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7794","30/Jul/15 16:20;mengxr;Issue resolved by pull request 7794
[https://github.com/apache/spark/pull/7794]",,,,,,,,,,,,,
BLAS.gemm fails to update matrix C when alpha==0 and beta!=1,SPARK-9175,12846127,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,meihuawu,meihuawu,meihuawu,18/Jul/15 23:30,21/Jul/15 00:11,15/Aug/18 23:03,21/Jul/15 00:11,1.2.2,1.3.1,1.4.1,,,,,,,,,,,,,1.2.3,1.3.2,1.4.2,1.5.0,,,MLlib,,,,,2,,,,,"In the BLAS wrapper, gemm is supposed to update matrix C to be alpha * A * B + beta * C. However, the current implementation will not update C as long as alpha == 0. This is incorrect when beta is not equal to 1. 

Example:
val p = 3 
val a = DenseMatrix.zeros(p,p)
val b = DenseMatrix.zeros(p,p)
var c = DenseMatrix.eye(p)
BLAS.gemm(0, a, b, 5, c)

c is unchanged in the Spark 1.4 even though it should be multiplied by 5 element-wise.

The bug is caused by the following in BLAS.gemm:
if (alpha == 0.0) {
  logDebug(""gemm: alpha is equal to 0. Returning C."")
}

Will submit PR to fix this.",,apachespark,meihuawu,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-07-18 23:53:09.299,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 00:11:11 UTC 2015,,,,,0|i2hfdb:,9223372036854775807,mengxr,,,,,1.2.3,1.3.2,1.4.2,1.5.0,,,,"18/Jul/15 23:53;apachespark;User 'rotationsymmetry' has created a pull request for this issue:
https://github.com/apache/spark/pull/7503","21/Jul/15 00:11;mengxr;Issue resolved by pull request 7503
[https://github.com/apache/spark/pull/7503]",,,,,,,,,,,,,,,,,,,,,,
Vectors.dense() in Python should accept numbers directly,SPARK-9138,12845941,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,17/Jul/15 17:36,17/Jul/15 19:44,15/Aug/18 23:03,17/Jul/15 19:44,,,,,,,,,,,,,,,,1.5.0,,,,,,MLlib,,,,,0,,,,,We already use this feature in doctests,,apachespark,davies,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-07-17 17:43:02.66,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 19:44:22 UTC 2015,,,,,0|i2he9b:,9223372036854775807,,,,,,1.5.0,,,,,,,"17/Jul/15 17:43;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/7476","17/Jul/15 19:44;mengxr;Issue resolved by pull request 7476
[https://github.com/apache/spark/pull/7476]",,,,,,,,,,,,,,,,,,,,,,
StopwatchSuite shouldn't use Thread.sleep(),SPARK-9126,12845775,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,17/Jul/15 03:29,17/Jul/15 06:02,15/Aug/18 23:03,17/Jul/15 06:02,1.5.0,,,,,,,,,,,,,,,1.5.0,,,,,,MLlib,,,,,0,,,,,Thread.sleep() might run overtime if Jenkins is busy.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-07-17 04:05:07.751,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 06:02:16 UTC 2015,,,,,0|i2hd93:,9223372036854775807,josephkb,,,,,1.5.0,,,,,,,"17/Jul/15 04:05;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/7457","17/Jul/15 06:02;mengxr;Issue resolved by pull request 7457
[https://github.com/apache/spark/pull/7457]",,,,,,,,,,,,,,,,,,,,,,
RegressionMetrics computing incorrect explainedVariance,SPARK-9005,12844470,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fliang,fliang,fliang,12/Jul/15 18:50,19/Jan/18 20:36,15/Aug/18 23:03,15/Jul/15 20:32,,,,,,,,,,,,,,,,1.5.0,,,,,,MLlib,,,,,0,,,,,"{{RegressionMetrics}} currently computes explainedVariance using {{summary.variance(1)}} (variance of the residuals) where the [Wikipedia definition|https://en.wikipedia.org/wiki/Fraction_of_variance_unexplained] uses the residual sum of squares {{math.pow(summary.normL2(1), 2)}}. The two coincide only when the predictor is unbiased (e.g. an intercept term is included in a linear model), but this is not always the case. We should change to be consistent.",,afarahat,apachespark,fliang,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-07-13 00:11:08.393,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 15 20:32:43 UTC 2015,,,,,0|i2h5jb:,9223372036854775807,,,,,,1.5.0,,,,,,,12/Jul/15 18:50;fliang;I will be working on this.,"13/Jul/15 00:11;apachespark;User 'feynmanliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/7361","14/Jul/15 18:26;afarahat;I compared the R2 and RMSE after fitting an ALS model . here are the results
rank 40  r2 =   0.993274964231 explained var =  0.993566133802 count =  94652197  meanres  -0.0606718131255 meanres2  0.085020285731
rank  50  r2 =   0.993547408858 explained var =  0.993826795105 count =  94652197  meanres  -0.0594314727572 meanres2  0.081575944201","15/Jul/15 20:32;josephkb;Issue resolved by pull request 7361
[https://github.com/apache/spark/pull/7361]",,,,,,,,,,,,,,,,,,,,
MLlib predict function error,SPARK-8631,12840548,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,slagade,slagade,25/Jun/15 13:57,25/Jun/15 14:13,15/Aug/18 23:03,25/Jun/15 14:13,1.4.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"def
predict(usersProducts: RDD[(Int, Int)]): RDD[Rating]
Predict the rating of many users for many products. The output RDD has an element per each element in the input RDD (including all duplicates) unless a user or product is missing in the training set.
usersProducts
RDD of (user, product) pairs.
returns
RDD of Ratings.
def
predict(user: Int, product: Int): Double
Predict the rating of one user for one product.",,slagade,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-06-25 14:13:01.934,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 25 14:13:01 UTC 2015,,,,,0|i2ghrz:,9223372036854775807,,,,,,,,,,,,,"25/Jun/15 13:59;slagade;based on the parameter type input to predict method does not behave in respective manner.
please check the functionality of this method in this class.",25/Jun/15 14:13;srowen;This is the third time you have opened this. As I explained this is not a valid JIRA. Please do not open any more. ,,,,,,,,,,,,,,,,,,,,,,
ALS model predict error,SPARK-8627,12840504,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,slagade,slagade,25/Jun/15 11:10,03/Dec/15 15:33,15/Aug/18 23:03,03/Dec/15 15:33,1.4.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"/**
 * Created by subhod lagade on 25/06/15.
 */
import org.apache.spark.SparkConf
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming._;

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._



import java.io.BufferedReader;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.PrintStream;
import java.net.ServerSocket;
import java.net.Socket;
import java.util.Properties;



import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating


object SparkStreamKafka {
  def main(args: Array[String]) {
	
    val conf = new SparkConf().setAppName(""Simple Application"");
    val sc = new SparkContext(conf);
	val data = sc.textFile(""/home/appadmin/Disney/data.csv"");
	val ratings = data.map(_.split(',') match { case Array(user, product, rate) =>	Rating(user.toInt, product.toInt, rate.toDouble)  });
	
	
	val rank = 3;
	val numIterations = 2;
	val model = ALS.train(ratings,rank,numIterations,0.01);
	

	val usersProducts = ratings.map{ case Rating(user, product, rate)  => (user, product)}

	// Build the recommendation model using ALS
	usersProducts.foreach(println)

	val predictions = model.predict(usersProducts)
	}
}

/*
ERROR Message
[ERROR] /home/appadmin/disneypoc/src/main/scala/org/capgemini/SparkKafka.scala:53: error: not enough arguments for method predict: (user: Int, product: Int)Double.
[INFO] Unspecified value parameter product.
[INFO]  val predictions = model.predict(usersProducts)
*/


",,mengxr,slagade,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-06-25 11:13:10.32,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 20:10:21 UTC 2015,,,,,0|i2ghi7:,9223372036854775807,,,,,,,,,,,,,25/Jun/15 11:13;srowen;This is a compile error in your own code.,"25/Jun/15 11:24;slagade;can you help me in resolving this ??
usersProducts is a RDD(int,int) it is still giving me error","07/Jul/15 20:10;mengxr;The code looks okay to me. Which Spark version did you use, and Scala version?",,,,,,,,,,,,,,,,,,,,,
ALS model predict error,SPARK-8626,12840503,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Duplicate,,slagade,slagade,25/Jun/15 11:02,25/Jun/15 11:14,15/Aug/18 23:03,25/Jun/15 11:14,1.4.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,,,slagade,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8627,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-06-25 11:14:03.74,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 25 11:14:03 UTC 2015,,,,,0|i2ghhz:,9223372036854775807,,,,,,,,,,,,,"25/Jun/15 11:05;slagade;/**
 * Created by subhod lagade on 25/06/15.
 */
import org.apache.spark.SparkConf
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming._;

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._



import java.io.BufferedReader;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.PrintStream;
import java.net.ServerSocket;
import java.net.Socket;
import java.util.Properties;



import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating


object SparkStreamKafka {
  def main(args: Array[String]) {
	
    val conf = new SparkConf().setAppName(""Simple Application"");
    val sc = new SparkContext(conf);
	val data = sc.textFile(""/home/appadmin/Disney/data.csv"");
	val ratings = data.map(_.split(',') match { case Array(user, product, rate) =>	Rating(user.toInt, product.toInt, rate.toDouble)  });
	
	
	val rank = 3;
	val numIterations = 2;
	val model = ALS.train(ratings,rank,numIterations,0.01);
	

	val usersProducts = ratings.map{ case Rating(user, product, rate)  => (user, product)}

	// Build the recommendation model using ALS
	usersProducts.foreach(println)

	val predictions = model.predict(usersProducts)
	}
}","25/Jun/15 11:06;slagade;INFO] Compiling 1 source files to /home/appadmin/disneypoc/target/classes at 1435229668035
[ERROR] /home/appadmin/disneypoc/src/main/scala/org/capgemini/SparkKafka.scala:53: error: not enough arguments for method predict: (user: Int, product: Int)Double.
[INFO] Unspecified value parameter product.
[INFO]  val predictions = model.predict(usersProducts)
",25/Jun/15 11:14;srowen;... and you opened it twice. Please read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark and take care before opening a JIRA,,,,,,,,,,,,,,,,,,,,,
Bug that IndexedRowMatrix.computeSVD() yields the U with wrong numCols,SPARK-8563,12839812,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lee19,lee19,lee19,23/Jun/15 11:08,02/Jul/15 18:41,15/Aug/18 23:03,30/Jun/15 21:14,1.0.2,1.1.1,1.2.2,1.3.1,1.4.1,,,,,,,,,,,1.0.3,1.1.2,1.2.3,1.3.2,1.4.1,1.5.0,MLlib,,,,,0,,,,,"IndexedRowMatrix.computeSVD() yields a wrong U which *U.numCols() = self.nCols*.

It should have been *U.numCols() = k = svd.U.numCols()*

{code}
self = U * sigma * V.transpose
(m x n) = (m x n) * (k x k) * (k x n)
-->
(m x n) = (m x k) * (k x k) * (k x n)
{code}


Proposed fix: https://github.com/apache/spark/pull/6953
",,apachespark,lee19,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-06-23 11:12:09.396,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 21:14:01 UTC 2015,,,,,0|i2gdpr:,9223372036854775807,,,,,,1.0.3,1.1.2,1.2.3,1.3.2,1.4.1,1.5.0,,"23/Jun/15 11:12;apachespark;User 'lee19' has created a pull request for this issue:
https://github.com/apache/spark/pull/6949","23/Jun/15 13:31;apachespark;User 'lee19' has created a pull request for this issue:
https://github.com/apache/spark/pull/6953","30/Jun/15 21:14;mengxr;Issue resolved by pull request 6953
[https://github.com/apache/spark/pull/6953]",,,,,,,,,,,,,,,,,,,,,
Bug in Streaming k-means documentation,SPARK-8525,12839521,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,fe2s,fe2s,fe2s,22/Jun/15 11:55,04/Jul/15 07:29,15/Aug/18 23:03,23/Jun/15 20:12,1.1.1,1.2.2,1.3.1,1.4.1,,,,,,,,,,,,1.1.2,1.2.3,1.3.2,1.4.1,1.5.0,,Documentation,MLlib,,,,0,,,,,"The expected input format is wrong in Streaming K-means documentation.
https://spark.apache.org/docs/latest/mllib-clustering.html#streaming-k-means

It might be a bug in implementation though, not sure.

There shouldn't be any spaces in test data points. I.e. instead of 
(y, [x1, x2, x3]) it should be
(y,[x1,x2,x3])

The exception thrown 
org.apache.spark.SparkException: Cannot parse a double from:  
	at org.apache.spark.mllib.util.NumericParser$.parseDouble(NumericParser.scala:118)
	at org.apache.spark.mllib.util.NumericParser$.parseTuple(NumericParser.scala:103)
	at org.apache.spark.mllib.util.NumericParser$.parse(NumericParser.scala:41)
	at org.apache.spark.mllib.regression.LabeledPoint$.parse(LabeledPoint.scala:49)


Also I would improve documentation saying explicitly that expected data types for both 'x' and 'y' is Double. At the moment it's not obvious especially for 'y'. 

",,apachespark,fe2s,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-06-22 12:23:22.138,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 20:12:44 UTC 2015,,,,,0|i2gbyv:,9223372036854775807,,,,,,,,,,,,,22/Jun/15 12:23;srowen;I imagine it should just accept spaces (and ignore them). It would be great if you can create a unit test and propose a fix in a PR.,"23/Jun/15 15:14;apachespark;User 'fe2s' has created a pull request for this issue:
https://github.com/apache/spark/pull/6954","23/Jun/15 15:17;fe2s;Makes sense, https://github.com/apache/spark/pull/6954","23/Jun/15 15:59;srowen;(Nit: this isn't really a bug in the docs right? and this can be more specific, about accepting white space in certain k-means input?)","23/Jun/15 20:12;mengxr;Issue resolved by pull request 6954
[https://github.com/apache/spark/pull/6954]",,,,,,,,,,,,,,,,,,,
BinaryClassificationMetrics in ML Lib has odd API,SPARK-8375,12837813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,sams,sams,15/Jun/15 11:43,15/Jun/15 12:57,15/Aug/18 23:03,15/Jun/15 12:57,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,1,,,,,"According to https://spark.apache.org/docs/1.0.0/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

The constructor takes `RDD[(Double, Double)]` which does not make sense it should be `RDD[(Double, T)]` or at least `RDD[(Double, Int)]`.

In scikit I believe they use the number of unique scores to determine the number of thresholds and the ROC.  I assume this is what BinaryClassificationMetrics is doing since it makes no mention of buckets.  In a Big Data context this does not make sense as the number of unique scores may be huge.  

Rather user should be able to either specify the number of buckets, or the number of data points in each bucket.  E.g. `def roc(numPtsPerBucket: Int)`

Finally it would then be good if either the ROC output type was changed or another method was added that returned confusion matricies, so that the hard integer values can be obtained.  E.g.

```
case class Confusion(tp: Int, fp: Int, fn: Int, tn: Int) {
  // bunch of methods for each of the things in the table here https://en.wikipedia.org/wiki/Receiver_operating_characteristic
}

...
def confusions(numPtsPerBucket: Int): RDD[Confusion]
```


",,charnabon,sams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-06-15 12:57:16.441,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 12:57:16 UTC 2015,,,,,0|i2g1lb:,9223372036854775807,,,,,,,,,,,,,"15/Jun/15 12:57;srowen;@sam This is a discussion for the mailing list rather than a JIRA.
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

You're looking at an API from 4 versions ago, too.
https://spark.apache.org/docs/1.4.0/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

The input are scores and ground-truth labels. I agree with the problem of many distinct values, but, this is part of the newer API.",,,,,,,,,,,,,,,,,,,,,,,
DecisionTreeModel.predict() return type not convenient!,SPARK-8335,12837530,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Won't Fix,,sfxardas,sfxardas,12/Jun/15 22:42,18/Jun/15 17:27,15/Aug/18 23:03,18/Jun/15 07:39,1.3.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,easyfix,machine_learning,,,"org.apache.spark.mllib.tree.model.DecisionTreeModel has a predict method:

def predict(features: JavaRDD[Vector]): JavaRDD[Double]

The problem here is the generic type of the return type JAVARDD[Double] because its a scala Double and I would expect a java.lang.Double. (to be convenient e.g. with org.apache.spark.mllib.classification.ClassificationModel)

I wanted to extend the DecisionTreeModel and use it only for Binary Classification and wanted to implement the trait org.apache.spark.mllib.classification.ClassificationModel . But its not possible because the ClassificationModel already defines the predict method but with an return type JAVARDD[java.lang.Double]. 
",,apachespark,josephkb,sfxardas,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-06-13 06:49:01.604,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 17:27:40 UTC 2015,,,,,0|i2fzuv:,9223372036854775807,,,,,,,,,,,,,"13/Jun/15 06:49;srowen;Yeah, the signature is actually a JavaRDD of scala.Double given how it's declared. Just changing the type to java.lang.Double probably invokes the right implicits to convert it, as happens elsewhere. You've verified it really is a scala.Double? ","15/Jun/15 16:09;sfxardas;Yeah I am sure, that is a really a scala.Double. I just looked it up again on github. So the problem still exists in on the current master branch. ",15/Jun/15 22:11;srowen;Go ahead and propose a PR. The sticky issue here is whether it's ok to change an experimental API at this point. I think so.,"17/Jun/15 08:38;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6854","18/Jun/15 00:27;josephkb;I've discussed this with [~mengxr] and we decided to leave it alone.  I agree it's annoying, but we figured that people will use the Pipelines API in the future (where this is not an issue) and not breaking people's code would be best.  Does that sound tolerable?","18/Jun/15 06:51;srowen;I don't feel strongly about it, but it is inconsistent with the same pattern in a few other classes. If we are allowed to fix the API and it's an easy fix, I figured, why not just fix it up for anyone that will use it for the rest of its lifetime? Or is the feeling that an API change, even where compatibility was not promised, just not worth it? this is a separate question from whether people should or would use a newer API.","18/Jun/15 07:24;josephkb;My initial thought was to fix it to make it consistent, but I was discouraged from making a breaking change for a nicety like this.  I think Matei & some of the original people in particular feel strongly about maintaining stable APIs as much as possible, even if items are marked as experimental/developer api.","18/Jun/15 07:39;srowen;Heh, then what does Experimental mean? if it can't be changed then we have people adding APIs thinking ""it's OK if it's not finished since I can change it"" when they can't. Maybe a lot of this stuff should be un-marked Experimental if it's not going to updated going forward anyway.

But yeah people consuming this from Java are probably already working around this with casts, and this change would break that. And I suppose you're saying you actually prefer people to migrate anyway.","18/Jun/15 17:27;josephkb;I agree we're too liberal with marking items as Experimental.  In general, I hope we can mark major new items as Experimental for 1 release after they are first added, and then unmark them except in special cases.",,,,,,,,,,,,,,,
add missing save load for python doc example and tune down MatrixFactorization iterations,SPARK-8308,12837102,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,11/Jun/15 10:19,01/Jul/15 18:18,15/Aug/18 23:03,01/Jul/15 18:18,,,,,,,,,,,,,,,,1.5.0,,,,,,MLlib,,,,,0,,,,,"1. add some missing save/load in python examples, LogisticRegression, LinearRegression, NaiveBayes
2. tune down iterations for MatrixFactorization, since current number will trigger StackOverflow.",,apachespark,josephkb,yuhaoyan,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-06-11 10:22:03.057,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 18:18:15 UTC 2015,,,,,0|i2fxaf:,9223372036854775807,josephkb,,,,,,,,,,,,"11/Jun/15 10:22;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/6760","01/Jul/15 18:18;josephkb;Issue resolved by pull request 6760
[https://github.com/apache/spark/pull/6760]",,,,,,,,,,,,,,,,,,,,,,
Exception in StreamingLinearAlgorithm on Stream with Empty RDD.,SPARK-8200,12836393,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,pparkkin,pparkkin,pparkkin,09/Jun/15 06:28,10/Jun/15 22:28,15/Aug/18 23:03,10/Jun/15 22:27,1.3.1,,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,DStreams,MLlib,,,,0,,,,,"When training a streaming logistic regression model or a streaming linear regression model, any empty RDDs in a stream will cause an exception.

  java.lang.UnsupportedOperationException: empty collection
  at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1288)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
  at org.apache.spark.rdd.RDD.first(RDD.scala:1285)
  at org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:215)
  at org.apache.spark.mllib.regression.StreamingLinearAlgorithm$$anonfun$trainOn$1.apply(StreamingLinearAlgorithm.scala:91)
  at org.apache.spark.mllib.regression.StreamingLinearAlgorithm$$anonfun$trainOn$1.apply(StreamingLinearAlgorithm.scala:85)
  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
","Ubuntu 14.04.2 LTS
Linux 3.13.0-45-generic #74-Ubuntu SMP Tue Jan 13 19:36:28 UTC 2015
java version ""1.8.0_25""
Java(TM) SE Runtime Environment (build 1.8.0_25-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)
Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL
",apachespark,pparkkin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-06-09 06:31:05.792,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 22:27:42 UTC 2015,,,,,0|i2ft0f:,9223372036854775807,,,,,,,,,,,,,09/Jun/15 06:30;pparkkin;PR: https://github.com/apache/spark/pull/6713,"09/Jun/15 06:31;apachespark;User 'pparkkin' has created a pull request for this issue:
https://github.com/apache/spark/pull/6713","10/Jun/15 22:27;srowen;Issue resolved by pull request 6713
[https://github.com/apache/spark/pull/6713]",,,,,,,,,,,,,,,,,,,,,
update NaiveBayes and SVM examples in doc,SPARK-8043,12834569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,02/Jun/15 12:55,03/Jun/15 06:18,15/Aug/18 23:03,03/Jun/15 06:16,1.4.0,,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,MLlib,,,,,0,,,,,"I found some issues during testing the save/load examples in markdown Documents, as a part of 1.4 QA plan

",,apachespark,mengxr,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7541,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-06-02 12:58:02.687,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 06:16:49 UTC 2015,,,,,0|i2fidr:,9223372036854775807,,,,,,1.4.1,1.5.0,,,,,,"02/Jun/15 12:58;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/6584","03/Jun/15 06:16;mengxr;Issue resolved by pull request 6584
[https://github.com/apache/spark/pull/6584]",,,,,,,,,,,,,,,,,,,,,,
Make NumPy version checking in mllib/__init__.py,SPARK-8032,12834498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,MechCoder,MechCoder,MechCoder,02/Jun/15 07:00,03/Jun/15 06:29,15/Aug/18 23:03,03/Jun/15 06:27,1.0.2,1.1.1,1.2.2,1.3.1,1.4.0,,,,,,,,,,,1.0.3,1.1.2,1.2.3,1.3.2,1.4.1,1.5.0,MLlib,PySpark,,,,0,,,,,"The current checking does version `1.x' is less than `1.4' this will fail if x has greater than 1 digit, since x > 4, however `1.x` < `1.4`",,apachespark,MechCoder,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-06-02 07:05:05.65,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 06:27:12 UTC 2015,,,,,0|i2fhy7:,9223372036854775807,,,,,,,,,,,,,"02/Jun/15 07:05;apachespark;User 'MechCoder' has created a pull request for this issue:
https://github.com/apache/spark/pull/6579","03/Jun/15 06:27;mengxr;Issue resolved by pull request 6579
[https://github.com/apache/spark/pull/6579]",,,,,,,,,,,,,,,,,,,,,,
DecayFactor wrongly set in StreamingKMeans,SPARK-7946,12833748,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,MechCoder,MechCoder,MechCoder,29/May/15 10:56,29/May/15 18:38,15/Aug/18 23:03,29/May/15 18:37,1.3.0,,,,,,,,,,,,,,,1.2.3,1.3.2,1.4.0,,,,DStreams,MLlib,,,,0,,,,,,,apachespark,MechCoder,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-29 10:59:07.032,,false,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 18:37:23 UTC 2015,,,,,0|i2fdgv:,9223372036854775807,,,,,,,,,,,,,"29/May/15 10:59;apachespark;User 'MechCoder' has created a pull request for this issue:
https://github.com/apache/spark/pull/6497","29/May/15 12:21;srowen;Set component please
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark","29/May/15 18:37;mengxr;Issue resolved by pull request 6497
[https://github.com/apache/spark/pull/6497]",,,,,,,,,,,,,,,,,,,,,
Make MLlib ChiSqSelector Serializable (& Fix Related Documentation Example).,SPARK-7920,12833507,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dusenberrymw,dusenberrymw,dusenberrymw,28/May/15 17:38,14/Jul/15 22:49,15/Aug/18 23:03,30/May/15 23:51,1.3.1,1.4.0,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,,,,,0,,,,,"The MLlib ChiSqSelector class is not serializable, and so the example in the ChiSqSelector documentation fails.  Also, that example is missing the import of ChiSqSelector.  ChiSqSelector should just extend Serializable.

Steps:
1. Locate the MLlib ChiSqSelector documentation example.
2. Fix the example by adding an import statement for ChiSqSelector.
3. Attempt to run -> notice that it will fail due to ChiSqSelector not being serializable. ",,apachespark,dusenberrymw,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-28 17:42:06.374,,false,,,,,,,,,,,,,9223372036854775807,,,Sat May 30 23:51:26 UTC 2015,,,,,0|i2fc13:,9223372036854775807,,,,,,,,,,,,,"28/May/15 17:42;apachespark;User 'dusenberrymw' has created a pull request for this issue:
https://github.com/apache/spark/pull/6462","30/May/15 23:51;josephkb;Issue resolved by pull request 6462
[https://github.com/apache/spark/pull/6462]",,,,,,,,,,,,,,,,,,,,,,
Fixing broken trainImplicit example in MLlib Collaborative Filtering documentation.,SPARK-7883,12832902,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dusenberrymw,dusenberrymw,dusenberrymw,26/May/15 23:29,14/Jul/15 22:49,15/Aug/18 23:03,27/May/15 01:10,1.0.2,1.1.1,1.2.2,1.3.1,1.4.0,,,,,,,,,,,1.0.3,1.1.2,1.2.3,1.3.2,1.4.0,,Documentation,MLlib,,,,0,,,,,"The trainImplicit Scala example near the end of the MLlib Collaborative Filtering documentation refers to an ALS.trainImplicit function signature that does not exist.  Rather than add an extra function, let's just fix the example.

Currently, the example refers to a function that would have the following signature: 
def trainImplicit(ratings: RDD[Rating], rank: Int, iterations: Int, alpha: Double) : MatrixFactorizationModel

Instead, let's change the example to refer to this function, which does exist (notice the addition of the lambda parameter):
def trainImplicit(ratings: RDD[Rating], rank: Int, iterations: Int, lambda: Double, alpha: Double) : MatrixFactorizationModel",,apachespark,dusenberrymw,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-26 23:31:02.244,,false,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 01:10:06 UTC 2015,,,,,0|i2f8c7:,9223372036854775807,,,,,,1.0.3,1.1.2,1.2.3,1.3.2,1.4.0,,,"26/May/15 23:31;apachespark;User 'dusenberrymw' has created a pull request for this issue:
https://github.com/apache/spark/pull/6422","27/May/15 01:10;mengxr;Issue resolved by pull request 6422
[https://github.com/apache/spark/pull/6422]",,,,,,,,,,,,,,,,,,,,,,
Broken tests in KernelDensity,SPARK-7844,12832395,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,MechCoder,MechCoder,MechCoder,24/May/15 06:27,27/May/15 07:03,15/Aug/18 23:03,26/May/15 20:22,,,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,,,,,0,,,,,"The densities in KernelDensity are scaled down by (number of parallel processes X number of points). This results in broken tests in KernelDensitySuite which haven't been tested properly. I think it should just be scaled down by (number of samples, i.e number of gaussian distributions)",,apachespark,MechCoder,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-24 06:32:04.645,,false,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 20:22:57 UTC 2015,,,,,0|i2f59b:,9223372036854775807,,,,,,1.4.0,,,,,,,24/May/15 06:31;MechCoder;ping [~josephkb],"24/May/15 06:32;apachespark;User 'MechCoder' has created a pull request for this issue:
https://github.com/apache/spark/pull/6383","26/May/15 20:22;mengxr;Issue resolved by pull request 6383
[https://github.com/apache/spark/pull/6383]",,,,,,,,,,,,,,,,,,,,,
Use getOrElse for getting the threshold of SVM model,SPARK-7793,12831866,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,coderxiang,coderxiang,coderxiang,21/May/15 17:02,21/May/15 19:11,15/Aug/18 23:03,21/May/15 19:11,,,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,,,,,0,,,,,same issue and fix as in Spark-7694,,apachespark,coderxiang,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-21 17:04:03.714,,false,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 19:11:27 UTC 2015,,,,,0|i2f233:,9223372036854775807,,,,,,,,,,,,,"21/May/15 17:04;apachespark;User 'coderxiang' has created a pull request for this issue:
https://github.com/apache/spark/pull/6321","21/May/15 19:11;mengxr;Issue resolved by pull request 6321
[https://github.com/apache/spark/pull/6321]",,,,,,,,,,,,,,,,,,,,,,
GradientBoostedTrees is missing maxBins parameter in pyspark,SPARK-7781,12831594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holdenk,dondrake,dondrake,21/May/15 03:50,23/Jun/15 05:41,15/Aug/18 23:03,23/Jun/15 05:40,1.3.1,,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,MLlib,,,,,0,,,,,"I'm running Spark v1.3.1 and when I run the following against my dataset:

{code}
model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catFeatures, maxDepth=6, numIterations=3)

The job will fail with the following message:
Traceback (most recent call last):
  File ""/Users/drake/fd/spark/mltest.py"", line 73, in <module>
    model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catFeatures, maxDepth=6, numIterations=3)
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/tree.py"", line 553, in trainRegressor
    loss, numIterations, learningRate, maxDepth)
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/tree.py"", line 438, in _train
    loss, numIterations, learningRate, maxDepth)
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py"", line 120, in callMLlibFunc
    return callJavaFunc(sc, api, *args)
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py"", line 113, in callJavaFunc
    return _java2py(sc, func(*args))
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
15/05/20 16:40:12 INFO BlockManager: Removing block rdd_32_95
py4j.protocol.Py4JJavaError: An error occurred while calling o69.trainGradientBoostedTreesModel.
: java.lang.IllegalArgumentException: requirement failed: DecisionTree requires maxBins (= 32) >= max categories in categorical features (= 1895)
	at scala.Predef$.require(Predef.scala:233)
	at org.apache.spark.mllib.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:128)
	at org.apache.spark.mllib.tree.RandomForest.run(RandomForest.scala:138)
	at org.apache.spark.mllib.tree.DecisionTree.run(DecisionTree.scala:60)
	at org.apache.spark.mllib.tree.GradientBoostedTrees$.org$apache$spark$mllib$tree$GradientBoostedTrees$$boost(GradientBoostedTrees.scala:150)
	at org.apache.spark.mllib.tree.GradientBoostedTrees.run(GradientBoostedTrees.scala:63)
	at org.apache.spark.mllib.tree.GradientBoostedTrees$.train(GradientBoostedTrees.scala:96)
	at org.apache.spark.mllib.api.python.PythonMLLibAPI.trainGradientBoostedTreesModel(PythonMLLibAPI.scala:595)
{code}

So, it's complaining about the maxBins, if I provide maxBins=1900 and re-run it:

{code}
model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catFeatures, maxDepth=6, numIterations=3, maxBins=1900)

Traceback (most recent call last):
  File ""/Users/drake/fd/spark/mltest.py"", line 73, in <module>
    model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catF
eatures, maxDepth=6, numIterations=3, maxBins=1900)
TypeError: trainRegressor() got an unexpected keyword argument 'maxBins'
{code}

It now says it knows nothing of maxBins.

If I run the same command against DecisionTree or RandomForest (with maxBins=1900) it works just fine.

Seems like a bug in GradientBoostedTrees. ",,apachespark,dondrake,holdenk,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-21 04:11:25.509,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 05:40:51 UTC 2015,,,,,0|i2f0wf:,9223372036854775807,,,,,,1.4.1,1.5.0,,,,,,21/May/15 04:11;holdenk;I can take this :),"21/May/15 21:05;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/6331","23/Jun/15 05:40;josephkb;Issue resolved by pull request 6331
[https://github.com/apache/spark/pull/6331]",,,,,,,,,,,,,,,,,,,,,
The intercept in LogisticRegressionWithLBFGS should not be regularized,SPARK-7780,12831586,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holdenk,dbtsai,dbtsai,21/May/15 02:45,27/Jan/16 01:59,15/Aug/18 23:03,27/Jan/16 01:59,,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,1,,,,,"The intercept in Logistic Regression represents a prior on categories which should not be regularized. In MLlib, the regularization is handled through `Updater`, and the `Updater` penalizes all the components without excluding the intercept which resulting poor training accuracy with regularization.

The new implementation in ML framework handles this properly, and we should call the implementation in ML from MLlib since majority of users are still using MLlib api. 

Note that both of them are doing feature scalings to improve the convergence, and the only difference is ML version doesn't regularize the intercept. As a result, when lambda is zero, they will converge to the same solution.",,apachespark,dbtsai,holdenk,josephkb,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-21 21:10:38.415,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 01:59:22 UTC 2016,,,,,0|i2f0un:,9223372036854775807,,,,,,,,,,,,,21/May/15 21:10;holdenk;I can take a crack at this if that would be cool :),"21/May/15 22:20;dbtsai;@holdenk Sure. This one will be fun to work on. If it's binary LoR, just call the one in ML. Need to be careful about the updater. If the updater is L1Updater, just set the elasticNetParam to 1.0, and L2Updater, set the elasticNetParam to 0.0. ",22/May/15 08:01;holdenk;Rad :) Yay for long weekends :),22/May/15 21:01;holdenk;I was thinking that since the user could override this with any updater a reasonable plan would be to switch on the updater and if it was not one of the known/supported ones default to the old behaviour. Does that sound reasonable?,22/May/15 21:03;josephkb;That sounds reasonable to me.,"22/May/15 21:04;dbtsai;++1
Although I don't think there are many users overriding the updater now, and most of the common use-cases are L1 and L2.",22/May/15 23:08;josephkb;Linked related issue on correctness of LogisticRegressionWithLBFGS,"24/May/15 08:07;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/6386","11/Jun/15 23:57;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/6771","19/Aug/15 05:24;rxin;I'm assuming this is going to 1.6 / 1.5.1?
","19/Aug/15 17:28;josephkb;Yep, 1.6 I'd say","16/Jan/16 17:11;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/10788","27/Jan/16 01:59;dbtsai;Issue resolved by pull request 10788
[https://github.com/apache/spark/pull/10788]",,,,,,,,,,,
Input vector should divide with the norm in Word2Vec's findSynonyms,SPARK-7765,12831450,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,viirya,viirya,20/May/15 17:45,21/May/15 08:39,15/Aug/18 23:03,20/May/15 17:59,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"In Word2Vec's findSynonyms, the computed cosine similarities should divide with the norm of the given vector since it is not normalized.",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-20 17:47:03.025,,false,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 08:39:26 UTC 2015,,,,,0|i2f01r:,9223372036854775807,,,,,,,,,,,,,"20/May/15 17:47;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6288","20/May/15 17:50;srowen;This duplicates https://issues.apache.org/jira/browse/SPARK-7617 right?

You have a great number of PRs open: https://spark-prs.appspot.com/users/viirya
Can you help close them or drive them to conclusion rather than focus on opening more?",21/May/15 08:39;viirya;I will try to close some obsolete PRs. Some PRs are there waiting for related APIs to be ready or reviewing from others. I usually will update them soon if they get responses.,,,,,,,,,,,,,,,,,,,,,
Use getOrElse for getting the threshold of LR model ,SPARK-7694,12830550,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,coderxiang,coderxiang,coderxiang,18/May/15 00:14,18/May/15 04:18,15/Aug/18 23:03,18/May/15 04:18,,,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,,,,,0,,,,,"The toString method of LogisticRegressionModel calls get method on an Option (threshold) without a safeguard. In spark-shell, the following code 
{code:title=lbfgs.scala|borderStyle=solid}
val model = algorithm.run(data).clearThreshold()
{code}
 in lbfgs code will fail as toString method will be called right after clearThreshold() to show the results in the REPL.",,apachespark,coderxiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-18 00:15:06.566,,false,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 00:15:06 UTC 2015,,,,,0|i2eujb:,9223372036854775807,,,,,,1.4.0,,,,,,,"18/May/15 00:15;apachespark;User 'coderxiang' has created a pull request for this issue:
https://github.com/apache/spark/pull/6224",,,,,,,,,,,,,,,,,,,,,,,
Matrix.map should preserve transpose property,SPARK-7668,12830184,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,15/May/15 14:41,15/May/15 17:04,15/Aug/18 23:03,15/May/15 17:04,1.3.1,1.4.0,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,MLlib,,,,,0,,,,,Currently calling map on both DenseMatrix and SparseMatrix will throw original transpose property away. It should be preserved.,,apachespark,mengxr,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-15 14:42:02.413,,false,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 17:04:10 UTC 2015,,,,,0|i2esbj:,9223372036854775807,,,,,,1.3.2,1.4.0,,,,,,"15/May/15 14:42;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6188","15/May/15 17:04;mengxr;Issue resolved by pull request 6188
[https://github.com/apache/spark/pull/6188]",,,,,,,,,,,,,,,,,,,,,,
Performance regression in naive Bayes prediction,SPARK-7652,12830027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,mengxr,mengxr,15/May/15 00:16,19/May/15 20:53,15/Aug/18 23:03,19/May/15 20:53,1.4.0,,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,,,,,0,,,,,Saw some performance regression in naive Bayes prediction. Since the multinomial code hasn't been changed since 1.3. I guess the root cause may be the breeze upgrade. So we may consider updating the implementation in prediction as well.,,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7612,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-15 14:53:04.938,,false,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 20:53:28 UTC 2015,,,,,0|i2ercn:,9223372036854775807,,,,,,1.4.0,,,,,,,"15/May/15 14:53;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6189","19/May/15 20:53;mengxr;Issue resolved by pull request 6189
[https://github.com/apache/spark/pull/6189]",,,,,,,,,,,,,,,,,,,,,,
"PySpark GMM predict, predictSoft should fail on bad input",SPARK-7651,12830009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,MeethuMathew,josephkb,josephkb,14/May/15 23:00,15/May/15 21:53,15/Aug/18 23:03,15/May/15 17:51,1.3.0,1.3.1,1.4.0,,,,,,,,,,,,,1.3.2,1.4.0,,,,,MLlib,PySpark,,,,0,,,,,"In PySpark, GaussianMixtureModel predict and predictSoft test if the argument is an RDD and operate correctly if so.  But if the argument is not an RDD, they fail silently, returning nothing.

[https://github.com/apache/spark/blob/11a1a135d1fe892cd48a9116acc7554846aed84c/python/pyspark/mllib/clustering.py#L176]

Instead, they should raise errors.",,apachespark,josephkb,MeethuMathew,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-15 04:31:27.077,,false,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 08:09:05 UTC 2015,,,,,0|i2er8n:,9223372036854775807,,,,,,1.3.2,1.4.0,,,,,,14/May/15 23:17;josephkb;Ping [~MeethuMathew]: Could you please make this fix?  It should be quick.  Thank you!,"15/May/15 04:31;MeethuMathew;[~josephkb] Yea, I wil fix it asap.","15/May/15 04:50;josephkb;Great, thank you!",15/May/15 04:53;MeethuMathew;Could you please tell me where I should make the changes? In master branch or 1.3.0?,15/May/15 04:58;josephkb;Please base your PR on master.  I can then merge it with master + the other branches.,15/May/15 04:59;MeethuMathew;Ok thank you,15/May/15 04:59;MeethuMathew;Ok thank you,"15/May/15 08:09;apachespark;User 'FlytxtRnD' has created a pull request for this issue:
https://github.com/apache/spark/pull/6180",,,,,,,,,,,,,,,,
MLLIB Word2Vec wordVectors divided by Euclidean Norm equals to zero ,SPARK-7615,12829688,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,ezli,ezli,13/May/15 21:16,12/Jan/16 13:28,15/Aug/18 23:03,12/Jan/16 13:28,1.3.1,,,,,,,,,,,,,,,1.6.1,2.0.0,,,,,MLlib,,,,,0,,,,,"In Word2VecModel, wordVecNorms may contains Euclidean Norm equals to zero. This will cause incorrect calculation for cosine distance. when you do cosineVec(ind) / wordVecNorms(ind). Cosine distance should be equal to 0 for norm = 0. ",,amartgon,apachespark,ezli,josephkb,michaelmalak,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-14 16:40:58.919,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 12 13:28:25 UTC 2016,,,,,0|i2ep9z:,9223372036854775807,,,,,,,,,,,,,"14/May/15 16:40;amartgon;Hi, is any body working on this (and SPARK-7617, SPARK-7618)? I have been using Spark for some time, and would like get started contributing. This looks like a trivial issue (good fo a newbie).

Shall I provide a pull request for this? Would it make sense a pull request for the three related issues?

","14/May/15 16:43;srowen;Yes the submitter is about to open another PR, as I understand it. ","18/May/15 21:05;apachespark;User 'ezli' has created a pull request for this issue:
https://github.com/apache/spark/pull/6245","11/Jan/16 10:51;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/10696",12/Jan/16 13:28;srowen;Resolved by https://github.com/apache/spark/pull/10696,,,,,,,,,,,,,,,,,,,
[MLLib] Using Kryo with FPGrowth fails with an exception,SPARK-7483,12828334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mark_yang,kretes,kretes,08/May/15 11:53,19/Jan/18 20:36,15/Aug/18 23:03,27/Feb/16 13:51,1.3.1,,,,,,,,,,,,,,,2.0.0,,,,,,MLlib,,,,,4,,,,,"When using FPGrowth algorithm with KryoSerializer - Spark fails with

{code}
Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 16, localhost): com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Can not set final scala.collection.mutable.ListBuffer field org.apache.spark.mllib.fpm.FPTree$Summary.nodes to scala.collection.mutable.ArrayBuffer
Serialization trace:
nodes (org.apache.spark.mllib.fpm.FPTree$Summary)
org$apache$spark$mllib$fpm$FPTree$$summaries (org.apache.spark.mllib.fpm.FPTree)
{code}

This can be easily reproduced in spark codebase by setting 
{code}
conf.set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
{code} and running FPGrowthSuite.


",,apachespark,aparcero,astral303,josephkb,Kaiyuan Yang,Kralph,kretes,mengxr,mlety2,simonlou,sunrenxu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-08 21:21:23.886,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 27 13:51:22 UTC 2016,,,,,0|i2eh4v:,9223372036854775807,,,,,,,,,,,,,"08/May/15 21:21;josephkb;(Updated) Maybe this is a bug...will look into it.
","08/May/15 21:25;josephkb;Does it fix anything if you give Kryo more info, such as explicit registration of relevant classes?","11/May/15 09:30;kretes;hmm, class org.apache.spark.mllib.fpm.FPTree is private and in spark-mllib. Spark is registering classes in spark-core in org.apache.spark.serializer.KryoSerializer#toRegister so it is not straightforward how to do that easily.

And I do not think this is the case of registering, looking at spark serialization doc ""Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful.""","11/May/15 17:56;josephkb;I agree; it should work, but I'm not sure why it's failing.  I'm not that familiar with Kryo, but I'll ask around.  Thanks for reporting this!","02/Jul/15 02:27;Kralph;I encountered the same bug.
Adding 
sparkConf.registerKryoClasses(Array(classOf[ArrayBuffer[String]], classOf[ListBuffer[String]]))
seems to fix the problem.","02/Jul/15 15:13;kretes;this solves the problem for me as well.

maybe those classes should be registered in kryo from spark when user code is using kryo serialization?","02/Jul/15 23:24;Kralph;This does NOT work.

Registering the classes stops it from crashing, but produces a bug in the FP-Growth algorithm.

Specifically, the frequency counts for itemsets are wrong.

:(",03/Jul/15 07:12;kretes;hmm that's right - adding it in FPGrowthSuite in spark fails first spec.,"05/Jul/15 22:14;sunrenxu;I've tried to change the nodes property of Summay class inside object FPTree from ListBuffer to ArrayBuffer. And it seems to fix the exception problem as well as producing the right item counts so far. Suspect the problem with KryoSerializer dealing with ListBuffer class, but haven't looked into details.  The code looks like:

private[fpm] object FPTree {

  /** Representing a node in an FP-Tree. */
  class Node[T](val parent: Node[T]) extends Serializable {
    var item: T = _
    var count: Long = 0L
    val children: mutable.Map[T, Node[T]] = mutable.Map.empty

    def isRoot: Boolean = parent == null
  }

  /** Summary of a item in an FP-Tree. */
  private class Summary[T] extends Serializable {
    var count: Long = 0L
    val nodes: mutable.ArrayBuffer[Node[T]] = mutable.ArrayBuffer.empty
  }
}

","24/Sep/15 11:32;simonlou;kyro not support ListBuffer because ListBuffer don't have any ""zero argument constructor"".
refer to : https://github.com/EsotericSoftware/kryo#using-standard-java-serialization

""By default, if a class has a zero argument constructor then it is invoked via ReflectASM or reflection, otherwise an exception is thrown. ""

Is that the reason?

When using kyro , use ArrayBuffer instead of ListBuffer","03/Feb/16 02:20;apachespark;User 'mark800' has created a pull request for this issue:
https://github.com/apache/spark/pull/11041","03/Feb/16 02:24;Kaiyuan Yang;Upgrade Chill to 0.7.2, then everything is fine.

New version registers more Scala classes, including ListBuffer to support Kryo with FPGrowth.","27/Feb/16 13:51;srowen;Issue resolved by pull request 11041
[https://github.com/apache/spark/pull/11041]",,,,,,,,,,,
need to sort when convert to array in topByKey,SPARK-7452,12828148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coderxiang,coderxiang,coderxiang,07/May/15 21:54,08/May/15 03:58,15/Aug/18 23:03,08/May/15 03:55,1.4.0,,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,,,,,0,,,,,"the toArray function of the PriorityQueue does not necessarily preserve order. Add a counter-example as the test, which would fail the original impl.",,apachespark,coderxiang,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-05-07 22:38:17.858,,false,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 03:55:37 UTC 2015,,,,,0|i2eg0n:,9223372036854775807,,,,,,1.4.0,,,,,,,"07/May/15 22:38;apachespark;User 'coderxiang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5990","08/May/15 03:55;josephkb;Issue resolved by pull request 5990
[https://github.com/apache/spark/pull/5990]",,,,,,,,,,,,,,,,,,,,,,
Spark Python 1.3.1 Mllib dataframe random forest problem,SPARK-7369,12827329,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,lisbethron,lisbethron,05/May/15 15:37,06/May/15 11:31,15/Aug/18 23:03,06/May/15 06:30,1.3.1,,,,,,,,,,,,,,,,,,,,,MLlib,PySpark,,,,0,hadoop,,,,"I'm working with Dataframes to train a random forest with mllib

and I have this error
  File ""/opt/mapr/spark/spark-1.3.1-bin-mapr4/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o58.sql.

somebody can help me...???",,josephkb,lisbethron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/May/15 11:31;lisbethron;random_forest_dataframe_spark_30042015.py;https://issues.apache.org/jira/secure/attachment/12730807/random_forest_dataframe_spark_30042015.py,,,1.0,,,,,,,,,,,,,,,,2015-05-05 19:54:56.827,,false,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 11:31:40 UTC 2015,,,,,0|i2eb5r:,9223372036854775807,,,,,,,,,,,,,"05/May/15 19:54;josephkb;Can you please post more info, such as what exactly was run + the full stack trace?","06/May/15 06:30;srowen;Have a look at https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

This sounds, on its face, like a py4j issue. These kinds of things can be reopened if there is more specific evidence it's Spark-related.","06/May/15 11:31;lisbethron;Hi Sean,

I still have problems with python spark here are the errors and also the
code that I'm using.

Thanks

Lisbeth



15/05/06 13:14:24 INFO ContextCleaner: Cleaned broadcast 1
15/05/06 13:14:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory
on node001.ca-innovation.fr:47882 (size: 11.0 KB, free: 8.3 GB)
15/05/06 13:14:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory
on node006.ca-innovation.fr:50830 (size: 11.0 KB, free: 8.3 GB)
15/05/06 13:14:25 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 5,
node001.ca-innovation.fr): java.lang.NullPointerException
        at
org.apache.spark.api.python.SerDeUtil$$anonfun$toJavaArray$1.apply(SerDeUtil.scala:106)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at
org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:123)
        at
org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:114)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at
org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:114)
        at
org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:421)
        at
org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
        at
org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
        at
org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

15/05/06 13:14:25 INFO TaskSetManager: Starting task 0.1 in stage 3.0 (TID
7, node001.ca-innovation.fr, NODE_LOCAL, 1409 bytes)
15/05/06 13:14:25 INFO TaskSetManager: Lost task 1.0 in stage 3.0 (TID 6)
on executor node006.ca-innovation.fr: java.lang.NullPointerException (null)
[duplicate 1]
15/05/06 13:14:25 INFO TaskSetManager: Starting task 1.1 in stage 3.0 (TID
8, node001.ca-innovation.fr, NODE_LOCAL, 1409 bytes)
15/05/06 13:14:26 INFO TaskSetManager: Lost task 0.1 in stage 3.0 (TID 7)
on executor node001.ca-innovation.fr: java.lang.NullPointerException (null)
[duplicate 2]
15/05/06 13:14:26 INFO TaskSetManager: Starting task 0.2 in stage 3.0 (TID
9, node006.ca-innovation.fr, NODE_LOCAL, 1409 bytes)
15/05/06 13:14:26 INFO TaskSetManager: Lost task 1.1 in stage 3.0 (TID 8)
on executor node001.ca-innovation.fr: java.lang.NullPointerException (null)
[duplicate 3]
15/05/06 13:14:26 INFO TaskSetManager: Starting task 1.2 in stage 3.0 (TID
10, node001.ca-innovation.fr, NODE_LOCAL, 1409 bytes)
15/05/06 13:14:27 INFO TaskSetManager: Lost task 0.2 in stage 3.0 (TID 9)
on executor node006.ca-innovation.fr: java.lang.NullPointerException (null)
[duplicate 4]
15/05/06 13:14:27 INFO TaskSetManager: Starting task 0.3 in stage 3.0 (TID
11, node006.ca-innovation.fr, NODE_LOCAL, 1409 bytes)
15/05/06 13:14:27 INFO TaskSetManager: Lost task 1.2 in stage 3.0 (TID 10)
on executor node001.ca-innovation.fr: java.lang.NullPointerException (null)
[duplicate 5]
15/05/06 13:14:27 INFO TaskSetManager: Starting task 1.3 in stage 3.0 (TID
12, node001.ca-innovation.fr, NODE_LOCAL, 1409 bytes)
15/05/06 13:14:28 INFO TaskSetManager: Lost task 0.3 in stage 3.0 (TID 11)
on executor node006.ca-innovation.fr: java.lang.NullPointerException (null)
[duplicate 6]
15/05/06 13:14:28 ERROR TaskSetManager: Task 0 in stage 3.0 failed 4 times;
aborting job
15/05/06 13:14:28 INFO TaskSchedulerImpl: Cancelling stage 3
15/05/06 13:14:28 INFO TaskSchedulerImpl: Stage 3 was cancelled
15/05/06 13:14:28 INFO DAGScheduler: Stage 3 (count at
/mapr/MapR-Cluster/casarisk/data/POCGRO/Codes/Spark_python/RF_Python_Spark_30042015/random_forest_dataframe_spark_30042015.py:79)
failed in 4.025 s
15/05/06 13:14:28 INFO DAGScheduler: Job 3 failed: count at
/mapr/MapR-Cluster/casarisk/data/POCGRO/Codes/Spark_python/RF_Python_Spark_30042015/random_forest_dataframe_spark_30042015.py:79,
took 4.052326 s
Traceback (most recent call last):
  File
""/mapr/MapR-Cluster/casarisk/data/POCGRO/Codes/Spark_python/RF_Python_Spark_30042015/random_forest_dataframe_spark_30042015.py"",
line 79, in <module>
    print trainingData.count()
  File ""/opt/mapr/spark/spark-1.3.1-bin-mapr4/python/pyspark/rdd.py"", line
932, in count
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File ""/opt/mapr/spark/spark-1.3.1-bin-mapr4/python/pyspark/rdd.py"", line
923, in sum
    return self.mapPartitions(lambda x: [sum(x)]).reduce(operator.add)
  File ""/opt/mapr/spark/spark-1.3.1-bin-mapr4/python/pyspark/rdd.py"", line
739, in reduce
    vals = self.mapPartitions(func).collect()
  File ""/opt/mapr/spark/spark-1.3.1-bin-mapr4/python/pyspark/rdd.py"", line
713, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File
""/opt/mapr/spark/spark-1.3.1-bin-mapr4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"",
line 538, in __call__
  File
""/opt/mapr/spark/spark-1.3.1-bin-mapr4/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"",
line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling
z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0
in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage
3.0 (TID 11, node006.ca-innovation.fr): java.lang.NullPointerException
        at
org.apache.spark.api.python.SerDeUtil$$anonfun$toJavaArray$1.apply(SerDeUtil.scala:106)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at
org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:123)
        at
org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:114)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at
org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:114)
        at
org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:421)
        at
org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)
        at
org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
        at
org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
        at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
        at scala.Option.foreach(Option.scala:236)
        at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
        at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
        at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)








-- 
*Lisbeth*
",,,,,,,,,,,,,,,,,,,,,
FPGrowth algo throwing OutOfMemoryError,SPARK-7337,12826893,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,amit.gupta.niit-tech,amit.gupta.niit-tech,04/May/15 10:03,03/Dec/15 15:33,15/Aug/18 23:03,03/Dec/15 15:33,1.3.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"When running FPGrowth algo with huge data in GBs and with numPartitions=500 then after some time it throws OutOfMemoryError.
Algo runs correctly upto ""collect at FPGrowth.scala:131"" where it creates 500 tasks. It fails at next stage ""flatMap at FPGrowth.scala:150"" where it fails to create 500 tasks and create some internal calculated 17 tasks.

Please refer to attachment - print screen.",Ubuntu,amit.gupta.niit-tech,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,04/May/15 10:06;amit.gupta.niit-tech;FPGrowthBug.png;https://issues.apache.org/jira/secure/attachment/12730148/FPGrowthBug.png,,,1.0,,,,,,,,,,,,,,,,2015-05-04 11:07:07.595,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 04:03:37 UTC 2015,,,,,0|i2e8nr:,9223372036854775807,mengxr,,,,,,,,,,,,"04/May/15 11:07;srowen;There's no info here on your input, your settings, your cluster, or the source of the out of memory error.","04/May/15 11:15;amit.gupta.niit-tech;I am running it in ""local"" mode. It should spill over hard-disk. I can clearly see that 500 tasks are not created by next stage hence OutOfMemoryError is coming.","04/May/15 11:20;amit.gupta.niit-tech;I am running it in ""local"" mode and using Java API. It should spill over hard-disk. I can clearly see that 500 tasks are not created by next stage hence OutOfMemoryError is coming.

Please let me know if you need further information. I tried to run same logic with custom code with 50 partitions and it worked. When I tried FPGrowth with same data with >50 partitions it failed. Perhaps you need to check why 500 tasks (specified using numPartitions=500) are not created for ""collect at FPGrowth.scala:131"" (source of error).

Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 118, localhost): java.lang.OutOfMemoryError: Java heap space
	at java.util.IdentityHashMap.resize(IdentityHashMap.java:471)
	at java.util.IdentityHashMap.put(IdentityHashMap.java:440)
	at org.apache.spark.util.SizeEstimator$SearchState.enqueue(SizeEstimator.scala:132)
	at org.apache.spark.util.SizeEstimator$$anonfun$visitArray$1.apply$mcVI$sp(SizeEstimator.scala:203)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.SizeEstimator$.visitArray(SizeEstimator.scala:202)
	at org.apache.spark.util.SizeEstimator$.visitSingleObject(SizeEstimator.scala:169)
	at org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate(SizeEstimator.scala:161)
	at org.apache.spark.util.SizeEstimator$$anonfun$visitArray$2.apply$mcVI$sp(SizeEstimator.scala:217)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.SizeEstimator$.visitArray(SizeEstimator.scala:210)
	at org.apache.spark.util.SizeEstimator$.visitSingleObject(SizeEstimator.scala:169)
	at org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate(SizeEstimator.scala:161)
	at org.apache.spark.util.SizeEstimator$.estimate(SizeEstimator.scala:155)
	at org.apache.spark.util.collection.SizeTracker$class.takeSample(SizeTracker.scala:78)
	at org.apache.spark.util.collection.SizeTracker$class.afterUpdate(SizeTracker.scala:70)
	at org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:33)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:205)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:56)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
15/05/04 17:04:12 INFO Executor: Running task 4.0 in stage 8.0 (TID 122)
15/05/04 17:04:12 ERROR Executor: Exception in task 4.0 in stage 8.0 (TID 122)
org.apache.spark.TaskKilledException
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/05/04 17:04:12 WARN TaskSetManager: Lost task 4.0 in stage 8.0 (TID 122, localhost): org.apache.spark.TaskKilledException
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
","04/May/15 20:27;josephkb;Do you know how many distinct items are being collected to the driver?  If it's too many, then you'll have to increase minCount.  If that's the issue, then this is not a bug, but a limitation of the implementation.","05/May/15 04:18;amit.gupta.niit-tech;Yes, I know that if anything collected on driver which doesn't fit memory will fail. Here I am talking about below line:

FPGrowthModel<String> model = new FPGrowth()
		.setMinSupport(minSupport)
		.setNumPartitions(numPartition)
		.run(transactions);

Where numPartition is >50 (i.e. 500). Please refer to print screen, one which is active stage (belongs to above API call) is throwing OutOfMemoryError. Now again referring to print screen, Stage just before active one (whose status is completed) has 500 tasks and Stage which is active has 17 tasks, instead it should have 500 tasks as I set numPartition as 500. And if you again refer to print screen, next Stage pending for execution again has 500 tasks. If you fix code around active Stage with 17 tasks to have tasks equal to numPartition then OutOfMemoryError issues will be fixed.",25/Jun/15 15:23;mengxr;How large is the `minSupport`? The number of frequent itemsets grows exponentially as minSupport decreases. So please start with a really large value (close to 1.0) and gradually reduce it.,07/Jul/15 20:13;mengxr;[~amit.gupta.niit-tech] Any updates?,"08/Jul/15 04:03;amit.gupta.niit-tech;I look at your comment as workaround, when you say make minSupport to 1.0 then you are asking me to get item sequences appears in all the transactions and then work backwards to get upto the breaking point.
Well I am not looking for workaround as I already have custom code written in core spark api which is working fine, its only FP api which breaks which I thought to try out.
When Tree grows beyond RAM then it should spill over to disk rather then it should throw outofmemory.

Try to use data in below site for recommendation/seq. of items, on single machine: https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data.",,,,,,,,,,,,,,,,
"Add Matrix, SparseMatrix to __all__ list in linalg.py",SPARK-7208,12825675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,josephkb,josephkb,josephkb,28/Apr/15 21:10,29/Apr/15 04:16,15/Aug/18 23:03,29/Apr/15 04:16,1.4.0,,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,PySpark,,,,0,,,,,,,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-28 21:13:16.417,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 04:16:19 UTC 2015,,,,,0|i2e1an:,9223372036854775807,,,,,,1.4.0,,,,,,,"28/Apr/15 21:13;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/5759","29/Apr/15 04:16;mengxr;Issue resolved by pull request 5759
[https://github.com/apache/spark/pull/5759]",,,,,,,,,,,,,,,,,,,,,,
GradientBoostTrees leaks a persisted RDD,SPARK-7100,12823526,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jimfcarroll,jimfcarroll,jimfcarroll,23/Apr/15 19:15,28/Apr/15 11:52,15/Aug/18 23:03,28/Apr/15 11:51,1.2.2,1.3.1,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,,,,,0,,,,,"It appears GradientBoostedTrees.scala can call 'persist' on an RDD and never unpersist it.

In the master branch it's in GradientBoostedTrees.boost method. It ""persists"" the input RDD if it's not already persisted but doesn't unpersist it.

I'll be submitting a PR with a fix.",,apachespark,jimfcarroll,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-23 19:21:26.814,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 11:51:12 UTC 2015,,,,,0|i2dohb:,9223372036854775807,,,,,,1.4.0,,,,,,,"23/Apr/15 19:21;apachespark;User 'jimfcarroll' has created a pull request for this issue:
https://github.com/apache/spark/pull/5669","23/Apr/15 19:36;srowen;(Don't set fix/target version, and I don't think this is major. Have a look at https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark)",23/Apr/15 21:08;jimfcarroll;Sorry about the fix/target version. I should have looked fist.,"28/Apr/15 11:51;srowen;Issue resolved by pull request 5669
[https://github.com/apache/spark/pull/5669]",,,,,,,,,,,,,,,,,,,,
Inconsistent default miniBatchFraction parameters in the train methods of RidgeRegression,SPARK-7085,12823339,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Kuromatsu,Kuromatsu,Kuromatsu,23/Apr/15 08:24,23/Apr/15 21:03,15/Aug/18 23:03,23/Apr/15 21:00,1.3.1,,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,,,,,0,,,,,"The miniBatchFraction parameter in the train method called with 4 arguments is 0.01, that is,

{code:title=RidgeRegression.scala|borderStyle=solid}
def train(
      input: RDD[LabeledPoint],
      numIterations: Int,
      stepSize: Double,
      regParam: Double): RidgeRegressionModel = {
    train(input, numIterations, stepSize, regParam, 0.01)
  }
{code}

but, the parameter is 1.0 in the other train methods. For example,
{code:title=RidgeRegression.scala|borderStyle=solid}
  def train(
      input: RDD[LabeledPoint],
      numIterations: Int): RidgeRegressionModel = {
    train(input, numIterations, 1.0, 0.01, 1.0)
  }
{code}


",,apachespark,josephkb,Kuromatsu,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-23 10:21:32.92,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 21:00:51 UTC 2015,,,,,0|i2dncv:,9223372036854775807,,,,,,1.4.0,,,,,,,"23/Apr/15 10:21;apachespark;User 'kuromatsu-nobuyuki' has created a pull request for this issue:
https://github.com/apache/spark/pull/5658","23/Apr/15 21:00;josephkb;Issue resolved by pull request 5658
[https://github.com/apache/spark/pull/5658]",,,,,,,,,,,,,,,,,,,,,,
LDA.setBeta calls itself,SPARK-7070,12823302,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,23/Apr/15 05:25,23/Apr/15 21:47,15/Aug/18 23:03,23/Apr/15 21:47,1.3.1,,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,MLlib,,,,,0,,,,,"Should call setTopicConcentration.

Reported by buring: http://apache-spark-user-list.1001560.n3.nabble.com/LDA-code-little-error-Xiangrui-Meng-td22621.html",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-23 05:28:14.953,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 21:47:14 UTC 2015,,,,,0|i2dn47:,9223372036854775807,,,,,,1.3.2,1.4.0,,,,,,"23/Apr/15 05:28;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5649","23/Apr/15 21:47;mengxr;Issue resolved by pull request 5649
[https://github.com/apache/spark/pull/5649]",,,,,,,,,,,,,,,,,,,,,,
"VectorAssembler should use NumericType, not NativeType",SPARK-7066,12823281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,23/Apr/15 02:42,23/Apr/15 04:35,15/Aug/18 23:03,23/Apr/15 04:35,,,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,,,,,0,,,,,,,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-23 02:43:11.897,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 02:43:11 UTC 2015,,,,,0|i2dmzj:,9223372036854775807,,,,,,1.4.0,,,,,,,"23/Apr/15 02:43;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5642",,,,,,,,,,,,,,,,,,,,,,,
ALS.train should support DataFrames in PySpark,SPARK-7036,12822750,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mengxr,mengxr,mengxr,21/Apr/15 18:31,21/Apr/15 23:45,15/Aug/18 23:03,21/Apr/15 23:45,1.3.1,,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,MLlib,,,,,0,,,,,ALS.train works with SchemaRDDs in 1.2. We should support DataFrames for compatibility.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-21 20:18:38.471,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 23:45:10 UTC 2015,,,,,0|i2djzj:,9223372036854775807,,,,,,1.3.2,1.4.0,,,,,,"21/Apr/15 20:18;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5619","21/Apr/15 23:45;mengxr;Issue resolved by pull request 5619
[https://github.com/apache/spark/pull/5619]",,,,,,,,,,,,,,,,,,,,,,
Make StreamingKMeans `Serializable`,SPARK-6998,12822159,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,19/Apr/15 17:55,20/Apr/15 03:34,15/Aug/18 23:03,20/Apr/15 03:34,1.2.2,1.3.1,,,,,,,,,,,,,,1.2.3,1.3.2,1.4.0,,,,MLlib,,,,,0,,,,,"If `StreamingKMeans` is not `Serializable`, we cannot do checkpoint for applications that using `StreamingKMeans`. So we should make it `Serializable`.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6979,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-19 17:58:43.327,,false,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 19 17:58:43 UTC 2015,,,,,0|i2dg9r:,9223372036854775807,,,,,,,,,,,,,"19/Apr/15 17:58;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5582",,,,,,,,,,,,,,,,,,,,,,,
Tiny bug in PowerIterationClusteringExample in which radius not accepted from command line,SPARK-6937,12821231,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,javadba,javadba,javadba,15/Apr/15 17:06,15/Apr/15 20:28,15/Aug/18 23:03,15/Apr/15 20:28,1.3.0,,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,MLlib,,,,,0,,,,,Tiny bug in PowerIterationClusteringExample in which radius not accepted from command line,,apachespark,javadba,josephkb,mengxr,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-15 17:07:34.394,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 20:28:27 UTC 2015,,,,,0|i2danb:,9223372036854775807,mengxr,,,,,,,,,,,,"15/Apr/15 17:07;apachespark;User 'javadba' has created a pull request for this issue:
https://github.com/apache/spark/pull/5531","15/Apr/15 20:28;mengxr;Issue resolved by pull request 5531
[https://github.com/apache/spark/pull/5531]",,,,,,,,,,,,,,,,,,,,,,
"Word2Vec's transform method throw IllegalStateException if a word not in vocabulary, but  findSynonyms(word: String, num: Int) method neither try catch exception nor throw exception. ",SPARK-6926,12821068,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Duplicate,,chrispy,chrispy,15/Apr/15 06:29,15/Apr/15 08:01,15/Aug/18 23:03,15/Apr/15 07:38,1.3.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,mllib,word2vec,,,"
{code}
  val doc = sc.textFile(""/xxx/segrestxt"", args(3).toInt).map(line => line.split("" "").toSeq)
  val model = new Word2Vec().setVectorSize(10).setSeed(42L).setNumIterations(3).fit(doc)
  val cpseeds = sc.textFile(""/xxx/seeds/cp.tag"", args(3).toInt)
{code} 
 	
it will get a {{IllegalStateException}} and then exit app when tag is a word not in vocabulary, because {{findSynonyms}} method handle the possible IllegalStateException.  

{code}
  cpseeds.map {
    tag =>
      val syn = model.findSynonyms(tag, 30).map(l => l._1)
      tag + "":"" + Joiner.on("" "").join(JavaConversions.asJavaIterator(syn.toIterator))
  }.saveAsTextFile(""/xxx/synonyms/cp"")
{code}
	
developer need to catch the IllegalStateException to find out what's going on here.

{code}
  cpseeds.map {
    tag =>
      try {
        val syn = model.findSynonyms(tag, 30).map(l => l._1)
        tag + "":"" + Joiner.on("" "").join(JavaConversions.asJavaIterator(syn.toIterator))
      } catch {
        case e: IllegalStateException => log.error(s""cp tag:$tag"", e)
      }
  }.saveAsTextFile(""/xxx/synonyms/cp"")
{code}",,chrispy,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6925,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-15 07:38:37.461,,false,,,,,Important,,,,,,,,9223372036854775807,,,Wed Apr 15 08:01:37 UTC 2015,,,,,0|i2d9nr:,9223372036854775807,mengxr,,,,,,,,,,,,15/Apr/15 07:38;srowen;Careful about opening issues twice.,15/Apr/15 08:01;chrispy;thank you so much,,,,,,,,,,,,,,,,,,,,,,
"Word2Vec's transform method throw IllegalStateException if a word not in vocabulary, but  findSynonyms(word: String, num: Int) method neither try catch exception nor throw exception. ",SPARK-6925,12821067,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Duplicate,,chrispy,chrispy,15/Apr/15 06:28,15/Apr/15 08:37,15/Aug/18 23:03,15/Apr/15 06:33,1.3.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,mllib,word2vec,,,"{code}
  val doc = sc.textFile(""/xxx/segrestxt"", args(3).toInt).map(line => line.split("" "").toSeq)
  val model = new Word2Vec().setVectorSize(10).setSeed(42L).setNumIterations(3).fit(doc)
  val cpseeds = sc.textFile(""/xxx/seeds/cp.tag"", args(3).toInt)
{code} 
 	
it will get a {{IllegalStateException}} and then exit app when tag is a word not in vocabulary, because {{findSynonyms}} method handle the possible IllegalStateException.  

{code}
  cpseeds.map {
    tag =>
      val syn = model.findSynonyms(tag, 30).map(l => l._1)
      tag + "":"" + Joiner.on("" "").join(JavaConversions.asJavaIterator(syn.toIterator))
  }.saveAsTextFile(""/xxx/synonyms/cp"")
{code}
	
developer need to catch the IllegalStateException to find out what's going on here.

{code}
  cpseeds.map {
    tag =>
      try {
        val syn = model.findSynonyms(tag, 30).map(l => l._1)
        tag + "":"" + Joiner.on("" "").join(JavaConversions.asJavaIterator(syn.toIterator))
      } catch {
        case e: IllegalStateException => log.error(s""cp tag:$tag"", e)
      }
  }.saveAsTextFile(""/xxx/synonyms/cp"")
{code}",,chrispy,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-15 07:30:18.862,,false,,,,,Important,,,,,,,,9223372036854775807,,,Wed Apr 15 08:37:49 UTC 2015,,,,,0|i2d9nj:,9223372036854775807,mengxr,,,,,,,,,,,,"15/Apr/15 07:30;srowen;I might be missing something, but isn't that expected behavior? what do you expect instead as the result of such an operation?",15/Apr/15 08:37;chrispy;i think model.findSynonyms method throws the IllegalStateException may be fine.,,,,,,,,,,,,,,,,,,,,,,
kmeans|| hangs for a long time if both k and vector dimension are large,SPARK-6706,12788240,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,mengxr,davidshen84,davidshen84,04/Apr/15 01:35,16/May/16 10:15,15/Aug/18 23:03,05/Apr/15 09:26,1.2.1,1.3.0,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,performance,,,,"When doing k-means cluster with the ""kmeans||"" algorithm which is the default one. The algorithm finished some {{collect()}} jobs, then the *driver* hangs for a long time.

Settings:

- k above 100
- feature dimension about 360
- total data size is about 100 MB

The issue was first noticed with Spark 1.2.1. I tested with both local and cluster mode. On Spark 1.3.0. I, I can also reproduce this issue with local mode. **However, I do not have a 1.3.0 cluster environment for me to test.**","Windows 64bit, Linux 64bit",apachespark,davidshen84,mouendless,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3220,,,,,,,,,,04/Apr/15 01:42;davidshen84;kmeans-debug.7z;https://issues.apache.org/jira/secure/attachment/12709371/kmeans-debug.7z,,,1.0,,,,,,,,,,,,,,,,2015-04-04 07:04:42.237,,false,,,,,,,,,,,,,9223372036854775807,,,Mon May 16 10:15:06 UTC 2016,,,,,0|i27s5z:,9223372036854775807,mengxr,,,,,,,,,,,,04/Apr/15 01:42;davidshen84;This package contains the code and dummy data. The test data is about 100 MB. I will update the bug once I find a place to host the test data.,04/Apr/15 01:49;davidshen84;The complete code and test data is available at http://1drv.ms/1FrNeye,"04/Apr/15 07:04;srowen;Hey Xi, I don't think this makes for a good JIRA as it does not sound like you've investigated what is happening. For example you can look at exactly what step is executing in the UI, and look at the source to know what is being computed. It's not clear whether you know it is stuck or simply still executing, or whether it's your RDDs that are being computed. Typically it's best to reproduce it locally vs master if at all possible. Although providing code is good, the whole code dump doesn't narrow it down.","04/Apr/15 08:08;davidshen84;I know it is more like a user report than a technical report. But I am not familiar with Spark code, and I am currently busy with my study. I am happy to look deep into this issue, but it may not happen very soon.

As for your question **It's not clear whether you know it is stuck or simply still executing**. I can confirm it is still executing. I observe one of my CPU is constantly busy a Java process, and if the *k* value is not very large, say 500, the job can finish after a long time.","04/Apr/15 09:10;srowen;I tried your code locally vs master with k=1000 (you say >100, but it works at 500, so I tried 1000), which you can do by building Spark and running the shell. I don't see it stuck in any {{collect()}} stage; those complete quickly. But, the driver does bog down for a long long time in {{LocalKMeans}}:

{code}
	at com.github.fommil.netlib.F2jBLAS.ddot(F2jBLAS.java:71)
	at org.apache.spark.mllib.linalg.BLAS$.dot(BLAS.scala:121)
	at org.apache.spark.mllib.linalg.BLAS$.dot(BLAS.scala:104)
	at org.apache.spark.mllib.util.MLUtils$.fastSquaredDistance(MLUtils.scala:311)
	at org.apache.spark.mllib.clustering.KMeans$.fastSquaredDistance(KMeans.scala:522)
	at org.apache.spark.mllib.clustering.KMeans$$anonfun$findClosest$1.apply(KMeans.scala:496)
	at org.apache.spark.mllib.clustering.KMeans$$anonfun$findClosest$1.apply(KMeans.scala:490)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.GenSeqViewLike$Sliced$class.foreach(GenSeqViewLike.scala:42)
	at scala.collection.mutable.IndexedSeqView$$anon$2.foreach(IndexedSeqView.scala:80)
	at org.apache.spark.mllib.clustering.KMeans$.findClosest(KMeans.scala:490)
	at org.apache.spark.mllib.clustering.KMeans$.pointCost(KMeans.scala:513)
	at org.apache.spark.mllib.clustering.LocalKMeans$$anonfun$kMeansPlusPlus$1$$anonfun$3.apply(LocalKMeans.scala:53)
	at org.apache.spark.mllib.clustering.LocalKMeans$$anonfun$kMeansPlusPlus$1$$anonfun$3.apply(LocalKMeans.scala:52)
	at scala.collection.GenTraversableViewLike$Mapped$$anonfun$foreach$2.apply(GenTraversableViewLike.scala:81)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:42)
	at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:43)
	at scala.collection.GenTraversableViewLike$Mapped$class.foreach(GenTraversableViewLike.scala:80)
	at scala.collection.SeqViewLike$$anon$3.foreach(SeqViewLike.scala:78)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:144)
	at scala.collection.SeqViewLike$AbstractTransformed.foldLeft(SeqViewLike.scala:43)
	at scala.collection.TraversableOnce$class.sum(TraversableOnce.scala:203)
	at scala.collection.SeqViewLike$AbstractTransformed.sum(SeqViewLike.scala:43)
	at org.apache.spark.mllib.clustering.LocalKMeans$$anonfun$kMeansPlusPlus$1.apply$mcVI$sp(LocalKMeans.scala:54)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.mllib.clustering.LocalKMeans$.kMeansPlusPlus(LocalKMeans.scala:49)
	at org.apache.spark.mllib.clustering.KMeans$$anonfun$22.apply(KMeans.scala:396)
	at org.apache.spark.mllib.clustering.KMeans$$anonfun$22.apply(KMeans.scala:393)
{code}

I think this is what Derrick was getting at in SPARK-3220, that this bit doesn't scale.","04/Apr/15 09:22;davidshen84;Yes, the {{collect()}} jobs finished, then hangs at the driver. Your words are more accurate.

But I don't observe this behavior with the *random initialization* of k-means. I think it is because the *kmeans||* algorithm has a more complex initialize algorithm.","16/May/16 09:25;mouendless;h2.Main Issue
I found the actually reason why GUI does not finish, which turns out that it's stuck with LocalKMeans. And there is a to be improved feature in LocalKMeans.scala in Mllib. After picking each new initial centers, it's unnecessary to compute the distances between all the points and the old centers as below
{code:scala}
val costArray = points.map { point =>
      KMeans.fastSquaredDistance(point, centers(0))
    }
{code}

Instead this we can keep the distance between all the points and their closest centers, and compare to the distance of them with the new center then update them.

h2.Test
Download [LocalKMeans.zip|https://dl.dropboxusercontent.com/u/83207617/LocalKMeans.zip]
I provided a attach ""LocalKMeans.zip"" which contains the code ""LocalKMeans.scala"" and dataset ""bigKMeansMedia"" 
LocalKMeans.scala contains both original version method KMeansPlusPlus and a modified version KMeansPlusPlusModify. (best fit with spark.mllib-1.6.0)
I added a tests and main function in it so that any one can run the file directly.

h3.How to Test
Replacing mllib.clustering.LocalKMeans.scala in your local repository with my LocalKMeans.scala. 
Modify the path in line 34 (loadAndRun()) with the path you restoring the data file bigKMeansMedia which is also provided in the patch. 
Tune the 2nd and 3rd parameter in line 34 (loadAndRun()) which are refereed to clustering number K and iteration number respectively. 
Then the console will print the cost time and SE of the two version of KMeans++ respectively.

h2.Test Results

This data is generated from a KMeans|| eperiment in spark, I add some inner function and output the result of KMeans|| initialization and restore.
The first line of the file with format ""%d:%d:%d:%d"" indicates ""the seed:feature num:iteration num (in original KMeans||):points num"" of the data. 

In my machine the experiment result is as below:

!https://cloud.githubusercontent.com/assets/10915169/15175957/6b21c3b0-179b-11e6-9741-66dfe4e23eb7.jpg!
 the x-axis is the clustering num k while y-axis is the time in seconds","16/May/16 10:15;apachespark;User 'mouendless' has created a pull request for this issue:
https://github.com/apache/spark/pull/13133",,,,,,,,,,,,,,,,
createDataFrame from RDD[Row] with UDTs cannot be saved,SPARK-6672,12787578,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,02/Apr/15 06:25,23/Apr/15 17:50,15/Aug/18 23:03,02/Apr/15 10:28,1.3.0,,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,MLlib,SQL,,,,0,,,,,"Reported by Jaonary (https://www.mail-archive.com/user@spark.apache.org/msg25218.html):

{code}
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.regression._
val df0 = sqlContext.createDataFrame(Seq(LabeledPoint(1.0, Vectors.dense(2.0, 3.0))))
df0.save(""/tmp/df0"") // works
val df1 = sqlContext.createDataFrame(df0.rdd, df0.schema)
df1.save(""/tmp/df1"") // error
{code}

throws

{code}
15/04/01 23:24:16 INFO DAGScheduler: Job 3 failed: runJob at newParquet.scala:686, took 0.288304 s
org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 3.0 failed 1 times, most recent failure: Lost task 3.0 in stage 3.0 (TID 15, localhost): java.lang.ClassCastException: org.apache.spark.mllib.linalg.DenseVector cannot be cast to org.apache.spark.sql.Row
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:191)
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:182)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:171)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:134)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$writeShard$1(newParquet.scala:668)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:686)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:686)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:212)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}",,apachespark,lian cheng,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-02 07:03:48.535,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 10:28:14 UTC 2015,,,,,0|i27oov:,9223372036854775807,,,,,,1.3.1,1.4.0,,,,,,"02/Apr/15 07:03;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5329","02/Apr/15 10:28;lian cheng;Issue resolved by pull request 5329
[https://github.com/apache/spark/pull/5329]",,,,,,,,,,,,,,,,,,,,,,
"Python type errors should print type, not object",SPARK-6661,12787519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,31z4,josephkb,josephkb,01/Apr/15 23:22,20/Apr/15 17:45,15/Aug/18 23:03,20/Apr/15 17:44,1.3.0,,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,PySpark,,,,0,,,,,"In MLlib PySpark, we sometimes test the type of an object and print an error if the object is of the wrong type.  E.g.:
[https://github.com/apache/spark/blob/f084c5de14eb10a6aba82a39e03e7877926ebb9e/python/pyspark/mllib/regression.py#L173]

These checks should print the type, not the actual object.  E.g., if the object cannot be converted to a string, then the check linked above will give a warning like this:
{code}
TypeError: not all arguments converted during string formatting
{code}
...which is weird for the user.

There may be other places in the codebase where this is an issue, so we need to check through and verify.",,31z4,apachespark,josephkb,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-04 20:45:50.566,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 20 17:44:31 UTC 2015,,,,,0|i27o9b:,9223372036854775807,,,,,,1.4.0,,,,,,,"04/Apr/15 20:45;apachespark;User '31z4' has created a pull request for this issue:
https://github.com/apache/spark/pull/5361","20/Apr/15 17:44;joshrosen;Issue resolved by pull request 5361
[https://github.com/apache/spark/pull/5361]",,,,,,,,,,,,,,,,,,,,,,
MLLibPythonAPI.pythonToJava doesn't recognize object arrays,SPARK-6660,12787516,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,01/Apr/15 23:05,02/Apr/15 01:31,15/Aug/18 23:03,02/Apr/15 01:31,,,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,MLlib,PySpark,,,,0,,,,,"{code}
points = MLUtils.loadLabeledPoints(sc, ""..."")
_to_java_object_rdd(points).count()
{code}

throws exception

{code}
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-22-5b481e99a111> in <module>()
----> 1 jrdd.count()

/home/ubuntu/databricks/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/home/ubuntu/databricks/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o510.count.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 114.0 failed 4 times, most recent failure: Lost task 18.3 in stage 114.0 (TID 1133, ip-10-0-130-35.us-west-2.compute.internal): java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to java.util.ArrayList
	at org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(PythonMLLibAPI.scala:1090)
	at org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(PythonMLLibAPI.scala:1087)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1472)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-01 23:14:12.808,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 01:31:36 UTC 2015,,,,,0|i27o8n:,9223372036854775807,,,,,,1.3.1,1.4.0,,,,,,"01/Apr/15 23:14;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5318","02/Apr/15 01:31;mengxr;Issue resolved by pull request 5318
[https://github.com/apache/spark/pull/5318]",,,,,,,,,,,,,,,,,,,,,,
Change the lambda weight to number of explicit ratings in implicit ALS,SPARK-6642,12787238,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,01/Apr/15 04:07,01/Apr/15 23:55,15/Aug/18 23:03,01/Apr/15 23:55,1.3.0,,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,MLlib,,,,,0,,,,,"Until SPARK-6637 is resolved, we should switch back to the 1.2 lambda weighting strategy to be consistent.",,apachespark,josephkb,mengxr,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6637,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-04-01 20:44:53.211,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 23:55:13 UTC 2015,,,,,0|i27mkv:,9223372036854775807,,,,,,1.3.1,1.4.0,,,,,,"01/Apr/15 20:44;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5314","01/Apr/15 23:55;mengxr;Issue resolved by pull request 5314
[https://github.com/apache/spark/pull/5314]",,,,,,,,,,,,,,,,,,,,,,
MatrixFactorizationModel created by load fails on predictAll,SPARK-6571,12786201,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,cchayden,cchayden,27/Mar/15 13:38,30/Mar/15 21:28,15/Aug/18 23:03,30/Mar/15 21:28,1.3.0,,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,MLlib,PySpark,,,,0,,,,,"This code, adapted from the documentation, fails when using a loaded model.
from pyspark.mllib.recommendation import ALS, Rating, MatrixFactorizationModel

r1 = (1, 1, 1.0)
r2 = (1, 2, 2.0)
r3 = (2, 1, 2.0)
ratings = sc.parallelize([r1, r2, r3])
model = ALS.trainImplicit(ratings, 1, seed=10)
print '(2, 2)', model.predict(2, 2)
#    0.43...
testset = sc.parallelize([(1, 2), (1, 1)])
print 'all', model.predictAll(testset).collect()
#    [Rating(user=1, product=1, rating=1.0...), Rating(user=1, product=2, rating=1.9...)]
import os, tempfile
path = tempfile.mkdtemp()
model.save(sc, path)
sameModel = MatrixFactorizationModel.load(sc, path)
print '(2, 2)', sameModel.predict(2,2)
sameModel.predictAll(testset).collect()


This gives
(2, 2) 0.443547642944
all [Rating(user=1, product=1, rating=1.1538351103381217), Rating(user=1, product=2, rating=0.7153473708381739)]
(2, 2) 0.443547642944
---------------------------------------------------------------------------
Py4JError                                 Traceback (most recent call last)
<ipython-input-18-af6612bed9d0> in <module>()
     19 sameModel = MatrixFactorizationModel.load(sc, path)
     20 print '(2, 2)', sameModel.predict(2,2)
---> 21 sameModel.predictAll(testset).collect()
     22 

/home/ubuntu/spark/python/pyspark/mllib/recommendation.pyc in predictAll(self, user_product)
    104         assert len(first) == 2, ""user_product should be RDD of (user, product)""
    105         user_product = user_product.map(lambda (u, p): (int(u), int(p)))
--> 106         return self.call(""predict"", user_product)
    107 
    108     def userFeatures(self):

/home/ubuntu/spark/python/pyspark/mllib/common.pyc in call(self, name, *a)
    134     def call(self, name, *a):
    135         """"""Call method of java_model""""""
--> 136         return callJavaFunc(self._sc, getattr(self._java_model, name), *a)
    137 
    138 

/home/ubuntu/spark/python/pyspark/mllib/common.pyc in callJavaFunc(sc, func, *args)
    111     """""" Call Java Function """"""
    112     args = [_py2java(sc, a) for a in args]
--> 113     return _java2py(sc, func(*args))
    114 
    115 

/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    302                 raise Py4JError(
    303                     'An error occurred while calling {0}{1}{2}. Trace:\n{3}\n'.
--> 304                     format(target_id, '.', name, value))
    305         else:
    306             raise Py4JError(

Py4JError: An error occurred while calling o450.predict. Trace:
py4j.Py4JException: Method predict([class org.apache.spark.api.java.JavaRDD]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)
	at py4j.Gateway.invoke(Gateway.java:252)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:744)
",,apachespark,cchayden,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-03-27 19:48:55.405,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 28 17:02:42 UTC 2015,,,,,0|i27g9r:,9223372036854775807,,,,,,1.3.1,1.4.0,,,,,,27/Mar/15 19:48;josephkb;Thanks for the detailed report!  I was able to reproduce the bug too.  Looking into it...,"28/Mar/15 17:02;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5243",,,,,,,,,,,,,,,,,,,,,,
Multinomial Logistic Regression failed when initialWeights is not null,SPARK-6496,12785135,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,24/Mar/15 11:10,25/Mar/15 17:07,15/Aug/18 23:03,25/Mar/15 17:06,1.3.0,,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,MLlib,,,,,0,,,,,"This bug is easy to reproduce, when use Multinomial Logistic Regression to train multiclass classification model with non-null initialWeights, it will throw an exception.
When you run
{code}
val lr = new LogisticRegressionWithLBFGS().setNumClasses(3)
val model = lr.run(input, initialWeights)
{code}
It will throw
{code}
requirement failed: LogisticRegressionModel.load with numClasses = 3 and numFeatures = -1 expected weights of length -2 (without intercept) or 0 (with intercept), but was given weights of length 10
java.lang.IllegalArgumentException: requirement failed: LogisticRegressionModel.load with numClasses = 3 and numFeatures = -1 expected weights of length -2 (without intercept) or 0 (with intercept), but was given weights of length 10
{code}
",,apachespark,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-03-24 11:22:56.813,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 17:07:08 UTC 2015,,,,,0|i279u7:,9223372036854775807,,,,,,,,,,,,,"24/Mar/15 11:22;srowen;The problem is numFeatures = -1, not initialWeights. I'm not clear how you are reproducing this? what are the input and initialWeights? those seem quite relevant.","24/Mar/15 11:49;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5167",24/Mar/15 12:07;yanboliang;[~srowen] I have address this issue at github. I guess you have seen it.,25/Mar/15 17:07;srowen;Resolved by https://github.com/apache/spark/pull/5167,,,,,,,,,,,,,,,,,,,,
Error when calling Pyspark RandomForestModel.load,SPARK-6457,12784676,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,josephkb,josephkb,josephkb,22/Mar/15 17:26,24/Apr/15 00:33,15/Aug/18 23:03,23/Mar/15 02:31,1.3.0,,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,MLlib,PySpark,,,,0,,,,,"Reported by [https://github.com/catmonkeylee]:

Summary: PySpark RandomForestModel.load fails in test script.  It appears that the saved model file is empty.

{quote}
When I run the sample code in cluster mode, there is an error.

Traceback (most recent call last):
File ""/data1/s/apps/spark-app/app/sample_rf.py"", line 25, in 
sameModel = RandomForestModel.load(sc, model_path)
File ""/home/s/apps/spark/python/pyspark/mllib/util.py"", line 254, in load
java_model = cls.load_java(sc, path)
File ""/home/s/apps/spark/python/pyspark/mllib/util.py"", line 250, in _load_java
return java_obj.load(sc._jsc.sc(), path)
File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call
File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.mllib.tree.model.RandomForestModel.load.
: java.lang.UnsupportedOperationException: empty collection
at org.apache.spark.rdd.RDD.first(RDD.scala:1191)
at org.apache.spark.mllib.util.Loader$.loadMetadata(modelSaveLoad.scala:125)
at org.apache.spark.mllib.tree.model.RandomForestModel$.load(treeEnsembleModels.scala:65)
at org.apache.spark.mllib.tree.model.RandomForestModel.load(treeEnsembleModels.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{quote}

{quote}
I run the code on a spark cluster , spark version is 1.3.0

The test code:
===================================
from pyspark import SparkContext, SparkConf
from pyspark.mllib.tree import RandomForest, RandomForestModel
from pyspark.mllib.util import MLUtils

conf = SparkConf().setAppName('LocalTest')
sc = SparkContext(conf=conf)
data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')
print data.count()
(trainingData, testData) = data.randomSplit([0.7, 0.3])
model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},
                                     numTrees=3, featureSubsetStrategy=""auto"",
                                     impurity='gini', maxDepth=4, maxBins=32)

# Evaluate model on test instances and compute test error
predictions = model.predict(testData.map(lambda x: x.features))
labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)
testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())
print('Test Error = ' + str(testErr))
print('Learned classification forest model:')
print(model.toDebugString())

# Save and load model
_model_path = ""/home/s/apps/spark-app/data/myModelPath""
model.save(sc, _model_path)
sameModel = RandomForestModel.load(sc, _model_path)
sc.stop()

===================
run command:
spark-submit --master spark://t0.q.net:7077 --executor-memory 1G sample_rf.py

======================
Then I get this error :


Traceback (most recent call last):
  File ""/data1/s/apps/spark-app/app/sample_rf.py"", line 25, in <module>
    sameModel = RandomForestModel.load(sc, _model_path)
  File ""/home/s/apps/spark/python/pyspark/mllib/util.py"", line 254, in load
    java_model = cls._load_java(sc, path)
  File ""/home/s/apps/spark/python/pyspark/mllib/util.py"", line 250, in _load_java
    return java_obj.load(sc._jsc.sc(), path)
  File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
  File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.mllib.tree.model.RandomForestModel.load.
: java.lang.UnsupportedOperationException: empty collection
at org.apache.spark.rdd.RDD.first(RDD.scala:1191)
at org.apache.spark.mllib.util.Loader$.loadMetadata(modelSaveLoad.scala:125)
at org.apache.spark.mllib.tree.model.RandomForestModel$.load(treeEnsembleModels.scala:65)
at org.apache.spark.mllib.tree.model.RandomForestModel.load(treeEnsembleModels.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
at py4j.Gateway.invoke(Gateway.java:259)
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
at py4j.commands.CallCommand.execute(CallCommand.java:79)
at py4j.GatewayConnection.run(GatewayConnection.java:207)
at java.lang.Thread.run(Thread.java:724)
{quote}
",,josephkb,lee.xiaobo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6330,,,,,,0.0,,,,,,,,,,,,,,,,2015-03-23 02:23:16.114,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 02:31:03 UTC 2015,,,,,0|i2779b:,9223372036854775807,,,,,,,,,,,,,"22/Mar/15 17:33;josephkb;I tried to reproduce this error myself using Spark 1.3 RC3, and I could not.

CC: [~mengxr]  Do you know if saveAsParquetFile() blocks?  I wonder if save() had not finished yet before load() was called.","22/Mar/15 17:36;josephkb;To catmonkeylee: Can you try inserting a sleep call between save and load as a quick test?  (That may not be the best long-term fix, but may be good enough to tell us what the bug is.)","23/Mar/15 02:23;lee.xiaobo;Thanks Joseph K. Bradley,   I change the code when save model,

model.save(sc, ""hdfs://t17.q.net:9000/user/webchecker/output/myModelPath"") 

This time I meet another error, So I searched in JIRA and found it's a bug. The issue had been resolved in https://issues.apache.org/jira/browse/SPARK-6330 , So I checkout the master code build and run the same test code, everything is OK.:

==================================================== 
15/03/20 23:27:06 INFO DAGScheduler: Stage 23 (saveAsTextFile at treeEnsembleModels.scala:310) finished in 1.026 s
15/03/20 23:27:06 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool default
15/03/20 23:27:06 INFO DAGScheduler: Job 11 finished: saveAsTextFile at treeEnsembleModels.scala:310, took 1.101135 s
Traceback (most recent call last):
  File ""/data1/s/apps/spark-app/app/test_rf_classify.py"", line 77, in <module>
    model.save(sc, ""hdfs://wdlog17.safe.bjt.qihoo.net:9000/user/webchecker/output/rf0"")
  File ""/home/s/apps/spark/python/pyspark/mllib/util.py"", line 202, in save
    self._java_model.save(sc._jsc.sc(), path)
  File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
  File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o75.save.
: java.lang.IllegalArgumentException: Wrong FS: hdfs://t17.q.net:9000/user/webchecker/output/rf0/data, expected: file:///
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
        at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:463)
        at org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.java:118)
        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.apply(newParquet.scala:252)
        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.apply(newParquet.scala:251)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.immutable.List.foreach(List.scala:318)


",23/Mar/15 02:29;josephkb;[~lee.xiaobo]  Thanks!  I'm glad you figured it out.  I'll close this JIRA.,23/Mar/15 02:31;josephkb;Fixed by [SPARK-6330],,,,,,,,,,,,,,,,,,,
_regression_train_wrapper does not test initialWeights correctly,SPARK-6421,12783421,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lewuathe,josephkb,josephkb,19/Mar/15 23:08,20/Mar/15 21:18,15/Aug/18 23:03,20/Mar/15 21:18,1.3.0,,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,MLlib,PySpark,,,,0,,,,,"There is a bug in this line: [https://github.com/apache/spark/blob/f17d43b033d928dbc46aef8e367aa08902e698ad/python/pyspark/mllib/regression.py#L138]

You can reproduce this bug as follows:
{code}
>>> from numpy import array
>>> a = array([1,2,3])
>>> b = a or [1.0] * 3
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
{code}
",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-03-20 06:30:10.228,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 21:18:45 UTC 2015,,,,,0|i270dj:,9223372036854775807,,,,,,1.3.1,1.4.0,,,,,,"20/Mar/15 06:30;apachespark;User 'Lewuathe' has created a pull request for this issue:
https://github.com/apache/spark/pull/5101","20/Mar/15 21:18;mengxr;Issue resolved by pull request 5101
[https://github.com/apache/spark/pull/5101]",,,,,,,,,,,,,,,,,,,,,,
Broken pipe error when training a RandomForest on a union of two RDDs,SPARK-6362,12782282,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,laskov,laskov,16/Mar/15 17:19,21/Mar/16 22:19,15/Aug/18 23:03,21/Mar/16 22:19,1.2.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,PySpark,,,,0,,,,,"Training a RandomForest classifier on a dataset obtained as a union of two RDDs throws a broken pipe error:

Traceback (most recent call last):
  File ""/home/laskov/code/spark-1.2.1/python/pyspark/daemon.py"", line 162, in manager
    code = worker(sock)
  File ""/home/laskov/code/spark-1.2.1/python/pyspark/daemon.py"", line 64, in worker
    outfile.flush()
IOError: [Errno 32] Broken pipe

Despite an error the job runs to completion. 

The following code reproduces the error:

from pyspark.context import SparkContext
from pyspark.mllib.rand import RandomRDDs
from pyspark.mllib.tree import RandomForest
from pyspark.mllib.linalg import DenseVector
from pyspark.mllib.regression import LabeledPoint
import random

if __name__ == ""__main__"":

    sc = SparkContext(appName=""Union bug test"")

    data1 = RandomRDDs.normalVectorRDD(sc,numRows=10000,numCols=200)
    data1 = data1.map(lambda x: LabeledPoint(random.randint(0,1),\
                                             DenseVector(x)))
    data2 = RandomRDDs.normalVectorRDD(sc,numRows=10000,numCols=200)
    data2 = data2.map(lambda x: LabeledPoint(random.randint(0,1),\
                                            DenseVector(x)))

    training_data = data1.union(data2)
    #training_data = training_data.repartition(2)
    model = RandomForest.trainClassifier(training_data, numClasses=2,
                                         categoricalFeaturesInfo={},
                                         numTrees=50, maxDepth=30)

Interestingly, re-partitioning the data after the union operation rectifies the problem (uncomment the line before training in the code above). 
","Kubuntu 14.04, local driver",josephkb,laskov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-03-16 23:46:57.111,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 21 22:19:11 UTC 2016,,,,,0|i26tif:,9223372036854775807,,,,,,,,,,,,,"16/Mar/15 23:46;josephkb;This may be caused by [SPARK-5973], which is fixed in 1.3.  Does upgrading to 1.3 fix it?",21/Mar/16 22:19;josephkb;I'm going to close this since it appears to be fixed (based on running it locally just now on master).,,,,,,,,,,,,,,,,,,,,,,
Model update propagation during prediction in Streaming Regression,SPARK-6345,12782120,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,freeman-lab,freeman-lab,freeman-lab,16/Mar/15 03:26,03/Apr/15 04:39,15/Aug/18 23:03,03/Apr/15 04:39,,,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,DStreams,MLlib,,,,0,,,,,"During streaming regression analyses (Streaming Linear Regression and Streaming Logistic Regression), model updates based on training data are not being reflected in subsequent calls to predictOn or predictOnValues, despite updates themselves occurring successfully. It may be due to recent changes to model declaration, and I have a working fix prepared to be submitted ASAP (alongside expanded test coverage).

A temporary workaround is to retrieve and use the updated model within a foreachRDD, as in:

{code}
model.trainOn(trainingData)
testingData.foreachRDD{ rdd =>
    val latest = model.latestModel()
    val predictions = rdd.map(lp => latest.predict(lp.features))
    ...print or other side effects...
}
{code}

Or within a transform, as in:

{code}
model.trainOn(trainingData)
val predictions = testingData.transform { rdd =>
      val latest = model.latestModel()
      rdd.map(lp => (lp.label, latest.predict(lp.features)))
}
{code}

Note that this does not affect Streaming KMeans, which works as expected for combinations of training and prediction.",,apachespark,freeman-lab,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-03-16 06:51:58.229,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 04:39:31 UTC 2015,,,,,0|i26sjz:,9223372036854775807,,,,,,1.1.2,1.2.2,1.3.1,1.4.0,,,,"16/Mar/15 06:51;apachespark;User 'freeman-lab' has created a pull request for this issue:
https://github.com/apache/spark/pull/5037","03/Apr/15 04:39;mengxr;Issue resolved by pull request 5037
[https://github.com/apache/spark/pull/5037]",,,,,,,,,,,,,,,,,,,,,,
spark-local dir not getting cleared during ALS,SPARK-6334,12781987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,antonymayi,antonymayi,14/Mar/15 10:22,05/Apr/15 22:44,15/Aug/18 23:03,05/Apr/15 22:44,1.2.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"when running bigger ALS training spark spills loads of temp data into the local-dir (in my case yarn/local/usercache/antony.mayi/appcache/... - running on YARN from cdh 5.3.2) eventually causing all the disks of all nodes running out of space (in my case I have 12TB of available disk capacity before kicking off the ALS but it all gets used (and yarn kills the containers when reaching 90%).

even with all recommended options (configuring checkpointing and forcing GC when possible) it still doesn't get cleared.

here is my (pseudo)code (pyspark):
{code}
sc.setCheckpointDir('/tmp')
training = sc.pickleFile('/tmp/dataset').repartition(768).persist(StorageLevel.MEMORY_AND_DISK)
model = ALS.trainImplicit(training, 50, 15, lambda_=0.1, blocks=-1, alpha=40)
sc._jvm.System.gc()
{code}

the training RDD has about 3.5 billions of items (~60GB on disk). after about 6 hours the ALS will consume all 12TB of disk space in local-dir data and gets killed. my cluster has 192 cores, 1.5TB RAM and for this task I am using 37 executors of 4 cores/28+4GB RAM each.

this is the graph of disk consumption pattern showing the space being all eaten from 7% to 90% during the ALS (90% is when YARN kills the container):
!als-diskusage.png!",,antonymayi,huasanyelao,josephkb,lisendong,mengxr,mlnick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Mar/15 10:25;antonymayi;als-diskusage.png;https://issues.apache.org/jira/secure/attachment/12704582/als-diskusage.png,01/Apr/15 15:11;antonymayi;gc.png;https://issues.apache.org/jira/secure/attachment/12708710/gc.png,,2.0,,,,,,,,,,,,,,,,2015-03-14 10:50:24.85,,false,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 05 22:44:38 UTC 2015,,,,,0|i26rqf:,9223372036854775807,,,,,,,,,,,,,"14/Mar/15 10:50;srowen;Do you have 12TB of disk available to the YARN local dir? or just 12TB in general?
You are setting checkpoint dir to /tmp, but talking about filling up the YARN local dir. Are you sure it's not /tmp that fills up?
What are the files that are filling up the disk, shuffle?
Did you try the ttl settings?","14/Mar/15 11:05;antonymayi;it is 12TB combined across all nodes available to YARN local-dirs.
checkpoint dir is on hdfs (/tmp used in the checkpoint is actually hdfs:///tmp). this is negligible in size. all the heavy volume growing during the ALS is on the local disks (not hdfs) under /diskX/yarn/local/usercache/antony.mayi/appcache/...

I have 4x500GB disks mounted as /diskX on each node. YARN is configured to use these disks for the local dirs:
{code:xml}
  <property>
    <name>yarn.nodemanager.local-dirs</name>
    <value>file:///disk1/yarn/local, file:///disk2/yarn/local, file:///disk3/yarn/local, file:///disk4/yarn/local</value>
  </property>
{code}

the usage on each of the disks before starting ALS is ~7%. During the ALS they all grow roughly same (all nodes across all disks) until the 90% threshold.","14/Mar/15 11:11;antonymayi;bq. What are the files that are filling up the disk, shuffle?
yes, it is all the shuffle data.

bq. Did you try the ttl settings?
do you mean spark.cleaner.ttl? yes, but that leads to loss of data required later and ALS then fails when trying to use it.","14/Mar/15 17:21;srowen;Hm, is this a case where it's necessary to cut the lineage fairly frequently with a persist(), so that older stages can be cleaned up safely? that might be the way forward if you have a long and very large computation.","14/Mar/15 17:29;josephkb;I don't think persist() will eliminate shuffle data; I think checkpoint is necessary to do that.  I agree that checkpointing more frequently seems the way to go.

One side comment: You are using a lot of partitions for this size computing cluster.  I'd recommend using fewer partitions (between # workers and # cores) for ALS.  That may not fix the main issue but may help some.","14/Mar/15 18:18;antonymayi;I had to increase the partitioning up to this level due to permanent OOM issues (GC overhead limit exceeded) - although there was enough RAM globally, the partitions were too big for individual executors (I have 28GB RAM + 4GB for spark.yarn.executor.memoryOverhead per each executor). with 728 partitions I got around the OOM problem.",14/Mar/15 18:22;antonymayi;btw. I see based on the sourcecode checkpointing should be happening every 3 iterations - how comes I don't see any drops in the disk usage at least once every three iterations? it just seems to be growing constantly... which worries me that even more frequent checkpointing wont help...,14/Mar/15 23:04;josephkb;I'm not sure about that.  [~mengxr] ?,"16/Mar/15 18:10;mengxr;https://issues.apache.org/jira/browse/SPARK-5955 is going to solve this issue. [~antonymayi] Could you tell more numbers about your test, e.g., number of users, number of ratings, rank, and number of iterations? Thanks!","17/Mar/15 08:33;antonymayi;users: 12.5 millions
ratings: 3.3 billions
rank: 50
iters: 15","17/Mar/15 20:33;mengxr;Couple suggestions before SPARK-5955 is implemented:

1. Upgrade to Spark 1.3. ALS receives a new implementation in 1.3, where the shuffle size is reduced.
2. Use less number of blocks, even you have more CPU cores. There is a trade-off between communication and computation. With k = 50, I think the communication still dominates.
3. Minor. Build Spark with -Pnetlib-lgpl to include native BLAS/LAPACK libraries.","20/Mar/15 10:45;antonymayi;bq. 2. Use less number of blocks, even you have more CPU cores. There is a trade-off between communication and computation. With k = 50, I think the communication still dominates.

thx, this has reduced the volume of the dumped shuffle data to 50% so I can complete the job, very helpful!","24/Mar/15 03:28;mengxr;SPARK-5955 was merged. So if you can use the latest master, you can set checkpoint interval to control the shuffle files. I'm closing this issue since there exists a workaround and it is fixed in master.","30/Mar/15 15:24;lisendong;I met the same problem with you, do you find why checkpoint does not take effect on delete the shuffle data in each iteration?","30/Mar/15 15:24;lisendong;I met the same problem with you, do you find why checkpoint does not take effect on delete the shuffle data in each iteration?","01/Apr/15 15:15;antonymayi;bq. btw. I see based on the sourcecode checkpointing should be happening every 3 iterations - how comes I don't see any drops in the disk usage at least once every three iterations? it just seems to be growing constantly... which worries me that even more frequent checkpointing wont help...

ok, I am now sure increasing the checkpointing interval is likely not going to help same as it is not helping now - the disk usage just grows even after 3x iterations. I just tried dirty hack - running parallel thread that forces GC every x minutes and suddenly I can notice the disk space gets cleared upon every three iterations when GC runs.

see this pattern - first run without forcing GC and then another one where there is noticeable disk usage drops every three steps (ALS iterations):
!gc.png!

so really what's needed to get the shuffles cleaned upon checkpointing is forcing GC.

this was my dirty hack:

{code}
from threading import Thread, Event
class GC(Thread):
    def __init__(self, context, period=600):
        Thread.__init__(self)
        self.context = context
        self.period = period
        self.daemon = True
        self.stopped = Event()
    def stop(self):
        self.stopped.set()
    def run(self):
        self.stopped.clear()
        while not self.stopped.is_set():
            self.stopped.wait(self.period)
            self.context._jvm.System.gc()

sc.setCheckpointDir('/tmp')

gc = GC(sc)
gc.start()

training = sc.pickleFile('/tmp/dataset').repartition(768).persist(StorageLevel.MEMORY_AND_DISK)
model = ALS.trainImplicit(training, 50, 15, lambda_=0.1, blocks=-1, alpha=40)

gc.stop()
{code}","05/Apr/15 22:43;mengxr;Right now (1.3.1 & master), calling `System.gc()` periodically is a workaround for this issue. We are discussing adding a utility to clean shuffle files explicitly, either in ALS or in Core, which would be the solution. I created SPARK-6171 for this.",05/Apr/15 22:44;mengxr;I marked this one as duplicated as the solution will be provided by SPARK-6717.,,,,,,
VectorUDT is displayed as `vecto` in dtypes,SPARK-6308,12781675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,MechCoder,mengxr,mengxr,13/Mar/15 01:00,23/Mar/15 20:30,15/Aug/18 23:03,23/Mar/15 20:30,,,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,SQL,,,,0,,,,,VectorUDT should override simpleString instead of relying on the default implementation.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-03-21 19:20:22.07,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 20:30:34 UTC 2015,,,,,0|i26puv:,9223372036854775807,,,,,,1.3.1,1.4.0,,,,,,"21/Mar/15 19:20;apachespark;User 'MechCoder' has created a pull request for this issue:
https://github.com/apache/spark/pull/5118","23/Mar/15 20:30;mengxr;Issue resolved by pull request 5118
[https://github.com/apache/spark/pull/5118]",,,,,,,,,,,,,,,,,,,,,,
Validate indices before constructing a SparseVector,SPARK-6150,12779273,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Won't Fix,,yuu.ishikawa@gmail.com,yuu.ishikawa@gmail.com,04/Mar/15 02:19,04/Mar/15 05:16,15/Aug/18 23:03,04/Mar/15 05:16,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"We can set the invalid indices when constructing a SparseVector. We should validate the indices before the construction.

It should be invalid, but we can make a SparseVector with below. It is not until we access the value with the index that an exception is raised.

{code}
Vectors.sparse(4, Array(100), Array(1.0))
{code}",,apachespark,yuu.ishikawa@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4956,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-03-04 02:46:46.255,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 02:46:46 UTC 2015,,,,,0|i26brr:,9223372036854775807,,,,,,,,,,,,,"04/Mar/15 02:46;apachespark;User 'yu-iskw' has created a pull request for this issue:
https://github.com/apache/spark/pull/4883",,,,,,,,,,,,,,,,,,,,,,,
Upgrade Breeze to 0.11 to fix convergence bug,SPARK-6141,12779152,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dbtsai,dbtsai,dbtsai,03/Mar/15 19:15,06/Mar/15 02:56,15/Aug/18 23:03,04/Mar/15 07:52,,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,"LBFGS and OWLQN in Breeze 0.10 has convergence check bug. This is fixed in 0.11, see the description in Breeze project for detail:

https://github.com/scalanlp/breeze/pull/373#issuecomment-76879760",,apachespark,dbtsai,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-03-03 19:18:44.544,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 07:52:21 UTC 2015,,,,,0|i26b1r:,9223372036854775807,,,,,,1.3.0,,,,,,,"03/Mar/15 19:18;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4879","04/Mar/15 07:52;mengxr;Issue resolved by pull request 4879
[https://github.com/apache/spark/pull/4879]",,,,,,,,,,,,,,,,,,,,,,
Python DataFrame type inference for LabeledPoint gets wrong type,SPARK-6121,12778887,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,josephkb,josephkb,02/Mar/15 22:35,03/Mar/15 01:14,15/Aug/18 23:03,03/Mar/15 01:14,1.3.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,PySpark,SQL,,,0,,,,,"In Pyspark, when an RDD of LabeledPoints is converted to a DataFrame using toDF(), the returned DataFrame has type ""null"" instead of VectorUDT.

To reproduce:
{code}
from pyspark.mllib.util import MLUtils
rdd = MLUtils.loadLibSVMFile(sc, ""data/mllib/sample_libsvm_data.txt"")
df = rdd.toDF()
{code}

Examine rdd and df to see:
{code}
>>> df
DataFrame[features: null, label: double]
{code}
",,apachespark,josephkb,mengxr,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-03-02 22:59:13.741,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 01:14:51 UTC 2015,,,,,0|i269gv:,9223372036854775807,,,,,,1.3.0,,,,,,,"02/Mar/15 22:59;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4858","03/Mar/15 01:14;mengxr;Issue resolved by pull request 4858
[https://github.com/apache/spark/pull/4858]",,,,,,,,,,,,,,,,,,,,,,
DecisionTree.save uses too much Java heap space for default spark shell settings,SPARK-6120,12778884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,02/Mar/15 22:27,03/Mar/15 09:07,15/Aug/18 23:03,03/Mar/15 06:35,1.3.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,"When the Python DecisionTree example in the programming guide is run, it runs out of Java Heap Space:

{code}
scala> model.save(sc, ""myModelPath"")
[Stage 12:>                                                                                                                                        (0 + 8) / 8]15/03/02 14:19:16 ERROR Executor: Exception in task 1.0 in stage 12.0 (TID 22)
java.lang.OutOfMemoryError: Java heap space
	at parquet.bytes.CapacityByteArrayOutputStream.initSlabs(CapacityByteArrayOutputStream.java:65)
	at parquet.bytes.CapacityByteArrayOutputStream.<init>(CapacityByteArrayOutputStream.java:57)
	at parquet.column.values.plain.PlainValuesWriter.<init>(PlainValuesWriter.java:45)
	at parquet.column.values.dictionary.DictionaryValuesWriter.<init>(DictionaryValuesWriter.java:102)
	at parquet.column.values.dictionary.DictionaryValuesWriter$PlainDoubleDictionaryValuesWriter.<init>(DictionaryValuesWriter.java:471)
	at parquet.column.ParquetProperties.getValuesWriter(ParquetProperties.java:111)
	at parquet.column.impl.ColumnWriterImpl.<init>(ColumnWriterImpl.java:74)
	at parquet.column.impl.ColumnWriteStoreImpl.newMemColumn(ColumnWriteStoreImpl.java:68)
	at parquet.column.impl.ColumnWriteStoreImpl.getColumnWriter(ColumnWriteStoreImpl.java:56)
	at parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.<init>(MessageColumnIO.java:178)
	at parquet.io.MessageColumnIO.getRecordWriter(MessageColumnIO.java:369)
	at parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:108)
	at parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:94)
	at parquet.hadoop.ParquetRecordWriter.<init>(ParquetRecordWriter.java:64)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:282)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:252)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$writeShard$1(newParquet.scala:620)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:641)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:641)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

When saving using JSON format instead of Parquet, this works.  It seems to be caused by Parquet requiring a lot of metadata to describe the schema.

I'm labeling this a bug since it should succeed with the default spark-shell settings.  Potential fixes are:
* increasing spark-shell default heap space settings (This is probably too hard to agree on currently.)
* not using Parquet for storage (This would be good for small examples but probably worse for large models, where Parquet would be more efficient than other formats.)
* compressing the schema (The various values in the DecisionTree model could be flattened into a single Seq of Double.  This may be the best option for now.)

Notes:
* This happens in both pyspark and Scala shells.
* Increasing driver memory to 1g (from the default of 512m) makes this succeed.
* Running other examples such as NaiveBayes with the default of 512m works.
* This is a bit strange in that the actual size of the saved model on disk is small (86K on disk for me).
",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-03-03 02:09:13.868,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 06:35:37 UTC 2015,,,,,0|i269gf:,9223372036854775807,,,,,,1.3.0,,,,,,,"03/Mar/15 01:20;josephkb;I checked, and this only happens in save(), not in load().

Increasing a tiny bit to 700m works.","03/Mar/15 01:22;josephkb;This also affects tree ensembles, of course.  (verified)

700m works for ensembles too, but 512m does not.","03/Mar/15 01:26;josephkb;Rather than one of the difficult options above, I'm sending a PR which will simply print a warning if the memory might be too low.","03/Mar/15 02:09;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/4864","03/Mar/15 06:35;mengxr;Issue resolved by pull request 4864
[https://github.com/apache/spark/pull/4864]",,,,,,,,,,,,,,,,,,,
"LogisticRegressionWithLBFGS in PySpark was assigned wrong ""regType"" parameter",SPARK-6080,12778457,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,28/Feb/15 08:34,02/Mar/15 18:17,15/Aug/18 23:03,02/Mar/15 18:17,,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,PySpark,,,,0,,,,,"Currently LogisticRegressionWithLBFGS in python/pyspark/mllib/classification.py will invoke callMLlibFunc with a wrong ""regType"" parameter.",,apachespark,josephkb,mengxr,tgraves,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-28 08:35:13.713,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 18:17:40 UTC 2015,,,,,0|i266vz:,9223372036854775807,,,,,,1.3.0,,,,,,,"28/Feb/15 08:35;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4831","28/Feb/15 08:47;yanboliang;This bug is easy to reproduce. In a PySpark environment, when you call
    {code} model = LogisticRegressionWithSGD.train(data, regType=None)  {code} 
it will return a LogisticRegressionModel which was trained with no regularization.

But when you run 
    {code} model = LogisticRegressionWithLBFGS.train(data, regType=None)  {code}
it will throw an exception
    {code} java.lang.IllegalArgumentException: Invalid value for 'regType' parameter. Can only be initialized using the following string values: ['l1', 'l2', None]. {code}

This is due to when invoke callMLlibFunc of LogisticRegressionWithLBFGS at python/pyspark/mllib/classification.py, the parameter was assigned to ""str(regType)"" which translate None(Python) to ""None""(Java/Scala). The right way should be translate None(Python) to null(Java/Scala). We need to do the same thing as LogisticRegressionWithSGD. ","02/Mar/15 18:17;mengxr;Issue resolved by pull request 4831
[https://github.com/apache/spark/pull/4831]",,,,,,,,,,,,,,,,,,,,,
ALS doc example fails randomly in PythonAccumulatorParam,SPARK-6071,12778363,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Cannot Reproduce,,josephkb,josephkb,27/Feb/15 23:05,03/Jun/15 23:06,15/Aug/18 23:03,03/Jun/15 23:06,1.3.0,,,,,,,,,,,,,,,,,,,,,MLlib,PySpark,,,,0,,,,,"When running the ALS example in [http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html#examples] on branch-1.3, I got a random failure which I have been unable to reproduce.

Specifically, I was running on the branch from this PR [https://github.com/apache/spark/pull/4811] at this commit: [https://github.com/mengxr/spark/commit/06140a48ec5bd55b329e9b7cf658bd3e43be4fe2]

However, that PR should not have affected the bug, so I suspect it is within branch-1.3 itself.

After a clean build, I ran:
{code}
from pyspark.mllib.recommendation import ALS, Rating, MatrixFactorizationModel

# Load and parse the data
data = sc.textFile(""data/mllib/als/test.data"")
ratings = data.map(lambda l: l.split(',')).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))

# Build the recommendation model using Alternating Least Squares
rank = 10
numIterations = 20
model = ALS.train(ratings, rank, numIterations)
{code}

And I got this error:
{code}
>>> model = ALS.train(ratings, rank, numIterations)
15/02/27 14:41:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/02/27 14:41:24 WARN LoadSnappy: Snappy native library not loaded
15/02/27 14:41:26 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
15/02/27 14:41:26 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
15/02/27 14:41:26 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
15/02/27 14:41:26 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
15/02/27 14:41:29 ERROR DAGScheduler: Failed to update accumulators for ResultTask(279, 2)
java.lang.ClassCastException: scala.None$ cannot be cast to java.util.List
	at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:745)
	at org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:82)
	at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:340)
	at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:335)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.Accumulators$.add(Accumulators.scala:335)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:892)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:974)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1398)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1362)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
15/02/27 14:41:29 ERROR DAGScheduler: Failed to update accumulators for ResultTask(279, 4)
java.lang.ClassCastException: scala.None$ cannot be cast to java.util.List
	at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:745)
	at org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:82)
	at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:340)
	at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:335)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.Accumulators$.add(Accumulators.scala:335)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:892)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:974)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1398)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1362)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}

However, re-running the same train() call immediately worked, and I have not yet been able to reproduce the bug.",,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 23:06:11 UTC 2015,,,,,0|i266bj:,9223372036854775807,,,,,,,,,,,,,04/Mar/15 23:33;josephkb;This is probably just a bug in Python accumulators and has nothing to do with ALS,03/Jun/15 23:06;josephkb;I'm going to close this since I haven't heard of this happening since my initial report.,,,,,,,,,,,,,,,,,,,,,,
KMeans Parallel test may fail,SPARK-6068,12778334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Won't Fix,,derrickburns,derrickburns,27/Feb/15 21:48,11/Jan/16 10:16,15/Aug/18 23:03,11/Jan/16 10:16,1.2.1,,,,,,,,,,,,,,,,,,,,,MLlib,Tests,,,,0,clustering,,,,"The test  ""k-means|| initialization in KMeansSuite can fail when the random number generator is truly random.

The test is predicated on the assumption that each round of K-Means || will add at least one new cluster center.  The current implementation of K-Means || adds 2*k cluster centers with high probability.  However, there is no deterministic lower bound on the number of cluster centers added.

Choices are:

1)  change the KMeans || implementation to iterate on selecting points until it has satisfied a lower bound on the number of points chosen.

2) eliminate the test

3) ignore the problem and depend on the random number generator to sample the space in a lucky manner. 

Option (1) is most in keeping with the contract that KMeans || should provide a precise number of cluster centers when possible. ",,derrickburns,josephkb,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-27 22:59:34.043,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 28 23:31:33 UTC 2015,,,,,0|i2665r:,9223372036854775807,,,,,,,,,,,,,"27/Feb/15 22:59;josephkb;This kind of failure seems very unlikely in practice.  For the test suite, it would be reasonable to use a few fixed random seeds as in some other test suites: [https://github.com/apache/spark/blob/master/mllib/src/test/scala/org/apache/spark/mllib/clustering/GaussianMixtureSuite.scala#L40]

If this is really an issue in practice, then (1) seems best.  However, I would vote against it as not worth it for realistic settings, especially since kmeans Parallel is only used for initialization.","28/Feb/15 00:47;derrickburns;Unit tests should never fail, so something must be done.

Changing the test to provide a magic seed value ties the test to the implementation.

Fixing the implementation is trivial.

Sent from my iPhone

","28/Feb/15 01:00;josephkb;Yes, unit tests should not be flaky.

True, fixed seeds are a bit of a hack but have worked pretty well so far.

That would be great if you fixed the implementation to prevent low-likelihood failures.","28/Feb/15 03:29;derrickburns;I've tried providing pull requests to Spark without success. However, you can find my fixes here: 

https://github.com/derrickburns/generalized-kmeans-clustering

Sent from my iPhone

","28/Feb/15 03:41;srowen;Has the test failed or is this theoretical?
Fixing the implementation to guarantee this contract is ideal, if there's no real downside. 

Something that fails once in a blue, blue moon due to random state isn't inherently a problem, so I would not delete the test over it, no. The alternative is usually to always test the same set of random states, with a fixed seed (where that is even possible), which isn't great either. Regular failure makes it an unuseful test though. Hopefully a moot point.

Derrick what PR are you having trouble with -- the big-bang multi-JIRA PR that's been going on for ages? targeted bite-size fixes to existing code here are much easier to get in. I hope you'll offer some changes for some (others) of the many JIRAs you've opened here. A lot look useful.","28/Feb/15 04:07;derrickburns;Not theoretical. The unit test failed for me in my project (branch of Spark 1.1 mllib). I spent 20 minutes stepping through the code before I realized that there was no (new) problem with my implementation of KMeansParallel but instead was a pre-existing problem. I confirmed via visual inspection that the problem still exists in a recent branch of the Spark.  

In this case the fix is simple: during kmeans parallel maintain a count of the number of centers actually added for each run. Terminate the iterations of the loop that add 2k points when both the step limit has been reached AND the minimum number of k points have been made centers. 

With this change, the high probability case will be  unaffected (I.e. no extra iterations) while the low probability case that gets hit in the unit test will be covered as well.

I've abandoned the ""big bang multi-JIRA PR"" that I submitted. It was too much at one time, as you point out. However, that is what the Spark clusterer requires in order to offer the flexibility that I demonstrate in my branch....  I put a lot of time into the re-architecture including many many hours of large scale testing. Perhaps others can benefit from that by forking or using an upcoming release of my branch. 

Sent from my iPhone

","28/Feb/15 04:30;srowen;Is this something for which a PR can easily be created then? it sounds like you're saying you have fixed it in your copy and that bit still resembles the original code here. Or if you'll point me at it I can try to extract the change.

On a broader question, IMHO:

I think you are ultimately creating a fairly different implementation in order to get in your improvements, and it's quite hard to propose a radical change to the implementation here. Especially if it changes the API or behaviors people are using. It's a shame that any library can only reasonably contain one implementation of a thing, but of course, nobody said MLlib is supposed to have everything or every bell and whistle, and we can and should be able to drop in other implementations in a Spark program as we like.

Overhauls are possible at inflection points in a project lifecycle, and there is of course the 'pipelines' API rewrite going on now, note. I don't have any view on how realistic it is to drop in your work as the new implementation there.

Failing that, I wonder if a lot of the improvements you've suggested here require a substantial rewrite, and won't realistically happen on the current impl? For those, I might suggested withdrawing the existing PRs / JIRAs and instead leave one placeholder JIRA summarizing the key features you'd like to see in a future rewrite, and track it as a feature request. It's up to your judgment but I suggest it since you say you abandoned the open PR.

It would still be good to get in any smaller clear-win changes that can be 'back-ported'.","28/Feb/15 08:29;derrickburns;Thanks Sean for your thoughtful comments!

The main requirement that drove my initial effort was to generalize the distance function used to include the provably largest class of distance functions for which the core algorithm works. This is the class of Bregman Divergences. 

Unfortunately, the current Spark implementation uses knowledge of the specific distance function in many places. Reversing that would result in code that is more general, just as efficient, much easier to read, and easier to prove correct. Alas, it would also touch many lines of code.

The other changes that I have made can be easily layered on the base change. They are largely independent. One could make those changes to either code base, (as one such change was recently implemented). 

However, I do not want to invest in supporting a code base that lacks my driving feature need.  Despite that, I report the issues that I find and fix that are shared in both implementations so that others may at least be aware of them. 

I think that my alternative implementation demonstrates that one can introduce my desired features with minimal impact to the user visible API, so this is not an API/backward compatibility issue like the new pipelines architecture. 

I'm happy to maintain a separate implementation and make it publicly available, particularly since my application requires a different distance function. Next week, I plan to release a version, if I can figure out how to do that easily. :)


Sent from my iPhone

","28/Feb/15 16:40;srowen;Yes, it seems like too much change to the existing version. From https://github.com/apache/spark/pull/2634 it seems like there are just some differences of opinion about what's worth doing and how. I think the only way forward would be to propose integration what you've done for the new version in the {{.ml}} package, because it's not clear the existing PR isn't going to proceed.

I'm hoping to just drive a resolution to what is almost one big issue rather than leave it hanging. I'm looking at the ~8 JIRAs for k-means you created:
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20reporter%20%3D%20%22Derrick%20Burns%22%20AND%20resolution%20%3D%20Unresolved

I assume a couple (like this one) are 'back-portable' from your work to the existing impl. Can we zap those and close them with a PR? This would be great and I'd like to help get those quick wins in.

The rest sound like interdependent aspects of one proposal: create a new k-means implementation with different design and properties X / Y / Z, and use it in the new pipelines API. (I can't say whether this would be accepted or not but that's what's on the table). I'd rather coherently collect that rather than have it live in pieces in JIRA, esp. since I'm getting the sense these remaining pieces won't otherwise move forward.","28/Feb/15 23:31;josephkb;[~derrickburns]  I'm sorry about how it can take a long time to get a PR into Spark, but sending small PRs with one PR per JIRA helps a lot.  For a reviewer to say ""LGTM,"" they need to fully understand and be prepared to ""own"" the code, which makes reviewing large patches *much* harder.  I've spent a lot of time breaking my patches into smaller pieces.

Looking over your JIRAs, the changes all sound useful.  It also seems like the most important change for you (supporting general Bregman divergences) could potentially be added in spark.ml or spark.mllib without making breaking changes.  Since there is no distance metric parameter currently, adding one based on a Bregman divergence API should be possible.  However, but it's pretty hard to figure out exactly what changes are needed because of the many issues being addressed in your big k-means PR.  A smaller PR would help a lot.

I hope it will prove worthwhile for you to help get these improvements into MLlib, piece by piece.  I don't think they will all require waiting for the spark.ml API, but if you do want to make major API changes, then this would be time to design the new API for the spark.ml package.
* [SPARK-6001] might require an API change since it would return a model which could not be serialized.  Perhaps it could follow a similar pattern as LDA, which returns a DistributedLDAModel (with info about the training dataset topic distributions), which in turn can be converted into a LocalLDAModel (which stores model parameters locally and drops the training dataset info).",,,,,,,,,,,,,,
DataFrame.collect() doesn't recognize UDTs,SPARK-5996,12777375,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,mengxr,mengxr,25/Feb/15 01:03,25/Feb/15 18:14,15/Aug/18 23:03,25/Feb/15 18:14,1.3.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,SQL,,,,0,,,,,"{code}
import org.apache.spark.mllib.linalg._
case class Test(data: Vector)
val df = sqlContext.createDataFrame(Seq(Test(Vectors.dense(1.0, 2.0))))
df.collect()[0].getAs[Vector](0)
{code}

throws an exception. `collect()` returns `Row` instead of `Vector`.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-25 03:49:39.202,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 25 03:49:39 UTC 2015,,,,,0|i260a7:,9223372036854775807,,,,,,1.3.0,,,,,,,"25/Feb/15 03:49;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4757",,,,,,,,,,,,,,,,,,,,,,,
Factors returned by ALS do not have partitioners associated.,SPARK-5976,12777282,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,24/Feb/15 20:16,26/Feb/15 07:43,15/Aug/18 23:03,26/Feb/15 07:43,1.3.0,,,,,,,,,,,,,,,1.3.0,,,,,,ML,MLlib,,,,0,,,,,"The model trained by ALS requires partitioning information to do quick lookup of a user/item factor for making recommendation on individual requests. In the new implementation, we don't put partitioners with ALS, which would cause performance regression.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-24 21:11:56.397,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 07:43:45 UTC 2015,,,,,0|i25zq7:,9223372036854775807,,,,,,1.3.0,,,,,,,"24/Feb/15 21:11;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4748","26/Feb/15 07:43;mengxr;Issue resolved by pull request 4748
[https://github.com/apache/spark/pull/4748]",,,,,,,,,,,,,,,,,,,,,,
[MLLIB] Python support for Power Iteration Clustering,SPARK-5963,12777024,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,javadba,javadba,24/Feb/15 02:15,24/Feb/15 23:41,15/Aug/18 23:03,24/Feb/15 23:41,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"Add python support for the Power Iteration Clustering feature.  Here is a fragment of the python API as we plan to implement it:

  /**
   * Java stub for Python mllib PowerIterationClustering.run()
   */
  def trainPowerIterationClusteringModel(
      data: JavaRDD[(java.lang.Long, java.lang.Long, java.lang.Double)],
      k: Int,
      maxIterations: Int,
      runs: Int,
      initializationMode: String,
      seed: java.lang.Long): PowerIterationClusteringModel = {
    val picAlg = new PowerIterationClustering()
      .setK(k)
      .setMaxIterations(maxIterations)

    try {
      picAlg.run(data.rdd.persist(StorageLevel.MEMORY_AND_DISK))
    } finally {
      data.rdd.unpersist(blocking = false)
    }
  }
",,javadba,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,9223372036854775807,,,2015-02-24 02:15:12.0,,,,,0|i25yef:,9223372036854775807,,,,,,1.4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make FPGrowth example app take parameters,SPARK-5939,12776637,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jackylk,jackylk,jackylk,21/Feb/15 13:56,23/Feb/15 16:48,15/Aug/18 23:03,23/Feb/15 16:47,1.2.1,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,,,apachespark,jackylk,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-21 14:01:48.244,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 23 16:47:59 UTC 2015,,,,,0|i25w1r:,9223372036854775807,,,,,,1.3.0,,,,,,,"21/Feb/15 14:01;apachespark;User 'jackylk' has created a pull request for this issue:
https://github.com/apache/spark/pull/4714","23/Feb/15 16:47;mengxr;Issue resolved by pull request 4714
[https://github.com/apache/spark/pull/4714]",,,,,,,,,,,,,,,,,,,,,,
Wrap the results returned by PIC and FPGrowth in case classes,SPARK-5900,12776013,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,18/Feb/15 22:43,20/Feb/15 02:06,15/Aug/18 23:03,20/Feb/15 02:06,1.3.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,We return tuples in the current version of PIC and FPGrowth. This is not very Java-friendly because the primitive types are translated into Objects.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-19 17:47:42.46,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 20 02:06:38 UTC 2015,,,,,0|i25san:,9223372036854775807,,,,,,1.3.0,,,,,,,"19/Feb/15 17:47;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4695","20/Feb/15 02:06;mengxr;Issue resolved by pull request 4695
[https://github.com/apache/spark/pull/4695]",,,,,,,,,,,,,,,,,,,,,,
Using first() to get feature size causes performance regression,SPARK-5858,12775535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,17/Feb/15 08:23,17/Feb/15 18:17,15/Aug/18 23:03,17/Feb/15 18:17,1.3.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,PySpark,,,,0,,,,,We call `.first()` to get the feature size. It causes performance regression because first() still runs on the driver.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-17 09:27:21.717,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 18:17:59 UTC 2015,,,,,0|i25phb:,9223372036854775807,,,,,,1.3.0,,,,,,,"17/Feb/15 09:27;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4647","17/Feb/15 18:17;mengxr;Issue resolved by pull request 4647
[https://github.com/apache/spark/pull/4647]",,,,,,,,,,,,,,,,,,,,,,
java.lang.OutOfMemoryError: Java heap space with RandomForest,SPARK-5743,12774245,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Won't Fix,,poiuytrez,poiuytrez,11/Feb/15 16:03,12/Feb/15 13:30,15/Aug/18 23:03,12/Feb/15 13:30,1.2.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"I am running a training a model with a RamdomForest using this code snippet:
model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo={},
                                    numTrees=5, featureSubsetStrategy=""auto"",
                                    impurity='variance', maxDepth=4)


The trainingData RDD looks like :
trainingData.take(2)
[LabeledPoint(0.0, [0.02,0.0,0.05,0.02,0.0,0.05,18.0,123675.0,189.09]), LabeledPoint(0.0, [0.04,0.1,0.1,0.04,0.1,0.1,4.0,631180.0,157.36])]
which seems ok. 

The csv file before loading is 30 MB.

When I run my code snippet, I get this error:
15/02/11 14:31:46 ERROR ActorSystemImpl: Uncaught fatal error from thread [sparkDriver-akka.actor.default-dispatcher-4] shutting down ActorSystem [sparkDriver]
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:2271)
	at java.io.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:178)
	at org.apache.spark.scheduler.Task$.serializeWithDependencies(Task.scala:143)
	at org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:461)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$7$$anonfun$apply$2.apply$mcVI$sp(TaskSchedulerImpl.scala:259)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$7.apply(TaskSchedulerImpl.scala:255)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$7.apply(TaskSchedulerImpl.scala:252)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply(TaskSchedulerImpl.scala:252)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply(TaskSchedulerImpl.scala:252)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:252)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor.makeOffers(CoarseGrainedSchedulerBackend.scala:163)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor$$anonfun$receiveWithLogging$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:127)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)
	at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:53)
	at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:42)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)
	at org.apache.spark.util.ActorLogReceive$$anon$1.applyOrElse(ActorLogReceive.scala:42)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverActor.aroundReceive(CoarseGrainedSchedulerBackend.scala:72)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
15/02/11 14:31:46 INFO DAGScheduler: Job 2 failed: count at DataValidators.scala:38, took 4.793047 s



PS : I am using Spark 1.2.0 with Python on machines of 8 cores and 32 GB of RAM. I have 1 master and 4 worker nodes. I have also tried with up to 30 machines of 16 cores and 104 GB each.",Debian 7 running on Google Compute Engine,poiuytrez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5772,,,,0.0,,,,,,,,,,,,,,,,2015-02-11 16:08:53.967,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 13:30:00 UTC 2015,,,,,0|i25hmf:,9223372036854775807,,,,,,,,,,,,,"11/Feb/15 16:08;srowen;How much memory do executors get though? your machine's memory doesn't matter if you're only giving them the default of 512m. That is, how are  you running Spark? You mean 104GB right?
It could also be you are accidentally serializing your whole data set in the closure too. Depends on what else your code is doing.","11/Feb/15 16:30;poiuytrez;You are right. I meant 104 GB.

I have tried again on my small cluster (4 workers, 8 cores of 32 GB of RAM). The spark.executor.memory	 value is 24179m. It is also displayed on the web UI.
",11/Feb/15 16:32;poiuytrez;I am running spark using spark-submit. ,12/Feb/15 08:54;poiuytrez;Let me know if you need any additional details. I can even provide you the data that were used.,"12/Feb/15 13:30;poiuytrez;Link to the underlying issue that was causing the problem:
https://issues.apache.org/jira/browse/SPARK-5772",,,,,,,,,,,,,,,,,,,
Size exceeds Integer.MAX_VALUE in File Map,SPARK-5739,12774136,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Duplicate,,DjvuLee,DjvuLee,11/Feb/15 09:02,15/Oct/15 19:06,15/Aug/18 23:03,27/Feb/15 12:54,1.1.1,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"I just run the kmeans algorithm using a random generate data,but occurred this problem after some iteration. I try several time, and this problem is reproduced. 

Because the data is random generate, so I guess is there a bug ? Or if random data can lead to such a scenario that the size is bigger than Integer.MAX_VALUE, can we check the size before using the file map?



015-02-11 00:39:36,057 [sparkDriver-akka.actor.default-dispatcher-15] WARN  org.apache.spark.util.SizeEstimator - Failed to check whether UseCompressedOops is set; assuming yes
[error] (run-main-0) java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE
java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE
	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:850)
	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105)
	at org.apache.spark.storage.DiskStore.putIterator(DiskStore.scala:86)
	at org.apache.spark.storage.MemoryStore.putIterator(MemoryStore.scala:140)
	at org.apache.spark.storage.MemoryStore.putIterator(MemoryStore.scala:105)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:747)
	at org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:598)
	at org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:869)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:79)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:68)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
	at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:270)
	at org.apache.spark.mllib.clustering.KMeans.runBreeze(KMeans.scala:143)
	at org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:126)
	at org.apache.spark.mllib.clustering.KMeans$.train(KMeans.scala:338)
	at org.apache.spark.mllib.clustering.KMeans$.train(KMeans.scala:348)
	at KMeansDataGenerator$.main(kmeans.scala:105)
	at KMeansDataGenerator.main(kmeans.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)
	at java.lang.reflect.Method.invoke(Method.java:619)","Spark1.1.1 on a cluster with 12 node. Every node with 128GB RAM, 24 Core. the data is just 40GB, and there is 48 parallel task on a node.",DjvuLee,kgierach,sams,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5928,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-11 09:43:27.086,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 17:54:55 UTC 2015,,,,,0|i25gyn:,9223372036854775807,,,,,,,,,,,,,"11/Feb/15 09:12;DjvuLee;the data is generated by the example KMeansDataGenerator in the Spark, I just change the parameter  for 

val parts = 480
val numPoints = 480 //1500
val k = 10 //args(3).toInt
val d = 10000000//args(4).toInt
val r = 1.0 //args(5).toDouble
val iter = 8","11/Feb/15 09:43;srowen;You are generating 10,000,000-dimensional data, so each vector is about 80MB. The init process will sample about 2k = 20 centers, and those centers are broadcast, so you're at 1.6GB or more, which is already flirting with the max byte array size in the JVM, 2GB, which is what is used under the hood to broadcast the serialized object. if you set runs > 1, it multiplies all of this even further.

That's a size that's not likely to turn up in practice, and I think the implementation assumes it can broadcast these points for speed. So I'd suggest that if you're merely benchmarking, pick a smaller d. Even 1M should be fine.","11/Feb/15 15:50;DjvuLee;Yes, 1M maybe enough for the Kmeans algorithm.

But if we consider other machine learning algorithm, such as  logistic regression, then 10^7 dimension is not such big. LR  in the ad click model in the real maybe common(I ever heard by my friends), so how Spark can deal well with this? 

Maybe the weight parameter in LR is only one, but when the dimension is up to billion, the data can up to GB.","11/Feb/15 15:51;srowen;Yes, but you're talking about extremely sparse vectors in problems like that. Here you've set up a fully dense 10M-dimensional input. That's not typical.","11/Feb/15 15:55;DjvuLee;Got it, thanks very much ! But can I understand that Spark do not support sparse vector very well now ?","11/Feb/15 15:59;srowen;No, it should be able to operate on sparse vectors, but what you generated and loaded was fully dense.","11/Feb/15 16:10;DjvuLee;Ok, Got it, I will look the code for more detail. 

I will close this issue, but add a check for the size is still a good advise?","11/Feb/15 16:14;srowen;It's hard to say because it depends on d, k, runs, and some of the details of the initialization. I am not sure where a warning should kick in. Small vectors + a huge k and # runs would also produce an error.","12/Feb/15 01:48;DjvuLee;Yes, I do not explain cleanly. What I mean is that we can add a check in the getBytes method in the DiskStore.scala file. 
just before call the following function.

Some(channel.map(MapMode.READ_ONLY, segment.offset, segment.length))",12/Feb/15 08:14;srowen;What would that really do though except change one error into another? I think the current exception is quite clear.,"12/Feb/15 09:43;DjvuLee;En, it have little difference. What I consider is that check size can reduce call the channel.map function, rather than step into it and throw a Exception, close this issue is also ok, because the exception hint is clear to find the problem now.",27/Feb/15 12:54;srowen;I think at best this reduces to just hitting the issue that blocks can't be >2GB,"15/Oct/15 17:54;kgierach;Is there anyway to increase this block limit?  I'm hitting the same issue during a UnionRDD operation.

Also, above this issue's state is ""resolved"" but I'm not sure what the resolution is?  Maybe a state of ""closed"" with a reference to the duplicate ticket would make it more clear.
",,,,,,,,,,,
Explore GPU-accelerated Linear Algebra Libraries,SPARK-5705,12773659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Duplicate,,sparks,sparks,10/Feb/15 02:05,10/Feb/15 09:27,15/Aug/18 23:03,10/Feb/15 09:27,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,,,dlyubimov,dusenberrymw,maropu,michaelmalak,prudenko,rezazadeh,sebastian.estevez@datastax.com,shivaram,sparks,,,,,,,,,,,,,,,,,,,,,SPARK-3785,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-10 09:27:10.191,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 10 09:27:10 UTC 2015,,,,,0|i25edr:,9223372036854775807,,,,,,,,,,,,,"10/Feb/15 02:29;sparks;This JIRA is a continuation of this thread: http://apache-spark-developers-list.1001551.n3.nabble.com/Using-CUDA-within-Spark-boosting-linear-algebra-td10481.html

To summarise - high-speed linear algebra operations including, but not limited to, matrix multiplies and solves have the potential to make certain machine learning operations faster on spark. However, we've got to be careful to balance the overheads of copying data/calling out to the GPU with other factors in the design of the system.

Additionally - getting these libraries compiled, linked, built, and configured on a target system is unfortunately not trivial. We should make sure we have a standard process for doing this (perhaps starting with this codebase: http://github.com/shivaram/matrix-bench).

Maybe we should start with some applications where we think GPU acceleration could help? Neural nets is one, LDA is another - others?",10/Feb/15 09:27;srowen;This already had a discussion going; let's merge the two.,,,,,,,,,,,,,,,,,,,,,,
NegativeArraySizeException in EigenValueDecomposition.symmetricEigs for large n and/or large k,SPARK-5656,12773085,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mbittmann,mbittmann,mbittmann,06/Feb/15 19:03,12/Feb/15 04:39,15/Aug/18 23:03,08/Feb/15 10:13,,,,,,,,,,,,,,,,1.4.0,,,,,,MLlib,,,,,0,,,,,"Large values of n or k in EigenValueDecomposition.symmetricEigs will fail with a NegativeArraySizeException. Specifically, this occurs when 2*n*k > Integer.MAX_VALUE. These values are currently unchecked and allow for the array to be initialized to a value greater than Integer.MAX_VALUE. I have written the below 'require' to fail this condition gracefully. I will submit a pull request. 

require(ncv * n.toLong < Integer.MAX_VALUE, ""Product of 2*k*n must be smaller than "" +
      s""Integer.MAX_VALUE. Found required eigenvalues k = $k and matrix dimension n = $n"")


Here is the exception that occurs from computeSVD with large k and/or n: 

Exception in thread ""main"" java.lang.NegativeArraySizeException
	at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:85)
	at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:258)
	at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:190)",,apachespark,mbittmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-06 19:15:52.29,,false,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 08 10:13:42 UTC 2015,,,,,0|i25ax3:,9223372036854775807,,,,,,,,,,,,,"06/Feb/15 19:15;apachespark;User 'mbittmann' has created a pull request for this issue:
https://github.com/apache/spark/pull/4433","08/Feb/15 10:13;srowen;Issue resolved by pull request 4433
[https://github.com/apache/spark/pull/4433]",,,,,,,,,,,,,,,,,,,,,,
PythonMLlibAPI trainGaussianMixture seed should use Java type,SPARK-5609,12772572,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,mengxr,josephkb,josephkb,05/Feb/15 03:10,05/Feb/15 05:43,15/Aug/18 23:03,05/Feb/15 05:43,1.3.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,PySpark,,,,0,,,,,"trainGaussianMixture takes parameter seed of type scala.Long but should take java.lang.Long.
Otherwise, the test for whether seed is null (None in Python) will be ineffective.  See compilation warning:
{code}
[warn] /Users/josephkb/spark/mllib/src/main/scala/org/apache/spark/mllib/api/python/PythonMLLibAPI.scala:304: comparing values of types Long and Null using `!=' will always yield true
[warn]     if (seed != null) gmmAlg.setSeed(seed)
[warn]              ^
{code}
",,josephkb,MeethuMathew,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-05 04:02:32.43,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 05 05:43:24 UTC 2015,,,,,0|i257sn:,9223372036854775807,,,,,,1.3.0,,,,,,,05/Feb/15 03:12;josephkb;Ping [~MeethuMathew] [~mengxr],"05/Feb/15 04:02;MeethuMathew;Please assign the ticket to me. [~josephkb]
",05/Feb/15 04:36;MeethuMathew;I think this is solved with this https://github.com/apache/spark/commit/679228b7f4c865147ac65099ffd6f5aa45e7126d,"05/Feb/15 05:37;josephkb;Oh, thanks!  I must have an outdated master...",05/Feb/15 05:43;josephkb;Fixed by [https://github.com/apache/spark/commit/679228b7f4c865147ac65099ffd6f5aa45e7126d],,,,,,,,,,,,,,,,,,,
Flaky test: Python regression,SPARK-5585,12772270,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,pwendell,pwendell,04/Feb/15 06:23,04/Feb/15 16:54,15/Aug/18 23:03,04/Feb/15 16:54,1.3.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,flaky-test,,,,"Hey [~davies] any chance you can take a look at this? The master build is having random python failures fairly often. Not quite sure what is going on:

{code}
0inputs+128outputs (0major+13320minor)pagefaults 0swaps
Run mllib tests ...
Running test: pyspark/mllib/classification.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.43user 0.12system 0:14.85elapsed 3%CPU (0avgtext+0avgdata 94272maxresident)k
0inputs+280outputs (0major+12627minor)pagefaults 0swaps
Running test: pyspark/mllib/clustering.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.35user 0.11system 0:12.63elapsed 3%CPU (0avgtext+0avgdata 93568maxresident)k
0inputs+88outputs (0major+12532minor)pagefaults 0swaps
Running test: pyspark/mllib/feature.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.28user 0.08system 0:05.73elapsed 6%CPU (0avgtext+0avgdata 93424maxresident)k
0inputs+32outputs (0major+12548minor)pagefaults 0swaps
Running test: pyspark/mllib/linalg.py
0.16user 0.05system 0:00.22elapsed 98%CPU (0avgtext+0avgdata 89888maxresident)k
0inputs+0outputs (0major+8099minor)pagefaults 0swaps
Running test: pyspark/mllib/rand.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.25user 0.08system 0:05.42elapsed 6%CPU (0avgtext+0avgdata 87872maxresident)k
0inputs+0outputs (0major+11849minor)pagefaults 0swaps
Running test: pyspark/mllib/recommendation.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.32user 0.09system 0:11.42elapsed 3%CPU (0avgtext+0avgdata 94256maxresident)k
0inputs+32outputs (0major+11797minor)pagefaults 0swaps
Running test: pyspark/mllib/regression.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.53user 0.17system 0:23.53elapsed 3%CPU (0avgtext+0avgdata 99600maxresident)k
0inputs+48outputs (0major+12402minor)pagefaults 0swaps
Running test: pyspark/mllib/stat/_statistics.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.29user 0.09system 0:08.03elapsed 4%CPU (0avgtext+0avgdata 92656maxresident)k
0inputs+48outputs (0major+12508minor)pagefaults 0swaps
Running test: pyspark/mllib/tree.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.57user 0.16system 0:25.30elapsed 2%CPU (0avgtext+0avgdata 94400maxresident)k
0inputs+144outputs (0major+12600minor)pagefaults 0swaps
Running test: pyspark/mllib/util.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.20user 0.06system 0:08.08elapsed 3%CPU (0avgtext+0avgdata 92768maxresident)k
0inputs+56outputs (0major+12474minor)pagefaults 0swaps
Running test: pyspark/mllib/tests.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
.........F/usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
  VisibleDeprecationWarning)
./usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
  VisibleDeprecationWarning)
/usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
  VisibleDeprecationWarning)
/usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
  VisibleDeprecationWarning)
/usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
  VisibleDeprecationWarning)
./usr/lib64/python2.6/site-packages/numpy/lib/utils.py:95: DeprecationWarning: `dot` is deprecated!
  warnings.warn(depdoc, DeprecationWarning)
.............
======================================================================
FAIL: test_regression (__main__.ListTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""pyspark/mllib/tests.py"", line 289, in test_regression
    self.assertTrue(rf_model.predict(features[0]) <= 0)
AssertionError: False is not true

----------------------------------------------------------------------
Ran 25 tests in 53.798s
{code}

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/1496/console",,apachespark,davies,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-04 07:06:27.924,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 04 16:54:38 UTC 2015,,,,,0|i255xz:,9223372036854775807,,,,,,,,,,,,,"04/Feb/15 07:06;davies;[~pwendell] I can not reproduce it locally, will add a seed for it, test it several times in jenkins.","04/Feb/15 07:13;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4358","04/Feb/15 16:54;mengxr;Issue resolved by pull request 4358
[https://github.com/apache/spark/pull/4358]",,,,,,,,,,,,,,,,,,,,,
Repartitioning DataFrame causes saveAsParquetFile to fail with VectorUDT,SPARK-5532,12771822,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,josephkb,josephkb,02/Feb/15 20:17,31/Mar/15 14:52,15/Aug/18 23:03,24/Feb/15 18:52,1.3.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,SQL,,,,0,,,,,"Deterministic failure:
{code}
import org.apache.spark.mllib.linalg._
import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc)
import sqlContext._
val data = sc.parallelize(Seq((1.0, Vectors.dense(1,2,3)))).toDataFrame(""label"", ""features"")
data.repartition(1).saveAsParquetFile(""blah"")
{code}
If you remove the repartition, then this succeeds.

Here's the stack trace:
{code}
15/02/02 12:10:53 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4, 192.168.1.230): java.lang.ClassCastException: org.apache.spark.mllib.linalg.DenseVector cannot be cast to org.apache.spark.sql.Row
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:186)
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:177)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:166)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:129)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:315)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:332)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:332)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

15/02/02 12:10:54 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 7, 192.168.1.230): java.lang.ClassCastException: org.apache.spark.mllib.linalg.DenseVector cannot be cast to org.apache.spark.sql.Row
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:186)
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:177)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:166)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:129)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:315)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:332)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:332)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1366)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1327)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}
",,apachespark,josephkb,marmbrus,rajao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-24 02:15:15.21,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 14:52:34 UTC 2015,,,,,0|i2539r:,9223372036854775807,,,,,,1.3.0,,,,,,,"24/Feb/15 02:15;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4738","24/Feb/15 18:52;marmbrus;Issue resolved by pull request 4738
[https://github.com/apache/spark/pull/4738]","31/Mar/15 14:52;rajao;I get the same problem with a DataFrame created with sqlContext.createDataFrame. Is this a related issue ? For example with the following code :

object TestDataFrame {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName(""RankingEval"").setMaster(""local[4]"")


    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)

    import sqlContext.implicits._

    val data = sc.parallelize(Seq(LabeledPoint(1, Vectors.zeros(10))))
    val dataDF = data.toDF

    dataDF.printSchema()
    //dataDF.save(""test1.parquet"")

    val dataDF2 = sqlContext.createDataFrame(dataDF.rdd, dataDF.schema)

    dataDF2.printSchema()

    dataDF2.saveAsParquetFile(""test3.parquet"")
  }
}",,,,,,,,,,,,,,,,,,,,,
ActorSystemImpl: Uncaught fatal error from thread [sparkDriver-akka.actor.default-dispatcher-22] shutting down ActorSystem [sparkDriver] java.lang.OutOfMemoryError: Java heap space,SPARK-5516,12771641,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Unresolved,,wuyukai,wuyukai,02/Feb/15 01:53,28/Apr/15 00:22,15/Aug/18 23:03,28/Apr/15 00:22,1.2.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"When we ran the model of Gradient Boosting Tree, it throwed this exception below. The data we used is only 45M. We ran these data on 4 computers that each have 4 cores and 16GB RAM. We set the parameter ""gradientboostedtrees.maxiteration"" 50.

15/02/01 01:39:48 INFO DAGScheduler: Job 965 failed: collectAsMap at DecisionTree.scala:653, took 1.616976 s
Exception in thread ""main"" org.apache.spark.SparkException: Job cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:702)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:701)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:701)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.postStop(DAGScheduler.scala:1428)
	at akka.actor.Actor$class.aroundPostStop(Actor.scala:475)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundPostStop(DAGScheduler.scala:1375)
	at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:210)
	at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)
	at akka.actor.ActorCell.terminate(ActorCell.scala:369)
	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:462)
	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:478)
	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:263)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
15/02/01 01:39:48 ERROR ActorSystemImpl: Uncaught fatal error from thread [sparkDriver-akka.actor.default-dispatcher-22] shutting down ActorSystem [sparkDriver]
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:2271)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)
	at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1876)
	at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1785)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1188)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at scala.collection.immutable.$colon$colon.writeObject(List.scala:379)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at scala.collection.immutable.$colon$colon.writeObject(List.scala:379)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
","centos 6.5   
",mengxr,poiuytrez,wuyukai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-20 23:02:29.217,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 00:22:24 UTC 2015,,,,,0|i25273:,9223372036854775807,,,,,,1.4.0,,,,,,,"20/Feb/15 23:02;mengxr;[~wuyukai] Could you provide all the parameters you used? The most important ones are number of features, maxDepth, and maxBins. Please also remember to set `--driver-memory` to a large number with spark-submit. ",28/Apr/15 00:22;mengxr;Closing this due to no activity.,,,,,,,,,,,,,,,,,,,,,,
"Allow both ""classification"" and ""Classification"" in Algo for trees",SPARK-5496,12771248,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,30/Jan/15 08:41,30/Jan/15 18:08,15/Aug/18 23:03,30/Jan/15 18:08,,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,"We use ""classification"" in tree but ""Classification"" in boosting. We switched to ""classification"" in both cases, but still need to accept ""Classification"" to be backward compatible.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-01-30 08:55:20.198,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 30 18:08:22 UTC 2015,,,,,0|i24ztb:,9223372036854775807,,,,,,1.3.0,,,,,,,"30/Jan/15 08:55;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4287","30/Jan/15 18:08;mengxr;Issue resolved by pull request 4287
[https://github.com/apache/spark/pull/4287]",,,,,,,,,,,,,,,,,,,,,,
KMeans clustering java.lang.NoSuchMethodError: scala.runtime.IntRef.create  (I)Lscala/runtime/IntRef;,SPARK-5489,12771169,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,dvohra,dvohra,29/Jan/15 23:19,30/Jan/15 18:24,15/Aug/18 23:03,30/Jan/15 13:00,1.2.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"The KMeans clustering generates following error, which also seems to be due version mismatch between Scala used for compiling Spark and Scala in Spark 1.2 Maven dependency. 

Exception in thread ""main"" java.lang.NoSuchMethodError: scala.runtime.IntRef.create

(I)Lscala/runtime/IntRef;
	at 

org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:282)
	at 

org.apache.spark.mllib.clustering.KMeans.runAlgorithm(KMeans.scala:155)
	at 

org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:132)
	at 

org.apache.spark.mllib.clustering.KMeans$.train(KMeans.scala:352)
	at 

org.apache.spark.mllib.clustering.KMeans$.train(KMeans.scala:362)
	at 

org.apache.spark.mllib.clustering.KMeans.train(KMeans.scala)
	at 

clusterer.kmeans.KMeansClusterer.main(KMeansClusterer.java:35)

","Spark 1.2 
Maven",dvohra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5483,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-01-30 13:00:28.152,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 30 18:24:27 UTC 2015,,,,,0|i24zbr:,9223372036854775807,,,,,,,,,,,,,"30/Jan/15 00:06;dvohra;Sean,

Made the Scala version the same, but still getting the error.
""For the Scala API, Spark 1.2.0 uses Scala 2.10. ""
http://spark.apache.org/docs/1.2.0/

Made Maven dependencies Scala version also 2.10.
<dependencies>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_2.10</artifactId>
			<version>1.2.0</version>
			<exclusions>
				<exclusion>
					<groupId>org.scala-lang</groupId>
					<artifactId>scala-library</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.scala-lang</groupId>
					<artifactId>scala-compiler</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-mllib_2.11</artifactId>
			<version>1.2.0</version>
			<exclusions>
				<exclusion>
					<groupId>org.scala-lang</groupId>
					<artifactId>scala-library</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.scala-lang</groupId>
					<artifactId>scala-compiler</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-library</artifactId>
			<version>2.10.0</version>
		</dependency>
		
			<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-compiler</artifactId>
			<version>2.10.0</version>
		</dependency>
	
	</dependencies>

thanks,
Deepak","30/Jan/15 00:26;dvohra;Sean,

Some dependency is making use of scala.runtime.IntRef.create, which was introduced in Scala 2.11.  
https://github.com/scala/scala/blob/v2.11.0/src/library/scala/runtime/IntRef.java

Scala 2.10.4, which is included with Spark 1.2, does not include the scala.runtime.IntRef.create method.
https://github.com/scala/scala/blob/v2.10.4/src/library/scala/runtime/IntRef.java

thanks,
Deepak","30/Jan/15 01:44;dvohra;If Scala 2.11.1 is used the scala.Cloneable is not found, which is available in Scala 2.10.4, but not not Scala 2.11.1. ","30/Jan/15 13:00;srowen;Although it seems to be clearly a problem with mixing artifacts for different versions of Scala, this is at least the same problem in SPARK-5483","30/Jan/15 16:34;dvohra;The issue is with the Maven dependency
<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-mllib_2.10</artifactId>
			<version>1.2.0</version>
		</dependency>
 
spark-mllib_2.10 does not include  the org.apache.spark.mllib.* packages, which it should according to 
http://mvnrepository.com/artifact/org.apache.spark/spark-mllib_2.10/1.2.0

Generates error at import statements:
import org.apache.spark.mllib.clustering.KMeans;
import org.apache.spark.mllib.clustering.KMeansModel;
import org.apache.spark.mllib.linalg.Vector;
import org.apache.spark.mllib.linalg.Vectors;

""The import org.apache.spark.mllib cannot be resolved"".

The 2.11 version spark-mllib_2.11 fixes the error but seems to be referring Scala 2.11.","30/Jan/15 16:45;srowen;No, the 2.10 artifact plainly has these classes. Download it ( https://repo1.maven.org/maven2/org/apache/spark/spark-mllib_2.10/1.2.0/spark-mllib_2.10-1.2.0.jar ) and grep for them. ","30/Jan/15 18:21;dvohra;Already did before posting the previous message and the jar does have the classes, but are indicated as not found with the Maven dependency. Gets fixed with MLLib 2.11. The Maven dependency MLlib 2.10 has some issue.","30/Jan/15 18:24;srowen;But that *is* the artifact, which you see has the class. It must be an issue in your classpath, right?",,,,,,,,,,,,,,,,
LocalLAPACK mode in RowMatrix.computeSVD should have much smaller upper bound,SPARK-5406,12770019,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,26/Jan/15 07:25,02/Feb/15 04:14,15/Aug/18 23:03,02/Feb/15 03:43,1.2.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,"In RowMatrix.computeSVD, under LocalLAPACK mode, the code would invoke brzSvd. Yet breeze svd for dense matrix has latent constraint. In it's implementation
( https://github.com/scalanlp/breeze/blob/master/math/src/main/scala/breeze/linalg/functions/svd.scala   ):

      val workSize = ( 3
        * scala.math.min(m, n)
        * scala.math.min(m, n)
        + scala.math.max(scala.math.max(m, n), 4 * scala.math.min(m, n)
          * scala.math.min(m, n) + 4 * scala.math.min(m, n))
      )
      val work = new Array[Double](workSize)

as a result, column num must satisfy 7 * n * n + 4 * n < Int.MaxValue
thus, n < 17515.

This jira is only the first step. If possbile, I hope spark can handle matrix computation up to 80K * 80K.
","centos, others should be similar",apachespark,mengxr,yuhaoyan,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-01-26 07:36:47.725,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 02 04:14:57 UTC 2015,,,,,0|i24sdj:,9223372036854775807,,,,,,1.3.0,,,,,,,"26/Jan/15 07:36;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/4200",02/Feb/15 03:43;mengxr;fixed by https://github.com/apache/spark/pull/4200,02/Feb/15 04:14;yuhaoyan;fix and merged. Thanks,,,,,,,,,,,,,,,,,,,,,
Vectors.sqdist return inconsistent result for sparse/dense vectors when the vectors have different lengths,SPARK-5384,12769669,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,23/Jan/15 16:30,27/Jan/15 00:05,15/Aug/18 23:03,26/Jan/15 06:18,1.3.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,"For two vectors of different lengths, Vectors.sqdist would return different result when the vectors are represented as sparse and dense respectively. Sample:   
    val s1 = new SparseVector(4, Array(0,1,2,3), Array(1.0, 2.0, 3.0, 4.0))
    val s2 = new SparseVector(1, Array(0), Array(9.0))
    val d1 = new DenseVector(Array(1.0, 2.0, 3.0, 4.0))
    val d2 = new DenseVector(Array(9.0))
    println(s1 == d1 && s2 == d2)
    println(Vectors.sqdist(s1, s2))
    println(Vectors.sqdist(d1, d2))
result:
     true
     93.0
     64.0

More precisely, for the extra part, Vectors.sqdist would include it for sparse vectors and exclude it for dense vectors. I'll send a PR and we can have more detailed discussion there.
","centos, others should be similar",apachespark,mengxr,yuhaoyan,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-01-23 16:32:23.735,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 26 06:21:42 UTC 2015,,,,,0|i24q9j:,9223372036854775807,,,,,,1.3.0,,,,,,,"23/Jan/15 16:32;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/4183","26/Jan/15 06:18;mengxr;Issue resolved by pull request 4183
[https://github.com/apache/spark/pull/4183]",26/Jan/15 06:21;yuhaoyan;fixed,,,,,,,,,,,,,,,,,,,,,
RowMatrix easily gets int overflow in the memory size warning,SPARK-5282,12768000,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,16/Jan/15 12:22,20/Jan/15 01:29,15/Aug/18 23:03,19/Jan/15 18:10,1.2.0,,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,MLlib,,,,,0,,,,,"The warning in the RowMatrix will easily get int overflow when the cols is larger than 16385.

minor issue.","centos, others should be similar",apachespark,mengxr,yuhaoyan,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-01-16 12:35:40.58,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 20 01:29:01 UTC 2015,,,,,0|i24gdj:,9223372036854775807,,,,,,1.3.0,,,,,,,"16/Jan/15 12:25;yuhaoyan;typical wrong message: Row matrix: 17000 cloumns will require at least -1982967296 bytes of memory!

PR on the way.","16/Jan/15 12:35;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/4069","19/Jan/15 18:10;mengxr;Issue resolved by pull request 4069
[https://github.com/apache/spark/pull/4069]",20/Jan/15 01:29;yuhaoyan;fixed,,,,,,,,,,,,,,,,,,,,
"In some cases ,The value of word's vector representation is too big",SPARK-5261,12767661,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,gq,gq,15/Jan/15 04:59,03/Dec/15 14:02,15/Aug/18 23:03,03/Dec/15 14:02,1.2.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"Get data:
{code:none}
normalize_text() {
  awk '{print tolower($0);}' | sed -e ""s/’/'/g"" -e ""s/′/'/g"" -e ""s/''/ /g"" -e ""s/'/ ' /g"" -e ""s/“/\""/g"" -e ""s/”/\""/g"" \
  -e 's/""/ "" /g' -e 's/\./ \. /g' -e 's/<br \/>/ /g' -e 's/, / , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\!/ \! /g' \
  -e 's/\?/ \? /g' -e 's/\;/ /g' -e 's/\:/ /g' -e 's/-/ - /g' -e 's/=/ /g' -e 's/=/ /g' -e 's/*/ /g' -e 's/|/ /g' \
  -e 's/«/ /g' | tr 0-9 "" ""
}
wget http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2013.en.shuffled.gz
gzip -d news.2013.en.shuffled.gz
normalize_text < news.2013.en.shuffled > data.txt
{code}
{code:none}
import org.apache.spark.mllib.feature.Word2Vec

val text = sc.textFile(""dataPath"").map { t => t.split("" "").toIterable }
val word2Vec = new Word2Vec()
word2Vec.
  setVectorSize(100).
  setSeed(42L).
  setNumIterations(5).
  setNumPartitions(36).
  setMinCount(5)

val model = word2Vec.fit(text)
model.getVectors.map { t => t._2.map(_.abs).sum }.sum / 100 / model.getVectors.size
=> 
res1: Float = 375059.84


val word2Vec = new Word2Vec()
word2Vec.
  setVectorSize(100).
  setSeed(42L).
  setNumIterations(5).
  setNumPartitions(36).
  setMinCount(100)

val model = word2Vec.fit(text)
model.getVectors.map { t => t._2.map(_.abs).sum }.sum / 100 / model.getVectors.size
=> 
res3: Float = 1661285.2 


 val word2Vec = new Word2Vec()
 word2Vec.
    setVectorSize(100).
    setSeed(42L).
    setNumIterations(5).
    setNumPartitions(1)

val model = word2Vec.fit(text)
model.getVectors.map { t => t._2.map(_.abs).sum }.sum / 100 / model.getVectors.size
=> 
 0.13889
{code}",,gq,lewuathe,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4846,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-01-27 00:49:10.852,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 14:02:12 UTC 2015,,,,,0|i24edj:,9223372036854775807,,,,,,,,,,,,,27/Jan/15 00:49;lewuathe;[~gq] Can you provide us data set? I tried with some patterns of num partitions but cannot reproduced it. ,"27/Jan/15 01:41;gq;[~lewuathe]
{code}
normalize_text() {
  awk '{print tolower($0);}' | sed -e ""s/’/'/g"" -e ""s/′/'/g"" -e ""s/''/ /g"" -e ""s/'/ ' /g"" -e ""s/“/\""/g"" -e ""s/”/\""/g"" \
  -e 's/""/ "" /g' -e 's/\./ \. /g' -e 's/<br \/>/ /g' -e 's/, / , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\!/ \! /g' \
  -e 's/\?/ \? /g' -e 's/\;/ /g' -e 's/\:/ /g' -e 's/-/ - /g' -e 's/=/ /g' -e 's/=/ /g' -e 's/*/ /g' -e 's/|/ /g' \
  -e 's/«/ /g' | tr 0-9 "" ""
}
wget http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2013.en.shuffled.gz
gzip -d news.2013.en.shuffled.gz
normalize_text < news.2013.en.shuffled > data.txt
{code}",23/Feb/15 22:41;mengxr;Could you try a larger minCount to reduce the vocabulary size?,05/Apr/15 01:49;gq;[~srowen] SPARK-5261 andSPARK-4846 are not the same problem. This is a  algorithm error.  The resulting vector is incorrect. ,"05/Apr/15 06:30;srowen;I think they both come down to a minCount that is too low. If you're going to reopen, can you please follow up on the request to try that? or provide your data set? I don't think it's actionable if there's no follow-up.","06/Apr/15 10:59;srowen;In the new code you pasted, I don't see a difference between the two runs. Is the point that the result isn't deterministic even with a fixed seed? that it might be sensitive to the order in which it encounters the words?","06/Apr/15 16:26;gq;I'm sorry, the  after  one 's  mincount is 100",03/Dec/15 14:02;srowen;Re-resolving as a duplicate of several subsequent issues,,,,,,,,,,,,,,,,
Use pickle instead of MapConvert and ListConvert in MLlib Python API,SPARK-5223,12767205,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,13/Jan/15 18:21,13/Jan/15 20:51,15/Aug/18 23:03,13/Jan/15 20:50,,,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,MLlib,PySpark,,,,0,,,,,"It will introduce problems if the object in dict/list/tuple can not support by py4j, such as Vector.

Also, pickle may have better performance for larger object (less RPC).

In some cases that the object in dict/list can not be pickled (such as JavaObject), we should still use MapConvert/ListConvert.

discussion: http://apache-spark-developers-list.1001551.n3.nabble.com/Python-to-Java-object-conversion-of-numpy-array-td10065.html",,apachespark,davies,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-01-13 18:33:51.987,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 13 20:50:50 UTC 2015,,,,,0|i24bkv:,9223372036854775807,,,,,,1.2.1,1.3.0,,,,,,"13/Jan/15 18:33;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4023","13/Jan/15 20:50;mengxr;Issue resolved by pull request 4023
[https://github.com/apache/spark/pull/4023]",,,,,,,,,,,,,,,,,,,,,,
Vector.equals  and Vector.hashCode are very inefficient and fail on SparseVectors with large size,SPARK-5186,12766555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuhaoyan,derrickburns,derrickburns,09/Jan/15 22:25,12/Mar/15 08:40,15/Aug/18 23:03,12/Mar/15 08:40,1.1.1,1.2.0,1.2.1,,,,,,,,,,,,,1.2.2,,,,,,MLlib,,,,,0,,,,,"The implementation of Vector.equals and Vector.hashCode are correct but slow for SparseVectors that are truly sparse.
",,apachespark,derrickburns,mengxr,yuhaoyan,,,,,,,,,,,,,,900,900,,0%,900,900,,,,,,,,,,SPARK-6288,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-01-09 22:29:56.562,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 08:40:53 UTC 2015,,,,,0|i247s7:,9223372036854775807,,,,,,1.1.2,1.2.2,,,,,,09/Jan/15 22:29;srowen;It looks like equals and hashCode are based on the result of toArray in all cases. The indices matter of course to the result of toArray. What's an example of two SparseVectors that aren't correct? ,"09/Jan/15 22:36;derrickburns;My mistake!  I mis-read the implementation of toArray!

However, this means that the implementation is NOT incorrect, just poor.
Converting sparse vectors to dense vectors to compare them can be extremely
costly.


","09/Jan/15 22:42;srowen;Agree, this could easily be specialized for much better performance. The indices are sorted anyway, so should just be a matter of comparing size, and the two arrays.","12/Jan/15 05:23;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/3997","15/Jan/15 07:17;derrickburns;The aforementioned pull request does fix part of the problem, however, hashCode is broken, not just inefficient. If one calls hashCode on a SparseVector with large size, one will get an insufficient memory error.  
",16/Jan/15 09:30;yuhaoyan;I just updated the PR with a hashCode fix. Please help review at will.,"20/Jan/15 23:20;mengxr;Issue resolved by pull request 3997
[https://github.com/apache/spark/pull/3997]","11/Mar/15 17:47;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4985",11/Mar/15 17:49;mengxr;Re-opened this issue for branch-1.2.,"12/Mar/15 08:40;mengxr;Issue resolved by pull request 4985
[https://github.com/apache/spark/pull/4985]",,,,,,,,,,,,,,
java.lang.ArrayIndexOutOfBoundsException on trying to train decision tree model,SPARK-5119,12765285,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lewuathe,vvkulkarni,vvkulkarni,07/Jan/15 03:47,27/Jan/15 19:38,15/Aug/18 23:03,27/Jan/15 02:03,1.1.0,1.2.0,,,,,,,,,,,,,,1.3.0,,,,,,ML,MLlib,,,,0,,,,,"First I tried to see if there was a bug raised before with similar trace. I found https://www.mail-archive.com/user@spark.apache.org/msg13708.html but the suggestion to upgarde to latest code bae ( I cloned from master branch) does not fix this issue.

Issue: try to train a decision tree classifier on some data.After training and when it begins colllect, it crashes:

15/01/06 22:28:15 INFO BlockManagerMaster: Updated info of block rdd_52_1
15/01/06 22:28:15 ERROR Executor: Exception in task 1.0 in stage 31.0 (TID 1895)
java.lang.ArrayIndexOutOfBoundsException: -1
        at org.apache.spark.mllib.tree.impurity.GiniAggregator.update(Gini.scala:93)
        at org.apache.spark.mllib.tree.impl.DTStatsAggregator.update(DTStatsAggregator.scala:100)
        at org.apache.spark.mllib.tree.DecisionTree$.orderedBinSeqOp(DecisionTree.scala:419)
        at org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$nodeBinSeqOp$1(DecisionTree.scala:511)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:536
)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:533
)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:109)
        at org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1(DecisionTree.scala:533)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:628)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:628)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)

Minimal code:
 data = MLUtils.loadLibSVMFile(sc, '/scratch1/vivek/datasets/private/a1a').cache()

model = DecisionTree.trainClassifier(data, numClasses=2, categoricalFeaturesInfo={}, maxDepth=5, maxBins=100)

Just download the data from: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a
",Linux ubuntu 14.04,apachespark,lewuathe,mengxr,ngarneau,vvkulkarni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-01-09 00:58:18.616,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 27 19:38:50 UTC 2015,,,,,0|i240o7:,9223372036854775807,,,,,,1.3.0,,,,,,,"09/Jan/15 00:58;lewuathe;I think impurity implemented MLlib cannot keep negative labels. In this case it is -1.
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/tree/impurity/Gini.scala#L93
Should impurity support negative label?","09/Jan/15 10:00;srowen;Yes, the input must contain categories that are nonnegative integers. I think this is a reasonable restriction. Although MLUtils.loadLibSVMFile will convert the 1-based feature numbers to 0-based, it leaves the -1 / 1 target untouched. Simply map your -1 labels to 0.","09/Jan/15 12:40;apachespark;User 'Lewuathe' has created a pull request for this issue:
https://github.com/apache/spark/pull/3975","27/Jan/15 02:03;mengxr;Issue resolved by pull request 3975
[https://github.com/apache/spark/pull/3975]","27/Jan/15 19:38;ngarneau;Hey guys, I am wondering what you think about letting the user control if its feature vectors are 0-based or 1-based. I used to have 0-based vectors for my datasets (worked a lot with scikit-learn) and I saw in the loadLibSVMFile function that you are ""converting"" any vectors to a 0-based...
Thought it would be cool to add a optional parameters or something...
Thanks guys, I'd be glad to give you some help :)",,,,,,,,,,,,,,,,,,,
Vector conversion broken for non-float64 arrays,SPARK-5089,12764938,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,freeman-lab,freeman-lab,freeman-lab,05/Jan/15 17:00,05/Jan/15 21:12,15/Aug/18 23:03,05/Jan/15 21:12,1.2.0,,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,MLlib,PySpark,,,,0,,,,,"Prior to performing many MLlib operations in PySpark (e.g. KMeans), data are automatically converted to {{DenseVectors}}. If the data are numpy arrays with dtype {{float64}} this works. If data are numpy arrays with lower precision (e.g. {{float16}} or {{float32}}), they should be upcast to {{float64}}, but due to a small bug in this line this currently doesn't happen (casting is not inplace). 

{code:none}
if ar.dtype != np.float64:
    ar.astype(np.float64)
{code}
 
Non-float64 values are in turn mangled during SerDe. This can have significant consequences. For example, the following yields confusing and erroneous results:

{code:none}
from numpy import random
from pyspark.mllib.clustering import KMeans
data = sc.parallelize(random.randn(100,10).astype('float32'))
model = KMeans.train(data, k=3)
len(model.centers[0])
>> 5 # should be 10!
{code}

But this works fine:

{code:none}
data = sc.parallelize(random.randn(100,10).astype('float64'))
model = KMeans.train(data, k=3)
len(model.centers[0])
>> 10 # this is correct
{code}

The fix is trivial, I'll submit a PR shortly.",,apachespark,freeman-lab,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-01-05 20:07:13.244,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 05 21:12:04 UTC 2015,,,,,0|i23ykn:,9223372036854775807,,,,,,,,,,,,,"05/Jan/15 20:07;apachespark;User 'freeman-lab' has created a pull request for this issue:
https://github.com/apache/spark/pull/3902","05/Jan/15 21:12;mengxr;Issue resolved by pull request 3902
[https://github.com/apache/spark/pull/3902]",,,,,,,,,,,,,,,,,,,,,,
"""train"" methods in object DecisionTree cannot work when using java reflection",SPARK-4998,12764124,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ljzzju,ljzzju,ljzzju,30/Dec/14 03:44,31/Dec/14 00:28,15/Aug/18 23:03,30/Dec/14 23:56,1.2.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,"When using the Java reflection, the several ""train"" methods defined in ""object DecisionTree"" cannot be identified due to the function with the same name in ""class DecisionTree"".

Since the namesake ""train"" method in class DecisionTree"" has already been a deprecated one, it should be commit out to make the ""train"" methods in ""object DecisionTree"" effective.",,apachespark,ljzzju,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-12-30 13:10:24.598,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 31 00:28:47 UTC 2014,,,,,0|i23u3r:,9223372036854775807,,,,,,,,,,,,,"30/Dec/14 04:45;ljzzju;I have make a PR to solve this issue.
https://github.com/apache/spark/pull/3836","30/Dec/14 13:10;apachespark;User 'ljzzju' has created a pull request for this issue:
https://github.com/apache/spark/pull/3836","30/Dec/14 23:56;mengxr;Issue resolved by pull request 3836
[https://github.com/apache/spark/pull/3836]","31/Dec/14 00:28;ljzzju;problem solved, thanks.",,,,,,,,,,,,,,,,,,,,
"Vector Initialization error when initialize a Sparse Vector by calling Vectors.sparse(size, indices, values)",SPARK-4956,12763679,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Won't Fix,,liaoyuxi,liaoyuxi,24/Dec/14 12:27,04/Mar/15 05:16,15/Aug/18 23:03,23/Feb/15 22:34,1.3.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,patch,,,,"When I initialize a sparse vector by calling the Vectors.sparse(size, indices, values), the vector will be all zeros if the indices is not ordered without giving any error or warning. 
A simple sentence to order the indicies with values can fix this bug",,liaoyuxi,mengxr,,,,,,,,,,,,,,,,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-12-24 12:55:07.783,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 23 22:34:41 UTC 2015,,,,,0|i23rfj:,9223372036854775807,,,,,,,,,,,,,"24/Dec/14 12:55;srowen;Yes, I see the same behavior. Breeze doesn't check the ordering (either) but requires ordered indices. I suggest simply failing if the indices are not ordered. To start, at least.","25/Dec/14 06:38;liaoyuxi;I've start a pull request by ordering the indices before the vector was created.
https://github.com/apache/spark/pull/3791
Also, I agreed to just check the ordering only. At least a error should be reported, otherwise the problem is hard to located in applications.",23/Feb/15 22:34;mengxr;I'm closing this PR per discussion on the PR page.,,,,,,,,,,,,,,,,,,,,,
Inconsistent loss and gradient in LeastSquaresGradient compared with R,SPARK-4907,12762988,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dbtsai,dbtsai,dbtsai,19/Dec/14 20:27,26/Dec/14 07:43,15/Aug/18 23:03,23/Dec/14 00:43,,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,"In most of the academic paper and algorithm implementations, people use L = 1/2n ||A weights-y||^2 instead of L = 1/n ||A weights-y||^2 for least-squared loss. See Eq. (1) in http://web.stanford.edu/~hastie/Papers/glmnet.pdf

Since MLlib uses different convention, this will result different residuals and all the stats properties will be different from GLMNET package in R. The model coefficients will be still the same under this change. ",,apachespark,dbtsai,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-12-19 20:29:43.975,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 26 07:43:27 UTC 2014,,,,,0|i23n9b:,9223372036854775807,,,,,,,,,,,,,"19/Dec/14 20:29;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/3746","22/Dec/14 17:26;srowen;PS I think you will want to update the docs too, for example, at http://spark.apache.org/docs/latest/mllib-linear-methods.html
There may be other places where the loss function formula is mentioned.","23/Dec/14 00:43;mengxr;Issue resolved by pull request 3746
[https://github.com/apache/spark/pull/3746]",26/Dec/14 07:43;dbtsai;[~sowen] It seems that the existing document has 1/2 factor there in the formula. ,,,,,,,,,,,,,,,,,,,,
MLlib SingularValueDecomposition ARPACK IllegalStateException ,SPARK-4900,12762822,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,mbofb,mbofb,19/Dec/14 11:23,04/Apr/15 15:55,15/Aug/18 23:03,04/Apr/15 15:54,1.1.1,1.2.0,1.2.1,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,"java.lang.reflect.InvocationTargetException
        ...
Caused by: java.lang.IllegalStateException: ARPACK returns non-zero info = 3 Please refer ARPACK user guide for error message.
        at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:120)
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:235)
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:171)
		...","Ubuntu 1410, Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)
spark local mode",apachespark,mbofb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-02-06 21:43:46.178,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 04 15:54:51 UTC 2015,,,,,0|i23m8f:,9223372036854775807,,,,,,,,,,,,,"20/Dec/14 11:01;mbofb;this exception occurs for various numbers of rows, columns, and k","06/Feb/15 21:43;srowen;Do you have any more info, like how to reproduce this? what were you computing?","09/Feb/15 20:05;mbofb;put a snapshot test data 1000x1000 matrix to https://dl.dropboxusercontent.com/u/8489998/test_matrix_1.zip

calling: 
			String filename = ""/custompath/27637/test_matrix_1"";
			RDD<Vector> vectorRDD = MLUtils.loadVectors(javaSparkContext.sc(), filename);
			vectorRDD.cache();
			System.out.println(""trtRowRDD.count():\t"" + vectorRDD.count());
			RowMatrix rowMatrix = new RowMatrix(vectorRDD);
			System.out.println(""rowMatrix.numRows():\t"" + rowMatrix.numRows());
			System.out.println(""rowMatrix.numCols():\t"" + rowMatrix.numCols());
			{
				int k = 10;
				boolean computeU = true;
				double rCond = 1.0E-9d;
				SingularValueDecomposition<RowMatrix, Matrix> svd = rowMatrix.computeSVD(k, computeU, rCond);
				RowMatrix u = svd.U();
				RDD<Vector> uRowsRDD = u.rows();
				System.out.println(""uRowsRDD.count():\t"" + uRowsRDD.count());
				Vector s = svd.s();
				System.out.println(""s.size():\t"" + s.size());
				Matrix v = svd.V();
				System.out.println(""v.numRows():\t"" + v.numRows());
				System.out.println(""v.numCols():\t"" + v.numCols());
			}

results in:


maxFeatureSpaceTermNumber:      1000
trtRowRDD.count():      1000
rowMatrix.numRows():    1000
rowMatrix.numCols():    1000
15/02/09 19:56:59 WARN PrimaryRunnerSpark:
java.lang.IllegalStateException: ARPACK returns non-zero info = 3 Please refer ARPACK user guide for error message.
        at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:120)
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:258)
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:190)
        at com.example.processing.spark.SVDProcessing2.createSVD_2(SVDProcessing2.java:184)
        at com.example.processing.spark.RunnerSpark.main(PrimaryRunnerSpark.java:27)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at sbt.Run.invokeMain(Run.scala:67)
        at sbt.Run.run0(Run.scala:61)
        at sbt.Run.sbt$Run$$execute$1(Run.scala:51)
        at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
        at sbt.Logger$$anon$4.apply(Logger.scala:85)
        at sbt.TrapExit$App.run(TrapExit.scala:248)
        at java.lang.Thread.run(Thread.java:745)
15/02/09 19:56:59 INFO TimeCounter: TIMER [com.example.processing.spark.PrimaryRunnerSpark] : 13.0 Seconds
TIMER [com.example.processing.spark.PrimaryRunnerSpark] : 13.0 Seconds
15/02/09 19:56:59 ERROR ContextCleaner: Error cleaning broadcast 20
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1039)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
        at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:107)
        at org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:137)
        at org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:227)
        at org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast(TorrentBroadcastFactory.scala:45)
        at org.apache.spark.broadcast.BroadcastManager.unbroadcast(BroadcastManager.scala:66)
        at org.apache.spark.ContextCleaner.doCleanupBroadcast(ContextCleaner.scala:185)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(
ContextCleaner.scala:147)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(
ContextCleaner.scala:138)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.sc
ala:138)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134
)
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134
)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1550)
        at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:133)
        at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:65)
15/02/09 19:56:59 ERROR Utils: Uncaught exception in thread SparkListenerBus
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:48)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1550)
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)
[success] Total time: 20 s, completed Feb 9, 2015 7:56:59 PM","09/Feb/15 20:11;mbofb;BTW: Matrix pc = rowMatrix.computePrincipalComponents(3); yields also an exception:

breeze.linalg.NotConvergedException:
        at breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:60)
        at breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:32)
        at breeze.generic.UFunc$class.apply(UFunc.scala:48)
        at breeze.linalg.svd$.apply(svd.scala:17)
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponents(RowMatrix.scala:375)
        at com.example.processing.spark.SVDProcessing2.createSVD_2(SVDProcessing2.java:236)
        at com.example.processing.spark.RunnerSpark.main(PrimaryRunnerSpark.java:27)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at sbt.Run.invokeMain(Run.scala:67)
        at sbt.Run.run0(Run.scala:61)
        at sbt.Run.sbt$Run$$execute$1(Run.scala:51)
        at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)
        at sbt.Logger$$anon$4.apply(Logger.scala:85)
        at sbt.TrapExit$App.run(TrapExit.scala:248)
        at java.lang.Thread.run(Thread.java:745)","09/Feb/15 20:19;srowen;So I think there is at least a small problem in the error reporting:

{code}
      info.`val` match {
        case 1 => throw new IllegalStateException(""ARPACK returns non-zero info = "" + info.`val` +
            "" Maximum number of iterations taken. (Refer ARPACK user guide for details)"")
        case 2 => throw new IllegalStateException(""ARPACK returns non-zero info = "" + info.`val` +
            "" No shifts could be applied. Try to increase NCV. "" +
            ""(Refer ARPACK user guide for details)"")
        case _ => throw new IllegalStateException(""ARPACK returns non-zero info = "" + info.`val` +
            "" Please refer ARPACK user guide for error message."")
      }
{code}

Really, what's called case 2 here corresponds to return value 3, which is what you get.

{code}
            =  0: Normal exit.
            =  1: Maximum number of iterations taken.
                  All possible eigenvalues of OP has been found. IPARAM(5)  
                  returns the number of wanted converged Ritz values.
            =  2: No longer an informational error. Deprecated starting
                  with release 2 of ARPACK.
            =  3: No shifts could be applied during a cycle of the 
                  Implicitly restarted Arnoldi iteration. One possibility 
                  is to increase the size of NCV relative to NEV. 
                  See remark 4 below.
{code}

I can fix the error message. Remark 4 that it refers to is:

{code}
    4. At present there is no a-priori analysis to guide the selection
       of NCV relative to NEV.  The only formal requrement is that NCV > NEV.
       However, it is recommended that NCV .ge. 2*NEV.  If many problems of
       the same type are to be solved, one should experiment with increasing
       NCV while keeping NEV fixed for a given test problem.  This will 
       usually decrease the required number of OP*x operations but it
       also increases the work and storage required to maintain the orthogonal
       basis vectors.   The optimal ""cross-over"" with respect to CPU time
       is problem dependent and must be determined empirically.
{code}

So I think that translates to ""k is too big"". Is the matrix low-rank?
In any event this is ultimately breeze code and I'm not sure if there's much that will done in Spark itself.","09/Feb/15 20:26;srowen;I think the other message means what it says; the SVD couldn't finish finding the 3rd principal component. The implementation you're calling for PCA is simple but may not be the most accurate. It's either a function of your input, or that you'd need a different algo or set of settings when calling into Breeze to run PCA.","09/Feb/15 23:15;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4485","10/Feb/15 05:14;mengxr;Issue resolved by pull request 4485
[https://github.com/apache/spark/pull/4485]","10/Feb/15 09:12;srowen;Yeah, that fixes the error at least. I imagine the rest is a function of the input and ARPACK, but if there's evidence that something is wrong in the Spark in between, you can reopen.","27/Feb/15 14:35;mbofb;i traced the problem back due to Double.NaN, Double.POSITIVE_INFINITY or  Double.NEGATIVE_INFINITY values of the java double values. In my opinion the user should care to have not such values in the data, but when an exception is thrown it should give a hint that this is a likely cause. If also other allgorithms in MLLib are not pretecting against failing due to Double.NaN, Double.POSITIVE_INFINITY or  Double.NEGATIVE_INFINITY it should be mentioned in the MLlib doc that the users has to ensure only proper double numbers.","27/Feb/15 14:39;srowen;Hm, I tend to agree that upfront error checking is good, and infinite / NaN values are not valid as input. There's a bit of a question of where you check, and whether it slows things down a lot to be always checking. Would you care to propose error checking for this case? I suppose at least the error message for this ARPACK error could hint at this possible cause.",27/Feb/15 14:47;mbofb;in my opinion there should be no automatic error checking but only hints on exception messages and in general MLlib Vector and Matrix doc. Maybe this should hint to an utility method (to be put somewhere) which would do such an check on vectors and matrix and which can be invoked by someone getting exceptions on his data.,27/Feb/15 15:03;srowen;This is still mostly counting on the user to validate their input. How about augmenting the error message here to suggest this as a cause - would that resolve the proximate issue more completely?,"27/Feb/15 15:07;mbofb;			
I would suggest a printValueStatistics(Vector/Matrix) method e.g. on MLUtils which should be hinted at the SVD exeption message (and on all exception methods from algorithms which are not able to process data with such double values - maybee also the PCA exception ?)
 
which says something like

values of the vector/matrix: 
x 0 values
y values are different from 0
sparsity is z %;

(an enhanced method could report more detailed stats one the distribution which can be costly to obtain but helpfull to grasp what one actully wants to process)

Non finite values (might cause problems on some (all?) algorithms):
m values are Double.NaN 
n values are Double.POSITIVE_INFINITY
o values are Double.NEGATIVE_INFINITY
","27/Feb/15 15:31;srowen;[~mengxr] what do you think on this one? I/we could implement whatever sounds good. We could keep this narrower by just implementing an explicit check on the argument to LAPACK, and also expanding its error message. I hesitate to build out more debugging utilities, even if they sound a bit useful.",04/Apr/15 15:54;srowen;Given lack of follow-up I think this is as resolved as we're going to make it.,,,,,,,,
Fix a bad unittest in LogisticRegressionSuite,SPARK-4887,12762666,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dbtsai,dbtsai,dbtsai,18/Dec/14 19:46,18/Dec/14 21:56,15/Aug/18 23:03,18/Dec/14 21:56,,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,"The original test doesn't make sense since if you step in, the lossSum already NaN, and the coefficients are diverging. That's because the step size is too large for SGD, so it doesn't work. The correct behavior is that you should get smaller coefficients than the one without regularization. Comparing the values using 20000.0 relative error doesn't make sense as well. ",,apachespark,dbtsai,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-12-18 19:48:27.186,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 18 21:56:00 UTC 2014,,,,,0|i23la7:,9223372036854775807,,,,,,,,,,,,,"18/Dec/14 19:48;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/3735","18/Dec/14 21:56;mengxr;Issue resolved by pull request 3735
[https://github.com/apache/spark/pull/3735]",,,,,,,,,,,,,,,,,,,,,,
"When the vocabulary size is large, Word2Vec may yield ""OutOfMemoryError: Requested array size exceeds VM limit""",SPARK-4846,12761684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,josephtang,josephtang,josephtang,15/Dec/14 05:26,29/Jan/16 02:15,15/Aug/18 23:03,30/Jan/15 18:07,1.1.1,1.2.0,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,"Exception in thread ""Driver"" java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:162)
Caused by: java.lang.OutOfMemoryError: Requested array size exceeds VM limit 
    at java.util.Arrays.copyOf(Arrays.java:2271)
    at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)
    at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
    at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)
    at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1870)
    at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1779)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1186)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
    at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)
    at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:73)
    at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:164)
    at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158)
    at org.apache.spark.SparkContext.clean(SparkContext.scala:1242)
    at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:610)
    at org.apache.spark.mllib.feature.Word2Vec$$anonfun$fit$1.apply$mcVI$sp(Word2Vec.scala:291)
    at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
    at org.apache.spark.mllib.feature.Word2Vec.fit(Word2Vec.scala:290)","Use Word2Vec to process a corpus(sized 3.5G) with one partition.
The corpus contains about 300 million words and its vocabulary size is about 10 million.",3cham,apachespark,josephkb,josephtang,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-12-15 05:38:28.297,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 02:15:40 UTC 2016,,,,,0|i23fav:,9223372036854775807,,,,,,1.1.2,1.2.1,1.3.0,,,,,"15/Dec/14 05:38;apachespark;User 'jinntrance' has created a pull request for this issue:
https://github.com/apache/spark/pull/3697","15/Dec/14 14:11;srowen;I think you're just running out of memory on your driver. It fails to have enough memory to copy and serialize two data structures, syn0Global and syn1Global which contain (vocab size * vector length) floats. With a default vector length of 100, and 10M vocab, that's at least 8GB of RAM, and the default for the driver isn't nearly that big.

I think this is just a matter of increasing your driver memory. I imagine you will need 16GB+","15/Dec/14 15:06;josephtang;In the very beginning, either I thought it's an issue of memory shortage in the driver or in the executors . But after I increase the driver's memory to 16G and all the executors' memory to 15G, this issue occurred in the same way.

Actually in the RDD.mapPartitionsWithIndex(higher-order-function), firstly SparkContext.clean() is invoked and then ClosureCleaner.ensureSerializable() tries to serialize the higher-order-function(including the initialized syn0Global and syn1Global) using ByteArrayOutputStream before it is executed. 

Because the ByteArrayOutputStream has a maximum size limit of Int.MaxValue(2^31), which means the higher-order-function can not be larger than 2GB.  So in this case, I'm trying to make syn0Global and syn1Global lazy, avoiding being serialized.","15/Dec/14 15:21;srowen;But being lazy doesn't really change whether it is serialized, right? one way or the other the recipients of the higher-order function have to get the same data. The function does use the data structures; it's not a question of simply keeping something out of the closure that shouldn't be there.

Is the problem that only part of this large data structure should go to each partition?","16/Dec/14 00:24;josephkb;I agree with [~srowen] that the current implementation has to serialize those big data structures, no matter what.  Splitting the big syn0Global and syn1Global data structures across partitions sounds possible, but I would guess that the Word2VecModel itself would then need to be distributed as well since it occupies the same order of memory.  A distributed Word2VecModel sounds like a much bigger PR.

In the meantime, a simpler & faster solution might be nice.  The easiest would be to catch the error and print a warning.  A fancier but better solution might be to automatically minCount as much as necessary (and print a warning about this automatic change).","16/Dec/14 13:51;josephtang;Hi  [~srowen] and [~josephkb].

You are right. I had another test about the 'lazy' and then found my misunderstanding of the object serialization in JVM. The 'lazy' does not work. I'll remove the 'lazy' later. 

[~josephkb]'s good advice about automatic minCount should work, meanwhile it costs less memory.  However, one potential issue may be that in the worst case we have to try many times to find the best minCount when changing the minCount step by step.
Also I think a quick fix can be automatically decreasing the vectorSize, but in this case it has lower accuracy. 

Or else, if a Spark user changes one of minCount and vectorSize, we automatically change the other one.  If  a user change neither, we just auto change vectorSize by default.  How do you think? 

","16/Dec/14 19:50;josephkb;Changing vectorSize sounds too aggressive to me.  I'd vote for either the simple solution (throw a nice error), or an efficient method which chooses minCount automatically.  For the latter, this might work:

1. Try the current method.  Catch errors during the collect() for vocab and during the array allocations.  If there is no error, skip step 2.
2. If there is an error, do 1 pass over the data to collect stats (e.g., a histogram).  Use those stats to choose a reasonable minCount.  Choose the vocab again, etc.
3. After the big array allocations, the algorithm can continue as before.
","23/Dec/14 11:05;josephtang;It sounds accomplishable.

I'll try this and make a PR later if it works pretty well .","30/Dec/14 23:11;mengxr;We merged `setMinCount()` in PR #3693. For this issue, throwing an error and asking users to try a larger minCount or a smaller vectorSize should be sufficient for now.","27/Jan/15 00:29;mengxr;[~josephtang] Are you working on this issue? If not, do you mind me sending a PR that throwing an exception if vectorSize is beyond limit?","27/Jan/15 02:14;josephtang;Sorry about the procrastination. I just thought you meant there is no need to implement a dynamic strategy. I'm still working on it and I'd like to quickly fix this issue.

Regarding your previous comment, should I throw a customized error in Spark or just an OOM besides the hint about minCount and vectorSize? ","27/Jan/15 02:43;josephtang;Hi Xiangrui, here is a problem.

PR #3693 that added the `setMinCount ` was merged to the branch `master`, while my PR #3697 was sent to `branch-1.1`.

Should I better close  PR #3697 and send a new PR based on PR #3693?","27/Jan/15 03:35;josephtang;I've added some code at https://github.com/jinntrance/spark/compare/w2v-fix?diff=split&name=w2v-fix

If it's OK, I would send a new PR to the branch `master`.

BTW, sorry for the horrible readability of the difference because of the space indent.","28/Jan/15 08:45;mengxr;We should throw a RuntimeException before allocating memory. Yes, it is easier to close #3697 and send a new PR to `master`.","28/Jan/15 11:14;apachespark;User 'jinntrance' has created a pull request for this issue:
https://github.com/apache/spark/pull/4247",28/Jan/15 11:23;josephtang;OK. I've added a piece of RuntimeException code and have sent a new PR as below.,"30/Jan/15 18:07;mengxr;Issue resolved by pull request 4247
[https://github.com/apache/spark/pull/4247]","02/Dec/15 07:55;3cham;I have a question regarding this issue: as far as I understand, word2vec.fit(input) will return a Word2VecModel object which will be stored on the driver's memory only? 

This leads to a painful consequence when experimenting on yarn-client mode, my driver (my computer) has limited memory and the object cannot fit into it. How could I improve the situation?

","07/Dec/15 20:56;josephkb;This sounds like a limitation of using yarn-client mode, if the client/driver has limited memory.

Eventually, we could support a distributed representation of the model, but that would make the algorithm slower for many use cases.  It'd be worth adding at some point, though, perhaps with a switching mechanism.  But I don't think it would be high priority unless there were many such problems.

Is this something you can get around by changing modes, or by using a different client computer?","28/Jan/16 13:59;3cham;[~josephkb]: I have changed the mode to yarn-cluster, however it seems that the implementation of word2vec has some problem with memory management. I give you some details about my experiment:

The dataset is only 2.8GB big with about 700K different words and vector length is only 200, so syn0Global and syn1Global should be around 1.2GB. For spark 1.5.1, I contantly receive this exception even with 100GB for driver (-Xmx80G), 120GB for each worker (10 total). I then switched to 1.6.0, it worked with just 8G for driver and 20GB for each worker (what I expected). However, if I increase the vector length to 400, I receive this exception again even with 100GB driver and 120GB worker.

The word2vec model should not be that big. Could you please give me some hint how I could solve this problem?","29/Jan/16 02:15;josephtang;Hi Tung,

As far as I can remember, the data is serialized by ByteArray that has the
length limit Integer.MAX_VALUE, which means ByteArray can only serialize
data less than 2GB.

May this piece of information help.

Joseph


",,,
pyspark.mllib.rand docs not generated correctly,SPARK-4821,12760878,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,10/Dec/14 22:24,12/Jan/15 22:51,15/Aug/18 23:03,17/Dec/14 22:13,1.2.0,,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,Documentation,MLlib,PySpark,,,0,,,,,"spark/python/docs/pyspark.mllib.rst needs to be updated to reflect the change in package names from pyspark.mllib.random to .rand

Otherwise, the Python API docs are empty.",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-12-10 22:36:07.357,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 12 22:51:40 UTC 2015,,,,,0|i23au7:,9223372036854775807,,,,,,1.2.1,1.3.0,,,,,,"10/Dec/14 22:36;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/3669","17/Dec/14 22:13;mengxr;Issue resolved by pull request 3669
[https://github.com/apache/spark/pull/3669]","12/Jan/15 22:51;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4011",,,,,,,,,,,,,,,,,,,,,
OOM when making bins in BinaryClassificationMetrics,SPARK-4547,12757111,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,21/Nov/14 23:17,31/Dec/14 21:37,15/Aug/18 23:03,31/Dec/14 21:37,1.1.0,,,,,,,,,,,,,,,1.3.0,,,,,,MLlib,,,,,0,,,,,"Also following up on http://mail-archives.apache.org/mod_mbox/spark-dev/201411.mbox/%3CCAMAsSdK4s4TNkf3_ecLC6yD-pLpys_PpT3WB7Tp6=yoXUxFpMA@mail.gmail.com%3E -- this one I intend to make a PR for a bit later. The conversation was basically:

{quote}
Recently I was using BinaryClassificationMetrics to build an AUC curve for a classifier over a reasonably large number of points (~12M). The scores were all probabilities, so tended to be almost entirely unique.

The computation does some operations by key, and this ran out of memory. It's something you can solve with more than the default amount of memory, but in this case, it seemed unuseful to create an AUC curve with such fine-grained resolution.

I ended up just binning the scores so there were ~1000 unique values
and then it was fine.
{quote}

and:

{quote}
Yes, if there are many distinct values, we need binning to compute the AUC curve. Usually, the scores are not evenly distribution, we cannot simply truncate the digits. Estimating the quantiles for binning is necessary, similar to RangePartitioner:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/Partitioner.scala#L104

Limiting the number of bins is definitely useful.
{quote}
",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-12-15 16:19:30.253,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 31 21:37:15 UTC 2014,,,,,0|i22oan:,9223372036854775807,,,,,,1.3.0,,,,,,,"15/Dec/14 16:19;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3702","31/Dec/14 21:37;mengxr;Issue resolved by pull request 3702
[https://github.com/apache/spark/pull/3702]",,,,,,,,,,,,,,,,,,,,,,
"GradientDescent get a wrong gradient value according to the gradient formula, which is caused by the miniBatchSize parameter.",SPARK-4530,12756909,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gq,gq,gq,21/Nov/14 06:29,12/Dec/14 12:58,15/Aug/18 23:03,12/Dec/14 12:58,1.0.0,1.1.0,1.2.0,,,,,,,,,,,,,1.2.0,,,,,,MLlib,,,,,0,,,,,"This bug is caused by {{RDD.sample}}
The number of  {{RDD.sample}}  returns is not fixed.",,apachespark,gq,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-11-22 23:31:37.148,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 12 12:58:32 UTC 2014,,,,,0|i22n33:,9223372036854775807,,,,,,1.2.0,,,,,,,22/Nov/14 23:31;srowen;See comments on the PR. I don't think these things rise to the level of 'blocker',25/Nov/14 10:03;mengxr;PR: https://github.com/apache/spark/pull/3399,"25/Nov/14 10:46;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/3399",12/Dec/14 12:58;srowen;This was evidently merged for master and 1.2: https://github.com/apache/spark/commit/f515f9432b05f7e090b651c5536aa706d1cde487,,,,,,,,,,,,,,,,,,,,
RandomForest classification uses wrong threshold,SPARK-4460,12755929,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,josephkb,josephkb,17/Nov/14 21:55,17/Nov/14 23:32,15/Aug/18 23:03,17/Nov/14 23:32,1.2.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"RandomForest was modified to use WeightedEnsembleModel, but it needs to use a different threshold for prediction than GradientBoosting does.

Fix: WeightedEnsembleModel.scala:70 should use threshold 0.0 for boosting and 0.5 for forests.",,josephkb,mengxr,yuu.ishikawa@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 17 23:32:16 UTC 2014,,,,,0|i22h73:,9223372036854775807,,,,,,1.2.0,,,,,,,"17/Nov/14 23:32;josephkb;Realized this was invalid.  Current implementation is fine, except for corner case at 0.5",,,,,,,,,,,,,,,,,,,,,,,
"In some cases, Vectors.fromBreeze get wrong results.",SPARK-4422,12755590,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gq,gq,gq,15/Nov/14 06:20,31/Dec/14 19:37,15/Aug/18 23:03,31/Dec/14 19:36,1.2.0,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,,,,,0,,,,,"{noformat}
import breeze.linalg.{DenseVector => BDV, DenseMatrix => BDM, sum => brzSum} 
var x = BDM.zeros[Double](10, 10)
val v = Vectors.fromBreeze(x(::, 0))
assert(v.size == x.rows)
{noformat}",,apachespark,gq,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-11-15 06:29:31.632,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 31 19:37:06 UTC 2014,,,,,0|i22f5j:,9223372036854775807,,,,,,1.2.0,,,,,,,"15/Nov/14 06:29;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/3281","17/Nov/14 05:32;mengxr;Issue resolved by pull request 3281
[https://github.com/apache/spark/pull/3281]",17/Nov/14 05:34;mengxr;Reopened for branch-1.0 and branch-1.1. Changed the priority to minor because `fromBreeze` is private and we don't use breeze matrix slicing in MLlib.,12/Dec/14 13:02;srowen;I think this should be marked for backport?,19/Dec/14 21:48;mengxr;fromBreeze is private and we don't use breeze vectors in a way that can trigger this bug. Actually it is not really necessary to backport this patch.,27/Dec/14 08:10;pwendell;[~mengxr] so should we just close this and decide not to backport it then?,31/Dec/14 19:37;mengxr;Marked as resolved.,,,,,,,,,,,,,,,,,
MLlib unit tests failed maven test,SPARK-4373,12754909,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,12/Nov/14 23:36,13/Nov/14 02:15,15/Aug/18 23:03,13/Nov/14 02:15,1.2.0,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,,,,,0,,,,,We should make sure there is at most one SparkContext running at any time inside the same JVM. Maven initializes all test classes first and then runs tests. So we cannot initialize sc as a member.,,apachespark,joshrosen,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-11-12 23:41:48.149,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 13 02:15:53 UTC 2014,,,,,0|i22b1z:,9223372036854775807,,,,,,1.2.0,,,,,,,"12/Nov/14 23:41;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3235","13/Nov/14 02:15;joshrosen;Issue resolved by pull request 3235
[https://github.com/apache/spark/pull/3235]",,,,,,,,,,,,,,,,,,,,,,
Make LR and SVM's default parameters consistent in Scala and Python ,SPARK-4372,12754869,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,12/Nov/14 19:57,13/Nov/14 21:55,15/Aug/18 23:03,13/Nov/14 21:54,1.2.0,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,,0,,,,,"The current default regParam is 1.0 and regType is claimed to be none in Python (but actually it is l2), while regParam = 0.0 and regType is L2 in Scala. We should make the default values consistent.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-11-12 20:08:42.755,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 13 21:54:53 UTC 2014,,,,,0|i22atj:,9223372036854775807,,,,,,1.2.0,,,,,,,"12/Nov/14 20:08;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3232","13/Nov/14 21:54;mengxr;Issue resolved by pull request 3232
[https://github.com/apache/spark/pull/3232]",,,,,,,,,,,,,,,,,,,,,,
TreeModel.predict does not work with RDD,SPARK-4369,12754820,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,12/Nov/14 17:58,12/Nov/14 21:57,15/Aug/18 23:03,12/Nov/14 21:57,1.2.0,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,,0,,,,,"{code}
Stack Trace
-------------
Traceback (most recent call last):
  File ""/home/rprabhu/Coding/github/SDNDDoS/classification/DecisionTree.py"",
line 49, in <module>
    predictions = model.predict(parsedData.map(lambda x: x.features))
  File ""/home/rprabhu/Software/spark/python/pyspark/mllib/tree.py"", line 42,
in predict
    return self.call(""predict"", x.map(_convert_to_vector))
  File ""/home/rprabhu/Software/spark/python/pyspark/mllib/common.py"", line
140, in call
    return callJavaFunc(self._sc, getattr(self._java_model, name), *a)
  File ""/home/rprabhu/Software/spark/python/pyspark/mllib/common.py"", line
117, in callJavaFunc
    return _java2py(sc, func(*args))
  File
""/home/rprabhu/Software/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"",
line 538, in __call__
  File
""/home/rprabhu/Software/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"",
line 304, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o39.predict. Trace:
py4j.Py4JException: Method predict([class
org.apache.spark.api.java.JavaRDD]) does not exist
        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333)
        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)
        at py4j.Gateway.invoke(Gateway.java:252)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:207)
        at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,davies,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-11-12 18:15:54.296,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 12 21:57:01 UTC 2014,,,,,0|i22aiv:,9223372036854775807,,,,,,1.2.0,,,,,,,"12/Nov/14 18:15;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3230","12/Nov/14 21:57;mengxr;Issue resolved by pull request 3230
[https://github.com/apache/spark/pull/3230]",,,,,,,,,,,,,,,,,,,,,,
OnlineSummarizer doesn't merge mean correctly,SPARK-4355,12754672,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,12/Nov/14 05:05,09/Mar/15 18:45,15/Aug/18 23:03,09/Mar/15 18:45,1.0.2,1.1.1,1.2.0,,,,,,,,,,,,,1.0.3,1.1.1,1.2.0,,,,MLlib,,,,,0,backport-needed,,,,It happens when the mean on one side is zero. I will send an PR with some code clean-up.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-11-12 06:21:02.845,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 30 23:01:32 UTC 2014,,,,,0|i229m7:,9223372036854775807,,,,,,1.0.3,1.1.1,1.2.0,,,,,"12/Nov/14 06:21;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3220","12/Nov/14 09:50;mengxr;Issue resolved by pull request 3220
[https://github.com/apache/spark/pull/3220]",12/Nov/14 09:53;mengxr;Reopen this issue because we haven't fixed branch-1.1 and branch-1.0 yet.,"13/Nov/14 19:08;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3251","13/Nov/14 23:36;mengxr;Issue resolved by pull request 3251
[https://github.com/apache/spark/pull/3251]",13/Nov/14 23:36;mengxr;branch-1.0 pending,"30/Dec/14 23:01;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3850",,,,,,,,,,,,,,,,,
pyspark.mllib.random conflicts with random module,SPARK-4348,12754594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,11/Nov/14 22:06,13/Jan/15 00:17,15/Aug/18 23:03,13/Nov/14 18:25,1.1.0,1.2.0,,,,,,,,,,,,,,1.1.0,1.2.0,,,,,MLlib,PySpark,,,,0,,,,,"There are conflict in two cases:

1. random module is used by pyspark.mllib.feature, if the first part of sys.path is not '', then the hack in pyspark/__init__.py will fail to fix the conflict.

2. Run tests in mllib/xxx.py, the '' should be popped out before import anything, or it will fail.

The first one is not fully fixed for user, it will introduce problems in some cases, such as:

{code}
>>> import sys
>>> import sys.insert(0, PATH_OF_MODULE)
>>> import pyspark
>>> # use Word2Vec will fail
{code}

I'd like to rename mllib/random.py as random/_random.py, then in mllib/__init.py

{code}
import pyspark.mllib._random as random
{code}


cc [~mengxr] [~dorx]",,apachespark,davies,dorx,joshrosen,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-11-11 22:25:48.106,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 13 00:17:49 UTC 2015,,,,,0|i2295z:,9223372036854775807,,,,,,1.2.0,,,,,,,"11/Nov/14 22:25;dorx;I fully support this. It took a lot of hacking just to override the default random module in Python, and it wasn't clear if the override was the ideal solution.","12/Nov/14 01:49;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/3216","12/Nov/14 02:02;davies;After some experiments, I found it's more harder than expected, it still need some hack to make it work (see the PR), but I think this hack is safer than before:

1. the rand.py module will not overwrite default random module, so it's safe to run the mllib/xxx.py without hacking, also we do not need hack to use random in mllib package.

2. the RandomModuleHook only installed when user try to import 'pyspark.mllib', it also only works for 'pyspark.mllib.random'.

Note: In order to use default random module, we need 'from __future__ import absolute_import' in the caller module, this also need as more. Without this, 'import random' can be translated as 'from pyspark.mllib import random'.  So, there is a bug in master (Word2Vec)","13/Nov/14 18:25;mengxr;Issue resolved by pull request 3216
[https://github.com/apache/spark/pull/3216]","14/Nov/14 03:06;mengxr;Note that after this fix, it is very likely that the bytecode file `random.pyc` still sits under `pyspark/mllib`. We need to remove it manually to prevent ""import random"" taking that file. ","12/Jan/15 22:51;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4011","13/Jan/15 00:17;joshrosen;I've also fixed this in 1.1.2 by backporting the 1.2 patch:

https://github.com/apache/spark/pull/4011",,,,,,,,,,,,,,,,,
MLLib BinaryClassificationMetricComputers try to divide by zero,SPARK-4256,12753182,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,abull,abull,abull,05/Nov/14 21:19,13/Nov/14 06:16,15/Aug/18 23:03,13/Nov/14 06:16,,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,,,,,0,,,,,MLLib BinaryClassificationMetricComputers try to divide by zero in cases where there are no positive or negative examples. Resulting in NaNs in metrics.,,abull,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-11-05 21:19:51.434,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 13 06:16:08 UTC 2014,,,,,0|i220s7:,9223372036854775807,,,,,,,,,,,,,"05/Nov/14 21:19;apachespark;User 'abull' has created a pull request for this issue:
https://github.com/apache/spark/pull/3118","13/Nov/14 06:16;mengxr;Issue resolved by pull request 3118
[https://github.com/apache/spark/pull/3118]",,,,,,,,,,,,,,,,,,,,,,
MovieLensALS example fails from including Params in closure,SPARK-4254,12753146,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,josephkb,josephkb,josephkb,05/Nov/14 19:34,06/Nov/14 03:51,15/Aug/18 23:03,06/Nov/14 03:51,,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,,,,,0,,,,,"MovieLensALS example fails when run.
The problem is that the parameter are included in the closure, but they are not Serializable.",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-11-05 19:45:38.081,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 06 03:51:56 UTC 2014,,,,,0|i220kf:,9223372036854775807,,,,,,1.2.0,,,,,,,"05/Nov/14 19:45;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/3116","06/Nov/14 03:51;mengxr;Issue resolved by pull request 3116
[https://github.com/apache/spark/pull/3116]",,,,,,,,,,,,,,,,,,,,,,
loadLibSVMFile does not handle extra whitespace,SPARK-4130,12751323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jegonzal,jegonzal,jegonzal,29/Oct/14 07:05,30/Oct/14 07:06,15/Aug/18 23:03,30/Oct/14 07:06,,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,,,,,0,,,,,"When testing MLlib on the splice site data (http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#splice-site) the loadSVM.  To reproduce in spark shell:

{code:scala}
import org.apache.spark.mllib.util.MLUtils
val data =  MLUtils.loadLibSVMFile(sc, ""hdfs://ec2-54-200-69-227.us-west-2.compute.amazonaws.com:9000/splice_site.t"")
{code}

generates the error:

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0:73 failed 4 times, most recent failure: Exception failure in TID 335 on host ip-172-31-31-54.us-west-2.compute.internal: java.lang.NumberFormatException: For input string: """"
        java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        java.lang.Integer.parseInt(Integer.java:504)
        java.lang.Integer.parseInt(Integer.java:527)
        scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
        scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
        org.apache.spark.mllib.util.MLUtils$$anonfun$4$$anonfun$5.apply(MLUtils.scala:81)
        org.apache.spark.mllib.util.MLUtils$$anonfun$4$$anonfun$5.apply(MLUtils.scala:79)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
        org.apache.spark.mllib.util.MLUtils$$anonfun$4.apply(MLUtils.scala:79)
        org.apache.spark.mllib.util.MLUtils$$anonfun$4.apply(MLUtils.scala:76)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:107)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,jegonzal,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-29 07:37:08.484,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 30 07:06:35 UTC 2014,,,,,0|i21pin:,9223372036854775807,,,,,,,,,,,,,"29/Oct/14 07:37;apachespark;User 'jegonzal' has created a pull request for this issue:
https://github.com/apache/spark/pull/2996","30/Oct/14 07:06;mengxr;Issue resolved by pull request 2996
[https://github.com/apache/spark/pull/2996]",,,,,,,,,,,,,,,,,,,,,,
Simplify serialization and call API in MLlib Python,SPARK-4124,12751258,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,28/Oct/14 22:34,31/Oct/14 05:25,15/Aug/18 23:03,31/Oct/14 05:25,,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,,0,,,,,"There are much repeated code  to similar things, convert RDD into Java object, convert arguments into java, convert to result rdd/object into python, they could be simplified to share the same code.",,apachespark,davies,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-29 07:26:03.986,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 31 05:25:43 UTC 2014,,,,,0|i21p47:,9223372036854775807,,,,,,,,,,,,,"29/Oct/14 07:26;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2995","31/Oct/14 05:25;mengxr;Issue resolved by pull request 2995
[https://github.com/apache/spark/pull/2995]",,,,,,,,,,,,,,,,,,,,,,
Master build failures after shading commons-math3,SPARK-4121,12751232,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mengxr,mengxr,mengxr,28/Oct/14 20:54,24/Nov/14 19:44,15/Aug/18 23:03,01/Nov/14 22:22,1.2.0,,,,,,,,,,,,,,,1.2.0,,,,,,Build,MLlib,Spark Core,,,0,,,,,"The Spark master Maven build kept failing after we replace colt with commons-math3 and shade the latter:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/

The error message is:

{code}
KMeansClusterSuite:
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark assembly has been built with Hive, including Datanucleus jars on classpath
- task size should be small in both training and prediction *** FAILED ***
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 9, localhost): java.io.InvalidClassException: org.apache.spark.util.random.PoissonSampler; local class incompatible: stream classdesc serialVersionUID = -795011761847245121, local class serialVersionUID = 4249244967777318419
        java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
        java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)
        java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:57)
        org.apache.spark.scheduler.Task.run(Task.scala:56)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:186)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}

This test passed in local sbt build. So the issue should be caused by shading. Maybe there are two versions of commons-math3 (hadoop depends on it), or MLlib doesn't use the shaded version at compile.

[~srowen] Could you take a look? Thanks!",,apachespark,joshrosen,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4022,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-28 21:01:51.588,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 01 22:22:16 UTC 2014,,,,,0|i21oyn:,9223372036854775807,,,,,,1.2.0,,,,,,,"28/Oct/14 21:01;pwendell;[~srowen] - can you help with this? This is likely happening because the PoissonSampler on the driver is using the classpath from Maven (with the unmodified version of PoissonSampler) and the executors are using the version from the assembly jar, which has package relocations of the commons math dependency in the byte code. This is a test that uses ""local-cluster"" mode.

Is there a reason we are doing these relocations in the assembly only? Would it be better to actually shade-and-inline commons-math in both the spark-core and spark-mllib package jars?

Having discrepancies between the assmebly and package jars I'm guessing could lead to problems other than just this test issue. It also means that applications which compile against Spark's dependencies rather than running through the Spark assembly packages won't get the benefit of the shading we've done.","28/Oct/14 21:03;joshrosen;Here's an easy command to reproduce this:

{code}
mvn -DskipTests package
mvn test -DwildcardSuites=org.apache.spark.mllib.clustering.KMeansClusterSuite
{code}","28/Oct/14 21:12;srowen;Yeah I was seeing this locally, but not on the Jenkins test build, so chalked it up to weirdness in my build.
I think the answer may indeed be to do the relocating in core/mllib itself. I'll get on that.","30/Oct/14 18:04;mengxr;[~srowen] Are you working on this? I played with the pom files yesterday. I found the serialVersionUID mismatch was because the `rng` in `PoissonSampler` is compiled to a public API in Java. Even I changed it to `private[this] val`, it is still public in Java. The problem is similar to:

http://www.scala-lang.org/old/node/11418.html

If I declare it as an `AnyRef`, it solved the serialVersionUID problem, then I got ClassNotFound from executors, because commons-math3 is not in the assembly jar nor on worker classpath. To not block the release, I suggest that we do not shade commons-math3 but set its version based on the hadoop profile. I can prepare a PR, but please let us know if you have a better solution.","30/Oct/14 19:52;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3023","30/Oct/14 19:57;srowen;Sorry I have been traveling without internet. When I tried revising the shading last night, tests hung. Probably due to network weirdness. 

My concern was that by shading just 2 modules you have to watch for other modules that accidentally start using Math3. 

I think a less drastic solution like modulating the version with hadoop profile sounds fine in principle. I'd have to look up what version goes with what. I think we are not using any very new methods. 

Sorry about that and please proceed as you see fit though I will have another look tonight. ","30/Oct/14 20:22;mengxr;No, we are not using fancy methods. v3.1.1 is good for us. I just sent out the PR. Please help review. Thanks!",01/Nov/14 22:22;pwendell;Okay I merged this. Let's see how it goes.,,,,,,,,,,,,,,,,
Inconsistent spelling 'MLlib' and 'MLLib',SPARK-4055,12749986,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sarutak,sarutak,sarutak,23/Oct/14 05:24,23/Oct/14 16:20,15/Aug/18 23:03,23/Oct/14 16:20,1.2.0,,,,,,,,,,,,,,,1.2.0,,,,,,Documentation,MLlib,,,,0,,,,,Thare are some inconsistent spellings 'MLlib' and 'MLLib' in some documents and source codes.,,apachespark,mengxr,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-23 05:25:45.243,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 23 16:20:19 UTC 2014,,,,,0|i21hg7:,9223372036854775807,,,,,,1.2.0,,,,,,,"23/Oct/14 05:25;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2903","23/Oct/14 16:20;mengxr;Issue resolved by pull request 2903
[https://github.com/apache/spark/pull/2903]",,,,,,,,,,,,,,,,,,,,,,
PySpark's stat.Statistics is broken,SPARK-4023,12749399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,mengxr,mengxr,21/Oct/14 01:00,21/Oct/14 16:30,15/Aug/18 23:03,21/Oct/14 16:30,1.2.0,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,,0,,,,,"{code}
from pyspark.mllib.stat import Statistics
from pyspark.mllib.random import RandomRDDs
data = RandomRDDs.uniformVectorRDD(sc, 100000, 10, 10)
Statistics.colStats(data)
{code}

throws 

{code}

Py4JJavaError: An error occurred while calling o37.colStats.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)
        net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
        net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
        net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
        net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
        net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
        org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(PythonMLLibAPI.scala:695)
        org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(PythonMLLibAPI.scala:694)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:144)
        scala.collection.AbstractIterator.foldLeft(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:201)
        scala.collection.AbstractIterator.aggregate(Iterator.scala:1157)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$4.apply(RDDFunctions.scala:99)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$4.apply(RDDFunctions.scala:99)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$5.apply(RDDFunctions.scala:100)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$5.apply(RDDFunctions.scala:100)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:599)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        org.apache.spark.scheduler.Task.run(Task.scala:56)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:744)
{code}",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3491,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-21 04:38:27.984,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 21 16:30:01 UTC 2014,,,,,0|i21dtr:,9223372036854775807,,,,,,1.2.0,,,,,,,"21/Oct/14 04:38;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2870","21/Oct/14 16:30;mengxr;Issue resolved by pull request 2870
[https://github.com/apache/spark/pull/2870]",,,,,,,,,,,,,,,,,,,,,,
Replace colt dependency (LGPL) with commons-math,SPARK-4022,12749398,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,srowen,pwendell,pwendell,21/Oct/14 00:56,28/Oct/14 20:57,15/Aug/18 23:03,27/Oct/14 17:53,,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,Spark Core,,,,0,,,,,"The colt library we use is LGPL-licensed:
http://acs.lbl.gov/ACSSoftware/colt/license.html

We need to swap this out for commons-math. It is also a very old library that hasn't been updated since 2004.",,apachespark,bcantoni,dorx,josephkb,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-21 01:03:32.637,,false,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 27 17:53:35 UTC 2014,,,,,0|i21dtj:,9223372036854775807,,,,,,1.2.0,,,,,,,"21/Oct/14 01:03;srowen;Yeah, looks like it is only in examples at least, so I don't know if Colt ever got technically distributed (? I forget whether it goes out with transitive deps). But best to change it. I can try it, since I know Commons Math well, unless someone's already on it.","21/Oct/14 01:10;mengxr;[~srowen] We use its random number generators and the stat.Probability package in core and mllib. It would be great if you can work on this JIRA. There are couple places we need to change:

1) sampling
2) chi-sq tests
3) random forests
4) some unit tests

We also need to shade the commons-math3 jar because hadoop depends on it. Fortunately, commons-math3 doesn't have any run-time deps.","21/Oct/14 01:22;srowen;Ah right there is Jet too, not just Colt.

The LGPL license actually only pertains to a few parts of Colt, in hep.aida.*, which aren't used by Spark.
Another solution is just to make sure these classes never become part of the distribution. Colt and Jet themselves don't appear to be LGPL, in the main.

Of course, if there was a desire to just stop using Colt+Jet anyway, I'm cool with that too.",21/Oct/14 01:26;mengxr;Colt is really old. Even the download link (http://acs.lbl.gov/software/colt/colt-download) is broken. It is good if we switch to an alternative that is active and has no license issues.,"23/Oct/14 16:17;srowen;I have begun work on this. You can see the base change here:

https://github.com/srowen/spark/commits/SPARK-4022
https://github.com/srowen/spark/commit/8246dbd39be7ff162392c59c28dee74a1419e236

There are 4 potential problems, each of which might need some assistance as to how to proceed.

*HypothesisTestSuite failure*

CC [~dorx]

{code}
HypothesisTestSuite:
  ...
- chi squared pearson RDD[LabeledPoint] *** FAILED ***
  org.apache.commons.math3.exception.NotStrictlyPositiveException: shape (0)
  at org.apache.commons.math3.distribution.GammaDistribution.<init>(GammaDistribution.java:168)
  ...
  at org.apache.spark.mllib.stat.test.ChiSqTest$.chiSquaredMatrix(ChiSqTest.scala:241)
  at org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$chiSquaredFeatures$4.apply(ChiSqTest.scala:134)
  at org.apache.spark.mllib.stat.test.ChiSqTest$$anonfun$chiSquaredFeatures$4.apply(ChiSqTest.scala:125)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
{code}

The commons-math3 implementation complains that a chi squared distribution is created with 0 degrees of freedom. It looks like that for col 645 in this test, there is just one feature, 0, and two labels. The contingency table should be at least 2x2 but it's 1x2 only, and that's not valid AFAICT. I spent some time staring at this and don't quite know what to make of fixing it.

*KMeansClusterSuite failure*

CC [~mengxr]

{code}
KMeansClusterSuite:
- task size should be small in both training and prediction *** FAILED ***
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 8, localhost): java.io.InvalidClassException: org.apache.spark.util.random.PoissonSampler; local class incompatible: stream classdesc serialVersionUID = -795011761847245121, local class serialVersionUID = 4249244967777318419
{code}

I understand what it's saying. PoissonSampler did indeed change and its serialized form changed, but, I don't see how two incompatible versions are turning up as a result of one clean build.

*RandomForestSuite failure*

CC [~josephkb]

{code}
RandomForestSuite:
...
- alternating categorical and continuous features with multiclass labels to test indexing *** FAILED ***
  java.lang.AssertionError: assertion failed: validateClassifier calculated accuracy 0.75 but required 1.0.
  at scala.Predef$.assert(Predef.scala:179)
  at org.apache.spark.mllib.tree.RandomForestSuite$.validateClassifier(RandomForestSuite.scala:227)
{code}

My guess on this one is that something is sampled differently as a result of this change, and happens to make the decision forest come out differently on this toy data set, and it happens to get 3/4 instead of 4/4 right now. This may be ignorable, meaning, the test was actually a little too strict and optimistic.

*Less efficient seeded sampling for series of Poisson variables*

CC [~dorx]

Colt had a way to seed the RNG, then generate a one-off sample from a Poisson distribution with mean m. commons-math3 lets you seed an instance of a Poisson distribution with mean m, but then not change that mean. To simulate, it's necessary to recreate a Poisson distribution with each successive mean with a deterministic series of seeds.

See here: https://github.com/srowen/spark/commit/8246dbd39be7ff162392c59c28dee74a1419e236#diff-0544248063499d8688c21f49be0918c8R285

This isn't a problem per se but could be slower. I am not sure if this code can be changed to not require constant reinitialization of the distribution.","23/Oct/14 19:00;josephkb;Hi Sean, Thanks for picking this up!  W.r.t. the RandomForestSuite failure, I agree that the test is likely flaky since it relies on choosing random subsets of the features.  That test actually does not really need to call validateClassifier; the test was to check for a bug in training which caused an exception previously.  It should be fine to change (in that test in RandomForestSuite.scala):
from {code}RandomForestSuite.validateClassifier(model, arr, 1.0){code}
to {code}RandomForestSuite.validateClassifier(model, arr, 0.0){code}
","23/Oct/14 20:20;mengxr;Hi [~srowen],

ChiSqTest:

This is an independence test. Since a constant distribution is independent of any distribution by definition, I think we should return pValue = 1.0 and statistic = 0.0 in that case.

KMeansClusterSuite:

It passed on my local machine. This test requires the assembly jar. Did you rebuild? Btw, breeze also depends on commons-math3. Even we plan to shade the jar, we should try to use the same version.

Poisson sampler:

The workaround won't generate good Poisson sequence because we change seed too frequently. We need two different mean values in stratified sampling. Refactoring the code is independent of this JIRA. So I would recommend the following:

1) maintain a map of mean -> PoissonDistribution in RandomDataGenerator
2) if the request mean is in the map, use the RNG there, otherwise, insert a new RNG into the map (and check the size of the map)
","24/Oct/14 13:48;srowen;[~mengxr] [~josephkb] Great, most of this is resolved now. 

{{KMeansSuite}} still fails for me, yes, after a clean build. I wonder if the problem is that the commons-math3 code is in a different package in the assembly? Before thinking too hard about it, let me open a PR to see what Jenkins makes of it.

I also implemented the different approach to Poisson sampler seeding. It would be good to cap the size of the cache, although, I wonder if that could lead to problems. If a sampler is removed and recreated, it will start generating the same sequence again from the same seed. If it is not seeded, it will be nondeterministic. It looks like {{RandomDataGenerator}} instances are short-lived and applied to a fixed set of mean values, which suggests this won't blow up readily. I admit I just glanced at the usages though.","24/Oct/14 13:50;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2928","27/Oct/14 17:53;mengxr;Issue resolved by pull request 2928
[https://github.com/apache/spark/pull/2928]",,,,,,,,,,,,,,
kryo.KryoException caused by ALS.trainImplicit in pyspark,SPARK-3990,12748895,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,gen,gen,17/Oct/14 14:17,07/Jul/15 23:39,15/Aug/18 23:03,24/Oct/14 07:01,1.1.0,,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,MLlib,PySpark,,,,0,test,,,,"When we tried ALS.trainImplicit() in pyspark environment, it only works for iterations = 1. What is more strange, it is that if we try the same code in Scala, it works very well.(I did several test, by now, in Scala ALS.trainImplicit works)

For example, the following code:
{code:title=test.py|borderStyle=solid}
  from pyspark.mllib.recommendation import *
  r1 = (1, 1, 1.0) 
  r2 = (1, 2, 2.0) 
  r3 = (2, 1, 2.0) 
  ratings = sc.parallelize([r1, r2, r3]) 
  model = ALS.trainImplicit(ratings, 1) 
'''by default iterations = 5 or model = ALS.trainImplicit(ratings, 1, 2)'''
{code}


It will cause the failed stage at count at ALS.scala:314 Info as:
{code:title=error information provided by ganglia}
Job aborted due to stage failure: Task 6 in stage 90.0 failed 4 times, most recent failure: Lost task 6.3 in stage 90.0 (TID 484, ip-172-31-35-238.ec2.internal): com.esotericsoftware.kryo.KryoException: java.lang.ArrayStoreException: scala.collection.mutable.HashSet
Serialization trace:
shouldSend (org.apache.spark.mllib.recommendation.OutLinkBlock)
        com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
        com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
        org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:133)
        org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:137)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
        scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
{code}
In the log of slave which failed the task, it has:

{code:title=error information in the log of slave}
14/10/17 13:20:54 ERROR executor.Executor: Exception in task 6.0 in stage 90.0 (TID 465)
com.esotericsoftware.kryo.KryoException: java.lang.ArrayStoreException: scala.collection.mutable.HashSet
Serialization trace:
shouldSend (org.apache.spark.mllib.recommendation.OutLinkBlock)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:133)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:137)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayStoreException: scala.collection.mutable.HashSet
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:338)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:293)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:648)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:605)
	... 36 more
{code}

","5 slaves cluster(m3.large) in AWS launched by spark-ec2
Linux
Python 2.6.8",apachespark,davies,gen,glenn.strycker@gmail.com,mengxr,tzhang101@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1977,SPARK-2652,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-17 22:58:59.188,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 23:39:11 UTC 2015,,,,,0|i21apj:,9223372036854775807,mengxr,,,,,1.1.1,1.2.0,,,,,,17/Oct/14 14:38;gen;I think this problem is related to https://issues.apache.org/jira/browse/SPARK-1977,"17/Oct/14 22:58;mengxr;In PySpark 1.1, we switched to Kryo serialization by default. However, ALS code requires special registration with Kryo in order to work. The error happens when there is not enough memory and ALS needs to store ratings or in/out blocks to disk. I will work on this issue. For now, the workaround is to use a cluster with enough memory.","18/Oct/14 08:36;gen;Thanks a lot for your reply. Right now, I use scala to realize my algorithm.
However, I don't think the problem is caused by the lack of memory, because my data has just 3 lines and 9 numbers and the slave node of cluster has 5 GB memory(I tried the same code with m3.xlarge this morning). For more details, the stage fails when certain slave tries to take the second task of count() but other slaves can finish the task. Hoping this information can help you (I don't know much about Scala.)

","20/Oct/14 06:07;mengxr;[~gen] Could you try the following and see whether it solves the problem or not:

{code}
bin/pyspark --master local-cluster[2,1,512] --conf 'spark.kryo.registrator=org.apache.spark.examples.mllib.MovieLensALS$ALSRegistrator' --jars lib/spark-examples-1.1.0-hadoop2.4.0.jar
{code}

Please replace master and the filename of the example jar to match yours.","20/Oct/14 10:27;gen;[~mengxr] I tried the code that you provided and it worked. Thanks a lot. 
In fact, I created a topic in the Apache Spark User List and I wanted to put your solution in this topic. It is OK for you?
","20/Oct/14 16:41;mengxr;Sure, please link to this JIRA so we can keep the discussion here. This is a temp fix. We should move `ALSRegistrator` back to `ALS.scala`. So if people want to use it, they don't need the examples jar. Are you interested in fixing it?","20/Oct/14 19:40;gen;Yeah, I will try. 
But I am afraid that I couldn't fix this problem efficiently, as I don't know much about programming.  
Thanks again for your help.
","20/Oct/14 21:54;davies;The default serializer change was introduced by https://issues.apache.org/jira/browse/SPARK-2652?filter=-2, This assume that all the data used in pyspark are array[byte], but this is not correct for MLlib, so I would like to revert it.

cc [~matei]",20/Oct/14 22:59;mengxr;[~davies] Is there a JIRA that we can link?,24/Oct/14 07:01;mengxr;This is fixed by reverting the default SerDe for PySpark in SPARK-2652.,"10/Nov/14 22:22;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/3190","11/Nov/14 06:41;mengxr;In Spark 1.1.1, I put a note on ALS and ask users to use the default serializer if they want to run ALS.","07/Jul/15 23:39;tzhang101@yahoo.com;Hi, I am using spark streaming 1.3.0 and I use scala. I am hitting a similar issue.
As long as I have List in my class definition, this happens here is the stack trace.
Serialization trace:
underlying (scala.collection.convert.Wrappers$JListWrapper)
bandwidth_kbps (com.oncue.rna.realtime.streaming.models.QosErrorEvent)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:648)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:605)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:138)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.avro.generic.GenericData$Array.add(GenericData.java:200)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:109)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:648)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:605)
	... 42 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
15/07/07 22:42:06 INFO spark.SparkContext: Starting job: foreachRDD at QosErrorExtractorJob2.scala:595
15/07/07 22:42:06 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 17.0 (TID 81, ip-10-118-10-71.ec2.internal): org.apache.spark.TaskKilledException
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)",,,,,,,,,,,
NNLS generates incorrect result,SPARK-3987,12748796,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coderxiang,debasish83,debasish83,17/Oct/14 04:28,16/May/15 11:55,15/Aug/18 23:03,16/May/15 11:55,1.1.0,,,,,,,,,,,,,,,1.1.1,1.2.0,,,,,MLlib,,,,,0,,,,,"Hi,

Please see the example gram matrix and linear term:

val P2 = new DoubleMatrix(20, 20, 333907.312770, -60814.043975, 207935.829941, -162881.367739, -43730.396770, 17511.428983, -243340.496449, -225245.957922, 104700.445881, 32430.845099, 336378.693135, -373497.970207, -41147.159621, 53928.060360, -293517.883778, 53105.278068, 0.000000, -85257.781696, 84913.970469, -10584.080103, -60814.043975, 13826.806664, -38032.612640, 33475.833875, 10791.916809, -1040.950810, 48106.552472, 45390.073380, -16310.282190, -2861.455903, -60790.833191, 73109.516544, 9826.614644, -8283.992464, 56991.742991, -6171.366034, 0.000000, 19152.382499, -13218.721710, 2793.734234, 207935.829941, -38032.612640, 129661.677608, -101682.098412, -27401.299347, 10787.713362, -151803.006149, -140563.601672, 65067.935324, 20031.263383, 209521.268600, -232958.054688, -25764.179034, 33507.951918, -183046.845592, 32884.782835, 0.000000, -53315.811196, 52770.762546, -6642.187643, -162881.367739, 33475.833875, -101682.098412, 85094.407608, 25422.850782, -5437.646141, 124197.166330, 116206.265909, -47093.484134, -11420.168521, -163429.436848, 189574.783900, 23447.172314, -24087.375367, 148311.355507, -20848.385466, 0.000000, 46835.814559, -38180.352878, 6415.873901, -43730.396770, 10791.916809, -27401.299347, 25422.850782, 8882.869799, 15.638084, 35933.473986, 34186.371325, -10745.330690, -974.314375, -43537.709621, 54371.010558, 7894.453004, -5408.929644, 42231.381747, -3192.010574, 0.000000, 15058.753110, -8704.757256, 2316.581535, 17511.428983, -1040.950810, 10787.713362, -5437.646141, 15.638084, 2794.949847, -9681.950987, -8258.171646, 7754.358930, 4193.359412, 18052.143842, -15456.096769, -253.356253, 4089.672804, -12524.380088, 5651.579348, 0.000000, -1513.302547, 6296.461898, 152.427321, -243340.496449, 48106.552472, -151803.006149, 124197.166330, 35933.473986, -9681.950987, 182931.600236, 170454.352953, -72361.174145, -19270.461728, -244518.179729, 279551.060579, 33340.452802, -37103.267653, 219025.288975, -33687.141423, 0.000000, 67347.950443, -58673.009647, 8957.800259, -225245.957922, 45390.073380, -140563.601672, 116206.265909, 34186.371325, -8258.171646, 170454.352953, 159322.942894, -66074.960534, -16839.743193, -226173.967766, 260421.044094, 31624.194003, -33839.612565, 203889.695169, -30034.828909, 0.000000, 63525.040745, -53572.741748, 8575.071847, 104700.445881, -16310.282190, 65067.935324, -47093.484134, -10745.330690, 7754.358930, -72361.174145, -66074.960534, 35869.598076, 13378.653317, 106033.647837, -111831.682883, -10455.465743, 18537.392481, -88370.612394, 20344.288488, 0.000000, -22935.482766, 29004.543704, -2409.461759, 32430.845099, -2861.455903, 20031.263383, -11420.168521, -974.314375, 4193.359412, -19270.461728, -16839.743193, 13378.653317, 6802.081898, 33256.395091, -30421.985199, -1296.785870, 7026.518692, -24443.378205, 9221.982599, 0.000000, -4088.076871, 10861.014242, -25.092938, 336378.693135, -60790.833191, 209521.268600, -163429.436848, -43537.709621, 18052.143842, -244518.179729, -226173.967766, 106033.647837, 33256.395091, 339200.268106, -375442.716811, -41027.594509, 54636.778527, -295133.248586, 54177.278365, 0.000000, -85237.666701, 85996.957056, -10503.209968, -373497.970207, 73109.516544, -232958.054688, 189574.783900, 54371.010558, -15456.096769, 279551.060579, 260421.044094, -111831.682883, -30421.985199, -375442.716811, 427793.208465, 50528.074431, -57375.986301, 335203.382015, -52676.385869, 0.000000, 102368.307670, -90679.792485, 13509.390393, -41147.159621, 9826.614644, -25764.179034, 23447.172314, 7894.453004, -253.356253, 33340.452802, 31624.194003, -10455.465743, -1296.785870, -41027.594509, 50528.074431, 7255.977434, -5281.636812, 39298.355527, -3440.450858, 0.000000, 13717.870243, -8471.405582, 2071.812204, 53928.060360, -8283.992464, 33507.951918, -24087.375367, -5408.929644, 4089.672804, -37103.267653, -33839.612565, 18537.392481, 7026.518692, 54636.778527, -57375.986301, -5281.636812, 9735.061160, -45360.674033, 10634.633559, 0.000000, -11652.364691, 15039.566630, -1202.539106, -293517.883778, 56991.742991, -183046.845592, 148311.355507, 42231.381747, -12524.380088, 219025.288975, 203889.695169, -88370.612394, -24443.378205, -295133.248586, 335203.382015, 39298.355527, -45360.674033, 262923.925938, -42012.606885, 0.000000, 79810.919951, -71657.856143, 10464.327491, 53105.278068, -6171.366034, 32884.782835, -20848.385466, -3192.010574, 5651.579348, -33687.141423, -30034.828909, 20344.288488, 9221.982599, 54177.278365, -52676.385869, -3440.450858, 10634.633559, -42012.606885, 13238.686902, 0.000000, -8739.845698, 16511.872845, -530.252003, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 118.430000, 0.000000, 0.000000, 0.000000, -85257.781696, 19152.382499, -53315.811196, 46835.814559, 15058.753110, -1513.302547, 67347.950443, 63525.040745, -22935.482766, -4088.076871, -85237.666701, 102368.307670, 13717.870243, -11652.364691, 79810.919951, -8739.845698, 0.000000, 26878.133950, -18588.407734, 3894.934299, 84913.970469, -13218.721710, 52770.762546, -38180.352878, -8704.757256, 6296.461898, -58673.009647, -53572.741748, 29004.543704, 10861.014242, 85996.957056, -90679.792485, -8471.405582, 15039.566630, -71657.856143, 16511.872845, 0.000000, -18588.407734, 23649.538538, -1951.083671, -10584.080103, 2793.734234, -6642.187643, 6415.873901, 2316.581535, 152.427321, 8957.800259, 8575.071847, -2409.461759, -25.092938, -10503.209968, 13509.390393, 2071.812204, -1202.539106, 10464.327491, -530.252003, 0.000000, 3894.934299, -1951.083671, 738.955915)
    val q2 = new DoubleMatrix(20, 1, 31755.057100, -13047.148129, 20191.244430, -25993.775800, -11963.550172, -4272.425977, -33569.856044, -33451.387021, 2320.764250, -5333.136834, 30633.764272, -49513.939049, -10351.230305, 872.276714, -37634.078430, -4628.338543, -0.000000, -18109.093788, 1856.725521, -3397.693211)

NNLS result:

0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0

PR result: https://github.com/apache/spark/pull/2705

QuadraticMinimizer result [0.130104; 0.126840; 0.072565; 0.154153; 0.144728; 0.129937; 0.121166; 0.161450; 0.199197; 0.187168; 0.159467; 0.144269; 0.117404; 0.109298; 0.086901; 0.221391; 0.000000; 0.174044; 0.162080; 0.045439]

Octave result:
octave:7> qp(x0, H, f, [], [], lb, ub)
ans =
   0.13010
   0.12684
   0.07256
   0.15415
   0.14473
   0.12994
   0.12117
   0.16145
   0.19920
   0.18717
   0.15947
   0.14427
   0.11740
   0.10930
   0.08690
   0.22139
   0.00000
   0.17404
   0.16208
   0.04544",,apachespark,coderxiang,debasish83,donnchadh,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-22 05:05:48.359,,false,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 11:55:53 UTC 2015,,,,,0|i21a3j:,9223372036854775807,,,,,,,,,,,,,"22/Oct/14 05:05;coderxiang;[~debasish83] By using a finer config (change 1e-6 on line 82 of NNLS.scala to 1e-7), NNLS will produce the expected result. ","22/Oct/14 15:07;debasish83;I will test it but this is how I called NNLS...assuming P2 and q2 are jblas matrices as mentioned up...

val nnlsResult2 = NNLS.solve(P2, q2.mul(-1), ws)
println(s""NNLS iters ${ws.iterations} result ${nnlsResult2.toList.mkString("","")}"")

val (posResult2, posConverged2) = qpIters.solve(P2, q2)
println(s""QuadraticMinimizer iters ${qpIters.iterations} result ${posResult2.toString()}"")

I did multiply q2 with -1 before it goes to NNLS...

Is that the right way to call NNLS ?",22/Oct/14 15:44;coderxiang;[~debasish83] I think you are correct. Could you please help test whether changing the stopping criterion works on your machine/data?,"22/Oct/14 18:49;debasish83;[~coderxiang] changing to 1e-6 to 1e-7 fixes this error but why bound step at 1e-6 or 1e-7...why not let the algorithm converge using the standard techniques used in BFGS / OWLQN (gradient norm and last 5 iterates)...you can also extend Breeze linear CG to implement NNLS...

NNLS iters 36 result 0.13010360222362655,0.1268399356245685,0.07256472682635416,0.15415258739697485,0.14472814692821925,0.12993720335014108,0.12116579552952525,0.16145040854270917,0.19919730253363563,0.18716812848138634,0.1594670311402431,0.1442692338314524,0.11740410727778867,0.10929848737016828,0.08690057753031168,0.22139114605899224,0.0,0.17404384335673376,0.16208039794069887,0.04543896291399707

QuadraticMinimizer iters 259 result [0.130104; 0.126840; 0.072565; 0.154153; 0.144728; 0.129937; 0.121166; 0.161450; 0.199197; 0.187168; 0.159467; 0.144269; 0.117404; 0.109298; 0.086901; 0.221391; 0.000000; 0.174044; 0.162080; 0.045439]

I will run more tests...Basically based on this PR I am testing https://issues.apache.org/jira/browse/SPARK-2426 userFeatures as SMOOTH and productFeatures as POSITIVE...and this particular case showed up there...For POSITIVE I use both NNLS and QuadraticMinimizer to see the iterations and optimize on that...","27/Oct/14 16:59;coderxiang;[~debasish83] IMHO, the current implementation of NNLS also alopts various terminating criteria, including small gradient and small step-size. I agree that general QP solver will definitely solve NNLS problem, it might be the case that a special solver may be advantageous in numerical consideration. For example, if you consider this case:

    val n = 5
    val ata = new DoubleMatrix(5, 5
      , 517399.13534, 242529.67289, -153644.98976, 130802.84503, -798452.29283
      , 242529.67289, 126017.69765, -75944.21743, 81785.36128, -405290.60884
      , -153644.98976, -75944.21743, 46986.44577, -45401.12659, 247059.51049
      , 130802.84503, 81785.36128, -45401.12659, 67457.31310, -253747.03819
      , -798452.29283, -405290.60884, 247059.51049, -253747.03819, 1310939.40814
    )
    val atb = new DoubleMatrix(5, 1,
      -31755.05710, 13047.14813, -20191.24443, 25993.77580, 11963.55017)

run MLlib/nnls, Matlab/quadprog and Matlab/lsqnonneg and use objective value to evaluate the algorithm, I got:
 * MLlib/nnls with step = 1e-6: *-18962.751251160757*
 * MLlib/nnls with step = 1e-7:  *-845478.0828914623*
 * Matlab/quadprog with all default settings: *-781762.406480181*
 * Matlab/quadprog with more iterations: *-845372.586681944*
 * Matlab/lsqnonneg with all default settings: *-845478.058414435*
","27/Oct/14 21:31;apachespark;User 'coderxiang' has created a pull request for this issue:
https://github.com/apache/spark/pull/2965","28/Oct/14 02:44;mengxr;Issue resolved by pull request 2965
[https://github.com/apache/spark/pull/2965]","31/Oct/14 03:07;debasish83;I can send you a further list of failures...this is one more example...I strongly suggest moving to a robust convergence criteria inside NNLS than adding hacks on step sizes...

P = [0.986619, 0.639909, 0.748906, 0.900377, 0.688079, 0.734711, 0.835164, 0.723973, 0.822436, 0.852591, 0.699979, 0.609533, 0.559504, 0.708015, 0.544744, 0.658359, 0.632510, 0.751316, 0.653993, 0.642734, 0.799106, 0.898689, 0.712825, 0.878405, 0.849565; 0.639909, 1.055175, 0.884940, 0.975502, 0.815121, 0.845699, 0.899780, 0.709264, 0.960949, 1.021108, 0.896508, 0.692635, 0.659746, 0.809355, 0.539466, 0.730501, 0.639971, 0.881502, 0.840159, 0.628515, 0.917052, 0.950677, 0.823301, 1.022355, 0.994935; 0.748906, 0.884940, 1.421868, 1.175203, 0.986093, 1.028669, 1.091437, 0.849192, 1.204776, 1.249037, 1.115160, 0.815680, 0.772715, 0.971263, 0.621197, 0.875925, 0.757031, 1.034483, 1.022001, 0.705077, 1.115237, 1.164796, 0.983688, 1.226847, 1.180170; 0.900377, 0.975502, 1.175203, 1.676666, 1.065369, 1.109756, 1.181625, 0.944176, 1.220418, 1.328803, 1.156144, 0.984967, 0.916500, 1.046903, 0.728221, 0.991042, 0.855095, 1.181719, 1.095485, 0.901193, 1.214851, 1.277434, 1.077374, 1.372354, 1.356724; 0.688079, 0.815121, 0.986093, 1.065369, 1.249908, 0.953824, 1.004765, 0.771115, 1.082255, 1.161322, 1.021400, 0.757474, 0.736266, 0.923406, 0.598005, 0.812629, 0.706870, 1.011984, 0.968135, 0.682813, 1.034818, 1.039625, 0.937088, 1.152792, 1.121475; 0.734711, 0.845699, 1.028669, 1.109756, 0.953824, 1.334869, 1.091564, 0.824709, 1.157992, 1.226413, 1.045522, 0.731169, 0.709382, 0.980030, 0.634635, 0.853988, 0.758256, 1.070744, 0.997542, 0.692832, 1.118828, 1.119519, 0.977618, 1.202464, 1.152186; 0.835164, 0.899780, 1.091437, 1.181625, 1.004765, 1.091564, 1.566492, 0.994066, 1.307932, 1.301575, 1.075206, 0.697477, 0.670553, 1.068449, 0.725819, 0.908391, 0.877151, 1.083915, 0.993647, 0.735423, 1.202928, 1.275592, 1.040266, 1.242863, 1.150213; 0.723973, 0.709264, 0.849192, 0.944176, 0.771115, 0.824709, 0.994066, 1.291185, 1.084907, 0.934737, 0.802436, 0.559117, 0.510659, 0.801938, 0.624240, 0.682461, 0.761152, 0.633965, 0.633248, 0.622034, 0.826798, 1.084260, 0.777897, 0.868127, 0.764337; 0.822436, 0.960949, 1.204776, 1.220418, 1.082255, 1.157992, 1.307932, 1.084907, 1.821667, 1.386725, 1.220398, 0.710929, 0.683088, 1.116057, 0.719380, 0.925482, 0.907590, 1.010205, 1.041999, 0.673723, 1.224967, 1.373886, 1.082813, 1.261720, 1.130003; 0.852591, 1.021108, 1.249037, 1.328803, 1.161322, 1.226413, 1.301575, 0.934737, 1.386725, 1.831419, 1.289097, 0.885418, 0.868685, 1.190024, 0.740180, 1.034238, 0.884778, 1.342867, 1.252762, 0.811797, 1.374303, 1.319088, 1.190385, 1.478896, 1.427420; 0.699979, 0.896508, 1.115160, 1.156144, 1.021400, 1.045522, 1.075206, 0.802436, 1.220398, 1.289097, 1.501713, 0.831799, 0.807718, 0.994071, 0.599941, 0.876646, 0.730630, 1.074102, 1.088886, 0.677923, 1.126406, 1.127440, 1.013104, 1.257194, 1.217591; 0.609533, 0.692635, 0.815680, 0.984967, 0.757474, 0.731169, 0.697477, 0.559117, 0.710929, 0.885418, 0.831799, 1.206659, 0.820404, 0.673313, 0.477307, 0.698907, 0.529141, 0.853290, 0.812957, 0.721698, 0.775190, 0.790003, 0.746242, 0.994816, 1.053857; 0.559504, 0.659746, 0.772715, 0.916500, 0.736266, 0.709382, 0.670553, 0.510659, 0.683088, 0.868685, 0.807718, 0.820404, 1.103586, 0.666515, 0.455429, 0.667077, 0.500250, 0.855895, 0.807716, 0.674035, 0.759131, 0.732206, 0.732658, 0.965557, 1.021876; 0.708015, 0.809355, 0.971263, 1.046903, 0.923406, 0.980030, 1.068449, 0.801938, 1.116057, 1.190024, 0.994071, 0.673313, 0.666515, 1.293629, 0.631919, 0.822011, 0.745899, 1.060809, 0.965440, 0.672388, 1.090059, 1.069719, 0.959136, 1.162012, 1.109763; 0.544744, 0.539466, 0.621197, 0.728221, 0.598005, 0.634635, 0.725819, 0.624240, 0.719380, 0.740180, 0.599941, 0.477307, 0.455429, 0.631919, 0.802572, 0.551340, 0.549393, 0.654652, 0.567237, 0.531461, 0.684655, 0.749979, 0.627718, 0.744096, 0.710310; 0.658359, 0.730501, 0.875925, 0.991042, 0.812629, 0.853988, 0.908391, 0.682461, 0.925482, 1.034238, 0.876646, 0.698907, 0.667077, 0.822011, 0.551340, 1.075722, 0.642962, 0.955470, 0.862026, 0.658120, 0.955639, 0.944118, 0.835790, 1.057347, 1.042521; 0.632510, 0.639971, 0.757031, 0.855095, 0.706870, 0.758256, 0.877151, 0.761152, 0.907590, 0.884778, 0.730630, 0.529141, 0.500250, 0.745899, 0.549393, 0.642962, 0.976258, 0.726041, 0.658693, 0.579842, 0.811285, 0.917944, 0.731996, 0.859613, 0.798469; 0.751316, 0.881502, 1.034483, 1.181719, 1.011984, 1.070744, 1.083915, 0.633965, 1.010205, 1.342867, 1.074102, 0.853290, 0.855895, 1.060809, 0.654652, 0.955470, 0.726041, 1.779551, 1.210339, 0.817870, 1.289494, 1.030990, 1.082950, 1.419900, 1.451896; 0.653993, 0.840159, 1.022001, 1.095485, 0.968135, 0.997542, 0.993647, 0.633248, 1.041999, 1.252762, 1.088886, 0.812957, 0.807716, 0.965440, 0.567237, 0.862026, 0.658693, 1.210339, 1.442263, 0.688333, 1.139661, 0.988895, 0.991869, 1.276105, 1.281761; 0.642734, 0.628515, 0.705077, 0.901193, 0.682813, 0.692832, 0.735423, 0.622034, 0.673723, 0.811797, 0.677923, 0.721698, 0.674035, 0.672388, 0.531461, 0.658120, 0.579842, 0.817870, 0.688333, 1.044625, 0.753991, 0.788593, 0.710741, 0.906241, 0.935628; 0.799106, 0.917052, 1.115237, 1.214851, 1.034818, 1.118828, 1.202928, 0.826798, 1.224967, 1.374303, 1.126406, 0.775190, 0.759131, 1.090059, 0.684655, 0.955639, 0.811285, 1.289494, 1.139661, 0.753991, 1.621034, 1.201765, 1.081841, 1.365902, 1.323655; 0.898689, 0.950677, 1.164796, 1.277434, 1.039625, 1.119519, 1.275592, 1.084260, 1.373886, 1.319088, 1.127440, 0.790003, 0.732206, 1.069719, 0.749979, 0.944118, 0.917944, 1.030990, 0.988895, 0.788593, 1.201765, 1.692948, 1.052744, 1.265542, 1.169388; 0.712825, 0.823301, 0.983688, 1.077374, 0.937088, 0.977618, 1.040266, 0.777897, 1.082813, 1.190385, 1.013104, 0.746242, 0.732658, 0.959136, 0.627718, 0.835790, 0.731996, 1.082950, 0.991869, 0.710741, 1.081841, 1.052744, 1.291439, 1.189060, 1.159221; 0.878405, 1.022355, 1.226847, 1.372354, 1.152792, 1.202464, 1.242863, 0.868127, 1.261720, 1.478896, 1.257194, 0.994816, 0.965557, 1.162012, 0.744096, 1.057347, 0.859613, 1.419900, 1.276105, 0.906241, 1.365902, 1.265542, 1.189060, 1.847657, 1.521522; 0.849565, 0.994935, 1.180170, 1.356724, 1.121475, 1.152186, 1.150213, 0.764337, 1.130003, 1.427420, 1.217591, 1.053857, 1.021876, 1.109763, 0.710310, 1.042521, 0.798469, 1.451896, 1.281761, 0.935628, 1.323655, 1.169388, 1.159221, 1.521522, 1.887527];

q = [-3.640583; -5.563638; -7.040787; -7.387618; -6.410455; -6.452543; -5.698284; -2.604581; -6.184568; -8.450254; -7.746985; -6.173699; -6.072464; -5.954310; -2.867163; -5.751653; -3.362224; -8.631145; -8.238493; -4.198840; -7.653769; -5.764195; -6.387377; -8.917469; -9.348541];

mosekx =

    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    0.0000
    1.0951
    0.5516
    0.6727
    0.0000
    0.0000
    0.0000
    0.0000
    1.0555
    2.0667
    0.0000
    0.0000
    0.0000
    0.0000
    0.2235
    1.1788

QuadraticMinimizer https://github.com/apache/spark/pull/2705

x =

         0
         0
         0
         0
         0
         0
         0
         0
         0
         0
    1.0951
    0.5516
    0.6727
         0
         0
         0
         0
    1.0555
    2.0666
         0
         0
         0
         0
    0.2235
    1.1787

I added (step < 1e-7) // too small or negative","31/Oct/14 03:10;debasish83;NNLS iters 36 result 0.13010360222362655,0.1268399356245685,0.07256472682635416,0.15415258739697485,0.14472814692821925,0.12993720335014108,0.12116579552952525,0.16145040854270917,0.19919730253363563,0.18716812848138634,0.1594670311402431,0.1442692338314524,0.11740410727778867,0.10929848737016828,0.08690057753031168,0.22139114605899224,0.0,0.17404384335673376,0.16208039794069887,0.04543896291399707","31/Oct/14 03:31;coderxiang;[~debasish83] Thanks for the follow up. Wondering can you provide your full test script, as I cannot replicate your result in my following tests:

{code:title=NNLSuite.scala|borderStyle=solid}
  test(""NNLS: extra test"") {
    val n = 25
    val ata = new DoubleMatrix(25, 25,
      0.986619, 0.639909, 0.748906, 0.900377, 0.688079, 0.734711, 0.835164, 0.723973, 0.822436, 0.852591,
      0.699979, 0.609533, 0.559504, 0.708015, 0.544744, 0.658359, 0.632510, 0.751316, 0.653993, 0.642734, 0.799106, 0.898689, 0.712825, 0.878405, 0.849565, 0.639909, 1.055175, 0.884940, 0.975502, 0.815121, 0.845699, 0.899780, 0.709264, 0.960949, 1.021108, 0.896508, 0.692635, 0.659746, 0.809355, 0.539466, 0.730501, 0.639971, 0.881502, 0.840159, 0.628515, 0.917052, 0.950677, 0.823301, 1.022355, 0.994935, 0.748906, 0.884940, 1.421868, 1.175203, 0.986093, 1.028669, 1.091437, 0.849192, 1.204776, 1.249037, 1.115160, 0.815680, 0.772715, 0.971263, 0.621197, 0.875925, 0.757031, 1.034483, 1.022001, 0.705077, 1.115237, 1.164796, 0.983688, 1.226847, 1.180170, 0.900377, 0.975502, 1.175203, 1.676666, 1.065369, 1.109756, 1.181625, 0.944176, 1.220418, 1.328803, 1.156144, 0.984967, 0.916500, 1.046903, 0.728221, 0.991042, 0.855095, 1.181719, 1.095485, 0.901193, 1.214851, 1.277434, 1.077374, 1.372354, 1.356724, 0.688079, 0.815121, 0.986093, 1.065369, 1.249908, 0.953824, 1.004765, 0.771115, 1.082255, 1.161322, 1.021400, 0.757474, 0.736266, 0.923406, 0.598005, 0.812629, 0.706870, 1.011984, 0.968135, 0.682813, 1.034818, 1.039625, 0.937088, 1.152792, 1.121475, 0.734711, 0.845699, 1.028669, 1.109756, 0.953824, 1.334869, 1.091564, 0.824709, 1.157992, 1.226413, 1.045522, 0.731169, 0.709382, 0.980030, 0.634635, 0.853988, 0.758256, 1.070744, 0.997542, 0.692832, 1.118828, 1.119519, 0.977618, 1.202464, 1.152186, 0.835164, 0.899780, 1.091437, 1.181625, 1.004765, 1.091564, 1.566492, 0.994066, 1.307932, 1.301575, 1.075206, 0.697477, 0.670553, 1.068449, 0.725819, 0.908391, 0.877151, 1.083915, 0.993647, 0.735423, 1.202928, 1.275592, 1.040266, 1.242863, 1.150213, 0.723973, 0.709264, 0.849192, 0.944176, 0.771115, 0.824709, 0.994066, 1.291185, 1.084907, 0.934737, 0.802436, 0.559117, 0.510659, 0.801938, 0.624240, 0.682461, 0.761152, 0.633965, 0.633248, 0.622034, 0.826798, 1.084260, 0.777897, 0.868127, 0.764337, 0.822436, 0.960949, 1.204776, 1.220418, 1.082255, 1.157992, 1.307932, 1.084907, 1.821667, 1.386725, 1.220398, 0.710929, 0.683088, 1.116057, 0.719380, 0.925482, 0.907590, 1.010205, 1.041999, 0.673723, 1.224967, 1.373886, 1.082813, 1.261720, 1.130003, 0.852591, 1.021108, 1.249037, 1.328803, 1.161322, 1.226413, 1.301575, 0.934737, 1.386725, 1.831419, 1.289097, 0.885418, 0.868685, 1.190024, 0.740180, 1.034238, 0.884778, 1.342867, 1.252762, 0.811797, 1.374303, 1.319088, 1.190385, 1.478896, 1.427420, 0.699979, 0.896508, 1.115160, 1.156144, 1.021400, 1.045522, 1.075206, 0.802436, 1.220398, 1.289097, 1.501713, 0.831799, 0.807718, 0.994071, 0.599941, 0.876646, 0.730630, 1.074102, 1.088886, 0.677923, 1.126406, 1.127440, 1.013104, 1.257194, 1.217591, 0.609533, 0.692635, 0.815680, 0.984967, 0.757474, 0.731169, 0.697477, 0.559117, 0.710929, 0.885418, 0.831799, 1.206659, 0.820404, 0.673313, 0.477307, 0.698907, 0.529141, 0.853290, 0.812957, 0.721698, 0.775190, 0.790003, 0.746242, 0.994816, 1.053857, 0.559504, 0.659746, 0.772715, 0.916500, 0.736266, 0.709382, 0.670553, 0.510659, 0.683088, 0.868685, 0.807718, 0.820404, 1.103586, 0.666515, 0.455429, 0.667077, 0.500250, 0.855895, 0.807716, 0.674035, 0.759131, 0.732206, 0.732658, 0.965557, 1.021876, 0.708015, 0.809355, 0.971263, 1.046903, 0.923406, 0.980030, 1.068449, 0.801938, 1.116057, 1.190024, 0.994071, 0.673313, 0.666515, 1.293629, 0.631919, 0.822011, 0.745899, 1.060809, 0.965440, 0.672388, 1.090059, 1.069719, 0.959136, 1.162012, 1.109763, 0.544744, 0.539466, 0.621197, 0.728221, 0.598005, 0.634635, 0.725819, 0.624240, 0.719380, 0.740180, 0.599941, 0.477307, 0.455429, 0.631919, 0.802572, 0.551340, 0.549393, 0.654652, 0.567237, 0.531461, 0.684655, 0.749979, 0.627718, 0.744096, 0.710310, 0.658359, 0.730501, 0.875925, 0.991042, 0.812629, 0.853988, 0.908391, 0.682461, 0.925482, 1.034238, 0.876646, 0.698907, 0.667077, 0.822011, 0.551340, 1.075722, 0.642962, 0.955470, 0.862026, 0.658120, 0.955639, 0.944118, 0.835790, 1.057347, 1.042521, 0.632510, 0.639971, 0.757031, 0.855095, 0.706870, 0.758256, 0.877151, 0.761152, 0.907590, 0.884778, 0.730630, 0.529141, 0.500250, 0.745899, 0.549393, 0.642962, 0.976258, 0.726041, 0.658693, 0.579842, 0.811285, 0.917944, 0.731996, 0.859613, 0.798469, 0.751316, 0.881502, 1.034483, 1.181719, 1.011984, 1.070744, 1.083915, 0.633965, 1.010205, 1.342867, 1.074102, 0.853290, 0.855895, 1.060809, 0.654652, 0.955470, 0.726041, 1.779551, 1.210339, 0.817870, 1.289494, 1.030990, 1.082950, 1.419900, 1.451896, 0.653993, 0.840159, 1.022001, 1.095485, 0.968135, 0.997542, 0.993647, 0.633248, 1.041999, 1.252762, 1.088886, 0.812957, 0.807716, 0.965440, 0.567237, 0.862026, 0.658693, 1.210339, 1.442263, 0.688333, 1.139661, 0.988895, 0.991869, 1.276105, 1.281761, 0.642734, 0.628515, 0.705077, 0.901193, 0.682813, 0.692832, 0.735423, 0.622034, 0.673723, 0.811797, 0.677923, 0.721698, 0.674035, 0.672388, 0.531461, 0.658120, 0.579842, 0.817870, 0.688333, 1.044625, 0.753991, 0.788593, 0.710741, 0.906241, 0.935628, 0.799106, 0.917052, 1.115237, 1.214851, 1.034818, 1.118828, 1.202928, 0.826798, 1.224967, 1.374303, 1.126406, 0.775190, 0.759131, 1.090059, 0.684655, 0.955639, 0.811285, 1.289494, 1.139661, 0.753991, 1.621034, 1.201765, 1.081841, 1.365902, 1.323655, 0.898689, 0.950677, 1.164796, 1.277434, 1.039625, 1.119519, 1.275592, 1.084260, 1.373886, 1.319088, 1.127440, 0.790003, 0.732206, 1.069719, 0.749979, 0.944118, 0.917944, 1.030990, 0.988895, 0.788593, 1.201765, 1.692948, 1.052744, 1.265542, 1.169388, 0.712825, 0.823301, 0.983688, 1.077374, 0.937088, 0.977618, 1.040266, 0.777897, 1.082813, 1.190385, 1.013104, 0.746242, 0.732658, 0.959136, 0.627718, 0.835790, 0.731996, 1.082950, 0.991869, 0.710741, 1.081841, 1.052744, 1.291439, 1.189060, 1.159221, 0.878405, 1.022355, 1.226847, 1.372354, 1.152792, 1.202464, 1.242863, 0.868127, 1.261720, 1.478896, 1.257194, 0.994816, 0.965557, 1.162012, 0.744096, 1.057347, 0.859613, 1.419900, 1.276105, 0.906241, 1.365902, 1.265542, 1.189060, 1.847657, 1.521522, 0.849565, 0.994935, 1.180170, 1.356724, 1.121475, 1.152186, 1.150213, 0.764337, 1.130003, 1.427420, 1.217591, 1.053857, 1.021876, 1.109763, 0.710310, 1.042521, 0.798469, 1.451896, 1.281761, 0.935628, 1.323655, 1.169388, 1.159221, 1.521522, 1.887527)

    val atb = new DoubleMatrix(25, 1, -3.640583, -5.563638, -7.040787, -7.387618, -6.410455, -6.452543, -5.698284, -2.604581, -6.184568, -8.450254, -7.746985, -6.173699, -6.072464, -5.954310, -2.867163, -5.751653, -3.362224, -8.631145, -8.238493, -4.198840, -7.653769, -5.764195, -6.387377, -8.917469, -9.348541)

    val refx = new DoubleMatrix(Array(0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
    0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0951, 0.5516, 0.6727, 0.0000, 0.0000,
    0.0000, 0.0000, 1.0555, 2.0667, 0.0000, 0.0000, 0.0000, 0.0000, 0.2235, 1.1788))
    val refObj = computeObjectiveValue(ata, atb.mul(-1.0), refx)

    val ws = NNLS.createWorkspace(n)
    val ans = NNLS.solve(ata, atb.mul(-1.0), ws)
    val x = new DoubleMatrix(ans)
    val obj = computeObjectiveValue(ata, atb.mul(-1.0), x)

    println((refObj, obj))
    println(ans.mkString("" ""))
    assert(obj < refObj + 1E-5)
  }
{code}

The test is OK with output:

[info] NNLSSuite:
[info] - NNLS: exact solution cases (145 milliseconds)
[info] - NNLS: nonnegativity constraint active (3 milliseconds)
[info] - NNLS: objective value test (0 milliseconds)
(-27.561557962647033,-27.561557968417716)
0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0950760986499062 0.5515642980855557 0.6727115532538089 0.0 0.0 0.0 0.0 1.0555025979837525 2.066667142460158 0.0 0.0 0.0 0.0 0.2235312185469203 1.178752675112592
[info] - NNLS: extra test (4 milliseconds)
",31/Oct/14 03:35;debasish83;Was there more changes that step size in your checkin ? I still have not updated my branch...changed the step size and ran...Let me update the branch and re-run full tests...,"31/Oct/14 04:59;mengxr;[~debasish83] For the sample data, did you check the condition number? The NNLS is to solve ALS sub-problems, where we assume that they are well-conditioned.",31/Oct/14 05:27;debasish83;[~mengxr] this came out of an internal dataset while running ALS...I can't point to the dataset...I might not have got all the changes so I am updating my branch and re-run this validation over the dataset...Basically I check cases where NNLS and QuadraticMinimization don't match and dump them out for further analysis...,31/Oct/14 06:24;mengxr;Please check the condition number of the matrix you sent. Did you run ALS with a very small lambda?,"31/Oct/14 06:54;debasish83;Nope...standard ALS...same as netflix params...0.065 as L2...My ratings are
not within 1-5 but more like 1-10...

Also what's a good condition number for NNLS ?

On Thu, Oct 30, 2014 at 11:25 PM, Xiangrui Meng (JIRA) <jira@apache.org>

",31/Oct/14 17:59;coderxiang;[~debasish83][~mengxr] The condition number for the latest test case is 74.5 and  the test case I put in my PR was 20000.,05/Apr/15 22:52;mengxr;[~coderxiang] [~debasish83] What is the status of this issue? Is the NNLS solver implemented in MLlib correct (or enough accurate)?,06/Apr/15 00:42;coderxiang;[~mengxr] I think it was an accuracy issue and it should be done now.,08/Apr/15 03:19;debasish83;[~mengxr] for this testcase it was fixed but I remember there was someone in user list who mentioned that he got incorrect result compared to some other tool...may be it's a good idea to ask for testcases...,08/Apr/15 23:28;coderxiang;[~debasish83] could you point me to those test cases?,16/May/15 11:55;srowen;From the discussion it sounds like the issue that this JIRA concerns was actually OK.,,,
Failed to deserialize Vector in cluster mode,SPARK-3971,12748661,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,16/Oct/14 17:56,05/Nov/14 15:59,15/Aug/18 23:03,16/Oct/14 21:57,1.2.0,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,,0,,,,,"The serialization of Vector/Rating did not work in cluster mode, because the initializer is not called in executor.",,apachespark,davies,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-16 19:44:22.687,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 05 15:59:44 UTC 2014,,,,,0|i219bb:,9223372036854775807,,,,,,1.2.0,,,,,,,"16/Oct/14 19:44;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/2830","16/Oct/14 21:57;mengxr;Issue resolved by pull request 2830
[https://github.com/apache/spark/pull/2830]","05/Nov/14 15:59;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/3113",,,,,,,,,,,,,,,,,,,,,
LogisticRegressionWithLBFGS should not use SquaredL2Updater ,SPARK-3942,12747959,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Won't Fix,,ZeroFM,ZeroFM,14/Oct/14 09:02,15/May/15 13:32,15/Aug/18 23:03,15/May/15 13:32,1.1.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"LBFGS method use line search for step size, in all mllib`s updater use step-size decreasing with the square root of the number of iterations, this may cause Wolfe condition not hold.",,josephkb,mengxr,ZeroFM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,9223372036854775807,,,2014-10-14 09:02:51.0,,,,,0|i2154f:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomForest bug in sanity check in DTStatsAggregator,SPARK-3934,12747807,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,13/Oct/14 18:45,17/Oct/14 22:03,15/Aug/18 23:03,17/Oct/14 22:03,,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,,,,,0,,,,,"When run with a mix of unordered categorical and continuous features, on multiclass classification, RandomForest fails.  The bug is in the sanity checks in getFeatureOffset and getLeftRightFeatureOffsets, which use the wrong indices for checking whether features are unordered.

Proposal: Remove the sanity checks since they are not really needed, and since they would require DTStatsAggregator to keep track of an extra set of indices (for the feature subset).",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-13 18:51:52.956,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 17 22:03:38 UTC 2014,,,,,0|i2149j:,9223372036854775807,,,,,,,,,,,,,"13/Oct/14 18:51;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/2785","13/Oct/14 23:24;srowen;Yep that fixes the issue I was seeing. Thanks! I can confirm it did not affect DecisionTree too, so it seems to match your analysis.","17/Oct/14 22:03;mengxr;Issue resolved by pull request 2785
[https://github.com/apache/spark/pull/2785]",,,,,,,,,,,,,,,,,,,,,
Forget Unpersist in RandomForest.scala(train Method),SPARK-3918,12747573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,Junlong Liu,Junlong Liu,12/Oct/14 11:59,11/Dec/14 17:02,15/Aug/18 23:03,11/Dec/14 17:02,1.1.0,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,,,,,0,decisiontree,train,unpersist,,"   In version 1.1.0 DecisionTree.scala, train Method, treeInput has been persisted in Memory, but without unpersist. It caused heavy DISK usage.
   In github version(1.2.0 maybe), RandomForest.scala, train Method, baggedInput has been persisted but without unpersisted too.",All,apachespark,josephkb,Junlong Liu,mengxr,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-13 18:51:54.429,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 11 17:02:37 UTC 2014,,,,,0|i212uf:,9223372036854775807,paulbits,,,,,1.2.0,,,,,,,"13/Oct/14 18:51;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/2785","10/Dec/14 06:48;srowen;[~josephkb] The PR you opened for this an another issue was merged, and notes indicate it should remove an unpersist call, but I didn't see that in the change? https://github.com/apache/spark/pull/2785/files

Is this just obsoleted or was the change not in the PR actually?","10/Dec/14 20:14;josephkb;Oops!  I forgot to update that PR's name.  It was originally in that PR, but [~Junlong Liu] sent a PR with the change first:
[https://github.com/apache/spark/commit/942847fd94c920f7954ddf01f97263926e512b0e]

(The PR linked above was not tagged with this JIRA.)","11/Dec/14 17:02;srowen;Great, looks like this was in fact fixed for 1.2 then.",,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException found in executing computePrincipalComponents,SPARK-3803,12745982,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,dobachi,dobachi,05/Oct/14 11:27,15/Jan/15 09:08,15/Aug/18 23:03,14/Oct/14 21:42,1.1.0,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,,,,,0,,,,,"When I executed computePrincipalComponents method of RowMatrix, I got java.lang.ArrayIndexOutOfBoundsException.

{code}
14/10/05 20:16:31 INFO DAGScheduler: Failed to run reduce at RDDFunctions.scala:111
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 611, localhost): java.lang.ArrayIndexOutOfBoundsException: 4878161
        org.apache.spark.mllib.linalg.distributed.RowMatrix$.org$apache$spark$mllib$linalg$distributed$RowMatrix$$dspr(RowMatrix.scala:460)
        org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$3.apply(RowMatrix.scala:114)
        org.apache.spark.mllib.linalg.distributed.RowMatrix$$anonfun$3.apply(RowMatrix.scala:113)
        scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
        scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:144)
        scala.collection.AbstractIterator.foldLeft(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:201)
        scala.collection.AbstractIterator.aggregate(Iterator.scala:1157)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$4.apply(RDDFunctions.scala:99)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$4.apply(RDDFunctions.scala:99)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$5.apply(RDDFunctions.scala:100)
        org.apache.spark.mllib.rdd.RDDFunctions$$anonfun$5.apply(RDDFunctions.scala:100)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
{code}

The RowMatrix instance was generated from the result of TF-IDF like the following.

{code}
scala> val hashingTF = new HashingTF()
scala> val tf = hashingTF.transform(texts)
scala> import org.apache.spark.mllib.feature.IDF
scala> tf.cache()
scala> val idf = new IDF().fit(tf)
scala> val tfidf: RDD[Vector] = idf.transform(tf)

scala> import org.apache.spark.mllib.linalg.distributed.RowMatrix
scala> val mat = new RowMatrix(tfidf)
scala> val pc = mat.computePrincipalComponents(2)
{code}

I think this was because I created HashingTF instance with default numFeatures and Array is used in RowMatrix#computeGramianMatrix method
like the following.

{code}
  /**
   * Computes the Gramian matrix `A^T A`.
   */
  def computeGramianMatrix(): Matrix = {
    val n = numCols().toInt
    val nt: Int = n * (n + 1) / 2

    // Compute the upper triangular part of the gram matrix.
    val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(
      seqOp = (U, v) => {
        RowMatrix.dspr(1.0, v, U.data)
        U
      }, combOp = (U1, U2) => U1 += U2)

    RowMatrix.triuToFull(n, GU.data)
  }
{code} 

When the size of Vectors generated by TF-IDF is too large, it makes ""nt"" to have undesirable value (and undesirable size of Array used in treeAggregate),
since n * (n + 1) / 2 exceeded Int.MaxValue.


Is this surmise correct?

And, of course, I could avoid this situation by creating instance of HashingTF with smaller numFeatures.
But this may not be fundamental solution.
",,apachespark,dobachi,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-05 16:43:18.877,,false,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 14 21:42:25 UTC 2014,,,,,0|i20t8n:,9223372036854775807,,,,,,,,,,,,,"05/Oct/14 16:43;srowen;I agree with your assessment. It would take some work, though not terribly much, to rewrite this method to correctly handle A with more than 46340 columns. At n = 46340, the Gramian already consumes about 8.5GB of memory, so it's kinda getting big to realistically use in core anyway. At the least, an error should be raised if n is too large. Any one else think this should be supported though? Would be nice, but, practically helpful?","07/Oct/14 00:41;mengxr;In `computeCovariance`, we generate a warning message if `numCols > 10000`. 

https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/RowMatrix.scala#L307

We could do the same in `Gram`, or we can throw an exception if `numCols` is too big.","07/Oct/14 15:13;dobachi;Thank you for your comments.
I agree with the idea to throw an exception.

This is because exiting with an appropriate exception and messages seems to be kind for users of MLlib.
It helps them to recognize which part of application they should change.

How about using sys.error() to throw RuntimeException in the same way as handling of empty rows.","14/Oct/14 11:58;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/2801","14/Oct/14 21:42;mengxr;Issue resolved by pull request 2801
[https://github.com/apache/spark/pull/2801]",,,,,,,,,,,,,,,,,,,
Some clean-up work after the refactoring of MLlib's SerDe for PySpark,SPARK-3701,12744395,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mengxr,mengxr,mengxr,26/Sep/14 20:29,01/Oct/14 00:10,15/Aug/18 23:03,01/Oct/14 00:10,,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,PySpark,,,,0,,,,,Fix some minor issues came with the refactoring of MLlib's SerDe for PySpark.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-09-26 20:35:34.366,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 01 00:10:54 UTC 2014,,,,,0|i20jh3:,9223372036854775807,,,,,,1.2.0,,,,,,,"26/Sep/14 20:35;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/2548","01/Oct/14 00:10;mengxr;Issue resolved by pull request 2548
[https://github.com/apache/spark/pull/2548]",,,,,,,,,,,,,,,,,,,,,,
DecisionTree overflow error in calculating maxMemoryUsage,SPARK-3494,12740892,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,11/Sep/14 22:40,12/Sep/14 08:39,15/Aug/18 23:03,12/Sep/14 08:38,1.1.0,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,,,,,0,,,,,"maxMemoryUsage can easily overflow.  It needs to use long ints, and also check for overflows afterwards.",,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-09-12 08:38:58.262,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 12 08:38:58 UTC 2014,,,,,0|i1zy33:,9223372036854775807,,,,,,,,,,,,,12/Sep/14 08:38;mengxr;https://github.com/apache/spark/pull/2341,,,,,,,,,,,,,,,,,,,,,,,
MulticlassMetrics is not serializable,SPARK-3459,12740182,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Cannot Reproduce,,mengxr,mengxr,09/Sep/14 16:44,09/Sep/14 17:28,15/Aug/18 23:03,09/Sep/14 17:28,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"Some task closures contains member variables and hence have reference to itself, which causes task not serializable exception on a real cluster.",,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,9223372036854775807,,,2014-09-09 16:44:32.0,,,,,0|i1zucn:,9223372036854775807,,,,,,1.1.1,1.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MLlib doesn't pass maven build / checkstyle due to multi-byte character contained in Gradient.scala,SPARK-3372,12738767,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,sarutak,sarutak,03/Sep/14 09:45,04/Sep/14 03:48,15/Aug/18 23:03,04/Sep/14 03:47,1.1.0,,,,,,,,,,,,,,,1.2.0,,,,,,MLlib,,,,,0,,,,,"Gradient.scala includes 2 UTF-8 hyphens.
Caused by this, mvn package falied on Windows8 because it cannot pass checkstyle.",Windows8 / Java 7 / Maven,apachespark,mengxr,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-09-03 09:50:31.889,,false,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 04 03:47:34 UTC 2014,,,,,0|i1zmfj:,9223372036854775807,,,,,,,,,,,,,"03/Sep/14 09:50;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/2248","04/Sep/14 03:47;mengxr;Issue resolved by pull request 2248
[https://github.com/apache/spark/pull/2248]",,,,,,,,,,,,,,,,,,,,,,
The loss of regularization in Updater should use the oldWeights,SPARK-3317,12737920,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Won't Fix,,dbtsai,dbtsai,29/Aug/14 21:19,31/Aug/14 22:08,15/Aug/18 23:03,31/Aug/14 22:08,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"The current loss of the regularization is computed from the newWeights which is not correct.  The loss, R(w) = 1/2 ||w||^2 should be computed with the oldWeights.
",,apachespark,dbtsai,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-08-29 21:30:28.115,,false,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 29 21:30:28 UTC 2014,,,,,0|i1zi8v:,9223372036854775807,,,,,,,,,,,,,"29/Aug/14 21:30;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/2207",,,,,,,,,,,,,,,,,,,,,,,
spark-example should be run-example in head notation of DenseKMeans and SparseNaiveBayes,SPARK-3296,12737741,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scwf,scwf,scwf,29/Aug/14 07:12,30/Aug/14 00:38,15/Aug/18 23:03,30/Aug/14 00:37,1.0.2,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,,,apachespark,mengxr,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-08-29 07:15:48.595,,false,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 30 00:37:51 UTC 2014,,,,,0|i1zh5z:,9223372036854775807,,,,,,1.1.0,,,,,,,"29/Aug/14 07:15;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/2193","30/Aug/14 00:37;mengxr;Issue resolved by pull request 2193
[https://github.com/apache/spark/pull/2193]",,,,,,,,,,,,,,,,,,,,,,
KMeans cluster will fail on large number of clusters/high dimensional data,SPARK-3253,12737139,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Invalid,,derrickburns,derrickburns,27/Aug/14 16:01,27/Aug/14 16:18,15/Aug/18 23:03,27/Aug/14 16:18,1.0.2,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"The latest changes to use broadcast to communicate cluster centers to workers keeps closure size small, but does not avoid the problem of returning the cluster centers to the master in the final collect() stage. At this step, the collect() may fail because the resulting cluster centers are larger than the akka framesize can accommodate.  What is frustrating about this is that there is no indication that the failure was caused by the frame size being exceeded.  This makes this a Major issue, even though there is a simple workaround, i.e. increasing the frame size. 

What would be helpful is a check BEFORE the clusterer begins the heavy lifting.  The check would compute the expected result size and compare it to the akka frame size.  If the result won't fit, at the very least it emits a warning.",,derrickburns,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,9223372036854775807,,,2014-08-27 16:01:06.0,,,,,0|i1zedj:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should not allow negative values in naive Bayes,SPARK-3130,12735102,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,19/Aug/14 17:08,20/Aug/14 04:02,15/Aug/18 23:03,20/Aug/14 04:02,1.1.0,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,because NB treats feature values as term frequencies.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-08-19 17:27:13.599,,false,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 20 04:02:47 UTC 2014,,,,,0|i1z1yf:,9223372036854775807,,,,,,1.1.0,,,,,,,"19/Aug/14 17:27;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/2038","20/Aug/14 04:02;mengxr;Issue resolved by pull request 2038
[https://github.com/apache/spark/pull/2038]",,,,,,,,,,,,,,,,,,,,,,
ChiSqTest only stores results in the first 100 columns.,SPARK-3087,12734626,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,17/Aug/14 04:26,18/Aug/14 03:53,15/Aug/18 23:03,18/Aug/14 03:53,,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,due to an indexing error in the implementation.,,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,412566,,,Mon Aug 18 03:53:40 UTC 2014,,,,,0|i1yz2f:,412553,,,,,,1.1.0,,,,,,,"18/Aug/14 03:53;mengxr;Issue resolved by pull request 1997
[https://github.com/apache/spark/pull/1997]",,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException in ALS for Large datasets,SPARK-3080,12734558,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Cannot Reproduce,mengxr,brkyvz,brkyvz,16/Aug/14 01:24,14/Feb/17 19:49,15/Aug/18 23:03,23/Feb/15 22:22,1.1.0,1.2.0,,,,,,,,,,,,,,,,,,,,MLlib,,,,,1,,,,,"The stack trace is below:

{quote}
java.lang.ArrayIndexOutOfBoundsException: 2716
        org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1.apply$mcVI$sp(ALS.scala:543)
        scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
        org.apache.spark.mllib.recommendation.ALS.org$apache$spark$mllib$recommendation$ALS$$updateBlock(ALS.scala:537)
        org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:505)
        org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:504)
        org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
        org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:138)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
        scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
{quote}
This happened after the dataset was sub-sampled. 
Dataset properties: ~12B ratings
Setup: 55 r3.8xlarge ec2 instances",,brkyvz,derenrich,devl.development,mbhynes,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-10-12 23:27:52.695,,false,,,,,,,,,,,,,412498,,,Mon Feb 23 22:15:52 UTC 2015,,,,,0|i1yynb:,412485,,,,,,,,,,,,,"12/Oct/14 23:27;derenrich;I ran into a similar bug. My datascale is smaller (only a few million entries on 5 r3.xlarges) but subsampling causes the bug to go away as does lowering the number of iterations. My dataset is public so if needed I could provide a test case.

I'll probably look into this when I have some free time.",27/Oct/14 21:27;ilganeli;I've seen the same error on a dataset of ~200 million ratings. I have tried lowering the number of iterations but unfortunately sub-sampling is not a viable solution in our case. ,"29/Oct/14 05:18;mengxr;Thanks for confirming the issue! I guess this could be a serialization issue. Did you observe any executor loss during the computation or in-memory cached RDDs switching to on-disk storage?

[~derenrich] Which public dataset are you using? Could you also let me know all the ALS parameters and custom Spark settings you used? Thanks!

[~ilganeli] If you do need to run ALS on the full dataset, I recommend using the new ALS implementation at

https://github.com/mengxr/spark-als/blob/master/src/main/scala/org/apache/spark/ml/SimpleALS.scala

It should perform better. But it is not merged yet.","29/Oct/14 13:51;ilganeli;Hello Xiangrui - happy to hear that you're on this! 

With regards to the first question, I have not seen any spillage to disk but I have seen executor loss (on a relatively frequent basis). I have not known whether this is a function of use on our cluster or an internal spark issue. 

With regards to upgrading ALS, can I simply replace the old SimpleALS.scala with the new one or will there be additional dependencies? I am interested in doing a piece-meal upgrade of ML Lib (without upgrading the rest of Spark from version 1.1). I want to do this to maintain compatibility with CDH 5.2. 

Please let me know, thank you. ",29/Oct/14 20:19;mengxr;SimpleALS is not merged yet. You need to build it and submit it as an application: http://spark.apache.org/docs/latest/submitting-applications.html,"29/Oct/14 21:33;ilganeli;Hi all - I have managed to make some substantial progress! What I discovered is that the default parallelization setting is critical. I did two things that got me around this blocker: 
1) I increased the amount of memory available to nodes - by itself this did not solve the problem
2) I set .set(""spark.default.parallelism"",""300"")

I believe the latter is critical because even if I partitioned the data before feeding it into ALS.train, the internal operations would produce RDDs that are coalesced into fewer partitions. Consequently, I believe these smaller (but presumably large in memory) partitions would create memory issues ultimately leading to this and other hard to pin-down issues. Forcing default parallelism ensured that even these internal operations would shard appropriately. ","29/Oct/14 22:00;mengxr;Btw, the `ArrayIndexOutOfBoundsException` is from the driver log. Could you also check the executor logs? It may contain the root cause.",02/Nov/14 22:23;derenrich;Bumping RAM and changing parallelism did fix the issue for me.,"14/Nov/14 02:01;mengxr;[~ilganeli] and [~derenrich], one more question: is there any non-deterministic factor in your job? For example, if you generate random numbers without fixing the seed per partition, the partition cannot be reproduced exactly.","14/Nov/14 05:49;derenrich;Yes I am using random numbers. The random numbers I'm using should only affect entries in the matrix I'm factorizing but not the number of non-zero entries.

(For clarity I'm using random numbers to sample 0s in my matrix which otherwise is all 1s or blank. I need to do this since my input only tells me when an action is taken not when it is not taken)

I won't have time for a little while to test if using a fixed seed would solve this problem.","14/Nov/14 06:09;mengxr;I see. If the procedure of sample negatives is not deterministic (by ""deterministic"" I mean it cannot be re-computed), then the problem may occur. For example,

{code}
rdd.filter(x => random.nextDouble() < 0.2)
{code}

is not deterministic (which violates the assumption of RDD), while

{code}
rdd.mapPartitionsWithIndex { (idx, iter) =>
  val random = new Random(idx) // fixed seed
  iter.filter(x => random.nextDouble() < 0.2)
}
{code}

is deterministic.",14/Nov/14 07:45;derenrich;Yes I understand. I am definitely doing it wrong. Didn't realize this would break things.,"14/Nov/14 20:55;mengxr;Thanks for the confirmation! If [~ilganeli] also confirms that this is the root cause, I'm going to close this JIRA as ""not a problem"" because this is the only way I could reproduce the issue, which violates the immutability assumption of RDD.",14/Nov/14 21:03;ilganeli;Hi Xiangrui - I was not doing any sort of randomization or sampling in the code that produced this issue. ,30/Jan/15 04:41;mbhynes;What is the status of this SimpleALS.scala rewrite? Are you planning to merge it into the master branch to replace the current implementation? ,"23/Feb/15 22:15;mengxr;I'm closing this issue since the only way that I can re-produce this bug is the have some non-deterministic factor in the input RDD, which violates the assumption of RDD. Feel free to re-open it if anyone has a way to produce this bug deterministically.",,,,,,,,
Make LRWithLBFGS API consistent with others,SPARK-3078,12734535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,15/Aug/14 22:17,16/Aug/14 04:05,15/Aug/18 23:03,16/Aug/14 04:05,1.1.0,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,Should ask users to use optimizer to set parameters.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-08-15 22:21:45.378,,false,,,,,,,,,,,,,412475,,,Sat Aug 16 04:05:07 UTC 2014,,,,,0|i1yyif:,412462,,,,,,1.1.0,,,,,,,"15/Aug/14 22:21;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/1973","16/Aug/14 04:05;mengxr;Issue resolved by pull request 1973
[https://github.com/apache/spark/pull/1973]",,,,,,,,,,,,,,,,,,,,,,
ChiSqTest bugs,SPARK-3077,12734529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,dorx,dorx,15/Aug/14 22:08,17/Aug/14 04:16,15/Aug/18 23:03,17/Aug/14 04:16,,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,"- promote nullHypothesis field in ChiSqTestResult to TestResult. Every test should have a null hypothesis
- Correct null hypothesis statement for independence test
- line 59 in TestResult: 0.05 -> 0.5",,apachespark,dorx,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-08-16 04:36:53.676,,false,,,,,,,,,,,,,412469,,,Sun Aug 17 04:16:50 UTC 2014,,,,,0|i1yyh3:,412456,,,,,,,,,,,,,"16/Aug/14 04:36;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/1982","17/Aug/14 04:16;mengxr;Issue resolved by pull request 1982
[https://github.com/apache/spark/pull/1982]",,,,,,,,,,,,,,,,,,,,,,
DecisionTree: isSampleValid indexing incorrect,SPARK-3041,12734120,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,14/Aug/14 18:12,15/Aug/14 21:56,15/Aug/18 23:03,15/Aug/14 21:56,1.1.0,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,"In DecisionTree, isSampleValid treats unordered categorical features incorrectly: It treated the bins as if indexed by featured values, rather than by subsets of values/categories.
This bug is exhibited for unordered features (multi-class classification with categorical features of low arity).
Proposed fix: Index bins correctly for unordered categorical features.
",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-08-14 20:16:47.69,,false,,,,,,,,,,,,,412147,,,Thu Aug 14 20:16:47 UTC 2014,,,,,0|i1ywif:,412136,,,,,,1.1.0,,,,,,,"14/Aug/14 20:16;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/1950",,,,,,,,,,,,,,,,,,,,,,,
"[MLlib] While running regression tests with dense vectors of length greater than 1000, the treeAggregate blows up after several iterations",SPARK-2916,12732741,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,brkyvz,brkyvz,08/Aug/14 01:54,16/Aug/14 05:56,15/Aug/18 23:03,16/Aug/14 05:56,,,,,,,,,,,,,,,,,,,,,,MLlib,Spark Core,,,,0,,,,,"While running any of the regression algorithms with gradient descent, the treeAggregate blows up after several iterations.

Observed on EC2 cluster with 16 nodes, matrix dimensions of 1,000,000 x 5,000

In order to replicate the problem, use aggregate multiple times, maybe over 50-60 times.

Testing lead to the possible workaround:
setting 
`spark.cleaner.referenceTracking false`

seems to help. So the problem is most probably related to the cleanup.
",,brkyvz,huasanyelao,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-08-08 08:57:28.774,,false,,,,,,,,,,,,,410769,,,Sat Aug 16 05:56:41 UTC 2014,,,,,0|i1yo6n:,410762,,,,,,1.1.0,,,,,,,"08/Aug/14 08:57;mengxr;[~brkyvz] I tried running computeColumnSummaryStatistics() 300 times with matrix size 1,000,000 x 10,000 on a 16 nodes m3.2xlarge cluster. I used the latest branch-1.1. Could you try again with the latest branch-1.1?",08/Aug/14 09:00;brkyvz;will do,15/Aug/14 22:26;pwendell;Just to document for posterity - this was narrowed down and is just a symptom of SPARK-3015.,16/Aug/14 05:56;pwendell;Fixed by virtue of SPARK-3015,,,,,,,,,,,,,,,,,,,,
fix random seed in Word2Vec,SPARK-2864,12732076,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,05/Aug/14 17:22,05/Aug/14 23:24,15/Aug/18 23:03,05/Aug/14 23:24,1.1.0,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,"The random seed is not fixed in word2vec, making the unit tests fail randomly.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-08-05 22:07:27.495,,false,,,,,,,,,,,,,410105,,,Tue Aug 05 23:24:21 UTC 2014,,,,,0|i1yk4n:,410099,,,,,,1.1.0,,,,,,,"05/Aug/14 22:07;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/1790","05/Aug/14 23:24;mengxr;Issue resolved by pull request 1790
[https://github.com/apache/spark/pull/1790]",,,,,,,,,,,,,,,,,,,,,,
DecisionTree bug with ordered categorical features,SPARK-2796,12731428,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,01/Aug/14 21:50,01/Aug/14 22:53,15/Aug/18 23:03,01/Aug/14 22:53,1.0.0,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,"In DecisionTree, the method sequentialBinSearchForOrderedCategoricalFeatureInClassification() indexed bins from 0 to (math.pow(2, featureCategories.toInt - 1) - 1).  This upper bound is the bound for unordered categorical features, not ordered ones.  The upper bound should be the arity (i.e., max value) of the feature.",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-08-01 21:56:12.632,,false,,,,,,,,,,,,,409500,,,Fri Aug 01 21:56:12 UTC 2014,,,,,0|i1yghb:,409495,,,,,,1.1.0,,,,,,,"01/Aug/14 21:56;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/1720",,,,,,,,,,,,,,,,,,,,,,,
Decision Tree bugs,SPARK-2756,12730940,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,30/Jul/14 22:58,01/Aug/14 03:53,15/Aug/18 23:03,01/Aug/14 03:53,1.0.0,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,"3 bugs:

Bug 1: Indexing is inconsistent for aggregate calculations for unordered features (in multiclass classification with categorical features, where the features had few enough values such that they could be considered unordered, i.e., isSpaceSufficientForAllCategoricalSplits=true).

* updateBinForUnorderedFeature indexed agg as (node, feature, featureValue, binIndex), where
** featureValue was from arr (so it was a feature value)
** binIndex was in [0,…, 2^(maxFeatureValue-1)-1)
* The rest of the code indexed agg as (node, feature, binIndex, label).

Bug 2: calculateGainForSplit (for classification):
* It returns dummy prediction values when either the right or left children had 0 weight.  These are incorrect for multiclass classification.

Bug 3: Off-by-1 when finding thresholds for splits for continuous features.
* When finding thresholds for possible splits for continuous features in DecisionTree.findSplitsBins, the thresholds were set according to individual training examples’ feature values.  This can cause problems for small datasets.
",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-07-30 23:01:16.629,,false,,,,,,,,,,,,,409012,,,Fri Aug 01 03:53:10 UTC 2014,,,,,0|i1ydi7:,409009,,,,,,1.1.0,,,,,,,"30/Jul/14 23:01;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/1673","01/Aug/14 03:53;mengxr;Issue resolved by pull request 1673
[https://github.com/apache/spark/pull/1673]",,,,,,,,,,,,,,,,,,,,,,
"Loss of precision for small arguments to Math.exp, Math.log",SPARK-2748,12730765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,30/Jul/14 11:33,15/Jan/15 09:08,15/Aug/18 23:03,30/Jul/14 15:57,1.0.1,,,,,,,,,,,,,,,1.1.0,,,,,,GraphX,MLlib,,,,0,,,,,"In a few places in MLlib, an expression of the form log(1.0 + p) is evaluated. When p is so small that 1.0 + p == 1.0, the result is 0.0. However the correct answer is very near p. This is why Math.log1p exists.

Similarly for one instance of exp(m) - 1 in GraphX; there's a special Math.expm1 method.

While the errors occur only for very small arguments, given their use in machine learning algorithms, this is entirely possible.

Also, while we're here, naftaliharris discovered a case in Python where 1 - 1 / (1 + exp(margin)) is less accurate than exp(margin) / (1 + exp(margin)). I don't think there's a JIRA on that one, so maybe this can serve as an umbrella for all of these related issues.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-07-30 11:40:59.063,,false,,,,,,,,,,,,,408838,,,Wed Jul 30 15:57:05 UTC 2014,,,,,0|i1ycfz:,408836,,,,,,1.1.0,,,,,,,"30/Jul/14 11:40;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1659","30/Jul/14 11:41;srowen;PR: https://github.com/apache/spark/pull/1659
See also: https://github.com/apache/spark/pull/1652","30/Jul/14 15:57;mengxr;Issue resolved by pull request 1659
[https://github.com/apache/spark/pull/1659]",,,,,,,,,,,,,,,,,,,,,
Correct doc and usage of preservesPartitioning,SPARK-2617,12728816,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,22/Jul/14 09:28,23/Jul/14 07:59,15/Aug/18 23:03,23/Jul/14 07:59,1.0.1,,,,,,,,,,,,,,,1.1.0,,,,,,Documentation,MLlib,Spark Core,,,0,,,,,"The name `preservesPartitioning` is ambiguous: 1) preserves the indices of partitions, 2) preserves the partitioner. The latter is correct and `preservesPartitioning` should really be called `preservesPartitioner`. Unfortunately, this is already part of the API and we cannot change.

We should be clear in the doc and fix wrong usages.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-07-22 09:36:07.55,,false,,,,,,,,,,,,,406890,,,Tue Jul 22 09:36:07 UTC 2014,,,,,0|i1y0nz:,406909,,,,,,1.1.0,,,,,,,"22/Jul/14 09:36;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/1526",,,,,,,,,,,,,,,,,,,,,,,
ALS has data skew for popular product,SPARK-2612,12728776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,peng.zhang,peng.zhang,peng.zhang,22/Jul/14 05:51,22/Jul/14 09:41,15/Aug/18 23:03,22/Jul/14 09:41,1.0.0,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,"Usually there are some popular products which are related with many users in Rating inputs. 
groupByKey() in updateFeatures() may cause one extra Shuffle stage to gather data of the popular product to one task, because it's RDD's partitioner may be not used as the join() partitioner. 
The following join() need to shuffle from the aggregated product data. The shuffle block can easily be bigger than 2G, and shuffle failed as mentioned in SPARK-1476
And increasing blocks number doesn't work.  

IMHO, groupByKey() should use the same partitioner as the other RDD in join(). So groupByKey() and join() will be in the same stage, and shuffle data from many previous tasks will not trigger ""2G"" limits.",,apachespark,glenn.strycker@gmail.com,mengxr,peng.zhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-07-22 06:10:24.985,,false,,,,,,,,,,,,,406850,,,Tue Jul 22 06:10:24 UTC 2014,,,,,0|i1y0f3:,406869,,,,,,,,,,,,,"22/Jul/14 06:10;apachespark;User 'renozhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/1521",,,,,,,,,,,,,,,,,,,,,,,
almostEquals mllib.util.TestingUtils does not behave as expected when comparing against 0.0,SPARK-2599,12728428,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Duplicate,,dorx,dorx,20/Jul/14 08:33,22/Jul/14 21:24,15/Aug/18 23:03,22/Jul/14 00:15,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"DoubleWithAlmostEquals.almostEquals, when used to compare a number with 0.0, would always produce an epsilon of 1 >> 1e-10, causing false failure when comparing very small numbers with 0.0.",,dbtsai,dorx,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-07-20 10:16:43.081,,false,,,,,,,,,,,,,406504,,,Tue Jul 22 21:24:50 UTC 2014,,,,,0|i1xyb3:,406524,,,,,,,,,,,,,"20/Jul/14 10:16;srowen;Yes, the relative error will never be more than 2.0; it wouldn't be possible to use epsilon >= 2.0 meaningfully -- think of expecting 0.05 but accepting anything 0.05 +/- 0.1. The range is also asymmetric, which might not be desired. With expected value 2.0 and epsilon = 0.1, you allow the range 1.8 to 2.2222 instead of 1.8 to 2.2.

The usual approach is to treat epsilon as an absolute error. This is fit for purpose in its default role of ignoring variance due to rounding or machine precision. The default can be near a ulp, and can be changed.

Is the intent really to make an *absolute* error bound whose default value is some *relative proportion* of the absolute value of the arguments?

If relative error, then it should be relative to the absolute value of the expected value, rather than just larger one, I think.

Of course, you would still be able to change the value of this relative epsilon. But what do you do when the expected value is 0? you would have no way of specifying an epsilon other than 0. And, working in relative terms might get clunky; if the expected value is 0.123 +/ 0.45, the test writer has to work out and document what the relative epsilon is doing.

Still, relative is useful. It seems like the right-est thing is two methods, one specifying absolute and the other relative complexity. And that is a little complex.

So I can see why every testing framework I've seen just lets you set an absolute error and that's it. At the least, it's straightforward to reason about and fully flexible. You do have to set the epsilon once in a while, but that's always the case.

Net-net, I'm suggesting making this an absolute error only to address all this.","21/Jul/14 02:04;dorx;Found this in-depth article discussing the different considerations for comparing floating point numbers: http://www.cygnus-software.com/papers/comparingfloats/comparingfloats.htm

My suggestion is the following (a blend of absolute and relative epsilon):

{code}
def almostEquals(y: Double, epsilon: Double = 1E-10): Boolean = {
      if(x == y || math.abs(x - y) < epsilon) {
        true
      } else if(math.abs(x) > math.abs(y)) {
        math.abs(x - y) / math.abs(x) < epsilon
      } else {
        math.abs(x - y) / math.abs(y) < epsilon
      }
  }
{code}

Not the most rigorous but covers most use cases I'd imagine (small numbers get caught by the first condition while large numbers with large absolute difference but small relative difference would still be considered equal by the subsequent conditions).","21/Jul/14 07:37;srowen;The problem I see with this is that it treats epsilon as both absolute and relative. You can imagine this leads to some weird situations. For example, how do I test for 0.005 +/- 100%? If I set epsilon to 1.0, I'll accept 0.005 +/- 1.0, which is far too large.","21/Jul/14 08:01;dorx;I completely agree that convoluting absolute and relative differences can be dangerous, but perhaps not so much in the case where we're just trying to see if two numbers are almost equal to each other (i.e. epsilon is really small, ~1e-10). To avoid the problem you suggested, we'd have to advise users to use absolute difference as a general rule of thumb, which is fairly conventional. The relative difference support can be thought of as the ""cherry on top"" for large number comparisons. ","21/Jul/14 10:34;srowen;The problem is I can't express a lot of test conditions. See above for an example, but here's another opposite one: what if I want to test for 50 +/- 1 (absolute error)? If epsilon = 1.0, and the result is 55, I want it to fail, but it will pass since it is within 1.0 (100%) as a relative error. I can't express this test at all and it's not an obscure case.

I'd still favor fixing this by sticking to absolute errors. Relative errors can always be expressed by the test caller since the test caller knows the right answer.

(Or, one can add another method for relative tests. I just don't think absolute and relative can go together in one method.)","21/Jul/14 19:03;dorx;Did some digging through the codebase, and it seems like DoubleWithAlmostEquals.almostEquals itself isn't used anywhere but VectorWithAlmostEquals.almostEquals, which calls DoubleWithAlmostEquals.almostEquals,  is exclusively used in one test suite MultivariateOnlineSummarizerSuite (by the same author). Not polluting the current implementation with absolute error is probably the safest thing to do, but naming then becomes an issue. We need something succinct and self explanatory for these test utils, and something like almostEqualsAbs/Rel might be too verbose? 
Either way, it'd be nice to have a centralized set of test utils soon.",22/Jul/14 00:15;dorx;Refer to this issue: https://issues.apache.org/jira/browse/SPARK-2479,"22/Jul/14 08:12;srowen;Yeah they're tracking roughly the same issue but it would be good to note this discussion in SPARK-2479, including the current issue with 0.

I'd favor one absolute error method, changing the one caller as needed. If necessary, a second relative error method to accommodate this one use case.","22/Jul/14 21:24;dbtsai;I'm the original guy implementing `almostEquals` for my unit-testing, and I also noticed that it will be suffering when comparing against 0.0. As [~srowen] pointed out, it's meaningless to comparing against 0.0 (or a really small number) with relative error. However, people may just want to write unittest using relative error for even comparing those small numbers. So I purpose the following APIs.

`a ~== b +- eps` for relative error, and when a or b near zero, let's say 1e-15, it falls back to absolute error.
`a === b +- eps` which is already in scalatest 2.0 for absolute error, but since we don't use scalatest 2.0 yet, we build the same APIs in mllib for absolute error.",,,,,,,,,,,,,,,
Stabilize the computation of logistic function in pyspark,SPARK-2552,12727878,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,17/Jul/14 08:14,21/Jul/14 01:41,15/Aug/18 23:03,21/Jul/14 01:41,,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,PySpark,,,,0,Starter,,,,"exp(1000) throws an error in python. For logistic function, we can use either 1 / ( 1 + exp( -x ) ) or 1 - 1 / (1 + exp( x ) ) to compute its value which ensuring exp always takes a negative value.",,mengxr,miccagiann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-07-19 02:11:12.052,,false,,,,,,,,,,,,,405983,,,Mon Jul 21 01:41:06 UTC 2014,,,,,0|i1xv4f:,406003,,,,,,1.1.0,,,,,,,"19/Jul/14 02:11;miccagiann;Hi Xiangrui,

From what I have seen so far, this error affects the prediction made using the
_predict_ method of _LogisticRegressionModel_ defined in 
_spark/python/pyspark/mllib/classification.py_ file. Is there any other occurence of
this issue in another file as well???

I can see two solutions in order to solve this issue:
a) Either check if the dot product between coeffs and data attributes gives a value in the desired range [ -745, 709 ] and if not to just set it to the one of these two values.

b) To create specific math functions in Java such as Logistic Function, SoftMax,
etc.. and call them via py4j in python2.7 and store the result in a 'decimal.Decimal' variable.

Thanks,
Michael","19/Jul/14 07:00;mengxr;It is not necessary to check the ranges because exp never underflows on a negative number. So the function is just

{code}
def logistic(x):
  if x > 0:
    return 1 / (1 + math.exp(-x))
  else
    return 1 - 1 / (1 + math.exp(x))
{code}
",19/Jul/14 08:53;mengxr;PR: https://github.com/apache/spark/pull/1493,"20/Jul/14 16:19;miccagiann;Xiangrui Meng,

Sorry about posting in this topic:
Would you find useful to create the following spark packages:
a) One that will include all the math functions commonly used for ML tasks.
b) One that will include distributions such as Uniform, Gaussian etc...

Let me know what you are thinking.

Thanks,
Michael","21/Jul/14 01:41;mengxr;Issue resolved by pull request 1493
[https://github.com/apache/spark/pull/1493]",,,,,,,,,,,,,,,,,,,
"In MLlib, implementation for Naive Bayes in Spark 0.9.1 is having an implementation bug.",SPARK-2433,12726625,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,rahul1993,rahul1993,10/Jul/14 16:37,17/Jul/14 05:27,15/Aug/18 23:03,17/Jul/14 03:12,0.9.1,,,,,,,,,,,,,,,0.9.2,,,,,,MLlib,PySpark,,,,0,easyfix,test,,,"Don't have much experience with reporting errors. This is first time. If something is not clear please feel free to contact me (details given below)

In the pyspark mllib library. 
Path : \spark-0.9.1\python\pyspark\mllib\classification.py

Class: NaiveBayesModel

Method:  self.predict

Earlier Implementation:
def predict(self, x):
    """"""Return the most likely class for a data vector x""""""
    return numpy.argmax(self.pi + numpy.log(dot(numpy.exp(self.theta),x)))
        

New Implementation:
No:1
def predict(self, x):
    """"""Return the most likely class for a data vector x""""""
    return numpy.argmax(self.pi + numpy.log(dot(numpy.exp(self.theta),x)))

No:2
def predict(self, x):
    """"""Return the most likely class for a data vector x""""""
    return numpy.argmax(self.pi + dot(x,self.theta.T))

Explanation:
No:1 is correct according to me. Don't know about No:2.

Error one:
The matrix self.theta is of dimension [n_classes , n_features]. 
while the matrix x is of dimension [1 , n_features].

Taking the dot will not work as its [1, n_feature ] x [n_classes,n_features].
It will always give error:  ""ValueError: matrices are not aligned""
In the commented example given in the classification.py, n_classes = n_features = 2. That's why no error.

Both Implementation no.1 and Implementation no. 2 takes care of it.

Error 2:
As basic implementation of naive bayes is: P(class_n | sample) = count_feature_1 * P(feature_1 | class_n ) * count_feature_n * P(feature_n|class_n) * P(class_n)/(THE CONSTANT P(SAMPLE)

and taking the class with max value.
That's what implementation 1 is doing.

In Implementation 2: 
Its basically class with max value :
( exp(count_feature_1) * P(feature_1 | class_n ) * exp(count_feature_n) * P(feature_n|class_n) * P(class_n))

Don't know if it gives the exact result.

Thanks
Rahul Bhojwani
rahulbhojwani2003@gmail.com",Any ,bdechoux,mengxr,rahul1993,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-07-10 17:59:42.531,,false,,,,,,,,,,,,,404732,,,Thu Jul 17 05:27:04 UTC 2014,,,,,0|i1xnl3:,404770,,,,,,0.9.2,,,,,,,"10/Jul/14 17:59;srowen;Your ""earlier implementation"" is identical to ""new implementation 1"". This does not appear to be the code in master, and I think it's only useful to propose fixes to the current version of code.","10/Jul/14 18:40;bdechoux;A Jira ticket is the first step, the second would have been to provide a diff patch or a github pull request. And you can also write a test to prove your point and make sure that the fix will stay longer.

I will second Sean :
1) work with last version (1.0)
2) you report is not clear, that's why diff patch or pull request are welcomed

And there is a transpose() in the current implementation,  the bug is actually already fixed, see https://github.com/apache/spark/pull/463

You might want to read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark for a next time.","10/Jul/14 19:02;rahul1993;Sorry my mistake.
By earlier implementation I mean how its already implemented.
and that is:
def predict(self, x):
        """"""Return the most likely class for a data vector x""""""
        return numpy.argmax(self.pi + dot(x,self.theta))








-- 
Rahul K Bhojwani
3rd Year B.Tech
Computer Science and Engineering
National Institute of Technology, Karnataka
","10/Jul/14 19:02;rahul1993;Okay fine. I will take care of the proper procedure from next time. Looks like its been already taken care.
 Thank you","16/Jul/14 08:04;mengxr;[~rahul1993] Thanks for reporting this bug! It was fixed in branch-1.0 but not in branch-0.9. So you can send a PR similar to https://github.com/apache/spark/pull/463 to branch-0.9, following https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark . I'll try to cut a release candidate for v0.9.2 tomorrow. So if you don't have time to send the PR, I may have to do that myself.",17/Jul/14 00:45;mengxr;PR for branch-0.9: https://github.com/apache/spark/pull/1453,"17/Jul/14 03:12;mengxr;Issue resolved by pull request 1453
[https://github.com/apache/spark/pull/1453]","17/Jul/14 05:05;rahul1993;Apologies for no response and not correcting it by myself. Situation was so.
The patch looks perfect.

Thanks,


On Thu, Jul 17, 2014 at 8:44 AM, Xiangrui Meng (JIRA) <jira@apache.org>




-- 

 [image: http://]
Rahul K Bhojwani
[image: http://]about.me/rahul_bhojwani
     <http://about.me/rahul_bhojwani>
","17/Jul/14 05:27;rahul1993;There is another small error in the documentation.

http://spark.apache.org/docs/0.9.1/mllib-guide.html#clustering-2

Have created the Issue


On Thu, Jul 17, 2014 at 10:34 AM, Rahul Bhojwani <




-- 

 [image: http://]
Rahul K Bhojwani
[image: http://]about.me/rahul_bhojwani
     <http://about.me/rahul_bhojwani>
",,,,,,,,,,,,,,,
Decision tree tests are failing,SPARK-2417,12726299,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jsondag,pwendell,pwendell,09/Jul/14 12:10,28/Jul/14 14:09,15/Aug/18 23:03,09/Jul/14 18:07,,,,,,,,,,,,,,,,1.0.1,1.1.0,,,,,MLlib,,,,,0,,,,,"After SPARK-2152 was merged, these tests started failing in Jenkins:

{code}
- classification stump with all categorical variables *** FAILED ***
  org.scalatest.exceptions.TestFailedException was thrown. (DecisionTreeSuite.scala:257)
- regression stump with all categorical variables *** FAILED ***
  org.scalatest.exceptions.TestFailedException was thrown. (DecisionTreeSuite.scala:284)
{code}

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/97/hadoop.version=1.0.4,label=centos/console",,jsondag,mengxr,Patrickmorton,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2152,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-07-09 13:38:31.794,,false,,,,,,,,,,,,,404406,,,Wed Jul 09 18:07:20 UTC 2014,,,,,0|i1xllb:,404445,,,,,,,,,,,,,09/Jul/14 13:38;jsondag;PR fix here: https://github.com/apache/spark/pull/1343,"09/Jul/14 18:07;mengxr;Issue resolved by pull request 1343
[https://github.com/apache/spark/pull/1343]",,,,,,,,,,,,,,,,,,,,,,
Check for the number of clusters to avoid ArrayIndexOutOfBoundsException,SPARK-2355,12725334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,viirya,viirya,03/Jul/14 17:07,30/Aug/14 00:55,15/Aug/18 23:03,03/Jul/14 18:32,1.0.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"When the number of clusters given to perform with org.apache.spark.mllib.clustering.KMeans under parallel initial mode is greater than data number, it will throw ArrayIndexOutOfBoundsException.

KMeans class should check the number of clusters that must not be greater than data number.

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1
        at org.apache.spark.mllib.clustering.LocalKMeans$$anonfun$kMeansPlusPlus$1.apply$mcVI$sp(LocalKMeans.scala:62)
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
        at org.apache.spark.mllib.clustering.LocalKMeans$.kMeansPlusPlus(LocalKMeans.scala:49)
        at org.apache.spark.mllib.clustering.KMeans$$anonfun$20.apply(KMeans.scala:297)
        at org.apache.spark.mllib.clustering.KMeans$$anonfun$20.apply(KMeans.scala:294)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.immutable.Range.foreach(Range.scala:141)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:294)
        at org.apache.spark.mllib.clustering.KMeans.runBreeze(KMeans.scala:143)
        at org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:126)
        at org.apache.spark.examples.mllib.DenseKMeans$.run(DenseKMeans.scala:102)
        at org.apache.spark.examples.mllib.DenseKMeans$$anonfun$main$1.apply(DenseKMeans.scala:72)
        at org.apache.spark.examples.mllib.DenseKMeans$$anonfun$main$1.apply(DenseKMeans.scala:71)
        at scala.Option.map(Option.scala:145)
        at org.apache.spark.examples.mllib.DenseKMeans$.main(DenseKMeans.scala:71)
        at org.apache.spark.examples.mllib.DenseKMeans.main(DenseKMeans.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:303)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
",,mengxr,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1215,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,403494,,,2014-07-03 17:07:02.0,,,,,0|i1xg1j:,403539,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
loadLibSVMFile doesn't handle regression datasets,SPARK-2341,12724894,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,eustache,eustache,02/Jul/14 07:21,15/Jan/15 09:08,15/Aug/18 23:03,31/Jul/14 00:35,1.0.0,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,easyfix,,,,"Many datasets exist in LibSVM format for regression tasks [1] but currently the loadLibSVMFile primitive doesn't handle regression datasets.

More precisely, the LabelParser is either a MulticlassLabelParser or a BinaryLabelParser. What happens then is that the file is loaded but in multiclass mode : each target value is interpreted as a class name !

The fix would be to write a RegressionLabelParser which converts target values to Double and plug it into the loadLibSVMFile routine.

[1] http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html ",,apachespark,eustache,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-07-02 08:23:08.113,,false,,,,,,,,,,,,,403077,,,Thu Jul 31 00:35:14 UTC 2014,,,,,0|i1xdiv:,403132,,,,,,1.1.0,,,,,,,02/Jul/14 08:23;mengxr;Just set `multiclass = true` to load double values.,"02/Jul/14 08:49;eustache;I see that LabelParser with multiclass=true works for the regression
setting.

What I fail to understand is how it is related to multiclass ? Is the
naming proper ?

In any case shouldn't we provide a naming that explicitly mentions
regression ?




","02/Jul/14 09:07;mengxr;It is a little awkward to have both `regression` and `multiclass` as input arguments. I agree that a correct name should be `multiclassOrRegression` or `multiclassOrContinuous`. But it is certainly too long. We tried to make this clear in the doc:

{code}
multiclass: whether the input labels contain more than two classes. If false, any label with value greater than 0.5 will be mapped to 1.0, or 0.0 otherwise. So it works for both +1/-1 and 1/0 cases. If true, the double value parsed directly from the label string will be used as the label value.
{code}

It would be good if we can improve the documentation to make it clearer. But for the API, I don't feel that it is necessary to change.
","02/Jul/14 09:33;eustache;Ok then would you mind that I work on a doc improvement for this ?

Perhaps a simple no-brainer like ""for regression set this to true"" could do
the job...

Personally I think `multiclassOrRegression` is a good option but I let it
to you to decide :)
","02/Jul/14 14:17;srowen;I've been a bit uncomfortable with how the MLlib API conflates categorical values and numbers, since they aren't numbers in general. Treating them as numbers is a convenience in some cases, and common in papers, but feels like suboptimal software design -- should a user have to convert categoricals to some numeric representation? To me it invites confusion, and this is one symptom. So I am not sure ""multiclass"" should mean ""parse target as double"" to begin with?

OK, it's not the issue here. But we're on the subject of an experimental API subject to change with an example of something related that could be improved along the way, and it's my #1 wish for MLlib at the moment. I'd really like to work on a change to try to accommodate classes as, say, strings at least, and not presume doubles. But I am trying to figure out if anyone agrees with that. ","03/Jul/14 07:39;mengxr;[~srowen] Instead of taking string labels directly, we can provide tools to convert them to integer labels (still Double typed). LIBLINEAR/LIBSVM do not support string labels either, but they are still among the top choices for logistic regression and SVM.

[~eustache] Unfortunately, the argument name in Scala is part of the API and loadLibSVMFile is not marked as experimental. So we cannot update the argument name to `multiclassOrRegression`, which is too long anyway. Could you update the doc and change the first sentence from ""multiclass: whether the input labels contain more than two classes"" to ""multiclass: whether the input labels are continuous-valued (for regression) or contain more than two classes""? ","03/Jul/14 08:42;srowen;[~mengxr] For regression, rather than further overloading ""multiclass"" to mean ""regression"", how about modifying the argument to take on three values (as an enum, string, etc.) to distinguish the three modes. The current method would stay, but be deprecated.

multiclass=false is for binary classification. libsvm uses ""0"" and ""1"" (or any ints) for binary classification. But this parses it as a real number, and rounds to 0/1. (Is that was libsvm does?) Maybe it's a convenient semantic overload when you want to transform a continuous value to a 0/1 indicator, but is that implied by libsvm format or just a transformation the caller should make? multiclass=true treats libsvm integer labels as doubles, but not continuous values. It seems like inviting more confusion to have this mode also double as the mode for parsing labels that are continuous values as continuous values.

libsvm is widely used but it's old; I don't think it's file format from long ago should necessarily inform API design now. There are other serializations besides libsvm (plain CSV for instance) and other algorithms (random decision forests).

You can make utilities to convert classes to numbers for benefit of the implementation on the front, and I'll have to in order to use this. Maybe we can start there -- at least if a utility is in the project people aren't all reinventing this in order to use an SVM with actual labels. The caller carries around a dictionary then to do the reverse mapping. The model seems like the place to hold that info, if in fact internally it converts classes to some other representation. Maybe the need would be clearer once the utility is created.

As you say I'm concerned that the API is already locked down early and some of these changes are going to be viewed as infeasible just for that reason.","16/Jul/14 05:04;mengxr;[~srowen] Using enum or string sounds good. As you already knew, using string may be better because of Python.

Rounding is used because people use either +1/-1 or 1/0 for binary classification in LIBSVM and we require 1/0 in MLlib. Actually the +1/-1 is the only corner case I wanted to cover when multiclass=false. We added LIBSVM support because there are many commonly used datasets we can download from LIBSVM/LIBLINEAR website and other places. It is easier for people to test MLlib's algorithms.

It would be nice if you have free cycles to implement a method that convert classes to numbers. For the long term, I'm thinking about for each dataset, we can attach metadata that contains feature names, feature types, number of non-zeros, and for every categorical feature we have a value <-> {0, 1, ...} map.","16/Jul/14 12:12;srowen;OK is it worth a pull request for changing the boolean multiclass argument to a string? I wanted to ask if that was your intent before I do that.

libsvm format support is certainly important. It happens to have to encode non-numeric input as numbers. It need not be that way throughout MLlib, since it isn't that way in other input formats. (In this API method, it's pretty minor, since libsvm does by definition use this encoding.) So yes that would be great if data sets or API objects didn't assume that categorical data was numeric, but encoded type in the data set or even in the object model itself. I think it's mostly a design and type-safety argument -- same reason we have String instead of just byte[] everywhere.

Sure I will have to build this conversion at some point anyway and can share the result then.","18/Jul/14 11:25;srowen;[~mengxr] Here is an example of changing the argument:
https://github.com/srowen/spark/commit/4a584ff9c0ada3d035d4668ecf22ec0e65ed16b6

I won't open a PR yet. I think this is a better API at this point, but the question is more whether the weight of deprecated methods are worth it or not. Another data point to keep in mind regarding how APIs can evolve.","29/Jul/14 17:26;mengxr;[~srowen] For the doc in your version:

{code}
If ""multiclass"", the numeric value parsed directly from the label string will be used as the label value.
If ""continuous"", the double value parsed directly from the string will be used as the label.
{code}

Would user feel confused since the two lines are essentially the same?

Another possible solution is that we parse the labels into doubles and remove the `multiclass` argument. Users can perform a map to transform the labels into binary 0/1 if needed.","29/Jul/14 17:33;srowen;To me, it's less confusing than writing ""multiclass"" for a regression problem. Yes I also think it could be simpler to remove multiclass; the idea I suppose is that binary is merely a special case of that, and the caller can write the required transformation to 0/1 if needed. At least the caller is aware of the transformation and I think that's good. At least, there you just let numbers be numbers and let downstream code figure out whether the number is a continuous value, or the number is a category.",30/Jul/14 05:48;mengxr;That sounds good. Do you mind creating a PR? We can deprecate the existing ones with `multiclass` and add a warning in the doc about the +1/-1 case.,"30/Jul/14 18:26;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1663","31/Jul/14 00:35;mengxr;Issue resolved by pull request 1663
[https://github.com/apache/spark/pull/1663]",,,,,,,,,
The algorithm of ALS in mlib lacks a parameter ,SPARK-2257,12723339,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,bing,bing,24/Jun/14 12:01,24/Jun/14 12:17,15/Aug/18 23:03,24/Jun/14 12:17,1.0.0,,,,,,,,,,,,,,,1.0.1,1.1.0,,,,,MLlib,,,,,0,patch,,,,"When I test ALS algorithm using netflix data, I find I cannot get the acurate results declared by the paper. The best  MSE value is 0.9066300038109709(RMSE 0.952), which is worse than the paper's result. If I increase the number of features or the number of iterations, I will get a worse result. After I studing the paper and source code, I find a bug in the updateBlock function of ALS.

orgin code is:
    while (i < rank) {
        // ---
       fullXtX.data(i * rank + i) += lambda

        i += 1
      }

The code doesn't consider the number of products that one user rates. So this code should be modified:
    while (i < rank) {
 
        //ratingsNum(index) equals the number of products that a user rates
        fullXtX.data(i * rank + i) += lambda * ratingsNum(index)
        i += 1
      } 

After I modify code, the MSE value has been decreased, this is one test result
conditions:
val numIterations =20
val features = 30
val model = ALS.train(trainRatings,features, numIterations, 0.06)

result of modified version:
MSE: Double = 0.8472313396478773
RMSE: 0.92045


results of version of 1.0
MSE: Double = 1.2680743123043832
RMSE: 1.1261

In order to add the vector ratingsNum, I want to change the InLinkBlock structure as follows:
private[recommendation] case class InLinkBlock(elementIds: Array[Int], ratingsNum:Array[Int], ratingsForBlock: Array[Array[(Array[Int], Array[Double])]])
So I could calculte the vector ratingsNum in the function of makeInLinkBlock. This is the code I add in the makeInLinkBlock:

........... 
//added 
  val ratingsNum = new Array[Int](numUsers)
   ratings.map(r => ratingsNum(userIdToPos(r.user)) += 1)
//end of added
  InLinkBlock(userIds, ratingsNum, ratingsForBlock)
........


Is this solution reasonable??",spark 1.0,bing,mengxr,,,,,,,,,,,,,,,,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-06-24 12:13:09.688,,false,,,,,,,,,,,,,401526,,,Tue Jun 24 12:16:56 UTC 2014,,,,,0|i1x43z:,401599,,,,,,1.0.0,,,,,,,"24/Jun/14 12:13;srowen;I don't think this is a bug, in the sense that it is just a different formulation of ALS. It's in the ALS-WR paper, but not the more well-known Hu/Koren/Volinsky paper. 

This is ""weighted regularization"" and it does help in some cases. In fact, it's already implemented in MLlib, although went in just after 1.0.0:

https://github.com/apache/spark/commit/a6e0afdcf0174425e8a6ff20b2bc2e3a7a374f19#diff-2b593e0b4bd6eddab37f04968baa826c

I think this is therefore already implemented.",24/Jun/14 12:16;bing;I find it and I will close this jira,,,,,,,,,,,,,,,,,,,,,,
MLLib Naive Bayes Example SparkException: Can only zip RDDs with same number of elements in each partition,SPARK-2251,12723262,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mengxr,xiejuncs,xiejuncs,24/Jun/14 02:46,11/Oct/15 18:22,15/Aug/18 23:03,27/Jun/14 04:47,1.0.0,,,,,,,,,,,,,,,1.0.1,1.1.0,,,,,MLlib,,,,,0,Naive-Bayes,,,,"I follow the exact code from Naive Bayes Example (http://spark.apache.org/docs/latest/mllib-naive-bayes.html) of MLLib.

When I executed the final command: 
val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()

It complains ""Can only zip RDDs with same number of elements in each partition"".

I got the following exception:
{code}
14/06/23 19:39:23 INFO SparkContext: Starting job: count at <console>:31
14/06/23 19:39:23 INFO DAGScheduler: Got job 3 (count at <console>:31) with 2 output partitions (allowLocal=false)
14/06/23 19:39:23 INFO DAGScheduler: Final stage: Stage 4(count at <console>:31)
14/06/23 19:39:23 INFO DAGScheduler: Parents of final stage: List()
14/06/23 19:39:23 INFO DAGScheduler: Missing parents: List()
14/06/23 19:39:23 INFO DAGScheduler: Submitting Stage 4 (FilteredRDD[14] at filter at <console>:31), which has no missing parents
14/06/23 19:39:23 INFO DAGScheduler: Submitting 2 missing tasks from Stage 4 (FilteredRDD[14] at filter at <console>:31)
14/06/23 19:39:23 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
14/06/23 19:39:23 INFO TaskSetManager: Starting task 4.0:0 as TID 8 on executor localhost: localhost (PROCESS_LOCAL)
14/06/23 19:39:23 INFO TaskSetManager: Serialized task 4.0:0 as 3410 bytes in 0 ms
14/06/23 19:39:23 INFO TaskSetManager: Starting task 4.0:1 as TID 9 on executor localhost: localhost (PROCESS_LOCAL)
14/06/23 19:39:23 INFO TaskSetManager: Serialized task 4.0:1 as 3410 bytes in 1 ms
14/06/23 19:39:23 INFO Executor: Running task ID 8
14/06/23 19:39:23 INFO Executor: Running task ID 9
14/06/23 19:39:23 INFO BlockManager: Found block broadcast_0 locally
14/06/23 19:39:23 INFO BlockManager: Found block broadcast_0 locally
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:0+24
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:24+24
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:0+24
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:24+24
14/06/23 19:39:23 ERROR Executor: Exception in task ID 9
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
14/06/23 19:39:23 ERROR Executor: Exception in task ID 8
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
14/06/23 19:39:23 WARN TaskSetManager: Lost TID 8 (task 4.0:0)
14/06/23 19:39:23 WARN TaskSetManager: Loss was due to org.apache.spark.SparkException
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
14/06/23 19:39:23 ERROR TaskSetManager: Task 4.0:0 failed 1 times; aborting job
14/06/23 19:39:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
14/06/23 19:39:23 INFO DAGScheduler: Failed to run count at <console>:31
14/06/23 19:39:23 INFO TaskSetManager: Loss was due to org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition [duplicate 1]
14/06/23 19:39:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
14/06/23 19:39:23 INFO TaskSchedulerImpl: Cancelling stage 4
org.apache.spark.SparkException: Job aborted due to stage failure: Task 4.0:0 failed 1 times, most recent failure: Exception failure in TID 8 on host localhost: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
        org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
        scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
        org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
        org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
        org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:724)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1038)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1022)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1020)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1020)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:638)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:638)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:638)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1212)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}","OS: Fedora Linux
Spark Version: 1.0.0. Git clone from the Spark Repository",apachespark,mengxr,pwendell,xiejuncs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-06-24 21:43:49.563,,false,,,,,,,,,,,,,401449,,,Sun Oct 11 18:22:02 UTC 2015,,,,,0|i1x3mv:,401522,,,,,,,,,,,,,"24/Jun/14 21:43;srowen;For what it's worth, I can reproduce this. In the sample, the ""test"" RDD has 2 partitions, containing 2 and 1 examples. The ""prediction"" RDD has 2 partitions, containing 1 and 2 examples respectively. So they aren't matched up, even though one is a 1-1 map() of the other. 

That seems like it shouldn't happen? maybe someone more knowledgeable can say whether that itself should occur. ""test"" is a PartitionwiseSampledRDD and ""prediction"" is a MappedRDD of course. 

If it is allowed to happen, then the example should be fixed, and I could easily supply a patch. It can be done without having to zip up RDDs to begin with.","25/Jun/14 05:27;xiejuncs;Hi, Sean. Thanks very much for your insight. I am new to Spark. So if you can easily supply a patch. Please. Really appreciate it.

I am digging it according to your suggestion to see what is going on. At the same time, familiar myself with Spark. ","25/Jun/14 08:07;srowen;Well the change to the examples is pretty straightforward. Instead of separately computing ""predictions"", you just:

{code}
val predictionAndLabel = test.map(x => (model.predict(x.features), x.label))
{code}

... and similarly for other languages, and other examples. In fact it seems more straightforward.

But I am wondering if this is actually a bug in PartitionwiseSampledRDD. [~mengxr] is this a bit of code you wrote or are familiar with?",26/Jun/14 06:47;mengxr;[~xiejuncs] Are you running the example on the latest master or v1.0.0? I tested it on v1.0.0 and it worked well. But it did fail in the latest master. `RDD.zip` was modified after v1.0.0. So could you confirm the version you are running? ,26/Jun/14 09:05;mengxr;Found a bug introduced by me in random sampler. PR: https://github.com/apache/spark/pull/1229,"26/Jun/14 20:33;pwendell;This is fixed in 1.0.1 via:
https://github.com/apache/spark/pull/1234/files","27/Jun/14 02:49;xiejuncs;I use the following command:

git log

The first entry is 
commit 601032f5bfe2dcdc240bfcc553f401e6facbf5ec
Author: Zongheng Yang <zongheng.y@gmail.com>
Date:   Tue Jun 10 21:59:01 2014 -0700

How to find out the current version of my branch? My current branch is in master, and I add Apache/Spark as the remote upstream.

Jun","27/Jun/14 04:47;pwendell;Issue resolved by pull request 1229
[https://github.com/apache/spark/pull/1229]","11/Oct/15 18:22;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/1229",,,,,,,,,,,,,,,
breeze DenseVector not serializable with KryoSerializer,SPARK-2200,12722514,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Later,,sinisa_lyh,sinisa_lyh,19/Jun/14 15:25,10/Jul/14 01:42,15/Aug/18 23:03,10/Jul/14 01:42,1.0.0,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"Spark 1.0.0 depends on breeze 0.7 and for some reason serializing DenseVector with KryoSerializer throws the following stack trace. Looks like some recursive field in the object. Upgrading to 0.8.1 solved this.
{code}
java.lang.StackOverflowError
	at java.lang.reflect.Field.getDeclaringClass(Field.java:154)
	at sun.reflect.UnsafeFieldAccessorImpl.ensureObj(UnsafeFieldAccessorImpl.java:54)
	at sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl.get(UnsafeQualifiedObjectFieldAccessorImpl.java:38)
	at java.lang.reflect.Field.get(Field.java:379)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:552)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
...
{code}

Code to reproduce:
{code}
import breeze.linalg.DenseVector
import org.apache.spark.SparkConf
import org.apache.spark.serializer.KryoSerializer

object SerializerTest {
  def main(args: Array[String]) {
    val conf = new SparkConf()
      .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
      .set(""spark.kryo.registrator"", classOf[MyRegistrator].getName)
      .set(""spark.kryo.referenceTracking"", ""false"")
      .set(""spark.kryoserializer.buffer.mb"", ""8"")

    val serializer = new KryoSerializer(conf).newInstance()
    serializer.serialize(DenseVector.rand(10))
  }
}
{code}",,mengxr,sinisa_lyh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1997,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-06-19 18:31:47.963,,false,,,,,,,,,,,,,400705,,,Thu Jul 10 01:42:57 UTC 2014,,,,,0|i1wz6n:,400799,,,,,,,,,,,,,19/Jun/14 15:26;sinisa_lyh;https://github.com/apache/spark/pull/940 addresses this.,"19/Jun/14 18:31;mengxr;[~neville] Do you know the root cause and how this is fixed in breeze 0.8.1? You disabled reference tracking, which may be the reason.","20/Jun/14 01:23;sinisa_lyh;With 0.7 the error went away when reference tracking is set to true.
With 0.8.1 it works either way.

Turns out in 0.7 the recursive references was caused by this:
{code}
  private final val innerUpdate: ((Int,E) => Unit) = if ((offset == 0) && (stride == 1)) { (i:Int,v:E) => {data(i) = v} } else {(i:Int,v:E) => {data(offset+i*stride)=v}  }
{code}

The function val has an closure $outer that references itself. It was removed in 0.8.1.",10/Jul/14 01:42;mengxr;Close this JIRA since this is caused by a non-default kryo setting.,,,,,,,,,,,,,,,,,,,,
Spark invoke DecisionTree by Java,SPARK-2197,12722462,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,onepiece,onepiece,19/Jun/14 10:16,03/Aug/14 17:38,15/Aug/18 23:03,03/Aug/14 17:38,,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,"Strategy strategy = new Strategy(Algo.Classification(), new Impurity() {
			@Override
			public double calculate(double arg0, double arg1, double arg2) {
				return Gini.calculate(arg0, arg1, arg2);
			}

			@Override
			public double calculate(double arg0, double arg1) {
				return Gini.calculate(arg0, arg1);
			}
		}, 5, 100, QuantileStrategy.Sort(), null, 256);
		DecisionTree decisionTree = new DecisionTree(strategy);
		final DecisionTreeModel decisionTreeModel = decisionTree.train(labeledPoints.rdd());

i try to run it on spark, but find an error on the console:
java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lorg.apache.spark.mllib.regression.LabeledPoint;
	at org.apache.spark.mllib.tree.DecisionTree$.findSplitsBins(DecisionTree.scala:990)
	at org.apache.spark.mllib.tree.DecisionTree.train(DecisionTree.scala:56)
	at org.project.modules.spark.java.SparkDecisionTree.main(SparkDecisionTree.java:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

i view source code, find  
val numFeatures = input.take(1)(0).features.size
this is a problem.",,apachespark,josephkb,joshrosen,mengxr,onepiece,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2478,,,,SPARK-2737,,,,,,0.0,,,,,,,,,,,,,,,,2014-07-30 00:17:51.49,,false,,,,,,,,,,,,,400653,,,Sun Aug 03 17:38:24 UTC 2014,,,,,0|i1wyv3:,400747,,,,,,1.1.0,,,,,,,30/Jul/14 00:17;josephkb;This error is at least partly caused by issues with collect() inside DecisionTree: https://issues.apache.org/jira/browse/SPARK-2737,"02/Aug/14 20:51;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/1740","03/Aug/14 17:38;mengxr;Issue resolved by pull request 1740
[https://github.com/apache/spark/pull/1740]",,,,,,,,,,,,,,,,,,,,,
Examples Data Not in Binary Distribution,SPARK-2192,12722429,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,cheffpj,cheffpj,19/Jun/14 07:28,01/Dec/14 08:34,15/Aug/18 23:03,01/Dec/14 08:32,1.0.0,,,,,,,,,,,,,,,1.2.0,,,,,,Build,MLlib,,,,0,,,,,The data used by examples is not packaged up with the binary distribution. The data subdirectory of spark should make it's way in to the distribution somewhere so the examples can use it.,,apachespark,cheffpj,hsaputra,mengxr,pat.mcdonough@databricks.com,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-06-20 01:06:56.141,,false,,,,,,,,,,,,,400620,,,Mon Dec 01 08:32:45 UTC 2014,,,,,0|i1wynr:,400714,,,,,,,,,,,,,19/Jun/14 07:31;cheffpj;[~pacoid] - thanks for pointing this out. I guess we'll have to fall back to using the data from src,20/Jun/14 01:06;pwendell;It might be good to have all the example data in src/main/resources.,25/Jun/14 01:35;hsaputra;I think several examples already have the data in the main/resources. Do you have list of which ones missing?,"25/Jun/14 18:06;pat.mcdonough@databricks.com;Based on a very quick and not thorough search, the only mention I found of
those files came in the docs (bagel-programming-guide.md -->
pagerank_data.txt). But you'll also note that SparkKMeans and SparkPageRank
seem to work with those files.


On Wed, Jun 25, 2014 at 10:52 AM, Henry Saputra (JIRA) <jira@apache.org>

","25/Nov/14 11:48;srowen;Data files are now consolidated under ""data/"", and they are not in the binary distribution. It would be easy to add them, and seems like a reasonable thing to do. However, I'm not clear all of those data files can be distributed; MovieLens data for example isn't supposed to be AFAIK. In fact, I'm not clear it should be in the Spark repo even.

Any support for me adding this to the distro, but removing examples based on things like Movielens that shouldn't be redistributed?","25/Nov/14 19:10;cheffpj;[~srowen] - I fully support that and agree that Movielens needs to be removed (unless the Spark project was granted permission to re-host it, which is very possible, but should probably be noted in a readme or license file).","26/Nov/14 13:07;srowen;Oops, on further inspection I see that the file is not Movielens data, but merely in the same format. The comments do say this in MovieLensALS.scala. I'll cook up a PR to add the example data to the distro.","26/Nov/14 13:34;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3480","26/Nov/14 22:04;cheffpj;Thanks [~srowen]. And yes, Xiangrui confirmed he just generated the data.","01/Dec/14 08:32;mengxr;Issue resolved by pull request 3480
[https://github.com/apache/spark/pull/3480]",,,,,,,,,,,,,,
PySpark cannot import mllib modules in YARN-client mode,SPARK-2172,12721809,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,frol,frol,17/Jun/14 22:54,25/Aug/14 06:05,15/Aug/18 23:03,26/Jun/14 05:04,1.0.0,1.1.0,,,,,,,,,,,,,,1.0.1,1.1.0,,,,,MLlib,PySpark,Spark Core,YARN,,1,mllib,python,,,"Here is the simple reproduce code:

{noformat}
$ HADOOP_CONF_DIR=/etc/hadoop/conf MASTER=yarn-client ./bin/pyspark
{noformat}

{code:title=issue.py|borderStyle=solid}
>>> from pyspark.mllib.regression import LabeledPoint

>>> sc.parallelize([1,2,3]).map(lambda x: LabeledPoint(1, [2])).count()
{code}

Note: The same issue occurs with .collect() instead of .count()

{code:title=TraceBack|borderStyle=solid}
Py4JJavaError: An error occurred while calling o110.collect.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8.0:0 failed 4 times, most recent failure: Exception failure in TID 52 on host ares: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/mnt/storage/bigisle/yarn/1/yarn/local/usercache/blb/filecache/18/spark-assembly-1.0.0-hadoop2.2.0.jar/pyspark/worker.py"", line 73, in main
    command = pickleSer._read_with_length(infile)
  File ""/mnt/storage/bigisle/yarn/1/yarn/local/usercache/blb/filecache/18/spark-assembly-1.0.0-hadoop2.2.0.jar/pyspark/serializers.py"", line 146, in _read_with_length
    return self.loads(obj)
ImportError: No module named mllib.regression

        org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:115)
        org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:145)
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:78)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

However, this code works as expected:

{code:title=noissue.py|borderStyle=solid}
>>> from pyspark.mllib.regression import LabeledPoint

>>> sc.parallelize([1,2,3]).map(lambda x: LabeledPoint(1, [2])).first()
>>> sc.parallelize([1,2,3]).map(lambda x: LabeledPoint(1, [2])).take(3)
{code}","Ubuntu 14.04
Java 7
Python 2.7
CDH 5.0.2 (Hadoop 2.3.0): HDFS, YARN
Spark 1.0.0 and git master",frol,joao,mengxr,piotrszul,shubhamc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-06-25 23:56:45.533,,false,,,,,,,,,,,,,400005,,,Mon Aug 25 06:05:12 UTC 2014,,,,,0|i1wuxj:,400107,,,,,,,,,,,,,"18/Jun/14 02:34;frol;I've tried to run the code in standalone and local modes. There is no such error, but I want to exercise YARN.
I've also tried to run similar code in spark-shell (Scala) and it does well:

{code}
scala> import org.apache.spark.mllib.regression.LabeledPoint
scala> import org.apache.spark.mllib.linalg.{Vector, Vectors}
scala> val array: Array[Double] = Array(1, 2)
scala> val vector: Vector = Vectors.dense(array)
scala> sc.parallelize(1 to 3).map(x => LabeledPoint(x, vector)).collect()
res2: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array(LabeledPoint(1.0, [1.0,2.0]), LabeledPoint(2.0, [1.0,2.0]), LabeledPoint(3.0, [1.0,2.0]))
{code}","25/Jun/14 23:56;piotrszul;I got the same problem while runing the kmeans.py from examples, i.e.:
{noformat}
$ spark-submit --master yarn-client examples/src/main/python/mllib/kmeans.py kmeans_data.txt 3
Spark assembly has been built with Hive, including Datanucleus jars on classpath
--args is deprecated. Use --arg instead.
14/06/26 09:52:59 WARN TaskSetManager: Lost TID 0 (task 0.0:0)
14/06/26 09:52:59 WARN TaskSetManager: Loss was due to org.apache.spark.api.python.PythonException
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/data/02/yarn/local/usercache/szu004/filecache/45/spark-assembly-1.0.0-hadoop2.2.0.jar/pyspark/worker.py"", line 73, in main
    command = pickleSer._read_with_length(infile)
  File ""/data/02/yarn/local/usercache/szu004/filecache/45/spark-assembly-1.0.0-hadoop2.2.0.jar/pyspark/serializers.py"", line 146, in _read_with_length
    return self.loads(obj)
ImportError: No module named mllib._common
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:115)
	at org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:78)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
{noformat}","26/Jun/14 00:01;piotrszul;I believe the problem is that the spark-assembly-1.0.0-xxx.jar does not include the pyspark/mllib package 
(only the pyspark python code is included).
It works in the local mode becasue then $SPARK_HOME/python is on PYTHONPATH and it has acces to both pyspark and pyspark/mllib modules.

To fix the assembly should include  pyspark/mllib 
",26/Jun/14 05:04;mengxr;Fixed in https://github.com/apache/spark/pull/1223 by [~piotrszul] .,25/Aug/14 06:05;joao;[~piotrszul] For the fix there is a workaround that I can use in my python script ?,,,,,,,,,,,,,,,,,,,
error of  Decision tree algorithm  in Spark MLlib ,SPARK-2160,12721563,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,mathcao,mathcao,17/Jun/14 00:54,08/Jul/14 20:26,15/Aug/18 23:03,08/Jul/14 20:26,1.0.0,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,patch,,,,"the error of comput rightNodeAgg about  Decision tree algorithm  in Spark MLlib  , in the function extractLeftRightNodeAggregates() ,when compute rightNodeAgg  used bindata index is error. in the DecisionTree.scala file about  Line980:

             rightNodeAgg(featureIndex)(2 * (numBins - 2 - splitIndex)) =
                binData(shift + (2 * (numBins - 2 - splitIndex))) +
                  rightNodeAgg(featureIndex)(2 * (numBins - 1 - splitIndex))    

 the   binData(shift + (2 * (numBins - 2 - splitIndex)))  index compute is error, so the result of rightNodeAgg  include  repeated data about ""bins""  ",,jsondag,mathcao,mengxr,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,SPARK-2152,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-06-17 08:25:17.232,,false,,,,,,,,,,,,,399759,,,Mon Jul 07 17:59:15 UTC 2014,,,,,0|i1wtfz:,399867,,,,,,,,,,,,,17/Jun/14 08:25;srowen;You already added this as https://issues.apache.org/jira/browse/SPARK-2152 right?,07/Jul/14 17:59;jsondag;https://github.com/apache/spark/pull/1316 (also resolves SPARK-2152),,,,,,,,,,,,,,,,,,,,,,
Not fully cached when there is enough memory,SPARK-2120,12720771,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,coderxiang,coderxiang,12/Jun/14 01:39,12/Jun/14 01:41,15/Aug/18 23:03,12/Jun/14 01:41,1.0.0,,,,,,,,,,,,,,,,,,,,,Block Manager,MLlib,Spark Core,,,0,,,,,"While factorizing a large matrix using the latest Alternating Least Squares (ALS) in mllib, from sparkUI it looks like that spark fail to cache all the partitions of some RDD while memory is sufficient. Please find [this post](http://apache-spark-user-list.1001560.n3.nabble.com/Not-fully-cached-when-there-is-enough-memory-tt7429.html) for screenshots. This may cause subsequent job failures while executing `userOut.Count()` or `productsOut.count`.",,coderxiang,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,398970,,,2014-06-12 01:39:48.0,,,,,0|i1wonr:,399087,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark/mllib is not compatible with numpy-1.4,SPARK-2091,12720283,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,10/Jun/14 06:30,11/Jun/14 07:54,15/Aug/18 23:03,11/Jun/14 07:54,1.0.0,,,,,,,,,,,,,,,1.0.1,,,,,,MLlib,PySpark,,,,0,,,,,"pyspark/mllib is not compatible with numpy 1.4. If the required changes are small, we should support numpy 1.4.",,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,398482,,,Tue Jun 10 08:18:33 UTC 2014,,,,,0|i1wlqn:,398607,,,,,,1.0.1,,,,,,,10/Jun/14 08:18;mengxr;PR: https://github.com/apache/spark/pull/1035,,,,,,,,,,,,,,,,,,,,,,,
mutable.BitSet in ALS not serializable with KryoSerializer,SPARK-1977,12717622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,sinisa_lyh,sinisa_lyh,30/May/14 18:58,14/Nov/14 06:30,15/Aug/18 23:03,07/Jul/14 22:08,1.0.0,,,,,,,,,,,,,,,1.0.2,1.1.0,,,,,MLlib,,,,,2,,,,,"OutLinkBlock in ALS.scala has an Array[mutable.BitSet] member.
KryoSerializer uses AllScalaRegistrar from Twitter chill but it doesn't register mutable.BitSet.

Right now we have to register mutable.BitSet manually. A proper fix would be using immutable.BitSet in ALS or register mutable.BitSet in upstream chill.

{code}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1724.0:9 failed 4 times, most recent failure: Exception failure in TID 68548 on host lon4-hadoopslave-b232.lon4.spotify.net: com.esotericsoftware.kryo.KryoException: java.lang.ArrayStoreException: scala.collection.mutable.HashSet
Serialization trace:
shouldSend (org.apache.spark.mllib.recommendation.OutLinkBlock)
        com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
        com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:43)
        com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:34)
        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
        org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:115)
        org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:125)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:155)
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:77)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:227)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        java.lang.Thread.run(Thread.java:662)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,apachespark,ceys,coderxiang,gen,huasanyelao,ilganeli,mengxr,sinisa_lyh,smolav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-06-04 05:54:56.168,,false,,,,,,,,,,,,,395826,,,Fri Nov 14 06:30:08 UTC 2014,,,,,0|i1w5br:,395943,,,,,,,,,,,,,"04/Jun/14 05:54;mengxr;I cannot reproduce this error in v1.0.0. There is an example called `MovieLensALS.scala` under `examples/`, which runs fine with kryo enabled. Did you include other dependencies in your application?","04/Jun/14 07:16;sinisa_lyh;Yeah that example worked fine for me in standalone mode but failed in YARN cluster mode with the same error.
Maybe serialization wasn't needed/triggered in standalone mode?","04/Jun/14 08:07;mengxr;This is more likely a version conflict in your dependencies. From the Spark WebUI, you can find the system classpath in the environment tab. Please verify that you don't have two different versions of spark, kryo, or any other related library. Classes may hide inside an assembly jar.","04/Jun/14 08:44;sinisa_lyh;We submit 1 spark-assembly and 1 job assembly jar via spark-submit and there are no other obvious scala/spark/kryo jars in the global classpath. I can reproduce the same exception locally with the following snippet, when kryo.register() is commented out.

I just added mutable BitSet to Twitter chill: https://github.com/twitter/chill/pull/185

{code}
import com.twitter.chill._
import org.apache.spark.serializer.{KryoSerializer, KryoRegistrator}
import org.apache.spark.SparkConf
import scala.collection.mutable

class MyRegistrator extends KryoRegistrator {
  override def registerClasses(kryo: Kryo) {
    // kryo.register(classOf[mutable.BitSet])
  }
}

case class OutLinkBlock(elementIds: Array[Int], shouldSend: Array[mutable.BitSet])

object KryoTest {
  def main(args: Array[String]) {
    println(""hello"")
    val conf = new SparkConf()
      .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
      .set(""spark.kryo.registrator"", classOf[MyRegistrator].getName)
    val serializer = new KryoSerializer(conf).newInstance()

    val bytes = serializer.serialize(OutLinkBlock(Array(1, 2, 3), Array(mutable.BitSet(2, 4, 6))))
    serializer.deserialize(bytes).asInstanceOf[OutLinkBlock]
  }
}
{code}","04/Jun/14 20:50;mengxr;In our example code, we only register `Rating` and it works. Could you try adding the following:

{code}
kryo.register(classOf[Rating])
{code}

I need to reproduce this problem with `ALS.train`.","04/Jun/14 20:53;sinisa_lyh;We are already doing that :)
Our job works on YARN with ""register(classOf[mutable.BitSet])"". Without it we get the reported exception.",04/Jun/14 21:43;mengxr;Did you register `Rating`? I think this is necessary.,"04/Jun/14 21:56;sinisa_lyh;Yes we did register 'Rating'. And we had to ""register(classOf[mutable.BitSet])"" in addition to make it work.","04/Jun/14 23:25;coderxiang;Hi [~neville], I just run the MovieLens example on my YARN cluster (hadoop-2.0.5-alpha) with kryo enabled and it works. I use the following command:

bin/spark-submit --master yarn-cluster  --class org.apache.spark.examples.mllib.MovieLensALS  --num-executors ** --driver-memory ** --executor-memory ** --executor-cores 1  spark-examples-1.0.0-hadoop2.0.5-alpha.jar  --rank 5 --numIterations 20 --lambda 1.0 --kryo /path/to/sample_movielens_data.txt","05/Jun/14 01:04;sinisa_lyh;Our YARN cluster runs 2.2.0. We built spark-assembly and spark-examples jars with 1.0.0 release source and the bundled make_distribution.sh. And here's my command:

{code}
spark-submit --master yarn-cluster --class org.apache.spark.examples.mllib.MovieLensALS --num-executors 2 --executor-memory 2g --driver-memory 2g dist/lib/spark-examples-1.0.0-hadoop2.2.0.jar --kryo --implicitPrefs sample_movielens_data.txt
{code}

Here's a complete list of classpath from the environment tab.
{code}
/etc/hadoop/conf
/usr/lib/hadoop-hdfs/hadoop-hdfs-2.2.0.2.0.6.0-76-tests.jar
/usr/lib/hadoop-hdfs/hadoop-hdfs-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-hdfs/hadoop-hdfs-nfs-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-hdfs/lib/asm-3.2.jar
/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar
/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar
/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar
/usr/lib/hadoop-hdfs/lib/commons-el-1.0.jar
/usr/lib/hadoop-hdfs/lib/commons-io-2.1.jar
/usr/lib/hadoop-hdfs/lib/commons-lang-2.5.jar
/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.1.jar
/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar
/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.8.8.jar
/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.8.8.jar
/usr/lib/hadoop-hdfs/lib/jasper-runtime-5.5.23.jar
/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar
/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar
/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.jar
/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.jar
/usr/lib/hadoop-hdfs/lib/jsp-api-2.1.jar
/usr/lib/hadoop-hdfs/lib/jsr305-1.3.9.jar
/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar
/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar
/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar
/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar
/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar
/usr/lib/hadoop-mapreduce/hadoop-archives-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-datajoin-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-distcp-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-extras-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-gridmix-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-app-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-common-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-core-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-hs-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-hs-plugins-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.2.0.2.0.6.0-76-tests.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-shuffle-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-rumen-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/hadoop-streaming-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar
/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar
/usr/lib/hadoop-mapreduce/lib/avro-1.7.4.jar
/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar
/usr/lib/hadoop-mapreduce/lib/commons-io-2.1.jar
/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar
/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar
/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.1.jar
/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.8.8.jar
/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.8.8.jar
/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar
/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar
/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar
/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar
/usr/lib/hadoop-mapreduce/lib/junit-4.10.jar
/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar
/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar
/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar
/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar
/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.4.1.jar
/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar
/usr/lib/hadoop-yarn/hadoop-yarn-api-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-applications-distributedshell-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-client-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-common-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-server-common-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-server-nodemanager-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-server-resourcemanager-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-server-tests-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-server-web-proxy-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/hadoop-yarn-site-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar
/usr/lib/hadoop-yarn/lib/asm-3.2.jar
/usr/lib/hadoop-yarn/lib/avro-1.7.4.jar
/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar
/usr/lib/hadoop-yarn/lib/commons-io-2.1.jar
/usr/lib/hadoop-yarn/lib/guice-3.0.jar
/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar
/usr/lib/hadoop-yarn/lib/hamcrest-core-1.1.jar
/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.8.8.jar
/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.8.8.jar
/usr/lib/hadoop-yarn/lib/javax.inject-1.jar
/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar
/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar
/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar
/usr/lib/hadoop-yarn/lib/junit-4.10.jar
/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar
/usr/lib/hadoop-yarn/lib/netty-3.6.2.Final.jar
/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar
/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar
/usr/lib/hadoop-yarn/lib/snappy-java-1.0.4.1.jar
/usr/lib/hadoop-yarn/lib/xz-1.0.jar
/usr/lib/hadoop/hadoop-annotations-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop/hadoop-auth-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop/hadoop-common-2.2.0.2.0.6.0-76-tests.jar
/usr/lib/hadoop/hadoop-common-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop/hadoop-nfs-2.2.0.2.0.6.0-76.jar
/usr/lib/hadoop/lib/activation-1.1.jar
/usr/lib/hadoop/lib/asm-3.2.jar
/usr/lib/hadoop/lib/avro-1.7.4.jar
/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar
/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar
/usr/lib/hadoop/lib/commons-cli-1.2.jar
/usr/lib/hadoop/lib/commons-codec-1.4.jar
/usr/lib/hadoop/lib/commons-collections-3.2.1.jar
/usr/lib/hadoop/lib/commons-compress-1.4.1.jar
/usr/lib/hadoop/lib/commons-configuration-1.6.jar
/usr/lib/hadoop/lib/commons-digester-1.8.jar
/usr/lib/hadoop/lib/commons-el-1.0.jar
/usr/lib/hadoop/lib/commons-httpclient-3.1.jar
/usr/lib/hadoop/lib/commons-io-2.1.jar
/usr/lib/hadoop/lib/commons-lang-2.5.jar
/usr/lib/hadoop/lib/commons-logging-1.1.1.jar
/usr/lib/hadoop/lib/commons-math-2.1.jar
/usr/lib/hadoop/lib/commons-net-3.1.jar
/usr/lib/hadoop/lib/guava-11.0.2.jar
/usr/lib/hadoop/lib/jackson-core-asl-1.8.8.jar
/usr/lib/hadoop/lib/jackson-jaxrs-1.8.8.jar
/usr/lib/hadoop/lib/jackson-mapper-asl-1.8.8.jar
/usr/lib/hadoop/lib/jackson-xc-1.8.8.jar
/usr/lib/hadoop/lib/jasper-compiler-5.5.23.jar
/usr/lib/hadoop/lib/jasper-runtime-5.5.23.jar
/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar
/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar
/usr/lib/hadoop/lib/jersey-core-1.9.jar
/usr/lib/hadoop/lib/jersey-json-1.9.jar
/usr/lib/hadoop/lib/jersey-server-1.9.jar
/usr/lib/hadoop/lib/jets3t-0.6.1.jar
/usr/lib/hadoop/lib/jettison-1.1.jar
/usr/lib/hadoop/lib/jetty-6.1.26.jar
/usr/lib/hadoop/lib/jetty-util-6.1.26.jar
/usr/lib/hadoop/lib/jsch-0.1.42.jar
/usr/lib/hadoop/lib/jsp-api-2.1.jar
/usr/lib/hadoop/lib/jsr305-1.3.9.jar
/usr/lib/hadoop/lib/junit-4.8.2.jar
/usr/lib/hadoop/lib/log4j-1.2.17.jar
/usr/lib/hadoop/lib/mockito-all-1.8.5.jar
/usr/lib/hadoop/lib/native/*
/usr/lib/hadoop/lib/netty-3.6.2.Final.jar
/usr/lib/hadoop/lib/paranamer-2.3.jar
/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar
/usr/lib/hadoop/lib/servlet-api-2.5.jar
/usr/lib/hadoop/lib/slf4j-api-1.7.5.jar
/usr/lib/hadoop/lib/slf4j-log4j12-1.7.5.jar
/usr/lib/hadoop/lib/snappy-java-1.0.4.1.jar
/usr/lib/hadoop/lib/stax-api-1.0.1.jar
/usr/lib/hadoop/lib/xmlenc-0.52.jar
/usr/lib/hadoop/lib/xz-1.0.jar
/usr/lib/hadoop/lib/zookeeper-3.4.5.jar
{code}","06/Jun/14 07:53;smolav;I can reproduce this depending on the size of the dataset:

{noformat}
spark-submit mllib-movielens-evaluation-assembly-1.0.jar --master spark://mllib1:7077
--class com.example.MovieLensALS --rank 10 --numIterations 20 --lambda 1.0 --kryo
hdfs:/movielens/oversampled.dat
{noformat}

The exception will not be thrown for small datasets. It will successfully run with MovieLens 100k and 10M. However, when I run it on a 100M dataset, the exception will be thrown.

My MovieLensALS is mostly the same as the one shipped with Spark. I just added cross-validation. Rating is registered in Kryo just as in the stock example.

{noformat}
# cat RELEASE 
Spark 1.0.0 built for Hadoop 2.2.0
{noformat}

",06/Jun/14 16:48;coderxiang;Update: I also reproduce similar error message for a larger data set (~ 3GB).,"06/Jun/14 22:28;mengxr;[~smolav] and [~coderxiang]:

Thanks for testing it! Could you post the exact error message you got with stack trace? Based on your description, it should be caused by the default serialization of kryo. It may treat BitSet as a general Java collection, then run into error in ser/de.","09/Jun/14 09:36;smolav;Xiangrui Meng, I can't reproduce it at the moment. It takes a quite big dataset to reproduce and I have my machines busy. But I'm pretty sure the stacktrace is exactly the same as the one posted by Neville Li. My bet is that this will be fixed with next Twitter Chill release: https://github.com/twitter/chill/commit/b47512c2c75b94b7c5945985306fa303576bf90d","07/Jul/14 18:40;mengxr;I think now I understand when it happens. We use storage level MEMORY_AND_DISK for user/product in/out links, which contains BitSet objects. If the dataset is large, these RDDs will be pushed from in memory storage to on disk storage, where the latter requires serialization. So the easiest way to re-produce this error is changing the storage level of inLinks/outLinks to DISK_ONLY and run with kryo.

[~neville] Instead of mapping mutable.BitSet to immutable.BitSet, which introduces overhead, we can register mutable.BitSet in our MovieLensALS example code and wait for the next Chill release. Does it sound good to you?",07/Jul/14 18:42;sinisa_lyh;[~mengxr] sounds good to me.,07/Jul/14 18:45;mengxr;Do you mind creating a PR registering mutable.BitSet in MovieLensALS.scala and close PR #925? Thanks!,"07/Jul/14 19:10;sinisa_lyh;There you go:
https://github.com/apache/spark/pull/1319","07/Jul/14 22:08;mengxr;Issue resolved by pull request 1319
[https://github.com/apache/spark/pull/1319]","23/Oct/14 08:26;gen;[~sinisa_lyh]
Sorry to bother you.
According to https://github.com/twitter/chill/pull/185, the twitter.chill have already had the support of mutable BitSet. However, I tried your code, it still doesn't work, if we make kryo as a comment. The task fails in the last line:
{code}
serializer.deserialize(bytes).asInstanceOf[OutLinkBlock]
{code}
Have you any ideas how it happens? The error information is as follow:
{code}
[error] (run-main) com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Can not set final scala.collection.mutable.BitSet field OutLinkBlock.elementIds to scala.collection.mutable.HashSet
[error] Serialization trace:
[error] elementIds (OutLinkBlock)
com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Can not set final scala.collection.mutable.BitSet field OutLinkBlock.elementIds to scala.collection.mutable.HashSet
Serialization trace:
elementIds (OutLinkBlock)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:626)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:162)
	at KroTest$.main(helloworld.scala:25)
	at KroTest.main(helloworld.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
Caused by: java.lang.IllegalArgumentException: Can not set final scala.collection.mutable.BitSet field OutLinkBlock.elementIds to scala.collection.mutable.HashSet
	at sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:164)
	at sun.reflect.UnsafeFieldAccessorImpl.throwSetIllegalArgumentException(UnsafeFieldAccessorImpl.java:168)
	at sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl.set(UnsafeQualifiedObjectFieldAccessorImpl.java:83)
	at java.lang.reflect.Field.set(Field.java:736)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:619)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:162)
	at KroTest$.main(helloworld.scala:25)
	at KroTest.main(helloworld.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)

{code}","23/Oct/14 12:37;gen;In fact, the problem about transformation between HashSet and BitSet in Kyro happens, if we don't register BitSet manually. 
{code}
com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Can not set final scala.collection.mutable.BitSet field OutLinkBlock.elementIds to scala.collection.mutable.HashSet
{code}
This will also cause the collapse of spark when we use spark HIVE.","27/Oct/14 16:04;ilganeli;Hi all - concise writeup on how to fix this bug here:
http://tbertinmahieux.com/wp/?author=1

Also related to:

http://apache-spark-user-list.1001560.n3.nabble.com/ALS-implicit-error-pyspark-td16595.html

Thanks. 
","14/Nov/14 06:30;apachespark;User 'nevillelyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/925",
Typo in org.apache.spark.mllib.tree.DecisionTree.isSampleValid,SPARK-1925,12716578,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,zsxwing,zsxwing,26/May/14 05:18,27/May/14 02:44,15/Aug/18 23:03,27/May/14 02:44,,,,,,,,,,,,,,,,1.0.0,,,,,,MLlib,,,,,0,easyfix,,,,"I believe this is a typo:

{code}
      if ((level > 0) & (parentFilters.length == 0)) {
        return false
      }
{code}

Should use ""&&"" here.",,mengxr,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,394786,,,Mon May 26 05:21:19 UTC 2014,,,,,0|i1vz13:,394921,,,,,,,,,,,,,26/May/14 05:21;zsxwing;PR: https://github.com/apache/spark/pull/879,,,,,,,,,,,,,,,,,,,,,,,
Clean up MLlib sample data,SPARK-1874,12715061,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,matei,matei,18/May/14 22:01,20/May/14 04:30,15/Aug/18 23:03,20/May/14 04:30,,,,,,,,,,,,,,,,1.0.0,,,,,,MLlib,,,,,0,,,,,"- Replace logistic regression example data with linear to make mllib.LinearRegression example easier to run
- Move files from mllib/data into data/mllib to make them easier to find
- Add a simple MovieLens data file",,matei,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-05-18 23:17:02.617,,false,,,,,,,,,,,,,393347,,,Mon May 19 22:55:29 UTC 2014,,,,,0|i1vqev:,393509,,,,,,,,,,,,,18/May/14 23:17;mengxr;Is `data/mllib` a better place than `mllib/data`?,"19/May/14 20:51;matei;Yes, cause there's other stuff in `data`. I think it's a more obvious location.","19/May/14 22:55;mengxr;There are three files under `data/`: `kmeans_data.txt`, `lr_data.txt`, and `pagerank_data.txt`, while more files under `mllib/data`. It feels more natural to me to keep the sample data under `mllib/data`. Anyway, I will create sample data first.",,,,,,,,,,,,,,,,,,,,,
RowMatrix.dspr is not using parameter alpha for DenseVector,SPARK-1696,12711773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mengxr,anishpatel,anishpatel,01/May/14 19:08,15/May/14 00:21,15/Aug/18 23:03,15/May/14 00:21,,,,,,,,,,,,,,,,1.0.0,,,,,,MLlib,,,,,0,,,,,"In the master branch, method dspr of RowMatrix takes parameter alpha, but does not use it when given a DenseVector.

This probably slid by because when method computeGramianMatrix calls dspr, it provides an alpha value of 1.0.",,anishpatel,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-05-14 22:06:56.394,,false,,,,,,,,,,,,,390094,,,Wed May 14 22:06:56 UTC 2014,,,,,0|i1v6xj:,390331,,,,,,,,,,,,,14/May/14 22:06;mengxr;Thanks! I sent a PR: https://github.com/apache/spark/pull/778,,,,,,,,,,,,,,,,,,,,,,,
Not robust Lasso causes Infinity on weights and losses,SPARK-1585,12709990,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yinxusen,yinxusen,yinxusen,23/Apr/14 09:45,19/May/14 06:00,15/Aug/18 23:03,19/May/14 06:00,0.9.1,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,26/Apr/14 00:00,0,,,,,"Lasso uses LeastSquaresGradient and L1Updater, but 

diff = brzWeights.dot(brzData) - label

in LeastSquaresGradient would cause too big diff, then will affect the L1Updater, which increases weights exponentially. Small shrinkage value cannot lasso weights back to zero then. Finally, the weights and losses reach Infinity.

For example, data = (0.5 repeats 10k times), weights = (0.6 repeats 10k times), then data.dot(weights) approximates 300+, the diff will be 300. Then L1Updater sets weights to approximate 300. In the next iteration, the weights will be set to approximate 30000, and so on.",,mengxr,yinxusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1859,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-05-15 19:07:23.866,,false,,,,,,,,,,,,,388312,,,Mon May 19 06:00:59 UTC 2014,,,,,0|i1uw2v:,388567,,,,,,,,,,,,,"15/May/14 19:07;mengxr;I think the gradient should pull the weights back. If I'm wrong, could you create an example code to demonstrate the problem? -Xiangrui",19/May/14 05:07;mengxr;All relates to the step size.,19/May/14 05:58;yinxusen;I see. I close it now.,"19/May/14 06:00;yinxusen;Parameter tuning is vital for LASSO, especially the step size. Large step size causes large updating value, then infinity occurs.",,,,,,,,,,,,,,,,,,,,
Assembly Jar with more than 65536 files won't work when compiled on  JDK7 and run on JDK6,SPARK-1520,12708995,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Unresolved,mengxr,pwendell,pwendell,17/Apr/14 06:49,20/Apr/15 09:54,15/Aug/18 23:03,04/May/14 19:23,,,,,,,,,,,,,,,,1.0.0,,,,,,MLlib,Spark Core,,,,0,,,,,"This is a real doozie - when compiling a Spark assembly with JDK7, the produced jar does not work well with JRE6. I confirmed the byte code being produced is JDK 6 compatible (major version 50). What happens is that, silently, the JRE will not load any class files from the assembled jar.

{code}
$> sbt/sbt assembly/assembly

$> /usr/lib/jvm/java-1.7.0-openjdk-amd64/bin/java -cp /home/patrick/Documents/spark/assembly/target/scala-2.10/spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar org.apache.spark.ui.UIWorkloadGenerator
usage: ./bin/spark-class org.apache.spark.ui.UIWorkloadGenerator [master] [FIFO|FAIR]

$> /usr/lib/jvm/java-1.6.0-openjdk-amd64/bin/java -cp /home/patrick/Documents/spark/assembly/target/scala-2.10/spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar org.apache.spark.ui.UIWorkloadGenerator
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/ui/UIWorkloadGenerator
Caused by: java.lang.ClassNotFoundException: org.apache.spark.ui.UIWorkloadGenerator
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: org.apache.spark.ui.UIWorkloadGenerator. Program will exit.

{code}

I also noticed that if the jar is unzipped, and the classpath set to the currently directory, it ""just works"". Finally, if the assembly jar is compiled with JDK6, it also works. The error is seen with any class, not just the UIWorkloadGenerator. Also, this error doesn't exist in branch 0.9, only in master.

h1. Isolation and Cause

The package-time behavior of Java 6 and 7 differ with respect to the format used for jar files:
||Number of entries||JDK 6||JDK 7||
|<= 65536|zip|zip|
|> 65536|zip*|zip64|

zip* is a workaround for the original zip format that [described in JDK-6828461|https://bugs.openjdk.java.net/browse/JDK-4828461] that allows some versions of Java 6 to support larger assembly jars.

The Scala libraries we depend on have added a large number of classes which bumped us over the limit. This causes the Java 7 packaging to not work with Java 6. We can probably go back under the limit by clearing out some accidental inclusion of FastUtil, but eventually we'll go over again.

The real answer is to force people to build with JDK 6 if they want to run Spark on JRE 6.

-I've found that if I just unpack and re-pack the jar (using `jar`) it always works:-

{code}
$ cd assembly/target/scala-2.10/
$ /usr/lib/jvm/java-1.6.0-openjdk-amd64/bin/java -cp ./spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar org.apache.spark.ui.UIWorkloadGenerator # fails
$ jar xvf spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar
$ jar cvf spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar *
$ /usr/lib/jvm/java-1.6.0-openjdk-amd64/bin/java -cp ./spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar org.apache.spark.ui.UIWorkloadGenerator # succeeds
{code}

-I also noticed something of note. The Breeze package contains single directories that have huge numbers of files in them (e.g. 2000+ class files in one directory). It's possible we are hitting some weird bugs/corner cases with compatibility of the internal storage format of the jar itself.-

-I narrowed this down specifically to the inclusion of the breeze library. Just adding breeze to an older (unaffected) build triggered the issue.-

-I ran a git bisection and this appeared after the MLLib sparse vector patch was merged:-
https://github.com/apache/spark/commit/80c29689ae3b589254a571da3ddb5f9c866ae534
SPARK-1212",,dougb,koert,koertkuipers,mengxr,paulrbrown,pwendell,qiuzhuang.lian,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1911,SPARK-1703,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-04-17 08:43:04.109,,false,,,,,,,,,,,,,387318,,,Mon May 05 11:33:15 UTC 2014,,,,,0|i1upzj:,387580,,,,,,,,,,,,,"17/Apr/14 08:43;srowen;Madness. One wild guess is that the breeze .jar files have something in META-INF that, when merged together into the assembly jar, conflicts with other META-INF items. In particular I'm thinking of MANIFEST.MF entries. It's worth diffing those if you can from before and after. However this would still require that Java 7 and 6 behave differently with respect to the entries, to explain your findings. It's possible.

Your last comment however suggests it's something strange with the byte code that gets output for a few classes. Java 7 is stricter about byte code. For example: https://weblogs.java.net/blog/fabriziogiudici/archive/2012/05/07/understanding-subtle-new-behaviours-jdk-7
However I would think these would manifest as quite different errors.

What about running with -verbose:class to print classloading messages? it might point directly to what's failing to load, if that's it.

Of course you can always build with Java 6 since that's supposed to be all that's supported/required now (see my other JIRA about making Jenkins do this), although I agree that it would be nice to get to the bottom of this, as there is no obvious reason this shouldn't work.","17/Apr/14 08:52;srowen;Regarding large numbers of files: are there INDEX.LST files used anywhere in the jars? If this gets munged or truncated while building the assembly jar, that might cause all kinds of havoc. It could be omitted.

http://docs.oracle.com/javase/7/docs/technotes/guides/jar/jar.html#Index_File_Specification","17/Apr/14 08:55;pwendell;[~srowen] heading to bed for the night... but would welcome help with this. I looked earlier and I don't think breeze is doing anything fancy with their manifest or meta-inf directories. I did a diff on the breeze directory itself between java 6 and java 7 compiled jars and they were identical. The classloading messages don't provide any useful output.

My best guess at present is we are hitting corner cases in the compatibility of the jar format itself due to having individual directories with thousands of class files. And these are causing the Java 6 RE to silently find the jar corrupt. I have no evidence to support that claim, however.","17/Apr/14 10:17;srowen;Java 6 had a limit of 65536 files per jar in total, but the limit is much higher in Java 7:
http://stackoverflow.com/questions/9616250/what-is-the-maximum-number-of-files-per-jar
https://blogs.oracle.com/xuemingshen/entry/zip64_support_for_4g_zipfile

When I build the assembly I find that it has 70948 files. I think you are certainly onto something.

I see the same behavior as you, and am using the latest Java 6/7. I also note that ""unzip -l"" succeeds for the Java 6 version, but fails with the following on the Java 7 version:

{code}
error:  expected central file header signature not found (file #70949).
  (please check that you have transferred or created the zipfile in the
  appropriate BINARY mode and that you have compiled UnZip properly)
{code}

This might not be Java's fault. It could be something to do with how SBT handles merging the zip files, and not handling Java 7's output (which is zip64) correctly.

As a short-term solution, I note that we can probably slim down the assembly jar. For example, fastutil is still in there for some reason, and accounts for 10,666 files. It shouldn't be there.

You can get a quick view into where the files are with:

{code}
jar tf spark-assembly-1.0.0-SNAPSHOT-hadoop1.0.4.jar | grep -oE ""(.+/)+"" | uniq -c | sort -rn | head -100
{code}

{code}
2883 breeze/linalg/operators/
2034 it/unimi/dsi/fastutil/objects/
1396 spire/std/
1379 scala/tools/nsc/typechecker/
1351 breeze/linalg/
1215 it/unimi/dsi/fastutil/longs/
1214 it/unimi/dsi/fastutil/ints/
1213 it/unimi/dsi/fastutil/doubles/
1211 it/unimi/dsi/fastutil/floats/
1210 it/unimi/dsi/fastutil/shorts/
1209 it/unimi/dsi/fastutil/chars/
1209 it/unimi/dsi/fastutil/bytes/
1187 scala/reflect/internal/
 896 com/google/common/collect/
 894 tachyon/thrift/
 886 spire/algebra/
 797 scala/tools/nsc/transform/
 749 scala/tools/nsc/interpreter/
 723 org/netlib/lapack/
 677 spire/math/
...
{code}","17/Apr/14 19:07;mengxr;I'm using Java 6 JDK located at /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home on a mac. It can create a jar with more than 65536 files. I also found this JIRA:

https://bugs.openjdk.java.net/browse/JDK-4828461 (Support Zip files with more than 64k entries)

which was fixed in version 6. Note that this is for openjdk.

I'm going to check the headers of assembly jars created by java 6 and 7.","17/Apr/14 19:29;mengxr;When I try to use jar-1.6 to untar the assembly jar created by java 7:

~~~
java.util.zip.ZipException: invalid CEN header (bad signature)
	at java.util.zip.ZipFile.open(Native Method)
	at java.util.zip.ZipFile.<init>(ZipFile.java:128)
	at java.util.zip.ZipFile.<init>(ZipFile.java:89)
	at sun.tools.jar.Main.list(Main.java:977)
	at sun.tools.jar.Main.run(Main.java:222)
	at sun.tools.jar.Main.main(Main.java:1147)
~~~

7z shows:

~~~
Path = spark-assembly-1.6.jar
Type = zip
Physical Size = 119682511

Path = spark-assembly-1.7.jar
Type = zip
64-bit = +
Physical Size = 119682587
~~~

I think the number of files limit is already increased in Java 6 (at least in the latest update), but Java 7 will use zip64 format for more than 64k  files, and this format cannot be recognized by Java 6.","17/Apr/14 19:59;mengxr;The quick fix may be removing fastutil, so Java 7 still generates the assembly jar in zip format instead of zip64.

In RDD#countApproxDistinct, we use HyperLogLog from com.clearspring.analytics:stream, which depends on fastutil. If this is the only place that introduces fastutil dependency, we should implement HyperLogLog and remove fastutil completely from Spark's dependencies.","17/Apr/14 21:55;mengxr;It seems HyperLogLog doesn't need fastutil, so we can exclude fastutil directly. Will send a patch.","02/May/14 18:21;koert;this one is a headache because i have not been able to actually make the unit tests pass with sbt and java 6 in a long time now, so i resorted to java 7 for the build process assuming the resulting jar could be run by java 6.
",04/May/14 19:23;pwendell;There is no known resolution for this problem. We've updated the build scripts to enforce JDK6 and added better error messages when this happens.,"04/May/14 19:27;srowen;[~pwendell] On this note, I wonder if it's also best to make Jenkins build with Java 6? I'm not quite sure if it catches things like this, but catches things with similar roots. I had a request open at https://issues.apache.org/jira/browse/SPARK-1437 but it's a Jenkins change rather than a code change.","05/May/14 00:06;mengxr;Koert, which JDK6 did you use? This problem was fixed in a later version of openjdk-6. If you are using openjdk-6, you can try upgrading it to the latest version and see whether the problem still exists.","05/May/14 02:24;koertkuipers;Xiangrui,
I have to stick to sun java. Currently on java version ""1.6.0_43""






","05/May/14 06:10;mengxr;The latest version of Sun JDK-6 is 1.6.0_45, though I didn't see any relevant changes between 43 and 45. After we remove fastutil from the dependencies, the assembly jar should have less than 65536 files. Did you try with the latest master?","05/May/14 11:33;koertkuipers;I will try latest master. thanks

",,,,,,,,,
Make MLlib work on Python 2.6,SPARK-1421,12706808,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matei,matei,matei,05/Apr/14 22:46,06/Apr/14 03:53,15/Aug/18 23:03,06/Apr/14 03:53,0.9.0,0.9.1,,,,,,,,,,,,,,0.9.2,1.0.0,,,,,MLlib,PySpark,,,,0,,,,,"Currently it requires Python 2.7 because it uses some new APIs, but they should not be essential for running our code.",,matei,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,385131,,,2014-04-05 22:46:15.0,,,,,0|i1ucjr:,385398,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GLM needs to check addIntercept for intercept and weights,SPARK-1327,12704874,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,,25/Mar/14 23:10,31/Mar/14 00:41,15/Aug/18 23:03,31/Mar/14 00:41,0.9.0,,,,,,,,,,,,,,,0.9.1,1.0.0,,,,,MLlib,,,,,0,,,,,GLM needs to check addIntercept for intercept and weights.,,mengxr,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-03-27 01:20:00.048,,false,,,,,,,,,,,,,382894,,,Mon Mar 31 00:41:52 UTC 2014,,,,,0|i1tyrz:,383162,,,,,,,,,,,,,25/Mar/14 23:24;mengxr;PR: https://github.com/apache/spark/pull/236,27/Mar/14 01:20;tdas;Partial fix for 0.9.1 where adding intercepts will fail fast. ,31/Mar/14 00:41;mengxr;Merged into master and backported to 0.9.x.,,,,,,,,,,,,,,,,,,,,,
Add support for  cross validation to MLLibb,SPARK-1310,12704793,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,holdenkarau,holdenk_amp,,24/Mar/14 17:13,18/Aug/14 05:54,15/Aug/18 23:03,16/Apr/14 16:36,,,,,,,,,,,,,,,,1.0.0,,,,,,MLlib,,,,,0,,,,,See MLI-2,,bing,holdenk_amp,mengxr,ryanleitaiwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-08-18 05:54:45.313,,false,,,,,,,,,,,,,382920,,,Mon Aug 18 05:54:45 UTC 2014,,,,,0|i1tyxr:,383188,,,,,,,,,,,,,18/Aug/14 05:54;bing;can you provide a link to your solution?,,,,,,,,,,,,,,,,,,,,,,,
Numerical drift in computation of matrix inverse leads to invalid results in ALS,SPARK-1262,12704859,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Cannot Reproduce,,msa,,17/Mar/14 16:34,21/Jan/15 06:58,15/Aug/18 23:03,18/Mar/14 16:41,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,0,,,,,"In what follows I cannot offer an expert analysis and remedy, however I will describe the problem I'm seeing and the strategy I've used to mitigate it.

The ALS {{updateBlock()}} method includes a call to {{Solve.solvePositive()}} from JBlas. Generally speaking, when we call {{solvePositive(A, B)}} for symmetric, positive definite {{A}}, this method computes {{x}} from the matrix equation {{Ax = B}}. Or, in other words, it computes {{A}}^-1^{{B}}. As mentioned, one of the preconditions on A is that it be symmetric. In ALS, we call this method on {{fullXtX}} or {{fullXtX.add(YtY.value.get)}}, both of which should be symmetric. However, for implicit ALS and rank > 1, some kind of imprecision in the computation of this inverse tends to make successive values of {{fullXtX.add(YtY.value.get)}} less and less symmetric. From my experience this can and does alter the model produced by ALS significantly, leading to very different and counterintuitive user-item recommendations compared to a solution in which this asymmetry does not exist.

An approach I've seen taken against this problem from the Oryx codebase is to cast each value in the vector returned from {{Solve.solvePositive()}} to a float. That has worked for me.

I've also tried alternative implementations of a linear system solver. The solver that Oryx uses, {{RRQRDecomposition}} from commons math v3, seems to lead to better solutions, but still drifts unless the results are cast to floats.

The Colt numerical computing libraries include a solver called {{QRDecomposition}}, which may be superior to the one used in JBlas. However, I haven't tested it.

I'm working on a unit test to exercise this bug. I discovered it by ""tracing"" the behavior of {{ALS.scala}} with various logging statements inserted into the code.

If nothing else, add a line before the calls to {{Solve.solvePositive()}} in {{updateBlock()}} to validate that {{fullXtX}} or {{fullXtX.add(YtY.value.get)}} are symmetric.",,mengxr,msa,sowen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-03-17 17:18:41.534,,false,,,,,,,,,,,,,382977,,,Tue Mar 18 16:41:36 UTC 2014,,,,,0|i1tzaf:,383245,,,,,,,,,,,,,"17/Mar/14 17:18;mengxr;Solve.solvePositive calls NativeBLAS.dposv, which only looks at the upper triangular part of the matrix to compute Cholesky factorization while ignoring the lower triangular part. It doesn't seem to be the root cause of the problem. It would be super helpful if you can create a unit test to re-produce the bug. -Xiangrui","17/Mar/14 18:02;sowen;Agree, though this might still be a symptom of something of interest. Michael is it YtY that is not symmetric? I can't see how fullXtX would not be symmetric. If it's not, somehow I'd want to figure out just why. Maybe I can look with a debugger too.

Are you saying you see this in Oryx too? which matrix ""still drifts""?
The use of floats is not driven by this issue but simply storage economy. It might mask some round-off error somewhere as a side-effect.

FWIW Oryx does not use jblas or the Cholesky decomposition. It uses the QR decomposition to solve Ax=B. QR is somewhat slower but might give better accuracy. This alone could be the difference. I had never run empirical tests to figure out how much it might impact the results. I suppose you could hack up the code to use CholeskyDecomposition to see if it gives more similar results (I can help offline with that.)","18/Mar/14 11:38;msa;Producing a unit test for this may be infeasible at this time. For one thing, the ""unit"" here is hidden behind private methods. For another, there's no way to initialize the algorithm to a known value. What if I provided a patched version of ALS.scala with log tracing and a driver program instead? I can make the problem visually obvious that way.",18/Mar/14 12:56;mengxr;A patched ALS with a sample data would be sufficient. Do worry about private methods.,"18/Mar/14 16:41;msa;The underlying assertion of this bug report is that the implementation is producing some asymmetric matrices that should be symmetric. However, I misread my debugging session output. The output matrices I was interpreting as asymmetric are in fact symmetric.",,,,,,,,,,,,,,,,,,,
Add proximal gradient updater.,SPARK-1217,12705042,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ameet,,25/Aug/13 18:21,07/Apr/14 17:46,15/Aug/18 23:03,07/Apr/14 17:46,,,,,,,,,,,,,,,,0.9.0,,,,,,MLlib,,,,,0,,,,,"Add proximal gradient updater, in particular for L1 regularization.",,ameet,jaggi,m8j,mengxr,padman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-04-07 16:48:27.296,,false,,,,,,,,,,,,,383037,,,Mon Apr 07 16:48:27 UTC 2014,,,,,0|i1tznr:,383305,,,,,,,,,,,,,"07/Apr/14 16:48;jaggi;The L1 updater is already proximal, as in the current code. Since it has no effect for L2, we could mark the issue as resolved for now.",,,,,,,,,,,,,,,,,,,,,,,
Clustering: Index out of bounds error,SPARK-1215,12704858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,dewshick,,17/Jan/14 05:24,27/Aug/14 08:57,15/Aug/18 23:03,17/Jul/14 22:05,,,,,,,,,,,,,,,,1.1.0,,,,,,MLlib,,,,,0,,,,,"code:
import org.apache.spark.mllib.clustering._

val test = sc.makeRDD(Array(4,4,4,4,4).map(e => Array(e.toDouble)))
val kmeans = new KMeans().setK(4)
kmeans.run(test) evals with java.lang.ArrayIndexOutOfBoundsException

error:
14/01/17 12:35:54 INFO scheduler.DAGScheduler: Stage 25 (collectAsMap at KMeans.scala:243) finished in 0.047 s
14/01/17 12:35:54 INFO spark.SparkContext: Job finished: collectAsMap at KMeans.scala:243, took 16.389537116 s
Exception in thread ""main"" java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.simontuffs.onejar.Boot.run(Boot.java:340)
	at com.simontuffs.onejar.Boot.main(Boot.java:166)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.spark.mllib.clustering.LocalKMeans$.kMeansPlusPlus(LocalKMeans.scala:47)
	at org.apache.spark.mllib.clustering.KMeans$$anonfun$19.apply(KMeans.scala:247)
	at org.apache.spark.mllib.clustering.KMeans$$anonfun$19.apply(KMeans.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.immutable.Range.foreach(Range.scala:81)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at scala.collection.immutable.Range.map(Range.scala:46)
	at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:244)
	at org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:124)
	at Clustering$$anonfun$1.apply$mcDI$sp(Clustering.scala:21)
	at Clustering$$anonfun$1.apply(Clustering.scala:19)
	at Clustering$$anonfun$1.apply(Clustering.scala:19)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.immutable.Range.foreach(Range.scala:78)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at scala.collection.immutable.Range.map(Range.scala:46)
	at Clustering$.main(Clustering.scala:19)
	at Clustering.main(Clustering.scala)
	... 6 more",,dewshick,dmaverick,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23/May/14 12:28;dmaverick;test.csv;https://issues.apache.org/jira/secure/attachment/12646502/test.csv,,,1.0,,,,,,,,,,,,,,,,2014-04-15 16:46:28.032,,false,,,,,,,,,,,,,383031,,,Thu Jul 17 22:05:30 UTC 2014,,,,,0|i1tzmf:,383299,,,,,,1.1.0,,,,,,,15/Apr/14 16:46;mengxr;The error was due to small number of points and large k. The k-means|| initialization doesn't collect more than k candidates. This is very unlikely to appear in practice because k is much smaller than number of points. I will re-visit this issue once we implement better weighted sampling algorithms.,"23/May/14 12:26;dmaverick;I don't think that the problem is about size of dataset. I've faced with similar issue on dataset  with about 900 items. As a workaround we've decided to fallback with random init mode.

","23/May/14 12:28;dmaverick;attach test dataset
MLLib failed to find 4 centers with k-means|| init mode on this data
","14/Jul/14 19:35;josephkb;Submitted fix as PR 1407: https://github.com/apache/spark/pull/1407

Made default behavior to return k clusters still, with some duplicated","17/Jul/14 22:05;mengxr;Issue resolved by pull request 1468
[https://github.com/apache/spark/pull/1468]",,,,,,,,,,,,,,,,,,,
0-1 labels ,SPARK-1214,12704581,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,ameet,,25/Aug/13 18:25,07/Apr/14 17:45,15/Aug/18 23:03,07/Apr/14 17:45,,,,,,,,,,,,,,,,0.9.0,,,,,,MLlib,,,,,0,,,,,"Use \{0,1\} labels for binary classification instead of {-1,1}. Advantages include:
(+) Consistency across algorithms
(+) Naturally extends to multi-class classification",,ameet,jaggi,mengxr,sandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-04-07 17:45:23.507,,false,,,,,,,,,,,,,383036,,,Mon Apr 07 17:45:23 UTC 2014,,,,,0|i1tznj:,383304,,,,,,,,,,,,,07/Apr/14 17:45;mengxr;Fixed in 0.9.0 or an earlier version.,,,,,,,,,,,,,,,,,,,,,,,
loss function error of logistic loss,SPARK-1213,12704857,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,xusen,,05/Jan/14 03:02,01/Apr/14 06:34,15/Aug/18 23:03,01/Apr/14 06:34,,,,,,,,,,,,,,,,0.9.0,,,,,,MLlib,,,,,0,"MLLib,",,,,"There might be a error in incubator-spark/mllib/src/main/scala/org/apache/spark/mllib/optimizatio/Gradient.scala

The loss function of class LogisticGradient might be wrong.

The original one is :
val loss =
  if (margin > 0) {
      math.log(1 + math.exp(0 - margin))
   } else {
      math.log(1 + math.exp(margin)) - margin
   }

But when we use this kind of loss function, we will find that the loss is increasing when optimizing, such as LogisticRegressionWithSGD.

I think it should be something like this:

val loss =
      if (label > 0) {
        math.log(1 + math.exp(margin))
      } else {
        math.log(1 + math.exp(margin)) - margin
      }

I tested the loss function. It works well.
",,crazyjvm,Dong Yan,mengxr,xusen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,383032,,,2014-01-05 03:02:00.0,,,,,0|i1tzmn:,383300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MLlib ALS gets stack overflow with too many iterations,SPARK-1006,12704480,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Duplicate,,matei,,21/Dec/13 11:02,06/Jan/16 17:19,15/Aug/18 23:03,23/Feb/15 22:12,,,,,,,,,,,,,,,,,,,,,,MLlib,,,,,3,,,,,"The tipping point seems to be around 50. We should fix this by checkpointing the RDDs every 10-20 iterations to break the lineage chain, but checkpointing currently requires HDFS installed, which not all users will have.

We might also be able to fix DAGScheduler to not be recursive.",,andrew xia,aremirata,auduwage,crazyjvm,Dong Yan,fji,huasanyelao,llai,matei,mdagost,mengxr,Qiuzhuang,qiuzhuang.lian,sandy,scorreia,smolav,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2013-12-31 00:25:08.254,,false,,,,,,,,,,,,,383225,,,Wed Jan 06 17:19:07 UTC 2016,,,,,0|i1u0tj:,383493,,,,,,,,,,,,,21/Dec/13 11:03;matei;Same issue will affect Bagel at around 100 iterations.,"31/Dec/13 00:25;andrew xia;Hi Matei

    ALS will be StackOverflow as too long linage chain(deserialization cause stackoverflow in executor), but I am not sure if we will fix this issue by changing DAGScheduler to not be recursive, because we don't break the linage chain. but it will avoid potential stackoverflow in driver(I have encountered this exception when recursively building stages) ",05/Nov/14 23:08;mdagost;Any plans to work on this or any pointers how one would go about making the needed modification?  I'm working with a dataset that doesn't appear to be converging before it runs into this limitation...,"24/Mar/15 02:31;mengxr;This is fixed as part of SPARK-5955, where we use checkpointing to cut off lineage.","11/Aug/15 18:26;auduwage;Is there away to find out which build fix this problem: I am on 1.3.1 and error 

5/08/11 14:08:20 ERROR TaskSetManager: Task 1 in stage 4204.0 failed 4 times; aborting job
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4204.0 failed 4 times, most recent failure: Lost task 1.3 in stage 4204.0 (COMPANYDOMAIN-TAKENOUT): java.lang.StackOverflowError
        at java.io.SerialCallbackContext.<init>(SerialCallbackContext.java:48)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1890)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

I am doing 50 iterations 
val numIterations = 50
ALS.trainImplicit(trainData, rank, numIterations, 0.01, 0.01)","06/Jan/16 17:19;aremirata;First of all, I would like to thank you guys for developing spark and putting it open source that we can use. I'm new to Spark and Scala, and working in a project involving matrix factorizations in Spark. I have a problem regarding running ALS in Spark. It has a stackoverflow due to long linage chain as per comments on the internet. One of their suggestion is to use the setCheckpointInterval so that for every 10-20 iterations, we can checkpoint the RDDs and it prevents the error. Just want to ask details on how to do checkpointing with ALS. I am using spark-kernel developed by IBM: https://github.com/ibm-et/spark-kernel instead of spark-shell.

Here are some of my specific questions regarding details on checkpoint:

1. In setting checkpoint directory through SparkContext.setCheckPointDir(), it needs to be a hadoop compatible directory. Can we use any available hdfs-compatible directory?
2. What do you mean by this comment on the code in ALS checkpointing:
If the checkpoint directory is not set in [[org.apache.spark.SparkContext]],
  * this setting is ignored.
3. Is the use of setCheckPointInterval the only code I needed to add to have checkpointing for ALS work?
4. I am getting this error: Name: java.lang.IllegalArgumentException, Message: Wrong FS: expected file :///. How can I solve this? What is the proper way of using checkpointing.

Thanks a lot!
",,,,,,,,,,,,,,,,,,
