https://api.github.com/repos/amplab/graphx/issues/148,https://api.github.com/repos/amplab/graphx/issues/148/comments,[Spark-10994] Add local clustering coefficient computation in GraphX,"The local clustering coefficient of a vertex (node) in a graph quantifies how close its neighbours are to being a clique (complete graph). 

More specifically, the local clustering coefficient C_i for a vertex v_i is given by the proportion of links between the vertices within its neighbourhood divided by the number of links that could possibly exist between them. 

Duncan J. Watts and Steven Strogatz introduced the measure in 1998 to determine whether a graph is a small-world network.
#### Usage

Here is a usage example for LocalClusteringCoefficient:

``` Scala
import org.apache.spark.graphx._
import org.apache.spark._

val conf = new SparkConf().setAppName(""testApp"")
val sc = new SparkContext(conf)
// load a graph
val graph = GraphLoader.edgeListFile(sc, ""graph.txt"").partitionBy(PartitionStrategy.RandomVertexCut)

// perform the local clustering coefficient computation 
val LccCounter = graph.localClusteringCoefficient()

// output results for each vertex
val verts = LccCounter.vertices
verts.collect().foreach { case (vid, count) =>
    println(vid + "": "" + count)
}
```
"
https://api.github.com/repos/amplab/graphx/issues/147,https://api.github.com/repos/amplab/graphx/issues/147/comments,Update README.md,"Hi,

I added section ""Books"" and a link to the Spark GraphX in Action written by Michael S. Malak and Robin East for Manning Publications. We believe this book is a great resource and all members of community will benefit from this information. If you find this inappropriate, please let me know where I can publish it.
"
https://api.github.com/repos/amplab/graphx/issues/145,https://api.github.com/repos/amplab/graphx/issues/145/comments,How to build project from https://github.com/apache/spark using sbt,"can anyone mention the steps which i need to follow for building spark application also commands to test the scala testcase.referring from https://github.com/apache/spark

I m getting build error 
## Stacktrace

[ERROR] Failed to execute goal on project spark-core_2.10: Could not resolve dependencies for project org.apache.spark:spark-core_2.10:jar:1.1.0-SNAPSHOT: Failed to collect dependencies at org.easymoc
k:easymockclassextension:jar:3.1: Failed to read artifact descriptor for org.easymock:easymockclassextension:jar:3.1: Could not transfer artifact org.easymock:easymockclassextension:pom:3.1 from/to ma
ven-repo (http://repo.maven.apache.org/maven2): Access denied to: http://repo.maven.apache.org/maven2/org/easymock/easymockclassextension/3.1/easymockclassextension-3.1.pom , ReasonPhrase:Forbidden. -

> [Help 1]

Thanx
"
https://api.github.com/repos/amplab/graphx/issues/139,https://api.github.com/repos/amplab/graphx/issues/139/comments,"There are some issues in article ""Launch a benchmarking cluster"" ","```
I want to run pagerank on Graphx, following the instructions(https://github.com/amplab/graphx/wiki/Launch-a-benchmarking-cluster), I encountered some problems.
First, the running command(~/graphx/run-example org.apache.spark.graph.Analytics spark://$MASTERS:7077 pagerank hdfs://$MASTERS:9000/soc-LiveJournal1.txt --numIter=20 --numEPart=128) is wrong, I changed it to ""./bin/run-example org.apache.spark.graphx.lib.Analytics spark://XXX:7077 pagerank hdfs://XXX:8020/soc-LiveJournal1.txt"". The parameter ""--numIter"" can not be found in running pagerank, I read the source code and find it is used in cc benchmark.
```

   when I run the command above, spark throw warnings and errors:
14/04/21 10:24:59 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
……
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Spark cluster looks down

   What I am sure is spark UI is ok, and other benchmarks can run rightly.

   What I expect is, if you can give some hint in deploying Graphx on standlone cluster? 
Thanks very very much, cause I have been blocked on this for 1 week. :)
"
https://api.github.com/repos/amplab/graphx/issues/135,https://api.github.com/repos/amplab/graphx/issues/135/comments,In memory shuffle,
https://api.github.com/repos/amplab/graphx/issues/134,https://api.github.com/repos/amplab/graphx/issues/134/comments,Store local vertex ids in EdgePartition,
https://api.github.com/repos/amplab/graphx/issues/123,https://api.github.com/repos/amplab/graphx/issues/123/comments,Add coarsen function to GraphOps,
https://api.github.com/repos/amplab/graphx/issues/122,https://api.github.com/repos/amplab/graphx/issues/122/comments,Graph Load/Save,
https://api.github.com/repos/amplab/graphx/issues/113,https://api.github.com/repos/amplab/graphx/issues/113/comments,Collapsed Gibbs Sampling for LDA,"This is a work in progress implementation of the collapsed Gibbs sampler for the LDA model using the GraphX abstraction primitives.  While this is based on the (non-ergodic) bulk synchronous Gibbs sampler, we do exploit local parameter sharing and if document vertex partitioning is used we recover the Newman et al. style sampler.

Remaining tasks:
- [ ] Unite tests
- [ ] Raw document processing
- [ ] Likelihood calculation (requires log gamma transcendental functions)
"
https://api.github.com/repos/amplab/graphx/issues/109,https://api.github.com/repos/amplab/graphx/issues/109/comments,Add K-Core to Analytics,
https://api.github.com/repos/amplab/graphx/issues/108,https://api.github.com/repos/amplab/graphx/issues/108/comments,Add LDA to Analytics,
https://api.github.com/repos/amplab/graphx/issues/106,https://api.github.com/repos/amplab/graphx/issues/106/comments,Improve pre-shuffle aggregation performance in mrTriplets,"I realized today that we can actually save the position of the source vertex and the target vertex in the array in EdgeTriplet, and then in aggregation, we can simply use that position to update the aggregation value (without actually doing any hash lookups).

Issue 101 will make this easier to implement. 

https://github.com/amplab/graphx/issues/101
"
https://api.github.com/repos/amplab/graphx/issues/67,https://api.github.com/repos/amplab/graphx/issues/67/comments,Compress vertex IDs and attributes during replication ,"Possible configurations:
1. Columnar replication with sorted vertex IDs and delta compression
2. Non-columnar replication with variable-length encoding for vertex IDs
"
https://api.github.com/repos/amplab/graphx/issues/66,https://api.github.com/repos/amplab/graphx/issues/66/comments,Add a flag to Analytics to keep the driver alive,"#65 adds a long delay to keep the Spark dashboard open. It would be better to have a flag for this that defaults to off.
"
https://api.github.com/repos/amplab/graphx/issues/52,https://api.github.com/repos/amplab/graphx/issues/52/comments,PageRank causes java.util.NoSuchElementException,"When running PageRank on a cluster, sometimes I hit a NoSuchElementException that's caused somewhere in VertexSetRDD. Full stack trace and command below. The line numbers may be slightly off due to debugging printlns.

Command:

```
/root/graphx/run-example org.apache.spark.graph.Analytics spark://ec2-54-224-159-106.compute-1.amazonaws.com:7077 pagerank hdfs://ec2-54-224-159-106.compute-1.amazonaws.com:9000/soc-LiveJournal1.txt --numIter=10 --numEPart=128
```

Stack Trace:

```
java.util.NoSuchElementException: End of stream
    at org.apache.spark.util.NextIterator.next(NextIterator.scala:83)
    at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:29)
    at org.apache.spark.graph.VertexSetRDD$$anonfun$8.apply(VertexSetRDD.scala:314)
    at org.apache.spark.graph.VertexSetRDD$$anonfun$8.apply(VertexSetRDD.scala:313)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:84)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:84)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:84)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:84)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:84)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:84)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:84)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:84)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:84)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:84)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:84)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.ZippedRDD.compute(ZippedRDD.scala:64)
    at org.apache.spark.graph.VertexSetRDD.compute(VertexSetRDD.scala:149)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:32)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:159)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:100)
    at org.apache.spark.scheduler.Task.run(Task.scala:53)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:212)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

```
"
https://api.github.com/repos/amplab/graphx/issues/45,https://api.github.com/repos/amplab/graphx/issues/45/comments,Graph Parsing,"Improve graph parsing and load performance. 
"
https://api.github.com/repos/amplab/graphx/issues/35,https://api.github.com/repos/amplab/graphx/issues/35/comments,Edge contraction,"Based on our discussion today it seems like it might be helpful to have a function of the form:

``` scala
def contractEdges(
  ePred: EdgeTriplet[VD,ED] => Boolean, 
  contractFun: EdgeTriplet[VD,ED] => VD,  
  mergeFun: (VD, VD) => VD): Graph[VD, ED]
```

where the user defined edge predicate `ePred` determines which edges to contract and the user defined `contractFun` renders a new vertex for the contracted edge and `mergeFun` merges multiple vertices that have been contracted together.
"
https://api.github.com/repos/amplab/graphx/issues/32,https://api.github.com/repos/amplab/graphx/issues/32/comments,Lazy evaluation of join and map operations,"The `VertexSetRDD[VD]` stores the vertex attributes as an `IndexedSeq[VD]`.  When a `VertexSetRDD` is first constructed from an `RDD[(Vid,VD)]` the attributes are stored in an `Array[VD]`.  When `mapValues` is in invoked on a `VertexSetRDD[VD]` a _new_ array is created and populated with the result of the map operation.  

https://github.com/amplab/graphx/blob/master/graph/src/main/scala/org/apache/spark/graph/VertexSetRDD.scala#L129

However when `leftJoin` is invoked an `IndexedSeqView` is created:

https://github.com/amplab/graphx/blob/master/graph/src/main/scala/org/apache/spark/graph/VertexSetRDD.scala#L192

Should both be implemented using views or should both be implemented using actual storage.  The tradeoffs are the following:
1. Using views means that long chains of computation might be invoked repeatedly.  
2. Using Arrays could lead to many long-lived allocations. 

I suspect all the operations should be implemented using the view but I am not sure what the implications are for caching. 
"
https://api.github.com/repos/amplab/graphx/issues/25,https://api.github.com/repos/amplab/graphx/issues/25/comments,Triplets.collect returns incorrect values,"The spark RDD.collect operation stores the output directly into an array.  Since we reuse the iterator values only a single edge triplet is stored (in duplicate) for each partition.
"
https://api.github.com/repos/amplab/graphx/issues/20,https://api.github.com/repos/amplab/graphx/issues/20/comments,Stack Overflow caused by repeated calls to default Java serialized data reader,"When we run PageRank for too many iterations (100 iterations consistently triggers it), we get a stack overflow that stems from reading deeply nested serial data using the default java serializer. Basically, we get repeated calls to the following sequence of methods

```
        at scala.collection.immutable.$colon$colon.readObject(List.scala:435)
        at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1015)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1891)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at scala.collection.immutable.$colon$colon.readObject(List.scala:435)
        at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1015)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1891)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1913)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
```

@ankurdave poked around a little bit and he said that Kryo seemed to be getting called on most things but apparently not whatever is causing this.

I did some more poking around today and when profiling PageRank running on livejournal, I found that a bunch of exceptions were being thrown in the Kryo serialization code. It seems that Kryo keeps trying to serialize something, fails with a NoSuchMethodException, and then falls back to Java serialization code. I'm thinking that these two might be related? The exception stack trace is here.

![screen shot 2013-10-11 at 3 22 15 pm 2](https://f.cloud.github.com/assets/431588/1318841/0b128070-32c6-11e3-9451-4cf56a55f93e.png)
"
https://api.github.com/repos/amplab/graphx/issues/16,https://api.github.com/repos/amplab/graphx/issues/16/comments,Fix Analytics.scala,"Remove broken commented code and fix primary functions: PageRank, Shortest Path, connected components etc...

Also split out ALS and other non-analytics tasks.
"
https://api.github.com/repos/amplab/graphx/issues/11,https://api.github.com/repos/amplab/graphx/issues/11/comments,Synthetic Graph Generators,"To help with benchmarking lets create some synthetic graph generators.  The Pregel paper describes a log-normal generator which is relatively easy to implement.  
"
https://api.github.com/repos/amplab/graphx/issues/9,https://api.github.com/repos/amplab/graphx/issues/9/comments,Add Mean Field Inference to Analytics,"Add a basic implementation of the variational mean field algorithm to Analytics. In addition create a synthetic noisy image generator.
"
