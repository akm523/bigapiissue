Summary,Issue key,Issue id,Issue Type,Project url,Priority,,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Container),Outward issue link (Duplicate),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Required),Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Date of First Response),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Shepherd),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Testcase included),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Fix batch gradient bug in SVDPlusPlus,SPARK-21130,13080616,Bug,http://spark.apache.org,Major,job,,"I think the mode of batch gradient in Graphx SVDPlusPlus is incorrect. The changes of this pull request are as blow:
* Use the mean of batch gradient instead of the sum of batch gradient.
* When calculating the bias of each rating during iteration,  the bias shouldn't be limit in (minVal, maxVal).
* The method ""calculateIterationRMSE"" is added to show the iteration effect clearly.",,xmlyfly@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,9.22E+18,,,18:12.0,,,,,0|i3gepz:,9.22E+18,,,,,,,,,,,,,,,,,,,,,,
[GRAPHX] Strange results for personalized pagerank if node is involved in a cycle,SPARK-20429,13065858,Bug,http://spark.apache.org,Major,,,"I'm trying to run the personalized PageRank implementation of GraphX on a simple test graph, which is the following: 

Image: https://i.stack.imgur.com/JDv1l.jpg

I'm a bit confused on some results that I get when I try to compute the PPR for a node that is involved in a cycle. For example, the final output for the node 12 is as follows:

(13, 0.0141)
(7,  0.0141)
(19, 0.0153)
(17, 0.0153)
(20, 0.0153)
(11, 0.0391)
(14, 0.0460)
(15, 0.0541)
(16, 0.0541)
(12, 0.1832)

I would clearly expect that the node 13 would have a much higher PPR value (in fact, I would expect it to be the first one after the starting node itself). The problem appears as well with other nodes involved in cycles, for example for starting node 13 the node 15 has a very low score. From all the testing that I have done it seems that for starting nodes that do not participate in a cycle the result is exactly how I expect.",,a1ray,cecconi,jackduluoz,,,,,,,,,,,,,,,,,,,,,,,SPARK-18847,,,,,,0,,,,,,,,,,,,,,,,59:38.6,,FALSE,,,,,,,,,,,,9.22E+18,,,Mon May 01 16:59:38 UTC 2017,,,,,0|i3dxd3:,9.22E+18,,,,,,,,,,01/May/17 16:59;a1ray;Can you retest your example with Spark 2.2/master. SPARK-18847 probably fixed your issue.,,,,,,,,,,,,
PageRank gives incorrect results for graphs with sinks,SPARK-18848,13027912,Bug,http://spark.apache.org,Major,job,,Sink vertices (those with no outgoing edges) should evenly distribute their rank to the entire graph but in the current implementation it is just lost.,,a1ray,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18847,,,,,,0,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,9.22E+18,,,32:50.0,,,,,0|i37k6v:,9.22E+18,,,,,,,,,,,,,,,,,,,,,,
PageRank gives incorrect results for graphs with sinks,SPARK-18847,13027911,Bug,http://spark.apache.org,Major,job,,Sink vertices (those with no outgoing edges) should evenly distribute their rank to the entire graph but in the current implementation it is just lost.,,a1ray,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,34:18.3,,FALSE,,,,,,,,,,,,9.22E+18,,,Fri Jan 06 06:49:04 UTC 2017,,,,,0|i37k6n:,9.22E+18,,,,,,,,,,13/Dec/16 21:34;srowen;Before you open more can you review old JIRAs about this? ,13/Dec/16 21:56;a1ray;I have and have not found any relevant. I'm currently working on a fix,"06/Jan/17 06:49;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/16483",,,,,,,,,,
PageRank has incorrect initialization value that leads to slow convergence,SPARK-18845,13027907,Bug,http://spark.apache.org,Major,,,"All variants of PageRank in GraphX have incorrect initialization value that leads to slow convergence. In the current implementations ranks are seeded with the reset probability when it should be 1. This appears to have been introduced a long time ago in https://github.com/apache/spark/commit/15a564598fe63003652b1e24527c432080b5976c#diff-b2bf3f97dcd2f19d61c921836159cda9L90

This also hides the fact that source vertices (vertices with no incoming edges) are not updated. This is because source vertices generally* have pagerank equal to the reset probability. Therefore both need to be fixed at once.

PR will be added shortly

*when there are no sinks -- but that's a separate bug",,a1ray,ankurd,apachespark,,,,,,,,,,,,,,,,,,,,SPARK-18847,SPARK-18848,,,,,,,,0,,,,,,,,,,,,,,,,26:22.9,,FALSE,,,,,,,,,,,,9.22E+18,,,Fri Dec 16 07:39:35 UTC 2016,,,,,0|i37k5r:,9.22E+18,,,,,,,,,,13/Dec/16 21:26;srowen;See https://issues.apache.org/jira/browse/SPARK-7005 ?,"13/Dec/16 21:49;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/16271","13/Dec/16 21:52;a1ray;[~srowen] No that's a different thing just whether the result sums to 1 or n. But to expand on that, since we are using the version that sums to n our initial ranks need to sum to n or it takes a lot longer to converge. ","16/Dec/16 07:39;ankurd;Issue resolved by pull request 16271
[https://github.com/apache/spark/pull/16271]",,,,,,,,,
GraphX Invalid initial capacity when running triangleCount,SPARK-18200,13016926,Bug,http://spark.apache.org,Major,memory,graphx,"Running GraphX triangle count on large-ish file results in the ""Invalid initial capacity"" error when running on Spark 2.0 (tested on Spark 2.0, 2.0.1, and 2.0.2).  You can see the results at: http://bit.ly/2eQKWDN

Running the same code on Spark 1.6 and the query completes without any problems: http://bit.ly/2fATO1M

As well, running the GraphFrames version of this code runs as well (Spark 2.0, GraphFrames 0.2): http://bit.ly/2fAS8W8

Reference Stackoverflow question:
Spark GraphX: requirement failed: Invalid initial capacity (http://stackoverflow.com/questions/40337366/spark-graphx-requirement-failed-invalid-initial-capacity)","Databricks, Ubuntu 16.04, macOS Sierra",apachespark,cybertaurean,dennyglee,dongjoon,sathyasrini,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,50:32.2,,FALSE,,,,,,,,,,,,9.22E+18,,,Sun Dec 04 02:48:14 UTC 2016,,,,,0|i35oef:,9.22E+18,,,,,2.0.3,2.1.0,,,,"02/Nov/16 22:50;dongjoon;Hi, [~dennyglee].
It's due to `OpenHashSet`. I'll make a PR for this.","02/Nov/16 22:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15741","02/Nov/16 22:58;dongjoon;Actually, there is a node who doesn't have any neighbors. So, it requested to create `VertexSet` with zero initial capacity.","02/Nov/16 23:19;dongjoon;The described scenario is also tested.
{code}
scala> import org.apache.spark.graphx.{GraphLoader, PartitionStrategy}

scala> val filepath = ""/tmp/ca-HepTh.txt""

scala> val graph = GraphLoader.edgeListFile(sc, filepath, true).partitionBy(PartitionStrategy.RandomVertexCut)

scala> val triCounts = graph.triangleCount().vertices

scala> triCounts.toDF().show()
+-----+---+
|   _1| _2|
+-----+---+
|50130|  2|
|20484| 11|
|10598|190|
|31760| 29|
{code}","03/Nov/16 01:54;sathyasrini;Thank you  Dongjoon Hyun and  Denny Lee for taking a very serious consideration to my question in Stack Overflow (http://stackoverflow.com/questions/40337366/spark-graphx-requirement-failed-invalid-initial-capacity).
I am in the process of implementing and testing the proposed solution and the reported answers work fine. Thanks",03/Nov/16 02:15;dongjoon;Good for you. :),"03/Nov/16 17:23;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/15754","04/Dec/16 01:37;cybertaurean;Does this issue exist currently in version 2.0.1?. I just ran a test and it's throwing the following exception.

User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 10.0 failed 4 times, most recent failure: Lost task 3.3 in stage 10.0 (TID 196, BD-S2F13): java.lang.IllegalArgumentException: requirement failed: Invalid initial capacity
at scala.Predef$.require(Predef.scala:224)
at org.apache.spark.util.collection.OpenHashSet$mcJ$sp.<init>(OpenHashSet.scala:51)
at org.apache.spark.util.collection.OpenHashSet$mcJ$sp.<init>(OpenHashSet.scala:57)
at org.apache.spark.graphx.lib.TriangleCount$$anonfun$5.apply(TriangleCount.scala:70)
at org.apache.spark.graphx.lib.TriangleCount$$anonfun$5.apply(TriangleCount.scala:69)
at org.apache.spark.graphx.impl.VertexPartitionBaseOps.map(VertexPartitionBaseOps.scala:61)
at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$mapValues$2.apply(VertexRDDImpl.scala:102)
at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$mapValues$2.apply(VertexRDDImpl.scala:102)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$3.apply(VertexRDDImpl.scala:156)
at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$3.apply(VertexRDDImpl.scala:154)
at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)
at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
at org.apache.spark.scheduler.Task.run(Task.scala:86)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)","04/Dec/16 02:12;dongjoon;Hi,
Yes, the bugs are there in 2.0.1.
The fix will be in upcoming Apache Spark 2.0.3 and 2.1.0.
We cannot backport into 2.0.1 because it's already released.",04/Dec/16 02:48;cybertaurean;Thanks much [~dongjoon],,,
Pregel does not keep vertex state properly; fails to terminate ,SPARK-17097,12997744,Bug,http://spark.apache.org,Major,job,,"Consider the following minimum example:
{code:title=PregelBug.scala|borderStyle=solid}
package testGraph

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.graphx.{Edge, EdgeTriplet, Graph, _}

object PregelBug {
  def main(args: Array[String]) = {
    //FIXME breaks if TestVertex is a case class; works if not case class
    case class TestVertex(inId: VertexId,
                     inData: String,
                     inLabels: collection.mutable.HashSet[String]) extends Serializable {
      val id = inId
      val value = inData
      val labels = inLabels
    }

    class TestLink(inSrc: VertexId, inDst: VertexId, inData: String) extends Serializable  {
      val src = inSrc
      val dst = inDst
      val data = inData
    }

    val startString = ""XXXSTARTXXX""

    val conf = new SparkConf().setAppName(""pregeltest"").setMaster(""local[*]"")
    val sc = new SparkContext(conf)

    val vertexes = Vector(
      new TestVertex(0, ""label0"", collection.mutable.HashSet[String]()),
      new TestVertex(1, ""label1"", collection.mutable.HashSet[String]())
    )
    val links = Vector(
      new TestLink(0, 1, ""linkData01"")
    )
    val vertexes_packaged = vertexes.map(v => (v.id, v))
    val links_packaged = links.map(e => Edge(e.src, e.dst, e))

    val graph = Graph[TestVertex, TestLink](sc.parallelize(vertexes_packaged), sc.parallelize(links_packaged))

    def vertexProgram (vertexId: VertexId, vdata: TestVertex, message: Vector[String]): TestVertex = {
      message.foreach {
        case `startString` =>
          if (vdata.id == 0L)
            vdata.labels.add(vdata.value)

        case m =>
          if (!vdata.labels.contains(m))
            vdata.labels.add(m)
      }
      new TestVertex(vdata.id, vdata.value, vdata.labels)
    }

    def sendMessage (triplet: EdgeTriplet[TestVertex, TestLink]): Iterator[(VertexId, Vector[String])] = {
      val srcLabels = triplet.srcAttr.labels
      val dstLabels = triplet.dstAttr.labels

      val msgsSrcDst = srcLabels.diff(dstLabels)
        .map(label => (triplet.dstAttr.id, Vector[String](label)))

      val msgsDstSrc = dstLabels.diff(dstLabels)
        .map(label => (triplet.srcAttr.id, Vector[String](label)))

      msgsSrcDst.toIterator ++ msgsDstSrc.toIterator
    }

    def mergeMessage (m1: Vector[String], m2: Vector[String]): Vector[String] = m1.union(m2).distinct

    val g = graph.pregel(Vector[String](startString))(vertexProgram, sendMessage, mergeMessage)

    println(""---pregel done---"")
    println(""vertex info:"")
    g.vertices.foreach(
      v => {
        val labels = v._2.labels
        println(
          ""vertex "" + v._1 +
            "": name = "" + v._2.id +
            "", labels = "" + labels)
      }
    )
  }
}
{code}


This code never terminates even though we expect it to. To fix, we simply remove the ""case"" designation for the TestVertex class (see FIXME comment), and then it behaves as expected.

(Apologies if this has been fixed in later versions; we're unfortunately pegged to 2.10.5 / 1.6.0 for now.)","Scala 2.10.5, Spark 1.6.0 with GraphX and Pregel",bromberger,ding,KevinRossi,michaelmalak,tritab@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,14:30.9,,FALSE,,,,,,,,,,,,9.22E+18,,,Fri Oct 21 18:07:23 UTC 2016,,,,,0|i32e9z:,9.22E+18,,,,,,,,,,"13/Sep/16 22:14;ding;I am afraid the attached sample code fail to terminate with case class is not caused by Pregel or graphx bug. As in the sample code, vertices(inLabels here) is updated inplace which supposed not to happen. In this way, after transform operations, the original vertices is also updated and it has exactly the same value with the new generated vertices in above code when VD is case class. It leads to fail to update EdgeTriplet as there is no difference of the vertices. So EdgeTriplet dstLabels is always empty while srcLabels contains a value. And there is always active message which lead to Pregel not terminate.

One way to fix the problem is remove inplace update in vertexProgram by clone the labels and make update in the new labels. I have tried below code and it works.
    def vertexProgram (vertexId: VertexId, vdata: TestVertex, message: Vector[String]): TestVertex = {
      val labels = vdata.labels.clone() 
      message.foreach {
        case `startString` =>
          if (vdata.id == 0L)
            labels.add(vdata.value )  

        case m =>
          if (!vdata.labels.contains(m))
            labels.add(m)
      }
      new TestVertex(vdata.id, vdata.value, labels)
    }

Hope this information is helpful to you.","29/Sep/16 14:59;bromberger;ding - thank you very much for the explanation and workaround. A followup question: why would removing the case class ""fix"" the problem?","29/Sep/16 20:35;ding;Because diff of case class behaves different with regular class. A case class implements the equals method while a class does not. When comparing two objects implemented as a class is actually comparing the memory address of the objects. In above code, if we remove ""case"",  after transform, the original vertices is still different with the new generated vertices although they have the same value. In this way, EdgeTriplet is able to be updated since there is difference and after 1 iteration there will be no active message and the application will terminate. ",30/Sep/16 16:06;bromberger;Great explanation. Thank you very much!,"21/Oct/16 16:05;tritab@gmail.com;If ding's solution resolves the issue, should this be marked ""not a bug""?",21/Oct/16 16:16;bromberger;OK by me. It's confusing behavior but definitely not a bug :),"21/Oct/16 18:07;tritab@gmail.com;Great, thanks for confirming and closing.",,,,,,
Cannot perform RDD operations on a checkpointed VertexRDD.,SPARK-15717,12974942,Bug,http://spark.apache.org,Major,job,,"A checkpointed (materialized) VertexRDD throws the following exception when collected:

bq. java.lang.ArrayStoreException: org.apache.spark.graphx.impl.ShippableVertexPartition

Can be replicated by running:
{code:java}
graph.vertices.checkpoint()
graph.vertices.count() // materialize
graph.vertices.collect()
{code}",,adeandrade,akrim,apachespark,josephkb,michaelmalak,rxin,,,,,,,,,,,,,,,,,,,,SPARK-14804,,,,,,0,,,,,,,,,,,,,,,,08:04.1,,FALSE,,,,,,,,,,,,9.22E+18,,,Tue Sep 27 17:15:35 UTC 2016,,,,,0|i2yuof:,9.22E+18,,,,,,,,,,"02/Jun/16 03:08;apachespark;User 'adeandrade' has created a pull request for this issue:
https://github.com/apache/spark/pull/13458",22/Sep/16 01:10;akrim;Any update on this issue? We are experiencing ClassCastExceptions when using checkpointing and LDA with the EM optimizer.,"27/Sep/16 17:15;josephkb;To confirm, this is a duplicate of [SPARK-14804], right?  I'll close this for now in favor of the other.  Thanks and sorry it took so long to see this.",,,,,,,,,,
ConnectedComponents fails to compute graph with 200 vertices (but long paths),SPARK-15042,12964092,Bug,http://spark.apache.org,Major,memory,,"ConnectedComponents takes forever and eventually fails with OutOfMemory when computing this graph: {code}{ (i, i+1) | i <- { 1..200 } }{code}

If you generate the example graph, e.g., with this bash command

{code}
for i in {1..200} ; do echo ""$i $(($i+1))"" ; done > input.graph
{code}

... then should be able to reproduce in the spark-shell by running:

{code}
import org.apache.spark.graphx._
import org.apache.spark.graphx.lib._
val graph = GraphLoader.edgeListFile(sc, ""input.graph"").cache()

ConnectedComponents.run(graph)
{code}

I seems to take forever, and spawns these warnings from time to time:

{code}
16/04/30 20:06:24 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@7af98fbd,BlockManagerId(driver, localhost, 43440))] in 1 attempts
{code}

For additional information, here is a link to my related question on Stackoverflow:
http://stackoverflow.com/q/36892272/783510

One comment so far, was that the number of skipping tasks grows exponentially.

---

Here is the complete output of a spark-shell session:

{noformat}
phil@terra-arch:~/tmp/spark-graph$ spark-shell 
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel(""INFO"")
Spark context available as sc.
SQL context available as sqlContext.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1
      /_/
         
Using Scala version 2.11.7 (OpenJDK 64-Bit Server VM, Java 1.8.0_92)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.graphx._
import org.apache.spark.graphx._

scala> import org.apache.spark.graphx.lib._
import org.apache.spark.graphx.lib._

scala> 

scala> val graph = GraphLoader.edgeListFile(sc, ""input.graph"").cache()
graph: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@1fa9692b

scala> ConnectedComponents.run(graph)
16/04/30 20:05:29 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@50432fd2,BlockManagerId(driver, localhost, 43440))] in 1 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:449)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:470)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:470)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 14 more
16/04/30 20:06:24 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@7af98fbd,BlockManagerId(driver, localhost, 43440))] in 1 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:449)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:470)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:470)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 14 more
16/04/30 20:13:00 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@7af98fbd,BlockManagerId(driver, localhost, 43440))] in 2 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:449)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:470)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:470)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 14 more
16/04/30 20:13:30 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 145068 ms exceeds timeout 120000 ms
16/04/30 20:24:46 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@7af98fbd,BlockManagerId(driver, localhost, 43440))] in 3 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:449)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:470)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:470)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:470)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 14 more
{noformat}","Local cluster (1 instance) running on Arch Linux
Scala 2.11.7, Java 1.8.0_92",jarnoraj,khwunchai,michaelmalak,Nick Schwartzmyer,philipp.classen,praetp,tritab@gmail.com,,,,,,,,,,,,,,,,,,,SPARK-10335,,,SPARK-5484,,,0,,,,,,,,,,,,,,,,27:49.9,,FALSE,,,,,,,,,,,,9.22E+18,,,Thu Oct 27 11:40:38 UTC 2016,,,,,0|i2x08f:,9.22E+18,,,,,,,,,,21/Oct/16 18:27;tritab@gmail.com;I tried your minimal example above and was able to get a near instant result using the latest master branch (2.1.0-snapshot). Could you attempt to recreate the issue and report your results?,22/Oct/16 08:54;srowen;Provisionally closing as it may have been fixed in between these versions.,"27/Oct/16 11:40;jarnoraj;I was able to reproduce this issue with 2.1.0-snapshot running locally using this:
{code:none}
import org.apache.spark.graphx._
val v=sc.parallelize(List.range(0,201).map(x => (x : VertexId,())))
val e=sc.parallelize(for (i <- List.range(0,200)) yield (Edge(i,i+1,())))
val graph=Graph(v,e)
val c=graph.connectedComponents()
{code}
",,,,,,,,,,
Graph vertexRDD/EdgeRDD checkpoint results ClassCastException: ,SPARK-14804,12960758,Bug,http://spark.apache.org,Minor,,,"{code}
    graph3.vertices.checkpoint()
    graph3.vertices.count()
    graph3.vertices.map(_._2).count()
{code}


16/04/21 21:04:43 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 4.0 (TID 13, localhost): java.lang.ClassCastException: org.apache.spark.graphx.impl.ShippableVertexPartition cannot be cast to scala.Tuple2
	at com.xiaomi.infra.codelab.spark.Graph2$$anonfun$main$1.apply(Graph2.scala:80)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1597)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1161)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1161)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1863)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1863)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:91)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:219)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)


look at the code:
{code}

  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =
  {
    if (isCheckpointedAndMaterialized) {
      firstParent[T].iterator(split, context)
    } else {
      compute(split, context)
    }
  }

 private[spark] def isCheckpointedAndMaterialized: Boolean = isCheckpointed

 override def isCheckpointed: Boolean = {
   firstParent[(PartitionID, EdgePartition[ED, VD])].isCheckpointed
 }

{code}

for VertexRDD or EdgeRDD, first parent is its partitionRDD  RDD[ShippableVertexPartition[VD]]/RDD[(PartitionID, EdgePartition[ED, VD])]

1. we call vertexRDD.checkpoint, it partitionRDD will checkpoint, so VertexRDD.isCheckpointedAndMaterialized=true.
2. then we call vertexRDD.iterator, because checkoint=true it called firstParent.iterator(which is not CheckpointRDD, actually is partitionRDD). 
 
so returned iterator is iterator[ShippableVertexPartition] not expect iterator[(VertexId, VD)]]
",,adeandrade,apachespark,eyalfa,felixcheung,josephkb,michaelmalak,nseggert,SuYan,tdas,,,,,,,,,,,,,,,,SPARK-17975,,,,,,,0,,,,,,,,,,,,,,,,47:03.3,,FALSE,,,,,,,,,,,,9.22E+18,,,Thu Jan 26 01:16:05 UTC 2017,,,,,0|i2wg93:,9.22E+18,,,,,,,,,,"22/Apr/16 02:47;apachespark;User 'suyanNone' has created a pull request for this issue:
https://github.com/apache/spark/pull/12576",02/Jun/16 03:34;adeandrade;The proposed PR is hacky. It disables independent checkpointing on VertexRDD and EdgeRDD. I have solved this issue for VertexRDD in https://issues.apache.org/jira/browse/SPARK-15717,"07/Oct/16 22:25;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/15396","10/Oct/16 06:33;eyalfa;I think this relates to SPARK-12431, is it possible to 'merge' both efforts?","12/Oct/16 05:57;apachespark;User 'apivovarov' has created a pull request for this issue:
https://github.com/apache/spark/pull/15447",04/Jan/17 19:19;josephkb;I think this is a separate issue from local checkpointing.  The problem appears to be that Graph checkpointing incorrectly uses internal checkpointing APIs.,"26/Jan/17 01:16;tdas;Issue resolved by pull request 15396
[https://github.com/apache/spark/pull/15396]",,,,,,
Fix `pickRandomVertex` not to fall into infinite loops for graphs with one vertex,SPARK-14219,12954140,Bug,http://spark.apache.org,Major,,,"Currently, `GraphOps.pickRandomVertex()` falls into infinite loops for graphs having only one vertex. This issue fixes it by modifying the following termination-checking condition.
{code}
-      if (selectedVertices.count > 1) {
+      if (selectedVertices.count > 0) {
{code}",,apachespark,dongjoon,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,58:04.0,,FALSE,,,,,,,,,,,,9.22E+18,,,Tue Mar 29 01:02:03 UTC 2016,,,,,0|i2vbfz:,9.22E+18,,,,,,,,,,"28/Mar/16 23:58;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/12018","29/Mar/16 01:02;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/12021",,,,,,,,,,,
Strongly connected components doesn't find all strongly connected components,SPARK-13313,12939086,Bug,http://spark.apache.org,Major,"job, memory",,"Strongly connected components algorithm doesn't find all strongly connected components. I was using Wikispeedia dataset (http://snap.stanford.edu/data/wikispeedia.html) and the algorithm found 519 SCCs and one of them had 4051 vertices, which in reality don't have any edges between them. 
I think the problem could be on line 89 of StronglyConnectedComponents.scala file where EdgeDirection.In should be changed to EdgeDirection.Out. I believe the second Pregel call should use Out edge direction, the same as the first call because the direction is reversed in the provided sendMsg function (message is sent to source vertex and not destination vertex).
If that is changed (line 89), the algorithm starts finding much more SCCs, but eventually stack overflow exception occurs. I believe graph objects that are changed through iterations should not be cached, but checkpointed.
",,glenn.strycker@gmail.com,michaelmalak,pzecevic,uzadude,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,01:41.5,,FALSE,,,,,,,,,,,,9.22E+18,,,Thu Mar 17 17:50:36 UTC 2016,,,,,0|i2stkf:,9.22E+18,,,,,,,,,,14/Feb/16 13:01;srowen;Can you be more specific? like specific examples from the data and a pull request?,"14/Feb/16 21:14;pzecevic;Yes, you need articles.tsv and links.tsv from this archive: http://snap.stanford.edu/data/wikispeedia/wikispeedia_paths-and-graph.tar.gz

Then parse the data, assign IDs to article names and create the graph:
val articles = sc.textFile(""articles.tsv"", 6).filter(line => line.trim() != """" && !line.startsWith(""#"")).zipWithIndex().cache()
val links = sc.textFile(""links.tsv"", 6).filter(line => line.trim() != """" && !line.startsWith(""#""))
val linkIndexes = links.map(x => { val spl = x.split(""\t""); (spl(0), spl(1)) }).join(articles).map(x => x._2).join(articles).map(x => x._2)
val wikigraph = Graph.fromEdgeTuples(linkIndexes, 0)

Then get strongly connected components:
val wikiSCC = wikigraph.stronglyConnectedComponents(100)

wikiSCC graph contains 519 SCCs, but there should be much more. The largest SCC in wikiSCC has 4051 vertices and that's obviously wrong.

The change in line 89, which I mentioned, seems to solve this problem, but then other issues arise (stack overflow etc) and I don't have time to investigate further. I hope someone will look into this.

","14/Feb/16 21:58;srowen;Dumb question, but is this the difference between directed and undirected graphs? like, GraphX is reading this as directed edges only?","15/Feb/16 07:23;pzecevic;No, I don't think it's got anything to do with that. That largest SCC's vertices are not connected in any way and they shouldn't be in the same group.","17/Mar/16 16:16;uzadude;Hi,
I am trying to use graphx's SCC and was very concerned with this issue, so I have taken this dataset and ran it with python's networkx strongly_connected_components function and got exactly the same results of 519 SCCs with maximal size = 4051.
So although I don't know what is the real result, the fact that both algorithms agree make me believe that they are correct.
I have also looked at the code and it looks fine to me, I don't agree that you should change the edge direction on line 89.","17/Mar/16 17:50;pzecevic;Ok, thanks for reporting. I'll look into this. ",,,,,,,
GraphX does not unpersist RDDs,SPARK-12655,12927351,Bug,http://spark.apache.org,Minor,job,,"Looks like Graph does not clean all RDDs from the cache on unpersist
{code}
// open spark-shell 1.5.2 or 1.6.0
// run

import org.apache.spark.graphx._

val vert = sc.parallelize(List((1L, 1), (2L, 2), (3L, 3)), 1)
val edges = sc.parallelize(List(Edge[Long](1L, 2L), Edge[Long](1L, 3L)), 1)

val g0 = Graph(vert, edges)
val g = g0.partitionBy(PartitionStrategy.EdgePartition2D, 2)
val cc = g.connectedComponents()

cc.unpersist()
g.unpersist()
g0.unpersist()
vert.unpersist()
edges.unpersist()
{code}
open http://localhost:4040/storage/
Spark UI 4040 Storage page still shows 2 items
{code}
VertexRDD      Memory Deserialized 1x Replicated   1  100%    1688.0 B   0.0 B  0.0 B
EdgeRDD        Memory Deserialized 1x Replicated   2  100%      4.7 KB   0.0 B  0.0 B
{code}",,apachespark,apivovarov,jasoncl,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,21:15.5,,FALSE,,,,,,,,,,,,9.22E+18,,,Tue Jun 07 08:28:18 UTC 2016,,,,,0|i2qthb:,9.22E+18,,,,,,,,,,08/Jan/16 23:21;jasoncl;The VertexRDD and EdgeRDD you see are created during the intermediate step of g.connectedComponents(). They are not properly unpersisted at the moment. I will look into this. ,"12/Jan/16 00:24;apachespark;User 'jasoncl' has created a pull request for this issue:
https://github.com/apache/spark/pull/10713","15/Jan/16 12:04;srowen;Issue resolved by pull request 10713
[https://github.com/apache/spark/pull/10713]",07/Jun/16 00:47;apivovarov;[~srowen] Can the fix be added to 1.6 branch for upcoming 1.6.2 release? It's just 3 new lines of code.,"07/Jun/16 08:28;srowen;OK, done",,,,,,,,
Personalized PageRank shouldn't use uniform initialization,SPARK-11432,12909342,Bug,http://spark.apache.org,Minor,parallelism,,"The current implementation of personalized pagerank in GraphX uses uniform initialization over the full graph - every vertex will get initially activated.

For example:

{code}
import org.apache.spark._
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
val users: RDD[(VertexId, (String, String))] =
  sc.parallelize(Array((3L, (""rxin"", ""student"")), (7L, (""jgonzal"", ""postdoc"")),
                       (5L, (""franklin"", ""prof"")), (2L, (""istoica"", ""prof""))))
val relationships: RDD[Edge[String]] =
  sc.parallelize(Array(Edge(3L, 7L, ""collab""),    Edge(5L, 3L, ""advisor""),
                       Edge(2L, 5L, ""colleague""), Edge(5L, 7L, ""pi"")))
val defaultUser = (""John Doe"", ""Missing"")
val graph = Graph(users, relationships, defaultUser)
graph.staticPersonalizedPageRank(3L, 0, 0.15).vertices.collect.foreach(println)
{code}

Leads to all vertices being set to resetProb (0.15), which is different from the behavior described in SPARK-5854, where only the source node should be activated. 

The risk is that, after a few iterations, the most activated nodes are the source node and the nodes that were untouched by the propagation. For example in the above example the vertex 2L will always have an activation of 0.15:

{code}
graph.personalizedPageRank(3L, 0, 0.15).vertices.collect.foreach(println)
{code}

Which leads into a higher score for 2L than for 7L and 5L, even though there's no outbound path from 3L to 2L.",,apachespark,dbtsai,michaelmalak,yraimond,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,48:03.7,,FALSE,,,,,,,,,,,,9.22E+18,,,Tue Nov 03 04:36:12 UTC 2015,,,,,0|i2nrl3:,9.22E+18,dbtsai,,,,1.6.0,,,,,"30/Oct/15 22:48;apachespark;User 'moustaki' has created a pull request for this issue:
https://github.com/apache/spark/pull/9386","03/Nov/15 04:36;dbtsai;Issue resolved by pull request 9386
[https://github.com/apache/spark/pull/9386]",,,,,,,,,,,
PageRank fails with unified memory manager,SPARK-11278,12907340,Bug,http://spark.apache.org,Critical,memory,,"PageRank (6-nodes, 32GB input) runs very slow and eventually fails with ExecutorLostFailure. Traced it back to the 'unified memory manager' commit from Oct 13th. Took a quick look at the code and couldn't see the problem (changes look pretty good). cc'ing [~andrewor14][~vanzin] who may be able to spot the problem quickly. Can be reproduced by running PageRank on a large enough input dataset if needed. Sorry for not being of much help here.",,andrewor14,andyd88,beettlle,er.guptavivek,glenn.strycker@gmail.com,michaelmalak,nezihyigitbasi,nravi,rxin,vanzin,,,,,,,,,,,,,,,,,,,,19/Nov/15 23:21;nravi;executor_log_legacyModeTrue.html;https://issues.apache.org/jira/secure/attachment/12773371/executor_log_legacyModeTrue.html,19/Nov/15 23:21;nravi;executor_logs_legacyModeFalse.html;https://issues.apache.org/jira/secure/attachment/12773372/executor_logs_legacyModeFalse.html,2,,,,,,,,,,,,,,,,44:38.7,,FALSE,,,,,,,,,,,,9.22E+18,,,Wed May 09 20:20:48 UTC 2018,,,,,0|i2nfan:,9.22E+18,,,,,,,,,,23/Oct/15 17:44;andrewor14;are there any exceptions in the executor logs? Does the problem go away if you run it again with `spark.memory.useLegacyMode = true`?,"24/Oct/15 00:04;nravi;Yeah, the problem goes away with useLegacyMode = true  (as expected).

In the executor logs, I see large spills:

{code}
15/10/23 14:27:13 INFO collection.ExternalSorter: Thread 145 spilling in-memory map of 1477.0 MB to disk (1 time so far)
{code}

and OOM errors:
{code}
15/10/23 14:47:44 ERROR executor.Executor: Exception in task 99.0 in stage 0.0 (TID 94)
java.lang.OutOfMemoryError: Java heap space
	at org.apache.spark.util.collection.AppendOnlyMap.growTable(AppendOnlyMap.scala:218)
	at org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.growTable(SizeTrackingAppendOnlyMap.scala:38)
	at org.apache.spark.util.collection.AppendOnlyMap.incrementSize(AppendOnlyMap.scala:204)
	at org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:151)
	at org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:210)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

Different values of spark.memory.fraction and spark.memory.storageFraction didn't help either.

With smaller executors the workload goes through but with a 1.6x performance degradation (as compared to without this commit). The spills are much smaller: 
15/10/23 15:43:26 INFO collection.ExternalSorter: Thread 117 spilling in-memory map of 5.0 MB to disk (1 time so far)

",04/Nov/15 12:10;srowen;[~nravi] Please don't set Blocker; this doesn't rise to that level anyway. ,"04/Nov/15 12:11;srowen;Oh, mea culpa [~nravi]. [~davies] set that. Is this really a blocker in your opinion?",11/Nov/15 01:12;nravi;Would it make sense to set useLegacyMode to true for 1.6 while we resolve this issue? ,11/Nov/15 02:05;andrewor14;I would make this at least critical; i's a regression.,19/Nov/15 02:13;andrewor14;[~nravi] can you try again with the latest 1.6 branch to see if this is still an issue? I wonder how this is different with https://github.com/apache/spark/commit/56419cf11f769c80f391b45dc41b3c7101cc5ff4.,"19/Nov/15 02:15;andrewor14;also, when you said 6 nodes what kind of nodes are they? How much memory / cores per node?","19/Nov/15 03:09;nravi;[~andrewor14] This was last tested on Nov 11th, which would include the commit you mentioned. Each node has 16 vcores and 48GB memory.",19/Nov/15 23:23;nravi;Attaching executor logs for most recent runs against master (with useLegacyMode=true and false respectively). ,20/Nov/15 00:45;andrewor14;thanks [~nravi] that's very helpful.,"28/Oct/16 09:54;er.guptavivek;We are facing an issue with Spark 1.6.0 whereby performance degrades severely  (with extensive Shuffle spill) using the Unified Memory Manager (spark.memory.fraction = 0.9 and spark.memory.storageFraction = 0.0).

However same application works with improved performance having switched to legacy mode (spark.memory.useLegacyMode=true, spark.shuffle.memoryFraction=0.9, spark.storage.memoryFraction=0.0).

Is this something related with this issue?","09/May/18 20:20;vanzin;I'm going to close this since it's way out of date at this point. We've been running PageRank for a while and while it needed some config tweaks, it seems to work fine."
Typo in the GraphX programming guide,SPARK-11174,12905782,Bug,http://spark.apache.org,Trivial,,,"There is a small typo in the GraphX documentation. In the EdgeRDD description it says ""Revere"" but should say ""Reverse"".",,apachespark,lpiepiora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,53:41.3,,FALSE,,,,,,,,,,,,9.22E+18,,,Sun Oct 18 13:26:12 UTC 2015,,,,,0|i2n5u7:,9.22E+18,,,,,,,,,,"18/Oct/15 12:53;srowen;This isn't even worth a JIRA, but please open a pull request.","18/Oct/15 12:59;apachespark;User 'lpiepiora' has created a pull request for this issue:
https://github.com/apache/spark/pull/9160","18/Oct/15 13:26;srowen;Issue resolved by pull request 9160
[https://github.com/apache/spark/pull/9160]",,,,,,,,,,
Integer overflow in VertexRDDImpl.count,SPARK-10228,12858679,Bug,http://spark.apache.org,Major,memory,,"VertexRDDImpl overrides RDD.count() but aggregates Int instead of Long:

/** The number of vertices in the RDD. */
  override def count(): Long = {
    partitionsRDD.map(_.size).reduce(_ + _)
  }

This causes Pregel to stop iterating when the number of messages is ""negative"", giving incorrect results.",,hpmv118,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3190,,,,,,0,,,,,,,,,,,,,,,,57:27.0,,FALSE,,,,,,,,,,,,9.22E+18,,,Tue Aug 25 16:57:27 UTC 2015,,,,,0|i2jcmf:,9.22E+18,,,,,,,,,,"25/Aug/15 16:57;srowen;Always best to look at master first, since you'd see it was already fixed:

https://github.com/apache/spark/blame/9e952ecbce670e9b532a1c664a4d03b66e404112/graphx/src/main/scala/org/apache/spark/graphx/impl/VertexRDDImpl.scala

https://issues.apache.org/jira/browse/SPARK-3190
",,,,,,,,,,,,
Unpersist a graph object does not work properly,SPARK-9109,12845635,Bug,http://spark.apache.org,Minor,job,,"Unpersist a graph object does not work properly.

Here is the code to produce 

{code}
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
import org.slf4j.LoggerFactory
import org.apache.spark.graphx.util.GraphGenerators

val graph: Graph[Long, Long] =
  GraphGenerators.logNormalGraph(sc, numVertices = 100).mapVertices( (id, _) => id.toLong ).mapEdges( e => e.attr.toLong)
  
graph.cache().numEdges
graph.unpersist()
{code}

There should not be any cached RDDs in storage (http://localhost:4040/storage/).",,apachespark,tien-dung.le,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,32:20.6,,FALSE,,,,,,,,,,,,9.22E+18,,,Sat Jul 18 21:28:14 UTC 2015,,,,,0|i2hcgf:,9.22E+18,,,,,,,,,,"16/Jul/15 22:32;srowen;[~tien-dung.le] you need to read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark  This JIRA has some problems I fixed, like priority and component.","16/Jul/15 22:41;srowen;unpersist() works correctly actually; it's that the cached RDDs are attached to other Graphs you create in this process. There are 3 here; think of it as 

{code}
val graph1 = GraphGenerators.logNormalGraph(sc, numVertices = 100)
val graph2 = graph1.mapVertices( (id, _) => id.toLong )
val graph3 = graph2.mapEdges( e => e.attr.toLong)
{code}

You'll find 1 of the 2 RDDs left over is from graph1, so call unpersist() on it.

There is a 'leaked' cached edges RDD in here somewhere though, I think, since I'm still left with one when I try this.
I think there are a number of issues like this in Graphx, where RDDs are cached but not obviously cleaned up by an unpersist().

Can you try this out, debug, and see if you can pinpoint where the edges RDD comes from? 
","17/Jul/15 08:05;tien-dung.le;Thanks [~sowen]. Indeed, there is still a cached edges RDD. I think this RDD is left from the graph construction in GraphImpl.scala, more precisely at fromEdgeRDD() function.

Here is the latest code follow your suggestion.
{code}

import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
import org.slf4j.LoggerFactory
import org.apache.spark.graphx.util.GraphGenerators

val graph1 = GraphGenerators.logNormalGraph(sc, numVertices = 100)
val graph2 = graph1.mapVertices( (id, _) => id.toLong )
val graph3 = graph2.mapEdges( e => e.attr.toLong)
  
graph3.cache().numEdges
graph3.unpersist()
graph2.unpersist()
graph1.unpersist()
graph2.unpersist()
graph3.unpersist()

sc.getPersistentRDDs.foreach( r => println( r._2.toString))

/*  
GraphImpl.scala

private def fromEdgeRDD[VD: ClassTag, ED: ClassTag](
      edges: EdgeRDDImpl[ED, VD],
      defaultVertexAttr: VD,
      edgeStorageLevel: StorageLevel,
      vertexStorageLevel: StorageLevel): GraphImpl[VD, ED] = {
    val edgesCached = edges.withTargetStorageLevel(edgeStorageLevel).cache()
    val vertices = VertexRDD.fromEdges(edgesCached, edgesCached.partitions.size, defaultVertexAttr)
      .withTargetStorageLevel(vertexStorageLevel)
    fromExistingRDDs(vertices, edgesCached)
  }
*/
{code}


","17/Jul/15 08:39;srowen;Yes, we should expect that calling unpersist() (just once) on all Graph objects that have been created should result in no cached RDDs. Either this should not be cached, or, it should somehow be un-persisted in unpersist(). Can you propose a PR with a change along these lines, for [~ankurd] to look at?","17/Jul/15 09:19;tien-dung.le;I think the cache is done in purpose so we should keep it. The solution is to keep all cached RDDs and unpersist them later (when the graph.unpersist is called. 

I can propose a change for that but it would be very kind of you to refer me to a document (produce) how to make a change and create a PR.",17/Jul/15 09:35;srowen;https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark,"17/Jul/15 14:33;apachespark;User 'tien-dungle' has created a pull request for this issue:
https://github.com/apache/spark/pull/7469","17/Jul/15 14:37;tien-dung.le;[~sowen] This is my first PR for spark so I am not sure how to process. Could you kindly have a first look?

PS: I did run ./dev/run-tests. It passed ""Scalastyle checks"" but il failed on python part

{code}
.../spark/dev/lint-python: line 64: syntax error near unexpected token `>'
.../spark/dev/lint-python: line 64: `    easy_install -d ""$PYLINT_HOME"" pylint==1.4.4 &>> ""$PYLINT_INSTALL_INFO""'
[error] running .../spark/dev/lint-python ; received return code 2
{code}","18/Jul/15 21:28;tien-dung.le;The change has been merged at https://github.com/apache/spark/pull/7469
",,,,
GraphLoader.edgeListFile does not populate Graph.vertices.,SPARK-8396,12838156,Bug,http://spark.apache.org,Minor,io,newbie,"With input data like this
18090 31237
31237 31225
31225 31285
31285 31200
31200 31197
31197 31195
31195 31346
31346 54013
54013 31256
31256 23121

The code 

val graph : Graph[Int, Int] = GraphLoader.edgeListFile(sc, hdfsNode + ""/data/misc/Sample_DirectedGraphData.ssv"")
graph.vertices.foreach{println}
graph.vertices.foreach{vertex: (VertexId, Int) => println(vertex._1.toString + "" *** "" + vertex._2.toString)}

prints nothing.",Mac OS X.  Spark-1.4.0 pre-compiled binary for Hadoop-2.4.0-bin.,bahbarrettmatthew,michaelmalak,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,9.22E+18,,,12:38.0,,,,,0|i2g3of:,9.22E+18,,,,,,,,,,,,,,,,,,,,,,
Wrong initial bias in GraphX SVDPlusPlus,SPARK-6710,12788294,Bug,http://spark.apache.org,Major,control paramter,,"In the initialization portion of GraphX SVDPlusPluS, the initialization of biases appears to be incorrect. Specifically, in line 
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/SVDPlusPlus.scala#L96 
instead of 
(vd._1, vd._2, msg.get._2 / msg.get._1, 1.0 / scala.math.sqrt(msg.get._1)) 
it should probably be 
(vd._1, vd._2, msg.get._2 / msg.get._1 - u, 1.0 / scala.math.sqrt(msg.get._1)) 

That is, the biases bu and bi (both represented as the third component of the Tuple4[] above, depending on whether the vertex is a user or an item), described in equation (1) of the Koren paper, are supposed to be small offsets to the mean (represented by the variable u, signifying the Greek letter mu) to account for peculiarities of individual users and items. 

Initializing these biases to wrong values should theoretically not matter given enough iterations of the algorithm, but some quick empirical testing shows it has trouble converging at all, even after many orders of magnitude additional iterations. 

This perhaps could be the source of previously reported trouble with SVDPlusPlus. 
http://apache-spark-user-list.1001560.n3.nabble.com/GraphX-SVDPlusPlus-problem-td12885.html ",,apachespark,josephkb,michaelmalak,rxin,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,0,,,,,,,,,,,,,,,,11:41.7,,FALSE,,,,,,,,,,,,9.22E+18,,,Sat Apr 11 01:31:40 UTC 2015,,,,,0|i27shj:,9.22E+18,,,,,,,,,,06/Apr/15 22:11;rxin;[~michaelmalak] would you like to submit a pull request for this?,"11/Apr/15 01:31;apachespark;User 'michaelmalak' has created a pull request for this issue:
https://github.com/apache/spark/pull/5464",,,,,,,,,,,
GraphX `diff` test incorrectly operating on values (not VertexId's),SPARK-6022,12777710,Bug,http://spark.apache.org,Major,,,"The current GraphX {{diff}} test operates on values rather than the VertexId's and, if {{diff}} were working properly (per [SPARK-4600|https://issues.apache.org/jira/browse/SPARK-4600]), it should fail this test. The code to test {{diff}} should look like the below as it correctly generates {{VertexRDD}}'s with different {{VertexId}}'s to {{diff}} against.

{code}
test(""diff functionality with small concrete values"") {
    withSpark { sc =>
      val setA: VertexRDD[Int] = VertexRDD(sc.parallelize(0L until 2L).map(id => (id, id.toInt)))
      // setA := Set((0L, 0), (1L, 1))
      val setB: VertexRDD[Int] = VertexRDD(sc.parallelize(1L until 3L).map(id => (id, id.toInt+2)))
      // setB := Set((1L, 3), (2L, 4))
      val diff = setA.diff(setB)
      assert(diff.collect.toSet == Set((2L, 4)))
    }
  }
{code}",,ankurd,boyork,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,55:13.6,,FALSE,,,,,,,,,,,,9.22E+18,,,Fri Mar 13 15:59:33 UTC 2015,,,,,0|i262br:,9.22E+18,,,,,,,,,,"25/Feb/15 23:54;boyork;FWIW I have this fix put in place under my branch for [SPARK-4600|https://issues.apache.org/jira/browse/SPARK-4600], but wanted to point it out here as I'm going to need to remove the original test. This is all assuming I'm not completely insane and missing something here.

cc [~ankurd]","03/Mar/15 07:55;maropu;Is the test correct?
According to the code below, 'diff' is assumed to have same indices in VertexPartitionBase.
That is,  SetA and SetB has the same set of VertexIDs (Im not sure that this behaviour totally correct though).  

https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/impl/VertexPartitionBaseOps.scala#L93",09/Mar/15 18:42;boyork;The test is correct (in what I believe {{diff}} should do). Maybe [~ankurd] can chime in here? And you're also correct in that the code implementing {{diff}} doesn't currently work properly which is why I believe this test should correctly assess whether {{diff}} is operating correctly.,"09/Mar/15 21:13;ankurd;[~maropu] is correct: the original intent of diff was to operate on values, not VertexIds. It was really written for internal use in [mapVertices|https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/impl/GraphImpl.scala#L133] and [outerJoinVertices|https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/impl/GraphImpl.scala#L284], which use it to find the set of vertices whose values have changed so they can ship only those to the edge partitions.

Based on your test you're looking for the set difference. Maybe you could introduce a new method called ""minus""?","11/Mar/15 07:55;maropu;Yeah, ISTM it'd be better to add set difference as Graph#minus.","13/Mar/15 15:59;boyork;Awesome, thanks for the clarity guys! I'll close this JIRA and introduce a new one for a Graph#minus that should better reflect what I was going for!",,,,,,,
Deprecate SVDPlusPlus APIs that expose DoubleMatrix from JBLAS,SPARK-5815,12775085,Bug,http://spark.apache.org,Major,,,It is generally bad to expose types defined in a 3rd-party package in Spark public APIs. We should deprecate those methods in SVDPlusPlus and replace them in the next release.,,apachespark,mengxr,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5814,,,,,0,,,,,,,,,,,,,,,,32:52.4,,FALSE,,,,,,,,,,,,9.22E+18,,,Mon Feb 16 17:05:24 UTC 2015,,,,,0|i25mqn:,9.22E+18,,,,,1.3.0,,,,,"15/Feb/15 13:32;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4614","16/Feb/15 04:41;mengxr;Issue resolved by pull request 4614
[https://github.com/apache/spark/pull/4614]","16/Feb/15 15:41;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4625",16/Feb/15 17:05;srowen;Also pushed a change to update run() and deprecated runSVDPlusPlus() for 1.4.0,,,,,,,,,
"EdgeRDD, VertexRDD getStorageLevel return bad values",SPARK-5534,12771857,Bug,http://spark.apache.org,Major,job,,"After caching a graph, its edge and vertex RDDs still return StorageLevel.None.

Reproduce error:
{code}
import org.apache.spark.graphx.{Edge, Graph}
val edges = Seq(
  Edge[Double](0, 1, 0),
  Edge[Double](1, 2, 0),
  Edge[Double](2, 3, 0),
  Edge[Double](3, 4, 0))
val g = Graph.fromEdges[Double,Double](sc.parallelize(edges), 0)
g.vertices.getStorageLevel  // returns value for StorageLevel.None
g.edges.getStorageLevel  // returns value for StorageLevel.None
g.cache()
g.vertices.count()
g.edges.count()
g.vertices.getStorageLevel  // returns value for StorageLevel.None
g.edges.getStorageLevel  // returns value for StorageLevel.None
{code}
",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,06:20.6,,FALSE,,,,,,,,,,,,9.22E+18,,,Tue Feb 03 01:02:47 UTC 2015,,,,,0|i253g7:,9.22E+18,,,,,1.3.0,,,,,"02/Feb/15 22:17;josephkb;Note: This is needed for [https://github.com/apache/spark/pull/4047], which is a PR for this JIRA: [https://issues.apache.org/jira/browse/SPARK-1405]","02/Feb/15 23:06;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/4317","03/Feb/15 01:02;mengxr;Issue resolved by pull request 4317
[https://github.com/apache/spark/pull/4317]",,,,,,,,,,
Pregel should checkpoint periodically to avoid StackOverflowError,SPARK-5484,12771092,Bug,http://spark.apache.org,Major,memory issue,,"Pregel-based iterative algorithms with more than ~50 iterations begin to slow down and eventually fail with a StackOverflowError due to Spark's lack of support for long lineage chains. Instead, Pregel should checkpoint the graph periodically.",,adeandrade,ankurd,apachespark,ding,lamerman,maropu,michael,praetp,prateekrungta,robineast,rohit13k,shreyagarwal@outlook.com,sophiasull,wesolows,,,,,,,,,,,,,SPARK-12431,SPARK-15739,,,,0,,,,,,,,,,,,,,,,27:44.1,,FALSE,,,,,,,,,,,,9.22E+18,,,Tue Oct 31 04:49:04 UTC 2017,,,,,0|i24yvb:,9.22E+18,,,,,2.2.0,2.3.0,,,,"29/Jan/15 19:27;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/4273",02/Jun/16 03:30;adeandrade;How is this not a priority? If we are not fixing Pregel then let's expose {{aggregateMessagesWithActiveSet}} to users so they can create their own version.,"16/Jul/16 09:32;wesolows;[~ankurd] do you plan to prepare another solution? I could see your pull request was closed, but did solve problem and was best thing at a time. The only thing I could see lacking was no ckeckpoint directory cleaning, and I guess I would change checkpoint iterations to 35 since it worked better during my tests.
More general solution proposed within SPARK-5561 needs a change within PeriodicRDDCheckpointer and PeriodicGraphCheckpointer - either to move it from mllib or change access modifier. ",16/Sep/16 02:11;ding;I will work on the issue if nobody took it.,"16/Sep/16 10:37;maropu;This component seems very inactive, so I think there is a little chance to review your code if you take on this.","16/Sep/16 17:18;ding;Thank you for your kindly reminder. However as the code is almost ready, I will still send PR in case someone has interest to review it.","17/Sep/16 02:52;apachespark;User 'dding3' has created a pull request for this issue:
https://github.com/apache/spark/pull/15125","03/Oct/16 17:38;shreyagarwal@outlook.com;Hi,

I am running a Spark 2.0 cluster and want to check if there is a way I can deploy this fix onto that. Also, it is kind of urgent :)

Regards,
Shreya","18/Jan/17 21:03;michael;Hi Guys,

@ding has rebased his PR, and it LGTM. Can a committer review it please? It's quite a helpful patch.","31/Oct/17 04:49;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/19618",,,
There will be an ArrayIndexOutOfBoundsException if the format of the source file is wrong,SPARK-5380,12769584,Bug,http://spark.apache.org,Minor,file format,,"When I build a graph with a file format error, there will be an ArrayIndexOutOfBoundsException",,apachespark,LiuHao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,34:20.7,,FALSE,,,,,,,,,,,,9.22E+18,,,Fri Feb 06 09:05:48 UTC 2015,,,,,0|i24prb:,9.22E+18,,,,,,,,,,"23/Jan/15 09:34;apachespark;User 'Leolh' has created a pull request for this issue:
https://github.com/apache/spark/pull/4176","06/Feb/15 09:05;srowen;I'm manually marking this fixed since the merge script wasn't able to do it for me, as I didn't have the right Python lib installed. Should work automatically next time.",,,,,,,,,,,
Can't zip RDDs with unequal numbers of partitions in ReplicatedVertexView.upgrade(),SPARK-5351,12769004,Bug,http://spark.apache.org,Major,data partition,,"If the value of 'spark.default.parallelism' does not match the number of partitoins in EdgePartition(EdgeRDDImpl), 
the following error occurs in ReplicatedVertexView.scala:72;

object GraphTest extends Logging {
def run[VD: ClassTag, ED: ClassTag](graph: Graph[VD, ED]): VertexRDD[Int] = {
graph.aggregateMessages[Int](
ctx => {
ctx.sendToSrc(1)
ctx.sendToDst(2)
},
_ + _)
}
}

val g = GraphLoader.edgeListFile(sc, ""graph.txt"")
val rdd = GraphTest.run(g)

java.lang.IllegalArgumentException: Can't zip RDDs with unequal numbers of partitions
	at org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:206)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:204)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:206)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:204)
	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:82)
	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:80)
	at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:193)
	at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:191)
    ...
",,ankurd,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,SPARK-2823,,,,,,0,,,,,,,,,,,,,,,,19:14.2,,FALSE,,,,,,,,,,,,9.22E+18,,,Sat Jan 24 03:36:34 UTC 2015,,,,,0|i24m9z:,9.22E+18,,,,,,,,,,"21/Jan/15 16:19;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/4136","24/Jan/15 03:36;ankurd;Issue resolved by pull request 4136
https://github.com/apache/spark/pull/4136",,,,,,,,,,,
ShortestPaths traverses backwards,SPARK-5343,12768872,Bug,http://spark.apache.org,Major,job,,"GraphX ShortestPaths seems to be following edges backwards instead of forwards:

import org.apache.spark.graphx._
val g = Graph(sc.makeRDD(Array((1L,""""), (2L,""""), (3L,""""))), sc.makeRDD(Array(Edge(1L,2L,""""), Edge(2L,3L,""""))))

lib.ShortestPaths.run(g,Array(3)).vertices.collect
res1: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map()), (3,Map(3 -> 0)), (2,Map()))

lib.ShortestPaths.run(g,Array(1)).vertices.collect

res2: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map(1 -> 0)), (3,Map(1 -> 2)), (2,Map(1 -> 1)))

The following changes may be what will make it run ""forward"":

Change one occurrence of ""src"" to ""dst"" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L64

Change three occurrences of ""dst"" to ""src"" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L65
",,ankurd,apachespark,boyork,michaelmalak,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,37:14.6,,FALSE,,,,,,,,,,,,9.22E+18,,,Tue Feb 10 23:02:40 UTC 2015,,,,,0|i24lgv:,9.22E+18,,,,,,,,,,"09/Feb/15 18:37;boyork;I'll take this issue, thanks.","09/Feb/15 19:04;apachespark;User 'brennonyork' has created a pull request for this issue:
https://github.com/apache/spark/pull/4478","10/Feb/15 23:02;ankurd;Issue resolved by pull request 4478
https://github.com/apache/spark/pull/4478",,,,,,,,,,
GraphX rmatGraph hangs,SPARK-5064,12764739,Bug,http://spark.apache.org,Major,job,,"org.apache.spark.graphx.util.GraphGenerators.rmatGraph(sc, 4, 8)

It just outputs ""0 edges"" and then locks up.

A spark-user message reports similar behavior:
http://mail-archives.apache.org/mod_mbox/spark-user/201408.mbox/%3C1408617621830-12570.post@n3.nabble.com%3E
",CentOS 7 REPL (no HDFS). Also tried Cloudera 5.2.0 QuickStart standalone compiled Scala with spark-submit.,ankurd,apachespark,kj-ki,michaelmalak,xhudik,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,40:38.7,,FALSE,,,,,,,,,,,,9.22E+18,,,Wed Jan 21 20:38:41 UTC 2015,,,,,0|i23xcn:,9.22E+18,,,,,,,,,,04/Jan/15 13:40;xhudik;the same behavior can be seen in version 1.3.0-snapshot as well. It get stuck and eat all resources,"08/Jan/15 13:32;apachespark;User 'kj-ki' has created a pull request for this issue:
https://github.com/apache/spark/pull/3950","21/Jan/15 20:38;ankurd;Issue resolved by pull request 3950
[https://github.com/apache/spark/pull/3950]",,,,,,,,,,
MimaExcludes should not exclude GraphX,SPARK-5032,12764362,Bug,http://spark.apache.org,Major,,,"Since GraphX is no longer alpha as of 1.2, MimaExcludes should not include this line for 1.3:
{code}
MimaBuild.excludeSparkPackage(""graphx""),
{code}
",,apachespark,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,36:49.4,,FALSE,,,,,,,,,,,,9.22E+18,,,Wed Dec 31 08:36:49 UTC 2014,,,,,0|i23vkf:,9.22E+18,,,,,1.3.0,,,,,"31/Dec/14 08:36;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/3856",,,,,,,,,,,,
Cut off the super long serialization chain in GraphX to avoid the StackOverflow error,SPARK-4672,12758581,Bug,http://spark.apache.org,Critical,io,,"While running iterative algorithms in GraphX, a StackOverflow error will stably occur in the serialization phase at about 300th iteration. In general, these kinds of algorithms have two things in common:

# They have a long computing chain.
{code:borderStyle=solid}
(e.g., “degreeGraph=>subGraph=>degreeGraph=>subGraph=>…=>”)
{code}
# They will iterate many times to converge. An example:
{code:borderStyle=solid}
//K-Core Algorithm
val kNum = 5

var degreeGraph = graph.outerJoinVertices(graph.degrees) {
		(vid, vd, degree) => degree.getOrElse(0)
}.cache()
	
do {
	val subGraph = degreeGraph.subgraph(
		vpred = (vid, degree) => degree >= KNum
	).cache()

	val newDegreeGraph = subGraph.degrees

	degreeGraph = subGraph.outerJoinVertices(newDegreeGraph) {
		(vid, vd, degree) => degree.getOrElse(0)
	}.cache()

	isConverged = check(degreeGraph)
} while(isConverged == false)
{code}

After about 300 iterations, StackOverflow will definitely occur with the following stack trace:

{code:borderStyle=solid}
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.StackOverflowError
java.io.ObjectOutputStream.writeNonProxyDesc(ObjectOutputStream.java:1275)
java.io.ObjectOutputStream.writeClassDesc(ObjectOutputStream.java:1230)
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1426)
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
{code}

It is a very tricky bug, which only occurs with enough iterations. Since it took us a long time to find out its causes, we will detail the causes in the following 3 paragraphs. 
 
h3. Phase 1: Try using checkpoint() to shorten the lineage

It's easy to come to the thought that the long lineage may be the cause. For some RDDs, their lineages may grow with the iterations. Also, for some magical references,  their lineage lengths never decrease and finally become very long. As a result, the call stack of task's serialization()/deserialization() method will be very long too, which finally exhausts the whole JVM stack.

In deed, the lineage of some RDDs (e.g., EdgeRDD.partitionsRDD) increases 3 OneToOne dependencies in each iteration in the above example. Lineage length refers to the  maximum length of OneToOne dependencies (e.g., from the finalRDD to the ShuffledRDD) in each stage.

To shorten the lineage, a checkpoint() is performed every N (e.g., 10) iterations. Then, the lineage will drop down when it reaches a certain length (e.g., 33). 

However, StackOverflow error still occurs after 300+ iterations!

h3. Phase 2:  Abnormal f closure function leads to a unbreakable serialization chain

After a long-time debug, we found that an abnormal _*f*_ function closure and a potential bug in GraphX (will be detailed in Phase 3) are the ""Suspect Zero"". They together build another serialization chain that can bypass the broken lineage cut by checkpoint() (as shown in Figure 1). In other words, the serialization chain can be as long as the original lineage before checkpoint().

Figure 1 shows how the unbreakable serialization chain is generated. Yes, the OneToOneDep can be cut off by checkpoint(). However, the serialization chain can still access the previous RDDs through the (1)->(2) reference chain. As a result, the checkpoint() action is meaningless and the lineage is as long as that before. 

!https://raw.githubusercontent.com/JerryLead/Misc/master/SparkPRFigures/g1.png|width=100%!

The (1)->(2) chain can be observed in the debug view (in Figure 2).

{code:borderStyle=solid}
_rdd (i.e., A in Figure 1, checkpointed) -> f -> $outer (VertexRDD) -> partitionsRDD:MapPartitionsRDD -> RDDs in  the previous iterations
{code}

!https://raw.githubusercontent.com/JerryLead/Misc/master/SparkPRFigures/g2.png|width=100%!


More description: While a RDD is being serialized, its f function 
{code:borderStyle=solid}
e.g., f: (Iterator[A], Iterator[B]) => Iterator[V]) in ZippedPartitionsRDD2
{code}

will be serialized too. This action will be very dangerous if the f closure has a member “$outer” that references its outer class (as shown in Figure 1). This reference will be another way (except the OneToOneDependency) that a RDD (e.g., PartitionsRDD) can reference the other RDDs (e.g., VertexRDD). Note that checkpoint() only cuts off the direct lineage, while the function reference is still kept. So, serialization() can still access the other RDDs along the f references. 

h3. Phase 3: Non-transient member variable of VertexRDD makes things worse

""Reference (1)"" in Figure 1 is caused by the abnormal f clousre, while ""Reference (2)"" is caused by the potential bug in GraphX: *PartitionsRDD is a non-transient member variable of VertexRDD*. 

With this _small_ bug, the f closure itself (without OneToOne dependency) can cause StackOverflow error, as shown in the red box in Figure 3:

# While _vertices:VertexRDD_ is being serialized, its member _PartitionsRDD_ will be serialized too.
# Next, while serializing this _partitionsRDD_, serialization() will simultaneously serialize its f’s referenced $outer. Here, it is another _partitionsRDD_.
# Finally, the chain 
{code:borderStyle=solid}
""f => f$3 => f$3 => $outer => vertices: VertexRDD => partitionsRDD => … => ShuffledRDD""
{code}

comes into shape. As a result, the serialization chain can be as long as the original lineage and finally triggers StackOverflow error.
  
!https://raw.githubusercontent.com/JerryLead/Misc/master/SparkPRFigures/g3.png|width=100%!


h2. Conclusions

In conclusion, the root cause of StackOverflow error is the long serialization chain, which cannot be cut off by _checkpoint()_. This long chain is caused by the multiple factors, including:

# long lineage
# $outer reference in the f closure
# non-transient member variable

h2. How to fix this error

We propose three pull requests as follows to solve this problem thoroughly.

# PR-3544
In this pr, we change the ""val PartitionsRDD"" to be transient in EdgeRDDImpl and VertexRDDImpl. As a result, while _vertices:VertexRDD_ is being serialized, its member _PartitionsRDD_ will not be serialized. In other words, the ""Reference (2)"" in Figure 1 will be cut off.
# PR-3545
In this pr, we set ""f = null"" if ZippedPartitionsRDD is checkpointed. As a result, when PartitionsRDD is checkpointed, its f closure will be cleared and the ""Reference (1)"" (i.e., f => $outer) in Figure 1 will no exist.
# PR-3549
To cut off the long lineage, we need to perform checkpoint()  on PartitionsRDD. However, current checkpoint() is performed on VertexRDD and EdgeRDD themselves. As a result, we need to override the checkpoint() methods in VertexRDDImpl and EdgeRDDImpl to perform checkpoint() on PartitionsRDD.







",,adeandrade,ankurd,apachespark,glenn.strycker@gmail.com,jason.dai,jerrylead,lianhuiwang,liyezhang556520,rxin,,,,,,,,,,,,,,,,,,SPARK-3623,,,,,0,,,,,,,,,,,,,,,,51:50.0,,FALSE,,,,,,,,,,,,9.22E+18,,,Wed Dec 03 10:11:11 UTC 2014,,,,,0|i22wtr:,9.22E+18,,,,,,,,,,"02/Dec/14 03:51;apachespark;User 'JerryLead' has created a pull request for this issue:
https://github.com/apache/spark/pull/3545","02/Dec/14 06:31;apachespark;User 'JerryLead' has created a pull request for this issue:
https://github.com/apache/spark/pull/3549","02/Dec/14 18:40;rxin;cc [~ankurdave]

Can you take a look at this ASAP? Would be great to fix for 1.2.
","03/Dec/14 03:35;jason.dai;We ran into the same issue, and this is a nice summary for the bug analysis. On the other hand, while this may fix the specific GraphX issue, I don't think it is generally applicable for dealing with super long lineage that can be generated in GraphX or other iterative algorithms. 

In particular, the user can define arbitrary functions, which can be called in RDD.compute() and refer to an arbitrary member variable that is an RDD, or can be used to construct another RDD, such as:
{noformat}
  class MyRDD (val rdd1, val rdd2, func1) extends RDD {
    val func2 = (f, iter1, iter2) => iter1– f(iter2)
    …
    override def compute(part, sc) {
      func2(func1, rdd1.iterator(part, sc), rdd2.iterator(part, sc))
    }
    …
   define newRDD(val rdd3, func3) = {
     val func4 = func2(func3)
     new AnotherRDD() {
       override def compute(part, sc) {
         func4(rdd1.iterator(part, sc) + rdd2.iterator(part, sc), rdd3.iterator(part, sc))
       }  
    }
  }
}
{noformat}
In this case, we will need to serialize rdd1 and rdd2 before MyRDD is checkpointed; after MyRDD is checkpointed, we don’t need to serialize rdd1 or rdd2, but we cannot clear func2 either. 

I think we can fix this more general issues as follows:
# As only RDD.compute(or RDD.iterator) should be called at the worker side, we only need to serialize anything that is referenced in that function (no matter it’s a member variable or not)
# After the RDD is checkpointed, the RDD.compute should be changed to read the checkpint file, which will not reference other variables – again, we only need to serialize whatever is referenced in that function now
","03/Dec/14 03:37;rxin;Yea it makes sense to remove all the function closure f from an RDD if it is checkpointed.
","03/Dec/14 03:44;jason.dai;[~rxin] what exactly do you mean by ""remove all the function closure f from an RDD if it is checkpointed""?

In my previous example, we should not clear func2 even if MyRDD is checkpointed, otherwise newRDD() will be no longer correct. Instead, we should make sure we only include RDD.compute(or RDD.iterator) in the closure (no matter whether it is checkpointed or not), and change RDD.compute to reading checkpoint files once it is checkpointed.","03/Dec/14 03:50;rxin;Ok I admit I wasn't reading your comment too carefully :)

Is there a concrete way you are proposing that would solve this problem for arbitrarily defined RDDs? I don't think it is solvable at this point.

That said, we can solve this for most of the built-in RDDs.


","03/Dec/14 04:08;jason.dai;I can see two possible ways to fix this:
# Define customized closure serialization mechanisms in task serializations, which can use reflections to carefully choose which to serialize (i.e., only those referenced by RDD.iterator); this potentially needs to deal with many details and can be error prone.
# In task serialization, each ""base"" RDD can generate a dual, ""shippable"" RDD, which only has transient member variables, and only implements the compute() function (which in turn calls the compute() function of the ""base"" RDD through ClosureCleaner.clean()); we can then probably rely on the Java serializer to handle this correctly.","03/Dec/14 07:54;ankurd;Issue resolved by pull request 3545
[https://github.com/apache/spark/pull/3545]","03/Dec/14 07:55;ankurd;[~jerrylead] Thanks for investigating this bug and the excellent explanation. Now that the PRs are merged, can you confirm that the bug is fixed for you? I haven't yet been able to reproduce it locally.","03/Dec/14 10:11;jerrylead;Thank you [~ankurdave]. Yes, the StackOverflow error disappears once the PRs are merged and checkpoint() is performed every N iterations. However, If the lineage of iterative algorithms are too long, we still need to do checkpoint() manually to cut off the lineage to avoid this error. Moreover, the fix suggestion given by [~jason.dai] is fine since it is a general problem. We need more elegant methods to avoid the long chain of task's serialization().",,
Too many TripletFields,SPARK-4627,12758043,Bug,http://spark.apache.org,Trivial,,,"The `TripletFields` class defines a set of constants for all possible configurations of the triplet fields.  However, many are not useful and as result the API is slightly confusing.  ",,apachespark,jegonzal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,00:16.1,,FALSE,,,,,,,,,,,,9.22E+18,,,Wed Nov 26 19:00:25 UTC 2014,,,,,0|i22tlb:,9.22E+18,,,,,,,,,,"26/Nov/14 19:00;apachespark;User 'jegonzal' has created a pull request for this issue:
https://github.com/apache/spark/pull/3472",26/Nov/14 19:00;jegonzal;Fixed in PR #3472.,,,,,,,,,,,
A problem of EdgePartitionBuilder in Graphx,SPARK-4249,12753031,Bug,http://spark.apache.org,Minor,data structure,,"https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/impl/EdgePartitionBuilder.scala#L48
in function toEdgePartition, code snippet     "" index.update(srcIds(0), 0)""
the elements in array srcIds are all not initialized and are all 0. The effect is that if vertex ids don't contain 0, indexSize is equal to (realIndexSize + 1) and 0 is added to the `index`
It seems that all versions have the problem",,ankurd,apachespark,Cookies,,,,,,,,,,,,,,,,,,,,,,,SPARK-4173,,,,,,0,,,,,,,,,,,,,,,,08:32.5,,FALSE,,,,,,,,,,,,9.22E+18,,,Thu Nov 06 18:50:58 UTC 2014,,,,,0|i21zvj:,9.22E+18,,,,,1.0.3,1.1.1,1.2.0,,,"06/Nov/14 14:08;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/3138","06/Nov/14 18:50;ankurd;Issue resolved by pull request 3138
[https://github.com/apache/spark/pull/3138]",,,,,,,,,,,
EdgePartitionBuilder uses wrong value for first clustered index,SPARK-4173,12752031,Bug,http://spark.apache.org,Major,data parition,,"Lines 48 and 49 in EdgePartitionBuilder reference {{srcIds}} before it has been initialized, causing an incorrect value to be stored for the first cluster.

https://github.com/apache/spark/blob/23468e7e96bf047ba53806352558b9d661567b23/graphx/src/main/scala/org/apache/spark/graphx/impl/EdgePartitionBuilder.scala#L48-49",,ankurd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,9.22E+18,,,Thu Nov 06 18:56:44 UTC 2014,,,,,0|i21ttb:,9.22E+18,,,,,,,,,,"06/Nov/14 18:56;ankurd;Issue resolved by pull request 3138
[https://github.com/apache/spark/pull/3138]",,,,,,,,,,,,
[GraphX] Modify option name according to example doc in SynthBenchmark ,SPARK-4146,12751584,Bug,http://spark.apache.org,Minor,,,"Now graphx.SynthBenchmark example has an option of iteration number named as ""niter"". However, in its document, it is named as ""niters"". The mismatch between the implementation and document causes certain IllegalArgumentException while trying that example.",,grace.huang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,9.22E+18,,,35:44.0,,,,,0|i21r53:,9.22E+18,,,,,,,,,,,,,,,,,,,,,,
Bad Default for GraphLoader Edge Partitions,SPARK-4142,12751562,Bug,http://spark.apache.org,Major,io,,The default number of edge partitions for the GraphLoader is set to 1 rather than the default parallelism.,,apachespark,jegonzal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,08:11.8,,FALSE,,,,,,,,,,,,9.22E+18,,,Thu Oct 30 00:08:11 UTC 2014,,,,,0|i21r07:,9.22E+18,,,,,,,,,,"30/Oct/14 00:08;apachespark;User 'jegonzal' has created a pull request for this issue:
https://github.com/apache/spark/pull/3006",,,,,,,,,,,,
Remove auto join elimination and introduce aggregateMessages,SPARK-3936,12747868,Bug,http://spark.apache.org,Blocker,,,"This is actually a ticket with two separate problems:

1. Remove auto join elimination

2. Introduce a new fundamental primitive aggregateMessages

For the first one, description provided by Pedro:

There seems to be a bug with the GraphX byte code inspection, specifically in BytecodeUtils. 

These are the unit tests I wrote to expose the problem:
https://github.com/EntilZha/spark/blob/a3c38a8329545c034fae2458df134fa3829d08fb/graphx/src/test/scala/org/apache/spark/graphx/util/BytecodeUtilsSuite.scala#L93-L121

The first two tests pass, the second two tests fail. This exposes a problem with inspection of methods in closures, in this case within maps. Specifically, it seems like there is a problem with inspection of non-inline methods in a closure.


For the 2nd one, see pull request https://github.com/apache/spark/pull/3100",,apachespark,maropu,pedrorodriguez,prateekrungta,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,17:43.0,,FALSE,,,,,,,,,,,,9.22E+18,,,Wed Nov 26 05:43:59 UTC 2014,,,,,0|i214lj:,9.22E+18,ankurd,,,,,,,,,"15/Oct/14 20:17;apachespark;User 'jegonzal' has created a pull request for this issue:
https://github.com/apache/spark/pull/2815","05/Nov/14 01:57;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/3100","26/Nov/14 05:43;rxin;BTW message from Taobao: 根据团队小伙伴的测试，[SPARK-3936] Add aggregateMessages, which supersedes mapReduceTriplets这个issue确实Work，可以提升30%的性能。以KCore为例，在standord的100w的数据集上运行作业，运行时间由 37mins减到22mins。使用GraphX的同学可以早日升级。#给周末此时还在加班做Spark和图计算的同学#

i.e. switching from mapReduceTriplets to aggregateMessages improved performance by 30% in their kcore implementation.",,,,,,,,,,
Triangle Count handles reverse edges incorrectly,SPARK-3650,12743360,Bug,http://spark.apache.org,Critical,job,,"The triangle count implementation assumes that edges are aligned in a canonical direction.  As stated in the documentation:

bq. Note that the input graph should have its edges in canonical direction (i.e. the `sourceId` less than `destId`)

However the TriangleCount algorithm does not verify that this condition holds and indeed even the unit tests exploits this functionality:

{code:scala}
val triangles = Array(0L -> 1L, 1L -> 2L, 2L -> 0L) ++
        Array(0L -> -1L, -1L -> -2L, -2L -> 0L)
      val rawEdges = sc.parallelize(triangles, 2)
      val graph = Graph.fromEdgeTuples(rawEdges, true).cache()
      val triangleCount = graph.triangleCount()
      val verts = triangleCount.vertices
      verts.collect().foreach { case (vid, count) =>
        if (vid == 0) {
          assert(count === 4)  // <-- Should be 2
        } else {
          assert(count === 2) // <-- Should be 1
        }
      }
{code}


",,apachespark,glenn.strycker@gmail.com,jegonzal,ovidiumarcu,robineast,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,30:27.3,,FALSE,,,,,,,,,,,,9.22E+18,,,Mon Feb 22 05:37:34 UTC 2016,,,,,0|i20d53:,9.22E+18,,,,,,,,,,"22/Sep/14 22:30;apachespark;User 'jegonzal' has created a pull request for this issue:
https://github.com/apache/spark/pull/2495","23/Jan/15 09:28;apachespark;User 'Leolh' has created a pull request for this issue:
https://github.com/apache/spark/pull/4176",26/Jun/15 15:11;robineast;What is the status of this issue? A user on the mailing list just ran into to this issue. It looks like PR-2495 should fix the issue. Is there a version that is being targeted for the fix?,27/Jun/15 05:38;srowen;[~robineast] looks like https://github.com/apache/spark/pull/2495 just never got merged for some reason. Dust it off and ping jegonzal and ankurdave for review (again),18/Feb/16 10:05;ovidiumarcu;Can someone look over this issue?,18/Feb/16 10:09;srowen;[~ovidiumarcu] what are you expecting here? you should try to revive the PR as I mentioned above if you're interested. I don't think anyone else is working on GraphX though.,"18/Feb/16 12:28;ovidiumarcu;I see interesting issues on GraphX, nobody working on, maybe low priority. too bad.","18/Feb/16 13:48;robineast;I did ask if the PR could be revived but never followed up on it. If I get a moment I'll try and submit the PR myself however have been a little busy on other GraphX things.

By the way there is a workaround to the issue which is to make sure your edges are in the canonical direction before calling triangleCount.","21/Feb/16 11:55;apachespark;User 'insidedctm' has created a pull request for this issue:
https://github.com/apache/spark/pull/11290",22/Feb/16 05:37;ovidiumarcu;Is it possible to apply this fix to a 1.5 version?,,,
ClassCastException in GraphX custom serializers when sort-based shuffle spills,SPARK-3649,12743355,Bug,http://spark.apache.org,Major,job,,"As [reported|http://apache-spark-user-list.1001560.n3.nabble.com/java-lang-ClassCastException-java-lang-Long-cannot-be-cast-to-scala-Tuple2-td13926.html#a14501] on the mailing list, GraphX throws

{code}
java.lang.ClassCastException: java.lang.Long cannot be cast to scala.Tuple2
        at org.apache.spark.graphx.impl.RoutingTableMessageSerializer$$anon$1$$anon$2.writeObject(Serializers.scala:39) 
        at org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:195) 
        at org.apache.spark.util.collection.ExternalSorter.spillToMergeableFile(ExternalSorter.scala:329)
{code}

when sort-based shuffle attempts to spill to disk. This is because GraphX defines custom serializers for shuffling pair RDDs that assume Spark will always serialize the entire pair object rather than breaking it up into its components. However, the spill code path in sort-based shuffle [violates this assumption|https://github.com/apache/spark/blob/f9d6220c792b779be385f3022d146911a22c2130/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala#L329].

GraphX uses the custom serializers to compress vertex ID keys using variable-length integer encoding. However, since the serializer can no longer rely on the key and value being serialized and deserialized together, performing such encoding would require writing a tag byte. Therefore it may be better to simply remove the custom serializers.",,ankurd,apachespark,LiuZeshan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,45:25.5,,FALSE,,,,,,,,,,,,9.22E+18,,,Tue Sep 23 04:45:25 UTC 2014,,,,,0|i20d3z:,9.22E+18,,,,,,,,,,"23/Sep/14 04:45;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/2503",,,,,,,,,,,,
Find Strongly Connected Components with Graphx has a small bug,SPARK-3635,12743107,Bug,http://spark.apache.org,Trivial,,,"The strongly connected components function (spark / graphx / src / main / scala / org / apache / spark / graphx / lib / StronglyConnectedComponents.scala) has a typo in the condition on line 78.
I think the condition should be ""if (e.srcAttr._1 < e.dstAttr._1)"" instead of ""if (e.srcId < e.dstId)""","VMWare, Centos 6.5",ankurd,apachespark,odedz,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,0,,,,,,,,,,,,,,,,05:30.4,,FALSE,,,,,,,,,,,,9.22E+18,,,Tue Sep 30 01:07:52 UTC 2014,,,,,0|i20blz:,9.22E+18,,,,,1.1.1,1.2.0,,,,"22/Sep/14 08:05;apachespark;User 'odedz' has created a pull request for this issue:
https://github.com/apache/spark/pull/2486","30/Sep/14 01:07;ankurd;Issue resolved by pull request 2486
[https://github.com/apache/spark/pull/2486]",,,,,,,,,,,
GraphGenerators.sampleLogNormal sometimes returns too-large result,SPARK-3578,12742312,Bug,http://spark.apache.org,Minor,,,"GraphGenerators.sampleLogNormal is supposed to return an integer strictly less than maxVal. However, it violates this guarantee. It generates its return value as follows:

{code}
var X: Double = maxVal

while (X >= maxVal) {
  val Z = rand.nextGaussian()
  X = math.exp(mu + sigma*Z)
}
math.round(X.toFloat)
{code}

When X is sampled to be close to (but less than) maxVal, then it will pass the while loop condition, but the rounded result will be equal to maxVal, which will fail the test.

For example, if maxVal is 5 and X is 4.9, then X < maxVal, but math.round(X.toFloat) is 5.

A solution is to round X down instead of to the nearest integer.",,ankurd,apachespark,jegonzal,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,20:55.3,,FALSE,,,,,,,,,,,,9.22E+18,,,Mon Sep 22 20:52:03 UTC 2014,,,,,0|i206pb:,9.22E+18,,,,,,,,,,"17/Sep/14 23:20;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/2439","18/Sep/14 04:42;pwendell;@ankurdave, could you tag stuff as GraphX component on JIRA? Thanks!","18/Sep/14 07:29;ankurd;[~pwendell] Sorry, I forgot to do that this time.",22/Sep/14 20:52;jegonzal;Resolved by https://github.com/apache/spark/pull/2439,,,,,,,,,
GraphX unit tests fail nondeterministically,SPARK-3400,12739029,Bug,http://spark.apache.org,Blocker,,,"GraphX unit tests have been failing since the fix to SPARK-2823 was merged: https://github.com/apache/spark/commit/9b225ac3072de522b40b46aba6df1f1c231f13ef. Failures have appeared as Snappy parsing errors and shuffle FileNotFoundExceptions. A local test showed that these failures occurred in about 3/10 test runs.

Reverting the mentioned commit seems to solve the problem. Since this is blocking everyone else, I'm submitting a hotfix to do that, and we can diagnose the problem in more detail afterwards.",,aash,ankurd,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,50:25.2,,FALSE,,,,,,,,,,,,9.22E+18,,,Mon Sep 22 01:28:42 UTC 2014,,,,,0|i1zo0f:,9.22E+18,,,,,,,,,,"04/Sep/14 06:50;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/2271","04/Sep/14 06:50;ankurd;Issue resolved by pull request 2271
[https://github.com/apache/spark/pull/2271]",16/Sep/14 17:35;aash;[~ankurd] is there another ticket to track the root cause diagnosis?  I'm seeing a similar error with a custom registrator (not GraphX's) and would like to follow along.,16/Sep/14 20:05;ankurd;[~aash] There isn't another ticket yet. Would you mind creating one?,22/Sep/14 01:28;aash;Filed as SPARK-3630,,,,,,,,
No unpersist callls in SVDPlusPlus,SPARK-3290,12737619,Bug,http://spark.apache.org,Major,memory issue,,"The implementation of SVDPlusPlus will cache graph produced by each iteration and do not unpersist them, so as iteration goes on, more and more useless graph will be cached and out of memory happens.",,ankurd,apachespark,Darlwen,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,16:26.1,,FALSE,,,,,,,,,,,,9.22E+18,,,Sat Feb 14 04:15:55 UTC 2015,,,,,0|i1zgvj:,9.22E+18,,,,,,,,,,"27/Jan/15 23:16;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4234","14/Feb/15 04:15;ankurd;Issue resolved by pull request 4234
https://github.com/apache/spark/pull/4234",,,,,,,,,,,
PR #720 broke GraphGenerator.logNormal,SPARK-3263,12737213,Bug,http://spark.apache.org,Major,job,,"PR #720 made multiple changes to GraphGenerator.logNormalGraph including:

* Replacing the call to functions for generating random vertices and edges with in-line implementations with different equations
* Hard-coding of RNG seeds so that method now generates the same graph for a given number of vertices, edges, mu, and sigma -- user is not able to override seed or specify that seed should be randomly generated.
* Backwards-incompatible change to logNormalGraph signature with introduction of new required parameter.
* Failed to update scala docs and programming guide for API changes

I also see that PR #720 added a Synthetic Benchmark in the examples.

Based on reading the Pregel paper, I believe the in-line functions are incorrect.  I proposed to:

* Removing the in-line calls
* Adding a seed for deterministic behavior (when desired)
* Keeping the number of partitions parameter.
* Updating the synthetic benchmark example",,ankurd,apachespark,rnowling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,13:25.9,,FALSE,,,,,,,,,,,,9.22E+18,,,Wed Sep 03 21:17:47 UTC 2014,,,,,0|i1zetb:,9.22E+18,,,,,,,,,,"27/Aug/14 21:13;apachespark;User 'rnowling' has created a pull request for this issue:
https://github.com/apache/spark/pull/2168","03/Sep/14 21:17;ankurd;Issue resolved by pull request 2168
[https://github.com/apache/spark/pull/2168]",,,,,,,,,,,
Error in PageRank values,SPARK-3206,12736555,Bug,http://spark.apache.org,Major,,,"I have found a small example where the PageRank values using run and runUntilConvergence differ quite a bit.

I am running the Pagerank module on the following graph:

Edge Table:

|| Node1 || Node2 ||
|1 | 2 |
|1 |	3|
|3 |	2|
|3 |	4|
|5 |	3|
|6 |	7|
|7 |	8|
|8 |	9|
|9 |	7|

Node Table (note the extra node):

|| NodeID  || NodeName  ||
|a |	1|
|b |	2|
|c |	3|
|d |	4|
|e |	5|
|f |	6|
|g |	7|
|h |	8|
|i |	9|
|j.longaddress.com |	10|

with a default resetProb of 0.15.
When I compute the pageRank with runUntilConvergence, running 

{{val ranks = PageRank.runUntilConvergence(graph,0.0001).vertices}}

I get the ranks
(4,0.29503124999999997)
(1,0.15)
(6,0.15)
(3,0.34124999999999994)
(7,1.3299054047985106)
(9,1.2381240056453071)
(8,1.2803346052504254)
(10,0.15)
(5,0.15)
(2,0.35878124999999994)

However, when I run page Rank with the run() method, running  

{{val ranksI = PageRank.run(graph,100).vertices}} 

I get the page ranks

(4,0.29503124999999997)
(1,0.15)
(6,0.15)
(3,0.34124999999999994)
(7,0.9999999387662847)
(9,0.9999999256447741)
(8,0.9999999256447741)
(10,0.15)
(5,0.15)
(2,0.29503124999999997)

These are quite different, leading me to suspect that one of the PageRank methods is incorrect. I have examined the source, but I do not know what the correct fix is, or which set of values is correct.",UNIX with Hadoop,ankurd,pfontana3w2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,46:50.1,,FALSE,,,,,,,,,,,,9.22E+18,,,Wed Nov 12 08:47:12 UTC 2014,,,,,0|i1zatb:,9.22E+18,,,,,,,,,,"03/Sep/14 21:46;ankurd;I think the `run` method is incorrect due to a bug in Pregel. I wrote a standalone (non-Pregel) version of PageRank that provides the functionality of `run` without using Pregel: https://github.com/ankurdave/spark/blob/low-level-PageRank/graphx/src/main/scala/org/apache/spark/graphx/impl/GraphImpl.scala#L274

I'd like to run that and check the results against these ones when I get a chance.","12/Nov/14 08:47;ankurd;I just tested this with the standalone version of PageRank that was introduced in SPARK-3427, and it seems to be fixed, so I'm closing this.

{code}
scala> val e = sc.parallelize(List(
  (1, 2), (1, 3), (3, 2), (3, 4), (5, 3), (6, 7), (7, 8), (8, 9), (9, 7)))

scala> val g = Graph.fromEdgeTuples(e.map(kv => (kv._1.toLong, kv._2.toLong)), 0)

scala> g.pageRank(0.0001).vertices.collect.foreach(println)
(8,1.2808550959634413)
(1,0.15)
(9,1.2387268204156412)
(2,0.35878124999999994)
(3,0.34124999999999994)
(4,0.29503124999999997)
(5,0.15)
(6,0.15)
(7,1.330417786200011)

scala> g.staticPageRank(100).vertices.collect.foreach(println)
(8,1.2803346052504254)
(1,0.15)
(9,1.2381240056453071)
(2,0.35878124999999994)
(3,0.34124999999999994)
(4,0.29503124999999997)
(5,0.15)
(6,0.15)
(7,1.3299054047985106)
{code}",,,,,,,,,,,
Creation of large graph(> 2.15 B nodes) seems to be broken:possible overflow somewhere ,SPARK-3190,12736210,Bug,http://spark.apache.org,Critical,memory issue,,"While creating a graph with 6B nodes and 12B edges, I noticed that 'numVertices' api returns incorrect result; 'numEdges' reports correct number. For few times(with different dataset > 2.5B nodes) I have also notices that numVertices is returned as -ive number; so I suspect that there is some overflow (may be we are using Int for some field?).

Here is some details of experiments  I have done so far: 
1. Input: numNodes=6101995593 ; noEdges=12163784626
   Graph returns: numVertices=1807028297 ;  numEdges=12163784626

2. Input : numNodes=2157586441 ; noEdges=2747322705
   Graph Returns: numVertices=-2137380855 ;  numEdges=2747322705

3. Input: numNodes=1725060105 ; noEdges=204176821
   Graph: numVertices=1725060105 ;  numEdges=2041768213

You can find the code to generate this bug here: 

https://gist.github.com/npanj/92e949d86d08715bf4bf

Note: Nodes are labeled are 1...6B .














 ",Standalone mode running on EC2 . Using latest code from master branch upto commit #db56f2df1b8027171da1b8d2571d1f2ef1e103b6 .,ankurd,apachespark,joshrosen,liyuance,npanj,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,31:01.7,,FALSE,,,,,,,,,,,,9.22E+18,,,Mon May 02 06:20:04 UTC 2016,,,,,0|i1z97b:,9.22E+18,,,,,1.3.2,1.4.2,1.5.0,,,"23/Aug/14 23:31;ankurd;I haven't tried to reproduce this yet, but counting the vertices occurs in [VertexRDD.scala:110|https://github.com/apache/spark/blob/3519b5e8e55b4530d7f7c0bcab254f863dbfa814/graphx/src/main/scala/org/apache/spark/graphx/VertexRDD.scala#L110], which sums up an Int from each partition and only promotes it to a Long when returning the result. Therefore it should fix the problem to change line 111 to
{code}
partitionsRDD.map(_.size.toLong).reduce(_ + _)
{code}","24/Aug/14 01:12;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/2106",25/Aug/14 06:17;npanj;Thanks Ankur for patch. I can confirm that this pull request fixed the issue.,"28/Aug/14 22:18;joshrosen;Issue resolved by pull request 2106
[https://github.com/apache/spark/pull/2106]",04/Aug/15 05:33;ankurd;Regression due to https://github.com/apache/spark/commit/a5ef58113667ff73562ce6db381cff96a0b354b0#diff-dded6985c865d7623d83ec5a860e1bd4R75,"04/Aug/15 05:39;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/7923","02/May/16 05:55;liyuance;The PR{2106，7923} can not fix the problem completely, as  the number of vertices in one of partition exceed Integer.MAX_VALUE also can repreduce this Bug。 The fundamental cause of this problem is the variable “size” is defined as type Int  in class VertexPartitionBase.","02/May/16 06:20;apachespark;User 'liyuance' has created a pull request for this issue:
https://github.com/apache/spark/pull/12835",,,,,
PartitionStrategy: VertexID hash overflow,SPARK-2981,12733383,Bug,http://spark.apache.org,Major,memory issue,,"In EdgePartition1D, a PartitionID is calculated by multiplying VertexId with a mixingPrime (1125899906842597L) then cast to Int, and mod numParts.

The Long is overflowed, and when cast to Int:

{quote}
scala> (1125899906842597L*1).toInt
res1: Int = -27

scala> (1125899906842597L*2).toInt
res2: Int = -54

scala> (1125899906842597L*3).toInt
res3: Int = -81
{quote}
As the cast produce number that are multiplies of 3, the partition is not useable when partitioning to multiples of 3.

for example when you partition to 6 or 9 parts:
{quote}
14/08/12 09:26:21 INFO GraphXPartition: GRAPHX: psrc Array((0,4347084), (1,0), (2,0), (3,3832578), (4,0), (5,0))
14/08/12 09:26:21 INFO GraphXPartition: GRAPHX: pdst Array((0,4347084), (1,0), (2,0), (3,3832578), (4,0), (5,0)) 

14/08/12 09:21:46 INFO GraphXPartition: GRAPHX: psrc Array((0,8179662), (1,0), (2,0), (3,0), (4,0), (5,0), (6,0), (7,0), (8,0))
14/08/12 09:21:46 INFO GraphXPartition: GRAPHX: pdst Array((0,8179662), (1,0), (2,0), (3,0), (4,0), (5,0), (6,0), (7,0), (8,0)) 

so the vertices are partitioned to 0,3 for 6; and 0 for 9
{quote}

I think solution is to cast after mod.
{quote}
scala> (1125899906842597L*3)
res4: Long = 3377699720527791

scala> (1125899906842597L*3) % 9
res5: Long = 3

scala> ((1125899906842597L*3) % 9).toInt
res5: Int = 3
{quote}",,ankurd,apachespark,larryxiao,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,0,,,,,,,,,,,,,,,,56:58.5,,FALSE,,,,,,,,,,,,411411,,,Mon Sep 08 08:26:12 UTC 2014,,,,,0|i1ys2n:,411402,,,,,,,,,,"12/Aug/14 06:56;apachespark;User 'larryxiao' has created a pull request for this issue:
https://github.com/apache/spark/pull/1902",08/Sep/14 08:26;ankurd;Reopening temporarily to correct the Fix Version.,,,,,,,,,,,
GraphX jobs throw IllegalArgumentException,SPARK-2823,12731678,Bug,http://spark.apache.org,Major,data partition,,"If the users set “spark.default.parallelism” and the value is different with the EdgeRDD partition number, GraphX jobs will throw IllegalArgumentException:

14/07/26 21:06:51 WARN DAGScheduler: Creating new stage failed due to exception - job: 1
java.lang.IllegalArgumentException: Can't zip RDDs with unequal numbers of partitions
        at org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:60)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
        at org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:54)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getShuffleMapStage(DAGScheduler.scala:1
97)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:272)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:269)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$visit$1(DAGScheduler.scala:269)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:274)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:269)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$visit$1(DAGScheduler.scala:269)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:274)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$visit$1$1.apply(DAGScheduler.s
cala:269)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$visit$1(DAGScheduler.scala:269)
        at org.apache.spark.scheduler.DAGScheduler.getParentStages(DAGScheduler.scala:279)
        at org.apache.spark.scheduler.DAGScheduler.newStage(DAGScheduler.scala:219)
        at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:672)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1184)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)",,ankurd,apachespark,joshrosen,luluorta,maropu,pedrorodriguez,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,16:52.8,,FALSE,,,,,,,,,,,,409707,,,Sat Jan 24 03:35:23 UTC 2015,,,,,0|i1yhpz:,409699,,,,,1.0.3,1.1.2,1.2.1,1.3.0,,"04/Aug/14 07:16;apachespark;User 'luluorta' has created a pull request for this issue:
https://github.com/apache/spark/pull/1763","03/Sep/14 02:30;ankurd;Issue resolved by pull request 1763
[https://github.com/apache/spark/pull/1763]",04/Sep/14 06:51;ankurd;I had to revert this because of SPARK-3400.,"16/Dec/14 00:17;joshrosen;I've removed the ""Fix Versions"" from this JIRA because its fix was reverted.","20/Jan/15 02:45;pedrorodriguez;I just ran into this bug while testing LDA code (hasn't ever happened before, calls into GraphX). I am not sure what causes it, but rather than running it in cluster or locally, I am running it on a Docker container. It also looks like if you run with local[2], the problem could be reproduced.

Is this being worked on or should I maybe try looking into it soon?","23/Jan/15 07:14;joshrosen;I think that the Snappy error was caused by a different change, so it should be safe to re-attempt the original fix.  There's a PR for this at https://github.com/apache/spark/pull/4136 (opened for SPARK-5351, a possible duplicate of this issue)",23/Jan/15 16:42;pedrorodriguez;I looked into this more and it looks like in addition to a bug in GraphX there is a bug in Spark core with handling textFile and zip https://issues.apache.org/jira/browse/SPARK-5385. Not certain if they may be related.,"24/Jan/15 03:35;ankurd;Issue resolved by pull request 4136
https://github.com/apache/spark/pull/4136",,,,,
"Loss of precision for small arguments to Math.exp, Math.log",SPARK-2748,12730765,Bug,http://spark.apache.org,Minor,,,"In a few places in MLlib, an expression of the form log(1.0 + p) is evaluated. When p is so small that 1.0 + p == 1.0, the result is 0.0. However the correct answer is very near p. This is why Math.log1p exists.

Similarly for one instance of exp(m) - 1 in GraphX; there's a special Math.expm1 method.

While the errors occur only for very small arguments, given their use in machine learning algorithms, this is entirely possible.

Also, while we're here, naftaliharris discovered a case in Python where 1 - 1 / (1 + exp(margin)) is less accurate than exp(margin) / (1 + exp(margin)). I don't think there's a JIRA on that one, so maybe this can serve as an umbrella for all of these related issues.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,40:59.1,,FALSE,,,,,,,,,,,,408838,,,Wed Jul 30 15:57:05 UTC 2014,,,,,0|i1ycfz:,408836,,,,,1.1.0,,,,,"30/Jul/14 11:40;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/1659","30/Jul/14 11:41;srowen;PR: https://github.com/apache/spark/pull/1659
See also: https://github.com/apache/spark/pull/1652","30/Jul/14 15:57;mengxr;Issue resolved by pull request 1659
[https://github.com/apache/spark/pull/1659]",,,,,,,,,,
VertexPartition is not serializable,SPARK-2455,12726883,Bug,http://spark.apache.org,Major,data partition,,"VertexPartition and ShippableVertexPartition are contained in RDDs but are not marked Serializable, leading to NotSerializableExceptions when using Java serialization.

The fix is simply to mark them as Serializable.",,ankurd,,,,,,,,,,,,,,,,,,,,,,SPARK-2347,,,,,,,,,0,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,404990,,,Fri Jul 11 22:17:03 UTC 2014,,,,,0|i1xp5r:,405026,,,,,,,,,,11/Jul/14 22:17;ankurd;Proposed fix: https://github.com/apache/spark/pull/1376,,,,,,,,,,,,
Graph object can not be set to StorageLevel.MEMORY_ONLY_SER,SPARK-2347,12725093,Bug,http://spark.apache.org,Major,job,,"I'm creating Graph object by using 

Graph(vertices, edges, null, StorageLevel.MEMORY_ONLY, StorageLevel.MEMORY_ONLY)

But that will throw out not serializable exception on both workers and driver. 

14/07/02 16:30:26 ERROR BlockManagerWorker: Exception handling buffer message
java.io.NotSerializableException: org.apache.spark.graphx.impl.VertexPartition
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)
	at org.apache.spark.serializer.SerializationStream$class.writeAll(Serializer.scala:106)
	at org.apache.spark.serializer.JavaSerializationStream.writeAll(JavaSerializer.scala:30)
	at org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:988)
	at org.apache.spark.storage.BlockManager.dataSerialize(BlockManager.scala:997)
	at org.apache.spark.storage.MemoryStore.getBytes(MemoryStore.scala:102)
	at org.apache.spark.storage.BlockManager.doGetLocal(BlockManager.scala:392)
	at org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:358)
	at org.apache.spark.storage.BlockManagerWorker.getBlock(BlockManagerWorker.scala:90)
	at org.apache.spark.storage.BlockManagerWorker.processBlockMessage(BlockManagerWorker.scala:69)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$2.apply(BlockManagerWorker.scala:44)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at org.apache.spark.storage.BlockMessageArray.foreach(BlockMessageArray.scala:28)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at org.apache.spark.storage.BlockMessageArray.map(BlockMessageArray.scala:28)
	at org.apache.spark.storage.BlockManagerWorker.onBlockMessageReceive(BlockManagerWorker.scala:44)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)
	at org.apache.spark.storage.BlockManagerWorker$$anonfun$1.apply(BlockManagerWorker.scala:34)
	at org.apache.spark.network.ConnectionManager.org$apache$spark$network$ConnectionManager$$handleMessage(ConnectionManager.scala:662)
	at org.apache.spark.network.ConnectionManager$$anon$9.run(ConnectionManager.scala:504)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

Even if the driver sometime does not throw this exception, it will throw 

java.io.FileNotFoundException: /tmp/spark-local-20140702151845-9620/2a/shuffle_2_25_3 (No such file or directory)

I know that VertexPartition not supposed to be serializable, so is there any workaround on this?",Spark standalone with 5 workers and 1 driver,ankurd,bxshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,58:39.8,,FALSE,,,,,,,,,,,,403276,,,Tue Jul 15 22:37:14 UTC 2014,,,,,0|i1xer3:,403329,,,,,,,,,,"11/Jul/14 21:58;ankurd;VertexPartition is actually supposed to be Serializable; it was an oversight not to mark it as such. A workaround is to use Kryo serialization instead of Java serialization, and I'm submitting a fix as well.","15/Jul/14 22:37;ankurd;SPARK-2455 should have fixed this, so I'm closing the issue. Feel free to reopen if there's still a problem.",,,,,,,,,,,
VertexRDD.apply does not use the mergeFunc,SPARK-2062,12718937,Bug,http://spark.apache.org,Major,,,Here: https://github.com/apache/spark/blob/b1feb60209174433262de2a26d39616ba00edcc8/graphx/src/main/scala/org/apache/spark/graphx/VertexRDD.scala#L410,,ankurd,apachespark,larryxiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,05:37.9,,FALSE,,,,,,,,,,,,397136,,,Mon Sep 22 19:37:44 UTC 2014,,,,,0|i1wdfb:,397259,,,,,1.1.1,1.2.0,,,,"11/Aug/14 03:05;larryxiao;Is anyone working on it? I want to take it.
My plan is to add a pass to do the merge, is it ok? [~ankurd]","12/Aug/14 08:47;apachespark;User 'larryxiao' has created a pull request for this issue:
https://github.com/apache/spark/pull/1903",22/Sep/14 19:37;ankurd;Resolved by https://github.com/apache/spark/pull/1903,,,,,,,,,,
EdgeRDD persists after pregel iteration,SPARK-2025,12718512,Bug,http://spark.apache.org,Major,parallelism,,"Symptoms: During execution of a pregel script/function a copy of an intermediate EdgeRDD object persists after each iteration as shown by the Spark WebUI - storage.

This is like a memory leak that affects in the Pregel function.

For example, after the first iteration I will have an EdgeRDD in addition to the EdgeRDD and VertexRDD that are kept for the next iteration. After 15 iterations I will have 15 EdgeRDDs in addition to the current/correct state represented by a single set of 1 EdgeRDD and 1 VertexRDD.

At the end of a Pregel loop the old EdgeRDD and VertexRDD are unpersisted, but there seems to be another EdgeRDD that is created somewhere that does not get unpersisted.

i _think_ this is from the replicateVertex function, but I cannot be sure.

Update - Dave Ankur says, in comments on SPARK-2011 - 
{quote}
... is a bug introduced by https://github.com/apache/spark/pull/497.
It occurs because unpersistVertices used to unpersist both the vertices and the replicated vertices, but after unifying replicated vertices with edges, there was no way to unpersist only one of them. I think the solution is just to unpersist both the vertices and the edges in Pregel.{quote}",RHEL6 on local and on spark cluster,ankurd,tweninge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,36:43.6,,FALSE,,,,,,,,,,,,396711,,,Thu Jun 05 01:36:43 UTC 2014,,,,,0|i1warz:,396830,,,,,,,,,,"05/Jun/14 01:03;tweninge;adding 

{{prevG.edges.unpersist(blocking=false)}}

after line 152 in Pregel.scala fixes the issue",05/Jun/14 01:04;tweninge;I'll leave it to you to make the bug fix. You seem to be a pro.,05/Jun/14 01:36;ankurd;Proposed fix: https://github.com/apache/spark/pull/972,,,,,,,,,,
VertexRDD can incorrectly assume index sharing,SPARK-1955,12717153,Bug,http://spark.apache.org,Minor,job,,"Many VertexRDD operations (diff, leftJoin, innerJoin) can use a fast zip join if both operands are VertexRDDs sharing the same index (i.e., one operand is derived from the other). This check is implemented by matching on the operand type and using the fast join strategy if both are VertexRDDs.

This is clearly fine when both do in fact share the same index. It is also fine when the two VertexRDDs have the same partitioner but different indexes, because each VertexPartition will detect the index mismatch and fall back to the slow but correct local join strategy.

However, when they have different numbers of partitions or different partition functions, an exception or even silently incorrect results can occur.

For example:

{code}
import org.apache.spark._
import org.apache.spark.graphx._

// Construct VertexRDDs with different numbers of partitions
val a = VertexRDD(sc.parallelize(List((0L, 1), (1L, 2)), 1))
val b = VertexRDD(sc.parallelize(List((0L, 5)), 8))
// Try to join them. Appears to work...
val c = a.innerJoin(b) { (vid, x, y) => x + y }
// ... but then fails with java.lang.IllegalArgumentException: Can't zip RDDs with unequal numbers of partitions
c.collect

// Construct VertexRDDs with different partition functions
val a = VertexRDD(sc.parallelize(List((0L, 1), (1L, 2))).partitionBy(new HashPartitioner(2)))
val bVerts = sc.parallelize(List((1L, 5)))
val b = VertexRDD(bVerts.partitionBy(new RangePartitioner(2, bVerts)))
// Try to join them. We expect (1L, 7).
val c = a.innerJoin(b) { (vid, x, y) => x + y }
// Silent failure: we get an empty set!
c.collect
{code}

VertexRDD should check equality of partitioners before using the fast zip join. If the partitioners are different, the two datasets should be automatically co-partitioned.",,ankurd,apachespark,boyork,e5c,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,34:10.4,,FALSE,,,,,,,,,,,,395360,,,Wed Feb 25 22:15:48 UTC 2015,,,,,0|i1w2jb:,395490,,,,,,,,,,15/Jul/14 22:46;ankurd;IndexedRDD fixes this issue.,13/Feb/15 06:34;boyork;[~ankurdave] if you haven't started on this I can take it since it relates heavily to [SPARK-4600|https://issues.apache.org/jira/browse/SPARK-4600] and [SPARK-5790|https://issues.apache.org/jira/browse/SPARK-5790]. If you've already started work though let me know :),"13/Feb/15 07:01;ankurd;[~boyork] Thanks, it would be great if you could take this. I think the fix should be pretty simple: in each of the VertexRDD functions, check partitioner equality and repartition the other RDD if unequal, as in https://github.com/amplab/spark-indexedrdd/blob/master/src/main/scala/edu/berkeley/cs/amplab/spark/indexedrdd/IndexedRDDLike.scala#L206","20/Feb/15 17:46;apachespark;User 'brennonyork' has created a pull request for this issue:
https://github.com/apache/spark/pull/4705","25/Feb/15 22:15;ankurd;Issue resolved by pull request 4705
https://github.com/apache/spark/pull/4705",,,,,,,,
Graph.partitionBy does not reconstruct routing tables,SPARK-1931,12716675,Bug,http://spark.apache.org,Major,data partition,,"Commit 905173df57b90f90ebafb22e43f55164445330e6 introduced a bug in partitionBy where, after repartitioning the edges, it reuses the VertexRDD without updating the routing tables to reflect the new edge layout. This causes the following test to fail:

{code}
      import org.apache.spark.graphx._
      val g = Graph(
        sc.parallelize(List((0L, ""a""), (1L, ""b""), (2L, ""c""))),
        sc.parallelize(List(Edge(0L, 1L, 1), Edge(0L, 2L, 1)), 2))
      assert(g.triplets.collect.map(_.toTuple).toSet ==
        Set(((0L, ""a""), (1L, ""b""), 1), ((0L, ""a""), (2L, ""c""), 1)))
      val gPart = g.partitionBy(PartitionStrategy.EdgePartition2D)
      assert(gPart.triplets.collect.map(_.toTuple).toSet ==
        Set(((0L, ""a""), (1L, ""b""), 1), ((0L, ""a""), (2L, ""c""), 1)))
{code}",,ankurd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,394883,,,Tue May 27 21:47:34 UTC 2014,,,,,0|i1vzmn:,395018,,,,,,,,,,26/May/14 18:12;ankurd;The fix is in PR #885: https://github.com/apache/spark/pull/885,"27/May/14 21:47;ankurd;Since the fix didn't make it into Spark 1.0.0, a workaround is to partition the edges before constructing the graph, as follows:

{code}
// Define our own version of partitionBy to work around SPARK-1931
import org.apache.spark.HashPartitioner
def partitionBy[ED](edges: RDD[Edge[ED]], partitionStrategy: PartitionStrategy): RDD[Edge[ED]] = {
  val numPartitions = edges.partitions.size
  edges.map(e => (partitionStrategy.getPartition(e.srcId, e.dstId, numPartitions), e))
    .partitionBy(new HashPartitioner(numPartitions))
    .mapPartitions(_.map(_._2), preservesPartitioning = true)
}

val vertices = ...
val edges = ...

// Instead of:
val g = Graph(vertices, edges).partitionBy(PartitionStrategy.EdgePartition2D) // broken in Spark 1.0.0

// Use:
val g = Graph(vertices, partitionBy(edges, PartitionStrategy.EdgePartition2D))
{code}",,,,,,,,,,,
Kryo Serialization Error in GraphX,SPARK-1786,12713430,Bug,http://spark.apache.org,Major,io,,"The following code block will generate a serialization error when run in the spark-shell with Kryo enabled:

{code}
import org.apache.spark.storage._
import org.apache.spark.graphx._
import org.apache.spark.graphx.util._

val g = GraphGenerators.gridGraph(sc, 100, 100)
val e = g.edges
e.persist(StorageLevel.MEMORY_ONLY_SER)
e.collect().foreach(println(_)) // <- Runs successfully the first time.

// The following line will fail:
e.collect().foreach(println(_))
{code}

The following error is generated:

{code}
scala> e.collect().foreach(println(_))
14/05/09 18:31:13 INFO SparkContext: Starting job: collect at EdgeRDD.scala:59
14/05/09 18:31:13 INFO DAGScheduler: Got job 1 (collect at EdgeRDD.scala:59) with 8 output partitions (allowLocal=false)
14/05/09 18:31:13 INFO DAGScheduler: Final stage: Stage 1(collect at EdgeRDD.scala:59)
14/05/09 18:31:13 INFO DAGScheduler: Parents of final stage: List()
14/05/09 18:31:13 INFO DAGScheduler: Missing parents: List()
14/05/09 18:31:13 INFO DAGScheduler: Submitting Stage 1 (MappedRDD[15] at map at EdgeRDD.scala:59), which has no missing parents
14/05/09 18:31:13 INFO DAGScheduler: Submitting 8 missing tasks from Stage 1 (MappedRDD[15] at map at EdgeRDD.scala:59)
14/05/09 18:31:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:0 as TID 8 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:0 as 1779 bytes in 3 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:1 as TID 9 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:1 as 1779 bytes in 4 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:2 as TID 10 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:2 as 1779 bytes in 4 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:3 as TID 11 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:3 as 1779 bytes in 4 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:4 as TID 12 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:4 as 1779 bytes in 3 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:5 as TID 13 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:5 as 1782 bytes in 4 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:6 as TID 14 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:6 as 1783 bytes in 4 ms
14/05/09 18:31:13 INFO TaskSetManager: Starting task 1.0:7 as TID 15 on executor localhost: localhost (PROCESS_LOCAL)
14/05/09 18:31:13 INFO TaskSetManager: Serialized task 1.0:7 as 1783 bytes in 4 ms
14/05/09 18:31:13 INFO Executor: Running task ID 9
14/05/09 18:31:13 INFO Executor: Running task ID 8
14/05/09 18:31:13 INFO Executor: Running task ID 11
14/05/09 18:31:13 INFO Executor: Running task ID 14
14/05/09 18:31:13 INFO Executor: Running task ID 10
14/05/09 18:31:13 INFO Executor: Running task ID 13
14/05/09 18:31:13 INFO Executor: Running task ID 15
14/05/09 18:31:13 INFO Executor: Running task ID 12
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_6 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_4 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_2 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_7 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_1 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_3 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_0 locally
14/05/09 18:31:13 INFO BlockManager: Found block rdd_12_5 locally
14/05/09 18:31:13 ERROR Executor: Exception in task ID 13
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 10
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 11
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 12
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 15
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 8
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 9
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR Executor: Exception in task ID 14
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 WARN TaskSetManager: Lost TID 11 (task 1.0:3)
14/05/09 18:31:13 WARN TaskSetManager: Loss was due to java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
	at org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14/05/09 18:31:13 ERROR TaskSetManager: Task 1.0:3 failed 1 times; aborting job
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 1]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 2]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 3]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 4]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO DAGScheduler: Failed to run collect at EdgeRDD.scala:59
14/05/09 18:31:13 INFO TaskSchedulerImpl: Cancelling stage 1
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 5]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 6]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
14/05/09 18:31:13 INFO TaskSetManager: Loss was due to java.lang.NullPointerException [duplicate 7]
14/05/09 18:31:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1.0:3 failed 1 times, most recent failure: Exception failure in TID 11 on host localhost: java.lang.NullPointerException
        org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:269)
        org.apache.spark.graphx.impl.EdgePartition$$anon$1.next(EdgePartition.scala:262)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        scala.collection.Iterator$class.foreach(Iterator.scala:727)
        scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        scala.collection.AbstractIterator.to(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
        org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:706)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1071)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:208)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:744)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107
{code}

We believe the error is associated with serialization of the EdgePartition.",,jegonzal,matei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,22:01.4,,FALSE,,,,,,,,,,,,391746,,,Mon May 12 02:22:01 UTC 2014,,,,,0|i1vguv:,391952,,,,,,,,,,12/May/14 02:22;matei;https://github.com/apache/spark/pull/724,,,,,,,,,,,,
EdgePartition is not serialized properly,SPARK-1750,12712938,Bug,http://spark.apache.org,Major,data partition,,"The GraphX design attempts to avoid moving edges across the network, instead shipping the vertices to the edge partitions. However, Spark sometimes needs to move the edges, such as for straggler mitigation.

All EdgePartition fields are currently declared transient, so the edges will not be serialized properly. Even if they are not marked transient, Kryo is unable to serialize the EdgePartition, failing with the following error:

{code}
java.lang.IllegalArgumentException: Can not set final org.apache.spark.graphx.util.collection.PrimitiveKeyOpenHashMap field org.apache.spark.graphx.impl.EdgePartition.index to scala.collection.immutable.$colon$colon
{code}

A workaround is to discourage Spark from moving the edges by setting {{spark.locality.wait}} to a high value such as 100000.",,ankurd,holdenk,jegonzal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,16:32.9,,FALSE,,,,,,,,,,,,391254,,,Mon May 26 18:15:35 UTC 2014,,,,,0|i1vdx3:,391474,,,,,,,,,,"12/May/14 03:16;jegonzal;I believe this issue is resolved with PR #724.

",26/May/14 18:15;ankurd;https://github.com/apache/spark/pull/742,,,,,,,,,,,
GraphX mapVertices with KryoSerialization,SPARK-1577,12709915,Bug,http://spark.apache.org,Major,job,,"If Kryo is enabled by setting:

{code}
SPARK_JAVA_OPTS+=""-Dspark.serializer=org.apache.spark.serializer.KryoSerializer ""
SPARK_JAVA_OPTS+=""-Dspark.kryo.registrator=org.apache.spark.graphx.GraphKryoRegistrator  ""
{code}

in conf/spark_env.conf and running the following block of code in the shell:

{code}
import org.apache.spark.graphx._
import org.apache.spark.graphx.lib._
import org.apache.spark.rdd.RDD

val vertexArray = Array(
  (1L, (""Alice"", 28)),
  (2L, (""Bob"", 27)),
  (3L, (""Charlie"", 65)),
  (4L, (""David"", 42)),
  (5L, (""Ed"", 55)),
  (6L, (""Fran"", 50))
  )
val edgeArray = Array(
  Edge(2L, 1L, 7),
  Edge(2L, 4L, 2),
  Edge(3L, 2L, 4),
  Edge(3L, 6L, 3),
  Edge(4L, 1L, 1),
  Edge(5L, 2L, 2),
  Edge(5L, 3L, 8),
  Edge(5L, 6L, 3)
  )

val vertexRDD: RDD[(Long, (String, Int))] = sc.parallelize(vertexArray)
val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)

val graph: Graph[(String, Int), Int] = Graph(vertexRDD, edgeRDD)

// Define a class to more clearly model the user property
case class User(name: String, age: Int, inDeg: Int, outDeg: Int)

// Transform the graph
val userGraph = graph.mapVertices{ case (id, (name, age)) => User(name, age, 0, 0) }
{code}

The following block of code works:

{code}
userGraph.vertices.count
{code}

and the following block of code generates a Kryo error:

{code}
userGraph.vertices.collect
{code}

There error:

{code}
java.lang.StackOverflowError
	at sun.reflect.UnsafeFieldAccessorImpl.ensureObj(UnsafeFieldAccessorImpl.java:54)
	at sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl.get(UnsafeQualifiedObjectFieldAccessorImpl.java:38)
	at java.lang.reflect.Field.get(Field.java:379)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:552)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
{code}

 ",,ankurd,jegonzal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,17:47.4,,FALSE,,,,,,,,,,,,388237,,,Mon May 26 18:17:47 UTC 2014,,,,,0|i1uvm7:,388492,,,,,,,,,,"23/Apr/14 05:53;jegonzal;I have narrowed the issued down to (line 46 in GraphKryoRegistrator):

{code} 
kryo.setReferences(false)
{code}

This creates an issue in the Spark REPL which leads to cyclic references.  Removing this line addresses the issue.  I will submit a pull request with the fix. 

In fact, I can reproduce the bug with the following much simpler block of code:

{code}
class A(a: String) extends Serializable
val x = sc.parallelize(Array.fill(10)(new A(""hello"")))
x.collect
{code}

tl;dr

Disabling reference tracking in Kryo will break the Spark Shell.  
",26/May/14 18:17;ankurd;Resolved by re-enabling Kryo reference tracking in #742: https://github.com/apache/spark/pull/742,,,,,,,,,,,
GraphX performs type comparison incorrectly,SPARK-1552,12709524,Bug,http://spark.apache.org,Major,data formats,,"In GraphImpl, mapVertices and outerJoinVertices use a more efficient implementation when the map function preserves vertex attribute types. This is implemented by comparing the ClassTags of the old and new vertex attribute types. However, ClassTags store _erased_ types, so the comparison will return a false positive for types with different type parameters, such as Option[Int] and Option[Double].

Thanks to Pierre-Alexandre Fonta for reporting this bug on the [mailing list|http://apache-spark-user-list.1001560.n3.nabble.com/GraphX-Cast-error-when-comparing-a-vertex-attribute-after-its-type-has-changed-td4119.html].

Demo in the Scala shell:

scala> import scala.reflect.{classTag, ClassTag}
scala> def typesEqual[A: ClassTag, B: ClassTag](a: A, b: B): Boolean = classTag[A] equals classTag[B]
scala> typesEqual(Some(1), Some(2.0)) // should return false
res2: Boolean = true

We can require richer TypeTags for these methods, or just take a flag from the caller specifying whether the types are equal.",,ankurd,npanj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,45:30.9,,FALSE,,,,,,,,,,,,387846,,,Wed Jun 04 03:39:07 UTC 2014,,,,,0|i1ut9b:,388107,,,,,,,,,,28/May/14 06:45;npanj;Does it make sense to get rid of this optimization until richer TypeTags are available? ,"28/May/14 07:17;ankurd;Alternatively, we could introduce type-preserving variants of these methods, maybe called mapVerticesSameType and outerJoinVerticesSameType.

We can't make it into 1.0.0, but it would be good to get a fix into 1.0.1.",04/Jun/14 03:39;ankurd;Proposed fix: https://github.com/apache/spark/pull/967,,,,,,,,,,
ArrayIndexOutOfBoundsException if graphx.Graph has more edge partitions than node partitions,SPARK-1329,12704632,Bug,http://spark.apache.org,Major,data partition,,"To reproduce, let's look at a graph with two nodes in one partition, and two edges between them split across two partitions:
scala> val vs = sc.makeRDD(Seq(1L->null, 2L->null), 1)
scala> val es = sc.makeRDD(Seq(graphx.Edge(1, 2, null), graphx.Edge(2, 1, null)), 2)
scala> val g = graphx.Graph(vs, es)

Everything seems fine, until GraphX needs to join the two RDDs:
scala> g.triplets.collect
[...]
java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.spark.graphx.impl.RoutingTable$$anonfun$2$$anonfun$apply$3.apply(RoutingTable.scala:76)
	at org.apache.spark.graphx.impl.RoutingTable$$anonfun$2$$anonfun$apply$3.apply(RoutingTable.scala:75)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.graphx.impl.RoutingTable$$anonfun$2.apply(RoutingTable.scala:75)
	at org.apache.spark.graphx.impl.RoutingTable$$anonfun$2.apply(RoutingTable.scala:73)
	at org.apache.spark.rdd.RDD$$anonfun$1.apply(RDD.scala:450)
	at org.apache.spark.rdd.RDD$$anonfun$1.apply(RDD.scala:450)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:34)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:71)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:85)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.Task.run(Task.scala:53)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

The bug is fairly obvious in RoutingTable.createPid2Vid() -- it creates an array of length vertices.partitions.size, and then looks up partition IDs from the edges.partitionsRDD in it.

A graph usually has more edges than nodes. So it is natural to have more edge partitions than node partitions.",,ankurd,darabos,glenn.strycker@gmail.com,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,45:00.2,,FALSE,,,,,,,,,,,,382871,,,Mon May 26 18:19:07 UTC 2014,,,,,0|i1tymv:,383139,,,,,,,,,,28/Mar/14 10:45;rxin;Thanks - [~darabos]. Do you mind submitting a pull request to fix this?,28/Mar/14 13:01;darabos;The bug being obvious does not mean the fix is obvious too :). But I'll give it a try!,28/Mar/14 13:03;rxin;Thanks - really appreciate it. Let me know if you run into problems. ,"26/May/14 11:38;darabos;Sorry, I haven't looked into fixing this. We ended up not using GraphX, so we are no longer affected by the bug.",26/May/14 18:19;ankurd;This was resolved by #368: https://github.com/apache/spark/pull/368,,,,,,,,
Use map side distinct in collect vertex ids from edges graphx,SPARK-1311,12704796,Bug,http://spark.apache.org,Minor,,,See GRAPH-1,,ankurd,holdenk_amp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,22:52.0,,FALSE,,,,,,,,,,,,382919,,,Mon May 26 18:22:51 UTC 2014,,,,,0|i1tyxj:,383187,,,,,,,,,,"26/May/14 18:22;ankurd;This was resolved by PR #497, which removed collectVertexIds and instead performed the operation as a side effect of constructing the routing tables: https://github.com/apache/spark/pull/497/files#diff-8ea535724b3f014cfef17284b3e783feR397",,,,,,,,,,,,
Cannot create graphx.Graph with no edges,SPARK-1249,12704717,Bug,http://spark.apache.org,Major,parallelism,,"Let's say I want a graph with a single node, no edges. (I actually want this for a unit test.)

scala> val vs:spark.rdd.RDD[(spark.graphx.VertexId, String)] = sc.makeRDD(Seq((1L, ""x"")))
scala> val es:spark.rdd.RDD[spark.graphx.Edge[String]] = sc.makeRDD(Seq())
scala> val g:spark.graphx.Graph[String, String] = spark.graphx.Graph(vs, es)
java.lang.IllegalArgumentException: Positive number of slices required
	at org.apache.spark.rdd.ParallelCollectionRDD$.slice(ParallelCollectionRDD.scala:116)
	at org.apache.spark.rdd.ParallelCollectionRDD.getPartitions(ParallelCollectionRDD.scala:95)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:205)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:31)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:207)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:205)
	at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:58)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:45)
	at org.apache.spark.graphx.EdgeRDD$$anonfun$1.apply(EdgeRDD.scala:45)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.graphx.EdgeRDD.<init>(EdgeRDD.scala:45)
	at org.apache.spark.graphx.impl.GraphImpl$.createEdgeRDD(GraphImpl.scala:373)
	at org.apache.spark.graphx.impl.GraphImpl$.apply(GraphImpl.scala:319)
	at org.apache.spark.graphx.Graph$.apply(Graph.scala:411)

My impression is that raising an IllegalArgumentException in ParallelCollectionRDD.slice() is a mistake. It should just return a Seq(Seq()). This would match the behavior of how """".split(""x"") returns Seq("""").

Let me know if I'm missing something. I'm new to Spark/GraphX. But it looks like there is no test for corner cases with no nodes/edges in graphx/GraphSuite.scala. Probably an oversight?

I'm happy to send a patch for this if you think I'm right.",,darabos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,383013,,,Fri Mar 14 08:16:20 UTC 2014,,,,,0|i1tzif:,383281,,,,,,,,,,"14/Mar/14 07:45;darabos;Oh, I've now discovered spark.rdd.EmptyRDD! So that's how you do it. It does not seem very convenient to have to treat empty data as a special case though. What if I'm reading the data from a file, and the graph may or may not have edges?

Thanks for bearing with me!","14/Mar/14 08:16;darabos;Looks like generally if an RDD ends up empty, it does not result in this exception, just when created with sc.makeRDD(). Don't mind me then.",,,,,,,,,,,
GraphX triplets not working properly,SPARK-1188,12704872,Bug,http://spark.apache.org,Major,job,,"I followed the GraphX tutorial at http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html 

on a local stand-alone cluster (Spark version 0.9.0) with two workers. Somehow, the graph.triplets is not returning what it should -- only Eds and Frans.

```
scala> graph.edges.toArray
14/03/04 16:15:57 INFO SparkContext: Starting job: collect at EdgeRDD.scala:51
14/03/04 16:15:57 INFO DAGScheduler: Got job 5 (collect at EdgeRDD.scala:51) with 1 output partitions (allowLocal=false)
14/03/04 16:15:57 INFO DAGScheduler: Final stage: Stage 27 (collect at EdgeRDD.scala:51)
14/03/04 16:15:57 INFO DAGScheduler: Parents of final stage: List()
14/03/04 16:15:57 INFO DAGScheduler: Missing parents: List()
14/03/04 16:15:57 INFO DAGScheduler: Submitting Stage 27 (MappedRDD[36] at map at EdgeRDD.scala:51), which has no missing parents
14/03/04 16:15:57 INFO DAGScheduler: Submitting 1 missing tasks from Stage 27 (MappedRDD[36] at map at EdgeRDD.scala:51)
14/03/04 16:15:57 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
14/03/04 16:15:57 INFO TaskSetManager: Starting task 27.0:0 as TID 11 on executor localhost: localhost (PROCESS_LOCAL)
14/03/04 16:15:57 INFO TaskSetManager: Serialized task 27.0:0 as 2068 bytes in 1 ms
14/03/04 16:15:57 INFO Executor: Running task ID 11
14/03/04 16:15:57 INFO BlockManager: Found block rdd_2_0 locally
14/03/04 16:15:57 INFO Executor: Serialized size of result for 11 is 936
14/03/04 16:15:57 INFO Executor: Sending result for 11 directly to driver
14/03/04 16:15:57 INFO Executor: Finished task ID 11
14/03/04 16:15:57 INFO TaskSetManager: Finished TID 11 in 13 ms on localhost (progress: 0/1)
14/03/04 16:15:57 INFO DAGScheduler: Completed ResultTask(27, 0)
14/03/04 16:15:57 INFO TaskSchedulerImpl: Remove TaskSet 27.0 from pool
14/03/04 16:15:57 INFO DAGScheduler: Stage 27 (collect at EdgeRDD.scala:51) finished in 0.015 s
14/03/04 16:15:57 INFO SparkContext: Job finished: collect at EdgeRDD.scala:51, took 0.023602266 s
res7: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(2,1,7), Edge(2,4,2), Edge(3,2,4), Edge(3,6,3), Edge(4,1,1), Edge(5,2,2), Edge(5,3,8), Edge(5,6,3))


scala> graph.vertices.toArray
14/03/04 16:16:18 INFO SparkContext: Starting job: toArray at <console>:27
14/03/04 16:16:18 INFO DAGScheduler: Got job 6 (toArray at <console>:27) with 1 output partitions (allowLocal=false)
14/03/04 16:16:18 INFO DAGScheduler: Final stage: Stage 28 (toArray at <console>:27)
14/03/04 16:16:18 INFO DAGScheduler: Parents of final stage: List(Stage 32, Stage 29)
14/03/04 16:16:18 INFO DAGScheduler: Missing parents: List()
14/03/04 16:16:18 INFO DAGScheduler: Submitting Stage 28 (VertexRDD[15] at RDD at VertexRDD.scala:52), which has no missing parents
14/03/04 16:16:18 INFO DAGScheduler: Submitting 1 missing tasks from Stage 28 (VertexRDD[15] at RDD at VertexRDD.scala:52)
14/03/04 16:16:18 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
14/03/04 16:16:18 INFO TaskSetManager: Starting task 28.0:0 as TID 12 on executor localhost: localhost (PROCESS_LOCAL)
14/03/04 16:16:18 INFO TaskSetManager: Serialized task 28.0:0 as 2426 bytes in 0 ms
14/03/04 16:16:18 INFO Executor: Running task ID 12
14/03/04 16:16:18 INFO BlockManager: Found block rdd_14_0 locally
14/03/04 16:16:18 INFO Executor: Serialized size of result for 12 is 947
14/03/04 16:16:18 INFO Executor: Sending result for 12 directly to driver
14/03/04 16:16:18 INFO Executor: Finished task ID 12
14/03/04 16:16:18 INFO TaskSetManager: Finished TID 12 in 13 ms on localhost (progress: 0/1)
14/03/04 16:16:18 INFO DAGScheduler: Completed ResultTask(28, 0)
14/03/04 16:16:18 INFO TaskSchedulerImpl: Remove TaskSet 28.0 from pool
14/03/04 16:16:18 INFO DAGScheduler: Stage 28 (toArray at <console>:27) finished in 0.015 s
14/03/04 16:16:18 INFO SparkContext: Job finished: toArray at <console>:27, took 0.027839851 s
res9: Array[(org.apache.spark.graphx.VertexId, (String, Int))] = Array((4,(David,42)), (2,(Bob,27)), (6,(Fran,50)), (5,(Ed,55)), (3,(Charlie,65)), (1,(Alice,28)))


scala> graph.triplets.toArray
14/03/04 16:16:30 INFO SparkContext: Starting job: toArray at <console>:27
14/03/04 16:16:30 INFO DAGScheduler: Got job 7 (toArray at <console>:27) with 1 output partitions (allowLocal=false)
14/03/04 16:16:31 INFO DAGScheduler: Final stage: Stage 33 (toArray at <console>:27)
14/03/04 16:16:31 INFO DAGScheduler: Parents of final stage: List(Stage 34)
14/03/04 16:16:31 INFO DAGScheduler: Missing parents: List()
14/03/04 16:16:31 INFO DAGScheduler: Submitting Stage 33 (ZippedPartitionsRDD2[32] at zipPartitions at GraphImpl.scala:60), which has no missing parents
14/03/04 16:16:31 INFO DAGScheduler: Submitting 1 missing tasks from Stage 33 (ZippedPartitionsRDD2[32] at zipPartitions at GraphImpl.scala:60)
14/03/04 16:16:31 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
14/03/04 16:16:31 INFO TaskSetManager: Starting task 33.0:0 as TID 13 on executor localhost: localhost (PROCESS_LOCAL)
14/03/04 16:16:31 INFO TaskSetManager: Serialized task 33.0:0 as 3322 bytes in 1 ms
14/03/04 16:16:31 INFO Executor: Running task ID 13
14/03/04 16:16:31 INFO BlockManager: Found block rdd_2_0 locally
14/03/04 16:16:31 INFO BlockManager: Found block rdd_31_0 locally
14/03/04 16:16:31 INFO Executor: Serialized size of result for 13 is 931
14/03/04 16:16:31 INFO Executor: Sending result for 13 directly to driver
14/03/04 16:16:31 INFO Executor: Finished task ID 13
14/03/04 16:16:31 INFO TaskSetManager: Finished TID 13 in 17 ms on localhost (progress: 0/1)
14/03/04 16:16:31 INFO DAGScheduler: Completed ResultTask(33, 0)
14/03/04 16:16:31 INFO TaskSchedulerImpl: Remove TaskSet 33.0 from pool
14/03/04 16:16:31 INFO DAGScheduler: Stage 33 (toArray at <console>:27) finished in 0.019 s
14/03/04 16:16:31 INFO SparkContext: Job finished: toArray at <console>:27, took 0.037909394 s
res10: Array[org.apache.spark.graphx.EdgeTriplet[(String, Int),Int]] = Array(((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3))
```
",,darabos,k0alak0der,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,25:59.5,,FALSE,,,,,,,,,,,,382877,,,Mon May 19 20:28:45 UTC 2014,,,,,0|i1tyo7:,383145,,,,,,,,,,"05/Mar/14 00:25;rxin;The problem is that we are reusing an EdgeTriplet object. Try force a copy before you do the collect.

e.g.

triplets.map(_.copy).collect()","13/Mar/14 17:42;darabos;This has bitten us as well. I am no expert, but my impression is that re-using an object in these iterators was a terrible idea.

Consider this example:

scala> val g = graphx.util.GraphGenerators.starGraph(sc, 5)
scala> g.edges.collect
Array(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1))
scala> g.edges.map(x => x).collect
Array(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1))

It can and does go very wrong in practice too. Consider this:

scala> g.edges.saveAsObjectFile(""edges"")
scala> sc.objectFile[graphx.Edge[Int]](""edges"").collect
Array(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1))

Indeed map(_.copy) is a good workaround. But seems like this is a nasty trap laid out for your users. Did you measure the performance gain? If not, or if it's insignificant, I would suggest not re-using the object in the iterators. It would simplify the GraphX source too. However, if it is significant, one idea is to offer separate mapFast() methods that do re-use, while the more commonly used map() would offer the more commonly expected (copying) semantics.

Also I wish GraphX offered graph save/load functionality. If it did, I would only have discovered this bug a few weeks from now :).","19/Mar/14 06:32;darabos;Sorry, I forgot to test your workaround before commenting. It does not work.

scala> g.triplets.map(_.copy).collect
<console>:16: error: missing arguments for method copy in class Edge;
follow this method with `_' if you want to treat it as a partially applied function
              g.triplets.map(_.copy).collect
                               ^
You can of course write a copy function yourself.

I'd be happy to work on this. Can I just get rid of the re-use, or would you prefer another approach?","28/Mar/14 08:19;darabos;Okay, it was just missing the parentheses:

scala> g.triplets.map(_.copy()).collect
res1: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1))

(Scala novice here :).)",31/Mar/14 11:13;darabos;I've sent a pull request to eliminate the re-use (https://github.com/apache/spark/pull/276). The change has no performance impact and simplifies the code.,23/Apr/14 09:29;darabos;The changes are in the master branch now. I can't figure out how to close a JIRA ticket :).,23/Apr/14 18:41;rxin;I added you to contributor list so you should be able to edit in the future. Cheers.,19/May/14 20:28;rxin;Adding a link to the commit: https://github.com/apache/spark/commit/78236334e4ca7518b6d7d9b38464dbbda854a777#diff-a2b19aac11cb2fbe9962b5d2290ea77e,,,,,
